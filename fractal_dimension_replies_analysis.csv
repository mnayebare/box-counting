post_id,conversation_type,fractal_dimension,fractal_dimension_type,reply,reply_level
post50hb,richly branching,1.6001453961458911,highest,"The following submission statement was provided by /u/soulpost:

---

According to new research, deep learning models based on artificial intelligence can identify someone's race merely by looking at their X-rays, which would be impossible for a human doctor looking at the same photos.  
  
The findings raise several serious concerns concerning AI's role in medical diagnosis, assessment, and treatment: Could computer algorithms mistakenly apply racial bias when analysing photographs like these?

---

 Please reply to OP's comment here: https://old.reddit.com/r/Futurology/comments/uvxpli/ai_can_predict_peoples_race_from_xray_images_and/i9o4ui0/",1
post50hb,richly branching,1.6001453961458911,highest,"I don't mean to show off, but I can do this just by looking at someone, no x-rays required.",1
post50hb,richly branching,1.6001453961458911,highest,"That's a less common skill than you'd think. The number of times my Hawaiian friend has been called Mexican, including by other Mexican people, is crazy 

And that was even before the guy also got a chihuahua and fixed up an old El Comino to drive around in. He actually didn't see why that would worsen things",2
post50hb,richly branching,1.6001453961458911,highest,"People always assume my buddy’s 100% Mexican dad is [East Asian] Indian, so much so when he goes into convenient stores the guys behind the counter start talking to him in Hindi or Bengali.",3
post50hb,richly branching,1.6001453961458911,highest,"My bro has this in reverse.  Everyone thinks he's Mexican.  People lean more towards black for me, but I get an occasional Mexican depending on the lighting.  We're only ethnically Indian though.... family hails from africa... can't speak Spanish or hindi ( or any of those languages).",4
post50hb,richly branching,1.6001453961458911,highest,My former boss is Moroccan. The number of Spanish speaking clients that lead with Spanish is pretty funny. Especially when my gringo white self was the one that can actually speak Spanish.,3
post50hb,richly branching,1.6001453961458911,highest,"To be fair Moroccans living near Ceuta,
Melilla, and In Tangier know Spanish as a second language and not French.",4
post50hb,richly branching,1.6001453961458911,highest,"Am Hawaiian, can confirm. When I was driving out west and stopping in Dennys or other diners to eat at I would frequently be greeted in Spanish.",3
post50hb,richly branching,1.6001453961458911,highest,"Am mixed chinese and white - in the summer when I am tan, i always get stopped by little abuelas in the grocery store speaking Spanish to me asking for help to read labels in English. My Spanish is limited to a basic understanding but I just oblige them and don’t bother explaining. Countless grandmas in new jersey have thought I’m just some second gen latina that can only respond in English 😂",4
post50hb,richly branching,1.6001453961458911,highest,Obviously the cultures are way different but that dna shares common roots. Based on the context from your 2nd paragraph I'm not too surprised. The context definitely paints the picture to the kind of style I'm sure this dude has haha.,3
post50hb,richly branching,1.6001453961458911,highest,"Had a Lebanese friend whose nickname in HS was ""The Mexican"" because after he grew a bit of a moustache everybody mistook him for Mexican",3
post50hb,richly branching,1.6001453961458911,highest,Maybe your friend should double check their skeleton.,3
post50hb,richly branching,1.6001453961458911,highest,My Hawaiian friend would get very upset and violent to good friends when people would call him Mexican. So as good friends we would always tell people that he was Mexican. Good times,3
post50hb,richly branching,1.6001453961458911,highest,"Those are localities (states/countries), not races. Races are black, white, yellow and brown, and they're [social groups](https://en.wikipedia.org/wiki/Race_\(human_categorization\)), not biological categories.

They're entirely defined by how a human would visually classify someone. It's not a common skill because there is no true underlying property to be revealed, and the classification is entirely arbitrary.",3
post50hb,richly branching,1.6001453961458911,highest,Are you making up the part about the dog and the car?,3
post50hb,richly branching,1.6001453961458911,highest,"I wish, ha. The car was his dad's from way back when so that much I get. But of all the dogs he could have got to keep his other dog (a generic healer mix) company he picked a Chihuahua",4
post50hb,richly branching,1.6001453961458911,highest,"You're kind of reinforcing the idea though because while he's often misidentified, it's almost always in the same way.",3
post50hb,richly branching,1.6001453961458911,highest,"Whenever this topic comes up I'm reminded of a study done in east asian countries that found while most the of the people surveyed thought they could tell the difference between nationalities and ethnicities on sight alone, the success rate was like 30%.",3
post50hb,richly branching,1.6001453961458911,highest,"As a standard issue white guy, the best I can offer is that several people in Istanbul confused me for a local. A tourist couple tried asking me for the way, in (presumably) Arab",3
post50hb,richly branching,1.6001453961458911,highest,I don't think Hawaiian is a race,3
post50hb,richly branching,1.6001453961458911,highest,"Correct, his race is Pacific Islander. But Hawaiian is easier to type and conveys the message well enough given the additional context provided",4
post50hb,richly branching,1.6001453961458911,highest,It is way more difficult than most imagine. In one of my anthropology labs we had an activity where we were given a list of ethnicities and tried to match them to faces.  No one got more than 50%.,3
post50hb,richly branching,1.6001453961458911,highest,"I think you meant El Camino
What you wrote says ""The cumin"" 😂

Also, my dad did the same with this kid that was in my school. He would go up to him repeatedly and try to talk to him in Spanish. I kept telling my dad he wasn't Mexican but he didn't listen and insisted he must speak Spanish 😂",3
post50hb,richly branching,1.6001453961458911,highest,"I’m ambiguous so I can go to like half the world and blend in, until they start speaking to me in their native tongues and I just stand there wide eyed",3
post50hb,richly branching,1.6001453961458911,highest,You racist then man. I only see genderless ageless raceless beings. /s,2
post50hb,richly branching,1.6001453961458911,highest,"Whoa there. So, you did not want to respect my identity by ignoring my gender, age and race?",3
post50hb,richly branching,1.6001453961458911,highest,Im an AI and I take offense to that.,3
post50hb,richly branching,1.6001453961458911,highest,"I see amorphous blobs. One race, the blob race.",3
post50hb,richly branching,1.6001453961458911,highest,"If everyone is just a blob you may need glasses, friend.",4
post50hb,richly branching,1.6001453961458911,highest,The true anti-racist,4
post50hb,richly branching,1.6001453961458911,highest,We are just blobs of LCL held together by AT fields I would say,4
post50hb,richly branching,1.6001453961458911,highest,Those are just people from Mississippi.,4
post50hb,richly branching,1.6001453961458911,highest,"I'm not trying to show off either, but if I know a person's race, I'm able to see into their body like an x-ray.",2
post50hb,richly branching,1.6001453961458911,highest,"Between us we could really get a Badgerparty happening

EDIT: to clarify this comment has little or nothing to do with A.I. being weird about skeletons",2
post50hb,richly branching,1.6001453961458911,highest,"We're the two(/four) most threatening parts of the BadgerMegaZord, so we ought to be able to get some amount of badgering happening.",3
post50hb,richly branching,1.6001453961458911,highest,"Until you remember that race is a social construct, and even Germans and Italians weren't considered 'white' a few decades ago

[White People Do Not Exist](https://youtu.be/EQikPmIdYyQ)",2
post50hb,richly branching,1.6001453961458911,highest,"I can do this by looking at craniums. You can tell age, sex, ethnic origin, pathologies, infer diet and nutrition, etc. It's undergrad level anthropology.",2
post50hb,richly branching,1.6001453961458911,highest,I can tell just by the voice and word choice.,2
post50hb,richly branching,1.6001453961458911,highest,"Wouldnt racial bias in this kind of AI be helpful?

I mean aren't there diseases that occur more in specific races than in others?",1
post50hb,richly branching,1.6001453961458911,highest,"Right, I'm a little confused why this is a concern.  This seems like a good thing if even doctors are unable to determine this.  There are absolutely medical conditions that are more likely to occur in certain races a.k.a. have specific genetic heritage.

If we are to use AI to diagnose patients, which surely is being worked on, this is a really valuable tool.  


EDIT: Also, if you're of a specific genetic heritage and you're planning on getting pregnant, sometimes you will be encouraged to do genetic testing for genetic diseases.  If you're not of those specific genetic groups, it's not a standard test to get done.",2
post50hb,richly branching,1.6001453961458911,highest,">I'm a little confused why this is a concern

Articles from 2 weeks ago had titles such as [MIT, Harvard scientists find AI can recognize race from X-rays — and nobody knows how](https://www.bostonglobe.com/2022/05/13/business/mit-harvard-scientists-find-ai-can-recognize-race-x-rays-nobody-knows-how/)

So I think sites take the real reporting and fill it full of buzzwords and eli5 commentary by the time it gets to reddit. Also scare tactics, easier to read writing and lack of paywalls all drive clicks which means more ad revenue. 

So that's probably the main reason why they are ""concerned""",3
post50hb,richly branching,1.6001453961458911,highest,That seems to describe a decent chunk of posts on this sub.,4
post50hb,richly branching,1.6001453961458911,highest,"I’m just trying to think of a scenario where someone would know what my skeleton looks like but not my skin, or where I’d be okay with them seeing my skull but not my face",4
post50hb,richly branching,1.6001453961458911,highest,"Yup, this is it. Nothing wrong with the tech. It's just modern trash ""journalism"".",4
post50hb,richly branching,1.6001453961458911,highest,“AI does thing and nobody knows how” is a pretty standard affirmation lmao,4
post50hb,richly branching,1.6001453961458911,highest,"Thing is - sometimes it's not a problem with AI but with data. Meaning that test data has some kind of bias that they are not aware of.

I always give wolf story as example. Someone taught AI to make a distinction between wolf and a dog. And because ai was not too complicated they analysed it and found out what contributed the most to the distinction.

And it was color white. You see... Photos if wolves were in their natural environment and most of them had snow in the background. So AI figured out that the more snow you have on the picture the higher possibility there is it's a wolf.

So biased data created biased result.",4
post50hb,richly branching,1.6001453961458911,highest,The media make everything worse.,4
post50hb,richly branching,1.6001453961458911,highest,"I worked in IP for a while & saw patent applications for False Femurs which were specifically for Asians (certainly it was more specific than just “Asian” but I forget.) 

That patent application was denied because you cant own the specifications of a Asian man’s femur

‘point is, I learned that your bones may be a little different depending on your race",4
post50hb,richly branching,1.6001453961458911,highest,"Idk man could lead to some very fucked up shit if not implemented properly, ie how naziz traced down Jewish families, even those with very small bloodlines. It would be great if it was never used for human racial prejudice, I just don't see all people using technology like this for what it should be used for",4
post50hb,richly branching,1.6001453961458911,highest,"The thing is that the ai teasing race out of the X-ray is somewhat irrelevant; someone administered this X-ray and would be able tell race already and, if race were clinically relevant to the investigation at hand, it would be in the chart.

The only place it gets weird is bigots trying to use the ability of ai to distinguish race as some kind of smoking gun to justify bias. But again, race is already pretty out in the open so that sort of argument quickly descends into phrenology.",4
post50hb,richly branching,1.6001453961458911,highest,"From your article, it does at least imply that there's concern.

>At a time when AI software is increasingly used to help doctors make diagnostic decisions, the research raises the unsettling prospect that AI-based diagnostic systems could unintentionally generate racially biased results.

>The research effort was born when the scientists noticed that an AI program for examining chest X-rays was more likely to miss signs of illness in Black patients.",4
post50hb,richly branching,1.6001453961458911,highest,"Also, ""aren't sure how"" is a stretch as well. AI researchers are developing tools to introspect the neural net weighting decisions such as providing a visual highlight of the areas it focused on. A human might take note that generally speaking, one race may have larger bones than another, or shorter arms, or squishy ribs etc.. but an AI may focus on background notice that's artifacts related to particular x-ray machines which occur in particular neighborhoods where a single race tends to cluster thereby revealing the source of the insight as not having anything to do with the actual bones but rather xray machine artifacts. When that happens the AI designer can erase the data and tell the AI to not focus on the bottom right corner pixels and try again.",4
post50hb,richly branching,1.6001453961458911,highest,To some degree it's about potentially creating a racist AI...,4
post50hb,richly branching,1.6001453961458911,highest,">and nobody knows how

Is this really surprising?  For certain x-ray angles, I assumed this was kind of easy?  Only reason I say that, is I was discussing a medical issue with a dentist and had a lateral x-ray of the patient's airway ... the dentist immediately commented ""ok, so this patient is asian"" and that was 100% correct.  The dentist had no other clues about the patient's identity prior to that.

I mean, if there are obvious soft tissue differences between causasian, black, asian, etc. humans ... would it be so unusual to expect that there may be some slight skeletal variances too?  Like (just making stuff up here) is the average interpupil width wider for asians or blacks than caucasians?  That could affect the size/shape/placement of the orbits.",4
post50hb,richly branching,1.6001453961458911,highest,What are these headlines lmao,4
post50hb,richly branching,1.6001453961458911,highest,"You make many valid points, but I prefer choosing to believe that robots are going to be really abrasive and uncomfortably upfront racists instead.",4
post50hb,richly branching,1.6001453961458911,highest,"Idk , googles AI ethics board is completely concerned with sniffing their own farts about PC bullshit instead of working on alignment and control.

I imagine most of the big players (so the ones most likely to birth a true AGI) are the same , maybe not tencent?

""Oops , we caused human extinction because we didnt want to trigger anyone""",4
post50hb,richly branching,1.6001453961458911,highest,"Race and skeletal structure are both genetic in origin and likely correlated as such. The genes involved are broad and inevitably overlap. A neural network is going to find that correlation, wherever it is, because that's literally it's entire job and designed purpose. To find patterns wherever it has been incentivized to find them. Subject to the third variable problem just the same.

An interesting but ultimately unsurprising and unconcerning result. Like you said, anyone claiming more is just scare tactics.",4
post50hb,richly branching,1.6001453961458911,highest,"This is a whole industry. Forensic anthropology. Surely if it can determine it from x-ray it can do it from pictures of bones. 

Sounds like an amazing tool for the identification of remains.",4
post50hb,richly branching,1.6001453961458911,highest,"Do these people not watch a lot of porn? With a lot of user driven content, people hide their faces. But I still get a mental image of what it is from their body. Japanese, Chinese, West European, Eastern European, Russian, American, Mexican, Brazilian, Indian, etc are usually distinct from my experience. It's like those people who are geo gurus with identifying the country from one random Google street view image. It would be hard to explain my reasoning, and that's just it with AI as well. It trains to identify, but it can't explain itself afterwards. I think that's a next step in AI: to explain its own logic.",4
post50hb,richly branching,1.6001453961458911,highest,... and they also frustrate those of us who can't justify the cost of a subscription to something like that yet who actually want to read the original *in detail* so that we can get the *real* truth.,4
post50hb,richly branching,1.6001453961458911,highest,"It’s a concern because of this taken directly from the article:

“Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons”",3
post50hb,richly branching,1.6001453961458911,highest,"There are several considerations:

1. Training data: If the data an algorithm is analyzing is of a fundamentally different type than the data it was trained on, it's prone to failure. When analyzing data specific to one demographic group, the algorithm should be trained specifically to analyze data from that group.

2. Diagnosis based on demographic instead of symptoms/physical condition: If one demographic has a higher prevalence of a condition, you want to control for that in a diagnostic algorithm. To use a rudimentary example, it's not helpful to me for an algorithm to say ""you're at 50% greater risk for testicular cancer"" just because the algorithm notices I have testicles, which half of the training data subjects didn't.

There are far more nuances to consider, too. The book ""The Alignment Problem"" is a fantastic read that goes into detail on dozens and dozens more.",4
post50hb,richly branching,1.6001453961458911,highest,"I'm still confused about why this particular new development is a problem. Isn't it actually a solution to that?

The sentence you quote is referring to earlier AI that missed indicators of sickness among black people, but didn't predict their race. So now if the AI can predict their race as well, any doctor interpreting it will know that there is a higher chance that the AI scanning for sickness has a higher chance of missing something, so they can compensate.

How is that not a good thing?",4
post50hb,richly branching,1.6001453961458911,highest,"This doesn’t make sense still. The AI knowing the race doesn’t have anything to do with missing the indicators of sickness for a race. 

Shouldn’t knowing the race be a boon to the diagnosis?

These two things don’t seem related",4
post50hb,richly branching,1.6001453961458911,highest,"As usual, 90% of the commenters here very obviously didn't read beyond the headline.",4
post50hb,richly branching,1.6001453961458911,highest,Which wouldn't make the AI much different from some doctors.,4
post50hb,richly branching,1.6001453961458911,highest,"The thing I don't understand is, surely the AI being able to predict race from x-rays is a good thing in this case? If it couldn't tell the difference in race but was more likely to miss indicators of sickness among black persons then there'd be nothing that could be done about it - it'd just be an AI that's only useful for diagnosing non-black people. The fact that it _can_ predict race means it can be taught to look more closely for indicators of sickness, or look for different indicators, if it recognises the person is likely to be black. Or am I missing something?",4
post50hb,richly branching,1.6001453961458911,highest,This type of interpretation is done by people who claim “math is racist” and who don’t understand how these algos work.,4
post50hb,richly branching,1.6001453961458911,highest,That doesn't make sense if it's correctly guessing the race near 100%. The real issue is as someone else listed they don't know how it's doing it. This would mean the aiadded the function itself.,4
post50hb,richly branching,1.6001453961458911,highest,That is the most interesting part to me. There us something there. Something must be different for it to he so accurate. And then for the imaging to also have the most missed diagnoses among certain populations. I'm wondering if there is a deeper indicator that makes tests of many sources less accurate that isn't visible to humans.,4
post50hb,richly branching,1.6001453961458911,highest,"AI is built on training data.

""Doctors more likely to miss sickness amoung Black persons""

My question is how are we only just noticing? That seems less than ideal.",4
post50hb,richly branching,1.6001453961458911,highest,"There is a reasonable dialogue around preventing machine learning models to focus on and reinforce biases that people have created. 

It's an entirely reasonable thing to be concerned about even when it has utility.",3
post50hb,richly branching,1.6001453961458911,highest,"It's not bias in the traditional sense though. What we see as bias, the AI merely sees as differentiation.",4
post50hb,richly branching,1.6001453961458911,highest,"Yeah but in this case the AI being able to make those distinctions does not seem to be rooted in a bias created by humans. It just sees bones and sorts them along some categories, some of which happen to roughly align with the thing we humans see as ""race"".

I don't think this is more concerning than AI being able to sort people into categories by photos of their face.",4
post50hb,richly branching,1.6001453961458911,highest,"There are several problems here that are difficult to disentangle.

Biases contained in training data can result in biased output:

https://www.vice.com/en/article/7kpxyy/this-image-of-a-white-barack-obama-is-ais-racial-bias-problem-in-a-nutshell

And when considering whether an output is biased or not, we have to take into consideration that we don't actually know what machine learning models know, since they create their own non-human internal representations:

https://www.vice.com/en/article/7kpxyy/this-image-of-a-white-barack-obama-is-ais-racial-bias-problem-in-a-nutshell

Many of these models (such as GANs) are trained using an adversarial system that rewards successful deception:

https://techcrunch.com/2018/12/31/this-clever-ai-hid-data-from-its-creators-to-cheat-at-its-appointed-task/

and the models seem to learn to memorize information in ways that challenge our understanding of information density (algorithmic information theory, kolmogorov complexity)

https://www.usenix.org/system/files/sec19-carlini.pdf

If doctors using these systems incorrectly assume the race of a patient, or if doctors are unaware of the types of biases ai models can have, an uncritical physician could easily do harm.",4
post50hb,richly branching,1.6001453961458911,highest,How in the world could AI create racial biases from looking at x-ray pictures? This sounds extremely delusional IMO.,4
post50hb,richly branching,1.6001453961458911,highest,The conversation is entirely reasonable. The eternal struggle of Risk vs Reward,4
post50hb,richly branching,1.6001453961458911,highest,I don't think you know what bias means.,4
post50hb,richly branching,1.6001453961458911,highest,"It's not bias to CORRECTLY identify something. Race is a real thing and it's intrinsically linked to our biological health. The AI didn't perform differently because it detected race, that would be bias. It's not like the airport scanner became self aware and started flagging black more. THATS NOT THE AI GUYS!  

Who has access to your x-rays that doesn't know your race? 

Whatever racial bias fears you have about AI, stop being dumb, humans will ALWAYS be more racially biased than AI.",4
post50hb,richly branching,1.6001453961458911,highest,[removed],4
post50hb,richly branching,1.6001453961458911,highest,"No it isn't. Machine Learning is, by definition, designed to alter itself into ever closer approaching the truth. If it reinforces biases that people have created then that is immaterial to anything.",4
post50hb,richly branching,1.6001453961458911,highest,"Just tone down the ""end justify the means"" variable and we're good to go right? Right?",4
post50hb,richly branching,1.6001453961458911,highest,It only took a few months for an AI with Twitter access to became racist.,4
post50hb,richly branching,1.6001453961458911,highest,It's our responsibility to think how a thing could be misused. I agree with you that if we aren't willing to discuss it then we have a very large problem. I think it largely matters where and by who this technology will/would be used. And who would access to the results. It's the same concern we should have for any medical information really.,4
post50hb,richly branching,1.6001453961458911,highest,"Well the concern isn't with the technology, it's with what happens when people who leave ethics at the door *use* the technology.",4
post50hb,richly branching,1.6001453961458911,highest,"Yeah but none of this is bias people created, the AI is with 90% accuracy spotting differences via xray between races. There is no human input into these xrays.",4
post50hb,richly branching,1.6001453961458911,highest,That sounds reasonable enough as a general statement but can you give any examples?  I don't really understand what it means.  What biases could AI focus on and how could it reinforce them?,4
post50hb,richly branching,1.6001453961458911,highest,"But... if it's create the same pattern as people do... why is it ""going the wrong way""?",4
post50hb,richly branching,1.6001453961458911,highest,"It’s not reinforcing biases humans have created, it’s recognizing the reality of the world with a level of detail we don’t notice. 

The skeletons of black people and white people are different. That’s a reality. That’s not a bias",4
post50hb,richly branching,1.6001453961458911,highest,Yes. Because there are already many negative biases built in to medical books and treatments that western medicine doesn’t even notice. The AI may reinforce that.,4
post50hb,richly branching,1.6001453961458911,highest,It’s not racist to state someone’s race. I’m a little concerned with people perceiving things to be racist when it’s just factual observation.,4
post50hb,richly branching,1.6001453961458911,highest,"We’re the Pharaohs black?  I think they were, but I would love a definitive answer one way or another.",4
post50hb,richly branching,1.6001453961458911,highest,If observable and quantifiable analysis from real data is called “bias” we might as well throw science away. I wonder if quarks feel upset that some of them are called “strange”.,4
post50hb,richly branching,1.6001453961458911,highest,"Additionally, racial biases in an AI are often caused by racial biases in the original training data it was given. The AI software itself is just math, but if you give it data that's non-reprisentitivice or a lil racist, it can act in problematic ways.

For example, facial upscaling software [thinking everyone is white.](https://cdn.vox-cdn.com/thumbor/v5eda-4BT6zJCywOxFGPlGx_0lI=/55x85:768x536/920x613/filters:focal(336x236:464x364):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/66972412/face_depixelizer_obama.0.jpg)

Race can certainly be a useful factor in diagnosing a patient, however, it is a factor that has been historically given too much importance. So it seems reasonable to see whether any of that bias accidentally made it into the AI.",4
post50hb,richly branching,1.6001453961458911,highest,Is it really bias though? It's just making predictions or observations based on real data.,4
post50hb,richly branching,1.6001453961458911,highest,This is the AI literally doing something a human doctor isn't capable of. It's impossible for a bias to be introduced because it's not trained by a doctor doing the same thing.,4
post50hb,richly branching,1.6001453961458911,highest,Then maybe they should stop training the things in a manner that leaves us with magical black boxes that produce objectively verifiably information in ways we cant ascertain.,4
post50hb,richly branching,1.6001453961458911,highest,"Once machine learning algorithms which are tasked with making predictions are fed data that's strongly correlated with broader societal/demographic trends, if you don't then control for those factors, you're going to see results that reflect those trends.

To use an example, black people in the US disproportionately live in areas with worse air quality.

If an algorithm designed to predict risk of, say, emphysema, gets fed race data, it can wind up predicting emphysema based on the race data alone, which isn't the purpose of diagnostic analysis. Ideally you want to make diagnoses based on the specific physical condition of the patient, while controlling for demographic data.",3
post50hb,richly branching,1.6001453961458911,highest,"So in your example the AI training ends up identifying a real world “bias”. Isn’t that good? Your suggesting that the model reflects reality, but it should be a model of some perfect world without any of the real discriminatory factors.",4
post50hb,richly branching,1.6001453961458911,highest,If you read the article you would know that the ai is guessing the race with remarkable accuracy from images humans could not be able to do the same with. They are also able to do it in incomplete or distorted images. **The ai is also missing illnesses in black people.** Scientists are confused and worry about racial bias affecting machine learning in unintended ways. If this tech is to be used in medicine this needs to be ironed out.,3
post50hb,richly branching,1.6001453961458911,highest,"I think it made mention that it failed to diagnose or detect sickness it skeletons that were of black people. It almost reminds me of the eGFR (estimated glomueral filtration rate) equation that is factored in for the African American population. If you ever get a CMP run, you might see the difference in the results between your value if you are white and the African American eGFR. It has been pointed out that using this type of bias has prevented proper treatment in patients with kidney failure, even delaying transplant eligibility.",3
post50hb,richly branching,1.6001453961458911,highest,"> Right, I'm a little confused why this is a concern. This seems like a good thing if even doctors are unable to determine this. There are absolutely medical conditions that are more likely to occur in certain races a.k.a. have specific genetic heritage.

Because when you base AI off of possibly bad info, that bad info follows.",3
post50hb,richly branching,1.6001453961458911,highest,The concern is always how shitty ppl will use this to discriminate based on race.,3
post50hb,richly branching,1.6001453961458911,highest,"No, the concern is that AI data used for diagnostics needs to produce results that control for everything other than the data specific to that patient.

If people with brown hair in my town have more cooties because one ""brown hair club of Springfield"" decided to visit a cooties ward, I don't want my doctor diagnosing me with a high risk of cooties without any care to whether I'm in that club or went to the cooties ward, just because I happen to have brown hair.",4
post50hb,richly branching,1.6001453961458911,highest,">Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research. Scientists must first figure out why this is happening.
Artificial intelligence (AI) is designed to replicate human thinking in order to discover patterns in data fast. However, this means it is susceptible to the same biases unintentionally. Worse, their intricacy makes it difficult to divorce our prejudices from them.",4
post50hb,richly branching,1.6001453961458911,highest,"I think the concern is that racial differences can alter data in subtle ways. For example I read a study where an AI was less likely to recommend an intensive treatment for black/minority patients at any given level of disease burden, even when such treatment was warranted. The issue with the algorithm turned out to be in the training data. Black/minority patients were less likely to spend money on future healthcare, perhaps due to being unable to afford care or from having negative experiences. The issue is that the AI had been trained to use healthcare SPENDING as a way to measure health. More spending in the AI mind meant worse health. Wealthy white patients spent more money on healthcare, so the AI judged them to be unhealthier and therefore allocated more intense treatment to them. Minority patients avoided future healthcare spending, so the AI thought that meant they were healthier. The AI was using race as a health predictor without understanding the socioeconomic context. Essentially, the program had been taught using biases data so it made biased decision. Learning algorithms make predictions based on data, but they don’t “understand” the data or it’s meaning. Race correlates with many, many things, so it’s a dangerous data point for an AI to have. As you’ve said, it can also be a really useful tool when diseases vary with race, but race probably needs to be something that AIs employ meaningfully and with the foreknowledge of clinicians and researchers.",3
post50hb,richly branching,1.6001453961458911,highest,"People are afraid of accepting that different races have measurable biological differences, lest they be seen as racist. It’s ridiculous but still a reality",3
post50hb,richly branching,1.6001453961458911,highest,"More like medical professionals want their diagnostic tests to diagnose their patient only, not simply reflect statistical trends associated with their demographic back at them.",4
post50hb,richly branching,1.6001453961458911,highest,But we live in a world where is preferable to ignore reality than even suggesting different humans may have differences,3
post50hb,richly branching,1.6001453961458911,highest,"No. That's not what's happening here.

The problem is that AI will repeat any bias from the data you train it on. And Black people in the US get poorer healthcare, including late or missed diagnoses. Whether that's due to individual racism, or systemic problems, it means any AI you make is likely to perpetuate that problem, instead of being the unbiased machine people prefer to think it is.",4
post50hb,richly branching,1.6001453961458911,highest,Bc society shits itself now when the words “race” and “tendency” enter the same conversation regardless of what the topic is.,3
post50hb,richly branching,1.6001453961458911,highest,"Probably because people who frequently use the words ""race"" and ""tendency"" in the same sentence are doing it to dehumanize and oppress. You can imagine why we might be twitchy.",4
post50hb,richly branching,1.6001453961458911,highest,"Reading the article helps...

TLDR - *AI is not transparent when making decisions*, knowing it can tell race apart _surprisingly_ accurately even from corrupted data creates pontential to perpetuate and amplify human bias

> Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research. Scientists must first figure out why this is happening.
Artificial intelligence (AI) is designed to replicate human thinking in order to discover patterns in data fast. However, this means it is susceptible to the same biases unintentionally. Worse, their intricacy makes it difficult to divorce our prejudices from them.",3
post50hb,richly branching,1.6001453961458911,highest,It’s a huge worry of science that research will dive into racial differences again. It didn’t work out well the last time it was heavily studied.,3
post50hb,richly branching,1.6001453961458911,highest,It’s a sociological concern of the anti science,3
post50hb,richly branching,1.6001453961458911,highest,They built a phrenology robot,3
post50hb,richly branching,1.6001453961458911,highest,"sounds absurd, but malicious people could weaponize this technology to target and eliminate people of a different race",3
post50hb,richly branching,1.6001453961458911,highest,"My limited understanding is there has been a shift in the medical community away from “race-based medicine” which seems to have resulted in inequitable care for patent is of different races. A persons race a social construct, not a biological one and you shouldn’t be making medical decisions based off it. Treatment and care should be evidence based.",3
post50hb,richly branching,1.6001453961458911,highest,Well let me inform you that activists within medical schools have lobbied (successfully) to stop teaching racial differences within a medical context (specifically relating to renal function). Whether or not this is beneficial development is yet to be seen.,3
post50hb,richly branching,1.6001453961458911,highest,"why could this be of concern? lets see... 

*queue wavy imagination lines*

A news Anchor:

'DHS announced today that a new security procedure should increase the accuracy of airport security scans. the system originally developed for scanning medical conditions should allow DHS TSA Agents to better chose subjects for closer inspection.'


*scene change, 2 years later*

The same Anchor:

'hearings today on capitol hill continue as DHS Secretary David Clarke testifies among accusations that TSA scanning machines were used to racially profile passengers of Arab ethnicity. the technology, now removed from many airports had been developed to scan for medical conditions associated with higher prevalence in certain populations.  critics claim TSA intentionally calibrated the machines to select passengers of specific ethnic groups for unfair scrutiny. Secretary Clarke denied this accusation saying, 'the fact that 2/3rds of all passengers selected for additional screening were of Arab ethnicity is coincidental and not the result of a intentional plan of discrimination.'

*the next day*

 Amazingly the same Anchor:

'shocking revelations in a congressional hearing today as TSA scanning whistle blower Todd Howard testified, 'It's my understanding Senator that the AI was simply instructed to ignore certain passengers and send others for additional screening.' 'and uh, mr. howard, was this instruction, uh, did this include any specific criteria?' 'Yes.' 'and uh, what was that?' ' the ai was connected to a  database assembled by DHS counter terrorism office.' 'and uh, did this database have any key features?' 'yes.' 'go ahead' ' the data included were heavily biased against  Arabs.'  *murmurs*

*back in the studio* ' we turn now to our national security correspondent Kyle Ritenhouse, Kyle. ' thanks Jim, first let me just say if they hadn't been doing anything wrong there wouldn't have been any problem...'

*wavy lines*",3
post50hb,richly branching,1.6001453961458911,highest,Is the concern that this challenges the notion of race being a social construct? Clearly this evidences it's rooted in biology.,3
post50hb,richly branching,1.6001453961458911,highest,"Yeah, I don't get it either. What do they think will happen? ""I can't tell from this persons skin colour if I can discriminate against them. Well, I better make an x ray and analyse it.""",3
post50hb,richly branching,1.6001453961458911,highest,"I mean, there's a long-ass history of governments using otherwise beneficial technology for malicious ends, so that might be cause for concern. Say a government decides it's time for a genocide; having AI that can detect non-obvious signs of ethnicity could prove pretty destructive in that instance.

Say you have a political party that doesn't like the idea of race mixing. Having an AI that can tell someone's if mixed heritage could prove pretty bad in that case.

Say you have a doctor that's racist and decides that a person with a certain ethnicity is less worth their time, or can be diagnosed based mostly on that ethnicity.

Then you have more practical concerns, like the fact that neither AI nor biology are perfect. This can exacerbate the above, or lead to less reliable diagnoses based on systemic discrimination.

There are wider considerations about a technology's use than whether it can be beneficial in specific circumstances.",3
post50hb,richly branching,1.6001453961458911,highest,Its a concern because the official brainwashing dictates that race is a completely made up social construct with no basis in biology.,3
post50hb,richly branching,1.6001453961458911,highest,Maybe they're concerned about having to write new grant proposals now that their paper is out since they already missed their daughter's big game last month and they promised this was the last time?,3
post50hb,richly branching,1.6001453961458911,highest,"Eugenics is why it's a concern.

The more we learn about biological differences in race and genes the closer we get to that awkward point in our evolution.",3
post50hb,richly branching,1.6001453961458911,highest,Bc the whole agenda of both political parties is to escalate a racial divide to distract people from realities.  Everyone needs a scapegoat.,3
post50hb,richly branching,1.6001453961458911,highest,This is only a concern for woke idiots who have been taught their whole lives that there is no difference between races.,3
post50hb,richly branching,1.6001453961458911,highest,It’s almost like this was meant to be clickbait,3
post50hb,richly branching,1.6001453961458911,highest,"Yeah, clearly the AI is seeing a pattern we are not, that is allowing it to identify a race via the X-Ray.

That's a good thing. It proves there are things we are missing that the AI can detect.

So when it comes to certain diseases, the AI will pick up those patterns too, whether we understand them or not.",3
post50hb,richly branching,1.6001453961458911,highest,"Yes, this is definitely a good thing. For example, transurethral prostatectomy is a great option for removing cancer while minimizing the potential for post-surgical complications like incontinence or erectile dysfunction. However, in Asian populations the urethra can be thinner around the bladder neck, and the same procedure could cause damage to the urethral sphincter",3
post50hb,richly branching,1.6001453961458911,highest,Because theres career cancellations for any correlation between science and hot button topics.,3
post50hb,richly branching,1.6001453961458911,highest,"You get clicks for seeing the rascism in X. All this is is advertising dollars, not a real concern.",3
post50hb,richly branching,1.6001453961458911,highest,"There is a lot of concern for folks around this. If our bones aren't exactly the same, we aren't exactly the same. When you examine the bones of ancient animals you classify them as different species if they aren't the same. Generally speaking.

An often ignored fact is that there is a lot of DNA between regions that isn't shared. Humans can have up to 7% of their DNA be from other ancestors that are not shared. Denisovan ancestors, Neanderthal, etc. As in some regions have 3% of this, some 7% of that, some 0% of neither. That's a lot of DNA variation. Which is fine really.

We are also finding more and more that specific genetic markers can heavily impact things such as violence/social interactions. Whether they are turn on/off, present/missing, etc.

So if you remove the human side of things, the emotional side, the side that connects us as a society and instead go on a machines raw logic it could be problematic. 

Remember humans themselves work off pattern recognition as well as tribalism and it leads to issues that are still causing strife in society today and probably always will. A robot won't second guess itself over something like ethics.",3
post50hb,richly branching,1.6001453961458911,highest,"Wait, so race isn't just a social construct? Wtf?",3
post50hb,richly branching,1.6001453961458911,highest,"It's probably just man's unwillingness to accept and therefore eventually admitting that even specialized fields like medicine and MDs will be less efficient than machines. AI and machine technology will also render those specialized fields obsolete, and that's probably an existential concern for medical professionals. It's not like MDs are blind and they can't evaluate a patient's race prior to an X-ray. *rolls eyes*",3
post50hb,richly branching,1.6001453961458911,highest,"The reason there concerned is, the skeleton is not different between ethnicities. Outside general overall build trends.

These sort of AI's are great at seeing patterns, including those a human would ignore because there irrelevant.

There is a real possibility that something regarding race is in the training set it was given that it's now applying wrongly.

Best example I have is an AI that was meant to identify a fish. The metric it used was human fingers... The specific fish was held into cameras a ton. Pattern found pattern applied",3
post50hb,richly branching,1.6001453961458911,highest,Because of murder bots.,3
post50hb,richly branching,1.6001453961458911,highest,"insurance companies could go ""well looks like our AI says you are X which have a predisposition to Y disease so we are going to cancel your policy once you hit 45.""

And the whole time the person has had no clue about what ""race"" they were because their great grandparents came from Europe.",3
post50hb,richly branching,1.6001453961458911,highest,"This is a legal argument, not a scientific one. 

They're already trying to do this with genetic analysis. The scary part is you don't even have to consent to this. If a close relative provides data, they'll have enough to accurately predict your predisposition as well. This should be a hot privacy issue. There is no password reset or anything to help you once your identity is obtained.

We would all be smart to rally against the insurance companies now, and prevent them from using this data against you before it's too late.",4
post50hb,richly branching,1.6001453961458911,highest,"The Article literally said:   
*Artificial intelligence (AI) is designed to replicate human thinking in order to discover patterns in data fast. However, this means it is susceptible to the same biases unintentionally. Worse, their intricacy makes it difficult to divorce our prejudices from them.*

A.I. still takes the information a human feeds it and if this is in fact true, you could see the great concern.  Would you want a bias computer making mortality decisions based on your race or gender alone?",3
post50hb,richly branching,1.6001453961458911,highest,"I mean, id be concerned if i made a robot and it taught itself how to do things that i didnt think were possible",3
post50hb,richly branching,1.6001453961458911,highest,Accidental occurrences they probably tried to make it unbiased but couldn’t or you know skynet.,3
post50hb,richly branching,1.6001453961458911,highest,"I think the issue here is about machine learning engineers coding in unconscious bias. 

Incorporating ethnicity or race into medical models to curate the best diagnosis is a good thing. Unknowingly or implicitly providing a lower standard of diagnosis or treatment based on ethnicity is a bad thing. 

Given the social bias at play in the medical industry already, which is where we get the data that trains these machine learning models to start with, the latter is a worrying possibility. And worse if you can’t identify it because ML algorithms can do things that medical professionals can’t…. Like deduce ethnicity from x-ray…",3
post50hb,richly branching,1.6001453961458911,highest,Yeah like being related to george bush and somehow develop a genetic mental disorder at age 29 because aliens aren't real,3
post50hb,richly branching,1.6001453961458911,highest,"Probably concerned because they don’t know how it’s able to tell. Machine learning runs off pattern recognition, so what pattern is the ai recognizing as unique to a specific race.",3
post50hb,richly branching,1.6001453961458911,highest,"The concern seems to be that if docters are diagnosing with a racial bias, the AI will too as it uses that data to train with.

Now I think there's a things to keep in mind. If the data-pool is large enough than that means that the ai produces results very similarly biased as averagely doctors would produce results right now. 
It is not the models job to filter out bias of any kind it is the model's job to predict with as much accuracy as possible what a doctor would diagnose.
If that prediction is biased, the training data was biased.
What you could do is train the ai with verified medical conditions and see what factors matter there.

I interpret bias here als an unfair way race or gender or something else would play a role in a misdiagnosis.",3
post50hb,richly branching,1.6001453961458911,highest,Because we have always assumed there was no “skeletal difference”. Ai is saying that’s incorrect.,3
post50hb,richly branching,1.6001453961458911,highest,"I’m pretty sure humans can predict someone’s race just by looking at them, never mind x rays. Is this impressive at all? Is it more accurate then just eyeballing someone?",3
post50hb,richly branching,1.6001453961458911,highest,"Let's say artificial intelligence can confirm a perpetrators race as black and armed. They bring in the amount of force used against a black person, but it's a young white girl who is a daughter of a police officer. 

That little girl died when they could have known early they didn't need the 5 assault rifles, the taser, the k9 unit, and a no knock warrant and it was technology that did that crime. 

Think about how much funding they would lose knowing she wasn't black and dangerously armed, she was white and just performing self defense.",3
post50hb,richly branching,1.6001453961458911,highest,It’ll be a good thing when insurance companies can change your rates based off of race and genetic diseases,3
post50hb,richly branching,1.6001453961458911,highest,"Well, we are seeing the laws change at the moment where doctors can now refuse treatment to patients where doing so goes against their believes.  So the racist doctor can now refuse to diagnose scan of people they don’t like.   Perhaps they will be able to determine other groups in the future as well.",3
post50hb,richly branching,1.6001453961458911,highest,"Because proxies.

You can design race-ignorant formulas and algorithms that end up utterly racist. It's happened before, and can be done both by accident and design. 

Your local neighborhood home prices and crime rates are both positively correlated to race, so if a bank uses these in deciding to give you a home loan, which sounds superficially very reasonable, they are generating racial discrimination on the fly. Your race can't get loans and can't get to better statistics as a result and so continue to be denied loans. 

Whether your parents have a college degree is correlated to race. So if a bank is deciding to give you a student loan, and they use your parent's education as a factor, say as an indicator of ability to complete the school, then once again: racial discrimination is now implicit in this act. You get denied a loan, can't get your degree, so neither can your descendants. 

So in this case imagine your health insurance is tied to your current health. Say they use x-rays to evaluate the health of your skeleton or something, to evaluate risks of breakage etc, or decide if they approve/deny your claim. But since this apparently is enough to detect race, you now have a (hidden) proxy for race, and using it can exacerbate racial discrimination despite any attempts to avoid it entirely. Your claim may get denied because race is correlated with getting denied, and the xray tells them that without ever having to make it explicit at any stage.",3
post50hb,richly branching,1.6001453961458911,highest,"If it is used to do automated radiology, it might end up biased by poor patient outcome of people from certain races and used to then justify denying people needed procedures and services. That sounds crazy to some but really, I don't think it is.. If the sample data says black people with this stage of cancer or whatever have a very low survivability rate, it might deny them for a procedure. Training data for medicine can be subject to racial bias.",3
post50hb,richly branching,1.6001453961458911,highest,I’m pretty sure I read that they are concerned over the fact that they cannot figure out exactly *how* the AI is able to accurately determine this from the X-ray.,3
post50hb,richly branching,1.6001453961458911,highest,Maybe if used for war? A drone that can differentiate between sides,3
post50hb,richly branching,1.6001453961458911,highest,"Disclaimer: I did not read the article but I have a masters in computer science and my thesis was in the realm of data mining which is one of those AI adjacent things. 

The issue could be in the training set. You usually give the AI some data to do the initial learning from and if that data is racially biased because doctors are racially biased the AI could pick up the bias even if you remove the explicit declaration of race.

If the training set is really good then what you said would happen happens. 

Moreover if any of the later data the AI collects while it’s working is racially biased the AI could learn the bias then. 

You cannot remove it because even if you remove the race attribute the AI knows the race from the X-ray and you can’t remove the X-ray because it’s important diagnostic data. 

In an ideal world race would be important diagnostic data too but we are where we are.",3
post50hb,richly branching,1.6001453961458911,highest,"For example, two jewish people with taysacs (sp?) or two black people with sickle cell",3
post50hb,richly branching,1.6001453961458911,highest,You have my upvote but I want to admit I'm a pretty high and worried /you're/ a robot.,3
post50hb,richly branching,1.6001453961458911,highest,I think it’s a concern because if there’s something like actually significantly different about certain races white supremacists and other racists will have a field day saying that this proves that white people are different and therefore superior,3
post50hb,richly branching,1.6001453961458911,highest,Saw this research come out a while ago and it seems like used for evil hypothetical Dr. Evil type stuff keeps whoever came up with the title up at night.,3
post50hb,richly branching,1.6001453961458911,highest,Cyborg Hitler.,3
post50hb,richly branching,1.6001453961458911,highest,"It’s a concern because it bumps up against our ideologies. Religion is largely dead, but it doesn’t stop the average person from thinking about and judging the world in the same ways as the deeply devout.

We should be committed to the truth no matter what. Even if it means the Jeff Bezos’s of the world are some kind of elvish master race. Or if I am some kind of lower species. Neither are true of course, but whatever subtle differences there are between groups is useful to know because we can exploit this knowledge to enhance our lives. And by “our” I mean everyone. It’s a shame to see sometimes people put their vision for the way things aught to be ahead of the way things are. This impulse has lead to a fair few reckless choices in the world - anti-nuclear being the one that comes to mind the quickest for me.

Whatever’s true, I want to know. Whether people should be judged as individuals, but treated equally at first meeting, Is not up for debate imo. Everything else is up for grabs",3
post50hb,richly branching,1.6001453961458911,highest,"Like many break throughs, it could help in so many ways, and yet, somehow they will use this for spying and other such evil shit.",3
post50hb,richly branching,1.6001453961458911,highest,"The concern isn't that the AI is good at making helpful discoveries based on knowing the race of the patient. They were doing this study because of past findings that AI was missing things for certain races; consistent with the source data it had learned from:

""Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research. Scientists must first figure out why this is happening.  
Artificial intelligence (AI) is designed to replicate human thinking in order to discover patterns in data fast. However, this means it is susceptible to the same biases unintentionally. Worse, their intricacy makes it difficult to divorce our prejudices from them.""

They already knew the AI was failing to diagnose certain illnesses in certain races at a similar rate to humans feeding the data. Contrary to the presumed goal of having a less biased way to review x-rays.  So they did this test to see if the AI was somehow figuring out the race from the x-ray even when they didn't want that to occur and had previously assumed there wasn't sufficient info there for the AI to make that determination. The ""concern"" is based on the fact that they seem to know based on the source data the AI learned from that IF the AI can figure out the race from an x-ray it will then rely on all the flawed racially skewed data and do just as bad a job as we've been doing.",3
post50hb,richly branching,1.6001453961458911,highest,its a concern because it’s not politically correct/woke. The mere suggestion that people could be biologically different due to their genetic makeup is now extremely racist.,3
post50hb,richly branching,1.6001453961458911,highest,"Applied ML researcher here: one reason a result like this is “concerning” relates to heuristics and how algorithms make predictions.

There are conditions that correlate heavily with income/socio-economic status, that may actually have no genetic propensity relating to race, but are nevertheless heavily correlated with race because of the correlation between race and SES.

If a model can infer race, that means that it may be using these features that indicate race as a proxy to predict certain conditions, even if those features that indicate race have nothing to do with the condition. The problem with that is that it limits the scope of when the model is useful—move to a context where the SES/race correlation is different, and the model will perform much worse.",3
post50hb,richly branching,1.6001453961458911,highest,"So not all races are equal?

I found the RACIST! 

You should be ashamed. Of course the scientists are concerned. If a robot is telling us things we don't want to hear without the ability to even consider racism, which is everywhere everyday, then we just need to stop advancing humanity immediately. 

People are being hurt here! Feelings are more important than scientific advancement.",3
post50hb,richly branching,1.6001453961458911,highest,"It would seem to me that activists and maybe some strains of sociologists may be concerned, but I don't see why others should be concerned.",3
post50hb,richly branching,1.6001453961458911,highest,Human doctors are concerned because they thought they were safe from robotics taking over.,3
post50hb,richly branching,1.6001453961458911,highest,Wouldn't your race already be determined in your scenario?,3
post50hb,richly branching,1.6001453961458911,highest,"> Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research. Scientists must first figure out why this is happening.",3
post50hb,richly branching,1.6001453961458911,highest,"This is a concern because it supports the race realist view adopted by White Nationalists. If there exist verifiable skeletal differences between the races, it is likely that there would also exist cognitive differences between the races.",3
post50hb,richly branching,1.6001453961458911,highest,[removed],2
post50hb,richly branching,1.6001453961458911,highest,"To me it read like: we know AI can be racist, we know this AI is good at detecting race in X-rays ~~(which should be impossible)~~ but aren't sure why, we also know AI misses more *medically relevant information* (""indicators of sickness"") in Black people in X-rays but aren't sure why.

This is a legitimate problem that can easily be expected to lead to real world problems if/when this AI is used without it being identified and corrected.",3
post50hb,richly branching,1.6001453961458911,highest,"This reminded me of the racial bias in facial recognition in regards to people of color. However, we should want an AI that is capable of detecting race as it does become medically important at some point. But to miss diagnosing illnesses in a subset or group of races at a disproportionate rate is indeed concerning and would lead me to ask about what training model was used and what dataset. Are we missing illnesses at the same rate in racial groups when a human is doing the diagnostics?",4
post50hb,richly branching,1.6001453961458911,highest,">we know this AI is good at detecting race in X-rays (which should be impossible) but aren't sure why

Except determining race from x-rays is absolutely possible and is done, reliably, by humans, currently, and we know why.

&#x200B;

Edit:  It looks like you were paraphrasing what the article is saying, not saying that yourself, my bad.  The article does make the claim you mention, which is just wrong.",4
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,"My SO is a pulm crit doctor and our area is a largely black population. During the pandemic doctors noticed the oximeter readings on POC were showing higher oxygen readings than the blood gas tests, so unless they ran the blood gas test they weren't treating them as hypoxic until they were more severe because they didn't know they needed to. There have now been several international papers written on the issue. These types of medical equipment biases could possibly be a factor in some of the disparities between medical outcomes for black people and other races.",4
post50hb,richly branching,1.6001453961458911,highest,"Considering genetics (race, by and large) plays a huge role in bone structure, facial structure, build etc... I don't see why an AI attached to X-rays, given a large enough sample size where it knows the answer...

It shouldn't be hard for an AI to predict genetic markers for a race indicative in bones.

I don't get it.",4
post50hb,richly branching,1.6001453961458911,highest,"This could be a problem with the learning set. Admittedly I'm a novice with this, but they likely started with real patient data. If the data being taught to the algorithm had worse ""correct"" diagnosis from racial bias of the doctors, we would end up teaching the computer to incorrectly diagnose people based on race",4
post50hb,richly branching,1.6001453961458911,highest,"That's really odd, and also makes me wonder if some of the reasons the AI does it are similar to why doctors misdiagnose patients of color more frequently",4
post50hb,richly branching,1.6001453961458911,highest,"Exactly, it's not that the scientists are afraid the AI isn't woke, it seemed like they're not sure why this is happening, what effects it could have on AI used for medical diagnostics, and any other unknown effects it could have.",4
post50hb,richly branching,1.6001453961458911,highest,"Re: the “aren’t sure why,” isn’t the prevailing theory that these codes are primarily created by non-Black men, so diseases that disproportionately affect women and BIPOC are comparatively less represented?",4
post50hb,richly branching,1.6001453961458911,highest,"Interesting take. 

I don't disagree that this is a problem however... I think this has way more to do with the data set its previously received other than anything nefarious at least in a direct way. 

My theory is that this tech has been introduced in areas that have a higher income per household than others. I want to clarify right here that in absolutely believe that anyone of any race or religion can reach any level (barring the same start which doesn't happen, I'm aware.) But statistically they are going to test more white people, at expensive hospitals. 

The odd thing though is that according to the cdc African Americans visit the ER at a rate double of white Americans... so I'm definitely not committed to this theory at all but I have a theory this was a statistical anomaly over some sort of direct attack... but that being said I've been far more disappointed in humanity so who knows. 

This is a lot more interesting than just ""ai thinks black people bad"".",4
post50hb,richly branching,1.6001453961458911,highest,"Agreed. Self driving cars can make mistakes, they just need to be better than human drivers to be a ner positive for society.

In the same way, AI healthcare will be racist, it’s almost impossible to eliminate. But, as long as it’s less racist than the existing healthcare system run by humans (a very low bar to clear), then these systems can still be good.

Making AI more equitable than human judgment is the next frontier of our algorithmic world, and that’s why studies like this are so important.",4
post50hb,richly branching,1.6001453961458911,highest,I would think that the fact that the algorythm is having a hard time detecting sickness in african american examples is because the data being fed into it is also full of examples of failed diagnoses of these same groups. Potentiality what this is showing us is a clear reflection of the inadiquicies of our own data that were too subtle to be noticed outside of a giant aggregated data set like the ones machine learning employs.,4
post50hb,richly branching,1.6001453961458911,highest,"To me it read : humans aren't all just clones, we're confused the IA noticed the difference. Wtf is wrong with people.",4
post50hb,richly branching,1.6001453961458911,highest,"AI is really good at being racist. Text AI's will say racist things straight from 4chan, Image Classification has a Gorilla problem that most have put off for now. The court sentencing ones suggest higher sentences for black people.",4
post50hb,richly branching,1.6001453961458911,highest,"Bruh, ai is not racist, the doctors that rely on it might",4
post50hb,richly branching,1.6001453961458911,highest,"Yeah, they could identify people who are against them, and make sure they lost their position of power...",4
post50hb,richly branching,1.6001453961458911,highest,"What's annoying about this whole thing is that doctors can misdiagnose patients given their race. So if the data set is flagging Black persons as healthy, it may be a problem with the data set? No? 

I remember years ago I saw a comment of some dude talking about how their AI was specialized in distinguishing dogs and wolves and it was good at it. But as soon as they showed pictures of wolves during summer, the AI failed. The answer was that most images of wolves were taking with snow in the background.... So it was basically detecting snow.",4
post50hb,richly branching,1.6001453961458911,highest,This is the answer,4
post50hb,richly branching,1.6001453961458911,highest,What if the AI missed the relevant medical info because it thought the patient's race was the disease?,4
post50hb,richly branching,1.6001453961458911,highest,"Rather than focusing on AI we should just compare it with non AI to give it a judgment. Does it miss more medically relevant information in Black people than the average doctor? Is it more susceptible to lying about it decision process than the average practician? Is it harder to audit, evaluate and act upon to improve the decision making?

If the answer is mostly no to these questions, AI is helping and people should be happy about that.",4
post50hb,richly branching,1.6001453961458911,highest,How can AI be racist?,4
post50hb,richly branching,1.6001453961458911,highest,"AI is not racist unless you make something like a chatbot. Period. It is totally an engineering problem which can be solved with certain methods or a dataset problem.

Also, as technically you mention it, AI is expected to identify race just because it is shown that it can be biased between data of different ethnicities.

As you also said, this is interesting in the fact that we can go for much better AI and Medicine as a field since these are not at all expected if there is decent engineering.

But I have to mention, the original paper should rule out Clever Hans effect by any means, the research means nothing if it is a Clever Hans effect (which sounds like it is not totally ruled out, as a quote in article said ""we have to wait"". I have no idea how they can totally rule out Clever Hans except by manual checking, and given that 100000s of images were analyzed there should definitely be a difference of what sort of X-rays are accessed unless it is a general truth that there is a miniscule of difference in x ray imaging.)",4
post50hb,richly branching,1.6001453961458911,highest,"Your last part has not been proven though?  (Unless this article is saying that?)

The consensus is that the AI can see the difference in skin tone / pigment on the X-RAY.  (Like maybe the fine grained grains on the X-ray have some info it was able to use for this purpose.)",4
post50hb,richly branching,1.6001453961458911,highest,"I'd conjecture that it's due to the data of the AI being provided, if earlier data were written by white men with a certain bias, the data is going to be skewed. Medical students these days are rarely given books that correctly identify the differences in bone structures and other physiological aspects of their patients. Most of the subjects in those books are white. I'm not pontificating upon certain predilections that earlier professionals may have had in specific, but ignoring that that sort of bias was a thing does nothing to alleviate the situation. 

(The deeper message underneath, that would have been said with actions like those much louder than any words is that individuals with those predilections don't want students to understand how to more accurately treat those patients. That's a level of twisted I'm not sure I can properly articulate.)",4
post50hb,richly branching,1.6001453961458911,highest,This AI was developed by white supremacists. Try Wakanda.,4
post50hb,richly branching,1.6001453961458911,highest,"Lmao, the leap would be AI purposefully puts in less effort in skeletons it believes to be minority races... uhhh to match human counterparts in care.",4
post50hb,richly branching,1.6001453961458911,highest,"With my limited knowledge of AI, I bet it's coming down to how they are training the AI. They are probably feeding it all humans as one group. In the US, this would lead to a majority of the training set being white with about 15% being black. However, if they individually trained the AI with ""this is a white person"" and ""this is a black person"" groupings, it could better detect the difference in treatment needed.

Just a guess, but I do work a lot with AI/ML engineers so not talking *completely* out of my ass.",4
post50hb,richly branching,1.6001453961458911,highest,"nah, the not knowing how its doing this is the problem and not that it is doing it. being able to differentiate between races by xray is a good thing since as you point out it misses things in xrays from black people more than it does for others. so now that it can identify race it can flag those xrays for extra scrutiny. 

this is in fact a good thing.",4
post50hb,richly branching,1.6001453961458911,highest,Isn't there a huge issue with their being way more published research with white participants than black? If that rings true in the training set for the neural network there's your problem.,4
post50hb,richly branching,1.6001453961458911,highest,"The study is a neat example of how seemingly unimportant information leaves trace information in data. 

There are also models that can ""back infer"" things like age, race, gender, ethnicity, personality, and facial structure from speech. I bet a sophisticated machine learning model could even predict someone's dental history from their speech too, like whether they got braces.

All such factors leave a little information in the data, and machine learning is good at approximating that relationship with decent accuracy. 

There was also one study showing that by connecting the timing of a few facebook likes, you can identify a person's age, gender and location with surprising accuracy. 

The OP's article doesn't even actually cite what research it is supposedly describing, or have an author, and it appears every article on the website is posted by the same blog account. At the bottom of the website, it says it was created by ""Blog"" and uses a template. So there is also that. Maybe it is computer generated or stolen content.",4
post50hb,richly branching,1.6001453961458911,highest,"Doctors often miss and dismiss medically relevant information in black people. 

I'm sure the people who write the code for the AI are just providing the AI already biased data and information from doctors. 

Too many people discredit how bias of the data or the programs are the reason why the bias exists within AI. The program can only have so much ""independent"" thought from what it was taught grow it's understanding of.",4
post50hb,richly branching,1.6001453961458911,highest,"So how about we feed this AI with data from around the world, instead of just data from the US? Would that not equal things out?",4
post50hb,richly branching,1.6001453961458911,highest,"""(which should be impossible)""

What's the highest level of biology you've taken in school? Primary?",4
post50hb,richly branching,1.6001453961458911,highest,You know that an anthropologist can make a decent guess at someone's ethnicity from their skull features right?,4
post50hb,richly branching,1.6001453961458911,highest,"All current ""AI"" is specialized to specific tasks. There is no abstract concepts like race present, so no possibility for the AI itself to be ""racist"".  The data used to train the AI on its specific task can have a racial bias (i.e. facial recognition software that is mostly trained with white faces) and that bias can cause the AI to underperform in specific cases (i.e. distinguishing faces of minorities).  This isn't really any different than the existing issue where diseases that mostly affect minorities are not studied enough in medicine due to most doctors (at least historically) being white.  It is human biases being systematically encoded into knowledge bases that are supposed to be applied to all humans.",4
post50hb,richly branching,1.6001453961458911,highest,"People always forget that AI is written by humans, too. Human biases are often unconsciously built into AI, algorithms, statistics, etc. Junk goes in and junk comes out.",4
post50hb,richly branching,1.6001453961458911,highest,Why would that be impossible?,4
post50hb,richly branching,1.6001453961458911,highest,"The clue is in the first paragraph

> ""which would be impossible for a human doctor looking at the same photos""

This just ian't true, humans are perfectly capable of telling the difference between racial skeletal morphology. Skull shape, arm length, teeth shape and jaw position, pelvic structure, bone density, eye socket orbital structure are all indicators.

Fore sic anthropology is full of racist history, but it tenda to be an area where experienced doctors are actually pretty good at it.",4
post50hb,richly branching,1.6001453961458911,highest,"Smaller data set. Segmented input data based on real care in populations. 

The folks working on it probably aren't able to engage in ethical sampling procedures because the data is flawed from unethical care.",4
post50hb,richly branching,1.6001453961458911,highest,"yeah, that's what I'm confused about. if you don't program racism into an AI, it will just see a distinction between races, and that's... it?

it's not like an AI will just become racist",3
post50hb,richly branching,1.6001453961458911,highest,DIRECTIVE 4: BE RACIST AF,4
post50hb,richly branching,1.6001453961458911,highest,*Tay (bot) entered the chat*,4
post50hb,richly branching,1.6001453961458911,highest,"AI will never be racist, but it can have racial biases which are definitely a real issue. I think this article is clickbaity as fuck, but racial bias in AI is an interesting topic",4
post50hb,richly branching,1.6001453961458911,highest,"But what if the data is racially biased? For instance, what if the correct identification of sickness from x-ray imaging is disproportionately lower in minority samples? Then the AI learns that flagging those correctly is both an issue of  identifying the disease and then passing that diagnosis through a racial filter. 

Nobody tells their AI to be racist, but if you give it racist data that's what you're gonna get.",4
post50hb,richly branching,1.6001453961458911,highest,"You don't need to ""program"" the racism in - that comes with your dataset. For example, if your data shows that high performing students tend to come from certain zip codes, and then train a model on that data for university admissions, then your model will reinforce the structural bias that already exists.


Maybe you want to use a model to figure out who should get organ transplants, maybe based on 5 year survivability rates or something. Then it turns out that a certain demographic is more prone to obesity based on socioeconomic factors of certain neighbourhoods, so your model learns not to give organs to that demographic.


""AI"" becomes racist very easily.",4
post50hb,richly branching,1.6001453961458911,highest,"It would be very easy for it to happen by mistake. If you're training a model based on other skeletal features it's possible that some of them could be correlated with race. Now you have a model that could potentially ""learn"" to treat people differently based on race. In some cases this may be fine or good, in some cases it could be bad. Bias in complex models is not so simple as ""you program it in or you don't""",4
post50hb,richly branching,1.6001453961458911,highest,"Here’s where it could become problematic. Let’s say that a company creates an algorithm to help with triage or prioritizing scheduling for life saving procedures. It combs through medical records and health outcomes and takes in current records to determine a priority. Most people will probably say that it is highly unethical to factor a patients race into those decisions. So the decision is made to not include patients race in the medical records. Some hospitals may even say it’s also an attempt to be neutral not let human bias cloud decisions. 

But let’s say the AI starts to accurately group patients by race based on X-rays and other diagnostic tests. It then goes out and finds similar patients in the data set.  In the US, racial minorities often have worse health outcomes because they often lack access to healthcare and systemic racism. The data set would show this. 

Because of this the algorithm would spit out a lower priority for some racial groups because they had worse health outcomes in the data set. They triage or procedure is delayed and the patient has a worse health outcome, which seemingly proves the algorithm’s assessment. 

Nobody told the AI to be racist. But the dataset and the AIs ability to accurately group races by X-rays made it so past and current inequities are pushed and reinforced. And the worst part is people can just throw up there hands and say that computers are making the decisions an not humans. 

As discussed in the book, Weapons of Math Destruction by Cathy O’Neil, these bad algorithms can and do reinforce existing inequities along racial and socioeconomic lines. So the fact that the AI can racially group people based on X-rays is problematic. Yes, there are medical conditions where race is a factor, but you don’t need an X-ray to tell you the patients race.",4
post50hb,richly branching,1.6001453961458911,highest,There is a lot of evidence that AI is racist in general. It’s designed by people after all,4
post50hb,richly branching,1.6001453961458911,highest,"Did you read the article?

>Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research. Scientists must first figure out why this is happening.

>Artificial intelligence (AI) is designed to replicate human thinking in order to discover patterns in data fast. However, this means it is susceptible to the same biases unintentionally. Worse, their intricacy makes it difficult to divorce our prejudices from them.",4
post50hb,richly branching,1.6001453961458911,highest,"Well that's just the problem, chief. We programmed the ai to be incredibly racist. I'm talking anti Welsh, Lovecraft levels of racism here",4
post50hb,richly branching,1.6001453961458911,highest,It will if you believe seeing a distinction between races is racist.,4
post50hb,richly branching,1.6001453961458911,highest,"From another comment below:

> So, in the case of the AI identifying race via X-ray, that might seem innocuous and a ""huh, that's interesting"" moment, but it could lead to problems down the road because we don't control the associations it makes. If you feed it current treatment plans which 
> are
>  subject to human biases, you could get problematic results. If African Americans are less likely to be believed about pain, for example, they'll get prescribed less medication to manage it. If the AI identifies them as African American through an X-ray, then it might also recommend no pain management medication even though there is evidence of disc issues in the X-ray, because it has created a spurious correlation between people with whatever features it's recognizing being prescribed less pain medication.",3
post50hb,richly branching,1.6001453961458911,highest,"So not inherently a problem with the AI itself, but the racial bias already present in the medical community? Sounds like a textbook systematic racism issue and not actually a problem with AI at all. Just don't teach your robot to be racist and we're all good.",4
post50hb,richly branching,1.6001453961458911,highest,"It’s not about the AI’s moral framework, but about the use of information by people, or the way a system is constructed by people. If there’s an assumption that data (and the tools for acquiring and manipulating data) is pure and unbiased, then it is easy to see how racial prejudice could come into play in medical treatment that results from this data/these tools.",3
post50hb,richly branching,1.6001453961458911,highest,"I’m still confused how this is going to cause an issue. In what world are scientists/doctors manipulating this data and don’t know the race of their patients/subjects for some reason and then somehow some kind of bias is caused by this observation?

Edit: please read my responses. The people reading this comment are not reading the headline correctly. I’m fully aware of data bias. This isn’t talking about bias from data we feed in, it’s talking about the AI being able to predict race based on X-Rays. This is not the same as feeding in biased data to the AI. This is output. Being able to determine race from X-Rays isn’t surprising. There are predictors in our skeletons.",4
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,No it's written as if you already understand the now widely known basic concept of racial bias can be inherent in AI trained by people with racial bias,3
post50hb,richly branching,1.6001453961458911,highest,"The problem is that if there is racial biases and we font know why, what other biases are there, and how will the racial biases impact its effectiveness.

Also leaves potential for governments or other malicious actors to potentially use the tech. To impose racist laws/acts.",3
post50hb,richly branching,1.6001453961458911,highest,Maybe they're using Blizzard's super woke racism calculator,3
post50hb,richly branching,1.6001453961458911,highest,">Woke

What exactly is this supposed to mean? Are you not familiar with the biases that have been found in many AI systems. For example facial recognition that can't identify black faces because it was not designed to? Is it wrong to want to include people from other backgrounds in technology. Is that as a concept offensive to you?",3
post50hb,richly branching,1.6001453961458911,highest,"This is a word I wish everyone would forget. ""Something I don't like?! WOKE ITS WOKE AGGHHHHHH!""",3
post50hb,richly branching,1.6001453961458911,highest,"More, they're concerned that the programming of the AI has the original creators' biases built in.  Bias, especially in the medical field, is bad.  It leads to things like more black women dying during childbirth because doctors has a bias about black women.",3
post50hb,richly branching,1.6001453961458911,highest,"AI by definition isn't woke. They are reflections of the status quo (problems and all). Luckily humans can be woke and say hey this shit ain't cool, let's make it better.",3
post50hb,richly branching,1.6001453961458911,highest,"AI is proven to be bias. 

When AI is bias, it removed objectivity of analysis and turns it into judgement instead of analysis.",3
post50hb,richly branching,1.6001453961458911,highest,"The whole racism scare considering AI recognition is a straw man argument, and ai believe it is being pushed by people with interests in that sector. When the improved algorithms will be used to dictate important aspects of a commoner's life, they can say the algorithms are good, because they aren't racist. The question of whether it is fundamentally ethical to apply algorithms in such way will be pushed aside; the developers will only have to defend and prove their products aren't racist.",3
post50hb,richly branching,1.6001453961458911,highest,"There are writers and/or people involved with these studies as HR more than as scientists who are trying to push the whole ""everything is a social construct and there's no X to prove that Y is anything but arbitrary."" They're ideologues who get nervous when their ideology is challenged by results. 
  
They use ""social construct"" as a synonym for ""completely made up hogwash"" because they fail to realize that almost everything can be considered a social construct (or that a social construct can be based on observable phenomena)",3
post50hb,richly branching,1.6001453961458911,highest,That's exactly what it seems.,3
post50hb,richly branching,1.6001453961458911,highest,[removed],3
post50hb,richly branching,1.6001453961458911,highest,[removed],4
post50hb,richly branching,1.6001453961458911,highest,"This.

Over and over again some of these people literally want to cancel the singularity because it's doesn't fit their own biases.",3
post50hb,richly branching,1.6001453961458911,highest,Where do you see in this article any concern for being woke? What specific combination of words make you think that? You're projecting your own politics onto  this issue.,3
post50hb,richly branching,1.6001453961458911,highest,You mean there is no precedent for woke and political nonsense to stand in the way of advancing AI or just technology in general? I really wish I could block that stuff out like that.,3
post50hb,richly branching,1.6001453961458911,highest,"Yes, but people have been socially conditioned to think that all racial bias is bad.

I'm a university professor, so I can sort of get away with asking the question, ""What are some example of positive racial bias?"" but some students are stricken aghast when you say that.  They are convinced that phenomes that alter appearance occurred in a vacuum and there can't possibly be any other differences in the races.",2
post50hb,richly branching,1.6001453961458911,highest,"Try being a psychology professor and mentioning that mens brains are physically bigger!!

You can feel an ice chill sweep the room with a hundred cold eyes staring daggers as they frantically try to explain there is no cognitive difference however as womens brains are more connected between hemispheres",3
post50hb,richly branching,1.6001453961458911,highest,"Tell them about the size and weight of mobile phones or computers in the last millennium.

They're getting mad on a false and premature assumption.

Then tell them the higher someone's IQ, the more likely it is a male. Watch the show again.

Then tell them the lower someone's IQ, the more likely it is a male.",4
post50hb,richly branching,1.6001453961458911,highest,"This is true, but people don’t like hearing it because they assume it implies that “bigger brain = more intelligent” which isn’t necessarily true. However in transgender females who medically transition, when they start taking testosterone it can cause brain inflammation. My former neighbor who is trans is going blind now as a result of taking testosterone since the brain swelling/inflammation is pushing against their eyes. A bigger brain isn’t always better.",4
post50hb,richly branching,1.6001453961458911,highest,"I hear men's brains also have a smaller hippocampus than women's, but I'm not sure whether or not that's true.

Either way, I find the physical differences between male and female brains fascinating.",4
post50hb,richly branching,1.6001453961458911,highest,It’s hilarious getting into a conversation about racial disparities across particular illnesses and getting called a racist.,3
post50hb,richly branching,1.6001453961458911,highest,"Well, you are. But its a good thing xD.

/s",4
post50hb,richly branching,1.6001453961458911,highest,">They are convinced that phenomes that alter appearance occurred in a vacuum and there can't possibly be any other differences in the races

Well it would be good if we stopped teaching that this was true in school.",3
post50hb,richly branching,1.6001453961458911,highest,"I mean, there are medical and genetic traits that do correlate with geographical origin, and thus, broadly, with race. I'm not saying thats what the guy you replied to meant, but this is one way that race affects more than just a person's appearance.",4
post50hb,richly branching,1.6001453961458911,highest,"“Bias” in general is thought of as a bad thing. Racial bias, recency bias, historical bias, etc are all thought of as obstacles to The Truth in an academic setting.",3
post50hb,richly branching,1.6001453961458911,highest,Isn’t race the wrong word to use when we’re talking about inherited traits? Shouldn’t we use ancestry or geographic origin? There are people who are “black” but have very different genetic backgrounds. It’s more useful to think in terms of populations of people than in made up racial categories.,3
post50hb,richly branching,1.6001453961458911,highest,"Sickle cell doesn’t care about your geographic origin. Black peoples in two different nations did not evolve independently of each other. At a macro level, they *do* share ancestry as far as predisposition for disease is concerned.",4
post50hb,richly branching,1.6001453961458911,highest,And… what are some positives?,3
post50hb,richly branching,1.6001453961458911,highest,"A common one, most people will agree to, is that a person of color may want a therapist of their own race.  Clearly, this is a racially biased perspective, but most people can see how it's beneficial because part of therapy is being as comfortable in the situation as possible.  Now, maybe there is a moral failing in the patient for being more comfortable around their own race, but that's a separate question; their will likely be a net positive in outcomes if they are allowed to select them.",4
post50hb,richly branching,1.6001453961458911,highest,"Screening blacks for sickle cell anemia might be considered a racial bias.

Avoiding a group of aggressive young black men in gang attire when walking alone at night is a racial bias.

Choosing a black person to be on your basketball team is a racial bias.",4
post50hb,richly branching,1.6001453961458911,highest,[removed],4
post50hb,richly branching,1.6001453961458911,highest,I suppose having an employment scheme aimed at employing more minorities to create a more diverse workforce would be a positive example of racial bias.,4
post50hb,richly branching,1.6001453961458911,highest,"You're assuming that the positive and the negative can be separated at all intersections. The people who this will ""positively benefit"" are going to pay the price because companies aren't going to understand or even give a shit to that degree. It's never going to be a priority UNLESS it affects the people working on it directly, and what kind of guarantee can we make for that? None. It's not like people have these opinions because it's not a constant in people's lives.",3
post50hb,richly branching,1.6001453961458911,highest,"You're sort of asking the wrong question, though. The relevant question here are 'do you want machine learning to reinforce those positive racial biases?' I would argue you don't want machine learning to reinforce any such biases, positive or negative, because any such biases are eventually going to have a socially undesirable consequence, even the positive ones.",3
post50hb,richly branching,1.6001453961458911,highest,"What are some positive examples or racial bias? This is a genuine question tbh. Does affirmative action count?

Do you mean inherent genetic benefits that come from race or social benefits? Isn’t all racism a positive example of racial bias towards the people that don’t experience racism?",3
post50hb,richly branching,1.6001453961458911,highest,"I'm not sure what answer they give, but Ashkenazi Jews are prone to certain genetic diseases.  An AI that also knows whether the patient is an Ashkenazi Jew might treat mild indicators of cystic fibrosis differently and make a better diagnosis.",4
post50hb,richly branching,1.6001453961458911,highest,"Irish predisposition to alcoholism means its hard as hell to poison an irishman with booze.

Not a lot of real life applications but if a bunch of assasins armed with scotch show up , guess who you'll be running to?",4
post50hb,richly branching,1.6001453961458911,highest,But racial bias exists in the medical world. This millennium medical students still believed Black people literally have thicker skin.,3
post50hb,richly branching,1.6001453961458911,highest,"That’s different that what we’re talking about here. Think: black people are more likely to develop sickle cell disease. That’s just, true",4
post50hb,richly branching,1.6001453961458911,highest,That isn't what biased is.,4
post50hb,richly branching,1.6001453961458911,highest,">""What are some example of positive racial bias?""

I think the thing that's making students pull back from your question is that there's growing conversation about how even 'positive' examples of racial bias can end up being a double-edged sword. 

For example - A university preferring asian candidates due to a perception that asian kids make for better students would be considered a positive bias. It means more asian kids will get into the school they applied to, right? But it will also end up feeding into the 'model minority' stereotypes - The school faculty and staff might be inclined to view an asian student who isn't doing well in their classes as someone who is 'clearly capable of more' and 'just being lazy', even if the real problem is that they're dealing with an unchecked learning disability or mental health issue that's impairing their performance. The school might be less willing to reach out with resources to assist, preferring to spend those resources on the students who 'really need them' instead, and suddenly the 'positive' racial bias is looking a lot like racism.",3
post50hb,richly branching,1.6001453961458911,highest,"Yep, it's a complicated an nuanced issue, but they struggle to engage the complexity.   When I was growing up, we were expected to be able to argue either side in a debate, regardless of our personal beliefs, in part, because it helps you understand an issue on a deeper level; now it's like they can't even bring themselves to admit there is another side.

I don't know what's wrong with them.  I've asked my seniors, ""Why is racism bad?"" and they struggle to put together a coherent response.  They've had going on 4 years of a liberal arts education, and they can't form rhetorical arguments.  They just say, ""It's obvious.""  I didn't ask if it was obvious; I asked you to articulate why something is so.

There also seems to be a diminished respect for the truth.  Here is a contrived example, I could see happening in one of my classes.

Person A: ""Hitler ate babies.""

Person B: ""There is no evidence that Hitler ate babies.""

Person A: ""Why are you defending Hitler?""

It's gone beyond *truthiness,* now it seems like misinformation is permissible, if not encouraged, as long as it's about the other side.",4
post50hb,richly branching,1.6001453961458911,highest,"In the real world, universities discriminate against Asians because they'd be even more overrepresented than they are if admission was purely  based on academic achievement/exam results.",4
post50hb,richly branching,1.6001453961458911,highest,The problem is most of the time you'll be wrong. Race is usually not the main factor for most medical variances. It is typically self reinforcing prophecies on diagnosis and socioeconomic differences.,3
post50hb,richly branching,1.6001453961458911,highest,[deleted],3
post50hb,richly branching,1.6001453961458911,highest,"I get the African bit but why the rest of the world as 1?  Aren't, say, indigenous south Americans as different to Europeans as two different African groups are from each other?",4
post50hb,richly branching,1.6001453961458911,highest,"Yeah something like sickle cell is more common in those with African ancestry. But that's also easily detectable by a blood draw. 

I'm not sure why the scientists are ""concerned"" by this, unless they are worried that racists will use this as a bases for their beliefs/arguments like ""see we are different, even a computer agrees""",2
post50hb,richly branching,1.6001453961458911,highest,"> Yeah something like sickle cell is more common in those with African ancestry. 

That is true in the US, but not in Africa.  That's because the sickle cell gene is primarily found in people living in [specific areas of Africa,](https://miro.medium.com/0*VKS36ceCtyoFiJ-a.png) as in its geographic, not racial.   Last I checked, the leading theory was that it tracks the distribution of malaria because the gene gives people protection from malaria.

The reason it is true in the US is because of slavery.  The majority of enslaved people were stolen from areas with high rates of the sickle cell gene like West Africa rather than places with low rates of the gene like South and East Africa.",3
post50hb,richly branching,1.6001453961458911,highest,this makes a lot of sense. thanks for sharing!,4
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,"That's not it. They are worried that the AI produces wrong results.  
In theory analysing stuff with an AI sounds great as an AI is perfectly neutral. For an AI everything is just data there is no difference at all. However in reality AIs are sort of like small children.  
If you teach them the wrong thing they are going to replicate it without any selfreflection.  


If an AI is able to detect the race it suddenly changes the dataset for its analysis. And these datasets are not unbiased. In a world where we humans created totally neutral datasets nothing of that would be an issue but we do not have such datasets. Diseases occuring more often in one group than in the other are a good example for this. If such a disease occurs often for one group we probably have a lot of data on it for said group with a realistic chance of representation.  
However for another group te data can be totally off just because we usually do not test them for this disease. They have it in a much higher number than anticipated we just do not know of it since we do not test people outside of a group that is more vulnerable to it.  


Generally whenever an AI is able to detect elements that are subject to human biases it is very cocnerning because they just make things worse. At that moment these biases become a self fulfilling prophecy which is horrible for a tool that is meant to be helpful.",3
post50hb,richly branching,1.6001453961458911,highest,"I feel this whole explanation is wrong. If the AI is built from the ground up using X image with X input and then told to fill in the Blanks it's self after numerous input is it human bias? 

If I give the AI 1000/10,000 images to look at and it's labeled correctly with patient information and then I feed it an unlabeled images and say ""tell me what you see"" is it actually human bias?",4
post50hb,richly branching,1.6001453961458911,highest,That's exactly the reason why. I remember reading somewhere that DNA studies for race specific genes weren't done or widely published so it doesn't create more divide,3
post50hb,richly branching,1.6001453961458911,highest,How is a lie gonna lead us to peace?,4
post50hb,richly branching,1.6001453961458911,highest,"I read the actual study, and the reason it's worrying is that since it's a neural network, we just don't know what's causing it and so we can't account for the bias. They suggested one possible reason could be differences in medical imaging equipment between races.",4
post50hb,richly branching,1.6001453961458911,highest,[removed],3
post50hb,richly branching,1.6001453961458911,highest,"But I mean if it's an learning AI and I give it 1000/10,000/100,000 x-rays filled with patient info such as age/gender/race and then I give it an unlabeled x-ray and ask it what it sees, and it tells me it's a philipino male in his 20s. Then it's doing what it's designed for. 

This is either a design flaw or they are just worried it's accurate.",4
post50hb,richly branching,1.6001453961458911,highest,"A lot of race-based studies are coming from ethnocentric biases, which have made a lot of race studies invalid. Ethnic differences are developed within periods of isolation from other populations. Two African population can be significantly different from each other, and yet we consider them the same group in the U.S. just based on skin tone. In the grand scheme of human genetics in the world, it hold little water. 

edit: words",3
post50hb,richly branching,1.6001453961458911,highest,"> unless they are worried that racists will use this as a bases for their beliefs/arguments

yeah, that's probably it.",3
post50hb,richly branching,1.6001453961458911,highest,"I’m honestly not clear on it either. Article makes it sound like the racial bias found in humans might be replicated in algorithms because they are apparently so good at finding racial differences in medical images that no one is quite sure how they even do it. That seems like nonsense tho. There aren’t that many legitimate or major medical biases between races, so when it comes to knowing the race to aiding diagnoses, it’s probably not skewing the results much. It’s a machine, not a nazi. Plus, any of these algorithms in use are probably going to either be given that information outright since it can be medically relevant, or made to ignore it completely.",3
post50hb,richly branching,1.6001453961458911,highest,"The problem is with the ""made to ignore it completely"" part of your comment.  If you determine that your training data has harmful racial biases built in, you might say ""Ok, just don't tell the AI what the race is.  Done, bias removed.""  But if it ends up determining the race via a back channel you don't know about, then the biases are still there even though you think you removed them.",4
post50hb,richly branching,1.6001453961458911,highest,Why are some people so afraid of the obvious? Historically isolated groups develop common pheno/genotypical traits. Humans did not stop evolving. Any isolated group continues to diverge from other populations every generation.,3
post50hb,richly branching,1.6001453961458911,highest,"Partially that, and partially out of fear that such observations will result in their funding (perhaps even careers) being canceled.",3
post50hb,richly branching,1.6001453961458911,highest,"I've already heard red pill fuckwits say this about visual systems that have trouble seeing people of color. ""well durr because the AI knows they aren't really people"".

Fucking morons",3
post50hb,richly branching,1.6001453961458911,highest,"Seeing how Nazis used eugenics and the amount of Nazis we currently have coming out of the woodwork, I’d be concerned too. Very.",3
post50hb,richly branching,1.6001453961458911,highest,"They're worried that this sort of technology will be used to racially profile people in the future. 

This sounds funny (""I don't need a machine to tell me that Malcolm X is black"") until you realize there were enslaved people in America who could escape into freedom because they ""passed as white"", and that the holocaust would have been so much more efficient *yet* if the Nazis had had a machine that automatically scans someone for their race and, just as automatically, takes measures depending on the desirability of the detected race.",3
post50hb,richly branching,1.6001453961458911,highest,"I read the actual study, and the reason it's worrying is that since it's a neural network, we just don't know what's causing it and so we can't account for the bias. They suggested one possible reason could be differences in medical imaging equipment between races.",3
post50hb,richly branching,1.6001453961458911,highest,Leadership under /u/spez - like navigating through a labyrinth blindfolded. Always an adventure!,3
post50hb,richly branching,1.6001453961458911,highest,"The scientists aren't saying, ""oh no, the machine can see race, that's bad."" They're saying, ""maybe the machine seeing race is part of how it's underperforming for black people.""

They're not implying the solution is to make the machine unable to see race. They're saying they need to figure out how race plays into what the machine sees, and hopefully use that to improve the machine before rolling it out.",3
post50hb,richly branching,1.6001453961458911,highest,"its not generally those with african ancestry, north and south africa actually doesnt have sickle cell anemia more commonly than europe does. its places with mosquitoes/malaria that have sickle cell, because the drawbacks of the anemia is outweighed by the benefits of it causing a resistance to malaria. west, central, and east africa, india,  southeast asia, all those places have higher genetic occurrence of the genes for sickle cell. america just happened to get its slaves from a part of the world malaria was common in so the descendants of those enslaved people still have those once-beneficial genes in their gene pool (its especially difficult to avoid passing it through a family because its recessive)

the reason scientists would be concerned is because ""race detection"" in ai occurs through biases of different kinds. sometimes its intentional, sometimes it isnt, but it still needs to be recognized and investigated and not simply brushed off as something that could be neutral or beneficial. its an unexpected result so it needs to be looked into",3
post50hb,richly branching,1.6001453961458911,highest,"No, you don't understand, the scientists, they're worried!",2
post50hb,richly branching,1.6001453961458911,highest,"I'm not.

Problem solved.",3
post50hb,richly branching,1.6001453961458911,highest,"I read the actual study, and the reason it's worrying is that since it's a neural network, we just don't know what's causing it and so we can't account for the bias. They suggested one possible reason could be differences in medical imaging equipment between races.",3
post50hb,richly branching,1.6001453961458911,highest,Operation Paperclip sheeple. Check mate nerds.,3
post50hb,richly branching,1.6001453961458911,highest,"Trust the experts!! You don't have the correct qualifications, don't speak!",3
post50hb,richly branching,1.6001453961458911,highest,Because they about to be exposed for creating racist Skynet lmao.,3
post50hb,richly branching,1.6001453961458911,highest,"They’re only worried because IF there can be a differentiation made for race, they are at risk of losing the grants to research and develop these datasets.",3
post50hb,richly branching,1.6001453961458911,highest,"In some cases, it maybe. In most cases, it causes problems.

""Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons""",2
post50hb,richly branching,1.6001453961458911,highest,"Why? When an AI camera cant register their skin tone, I understand the problem. But why should being able to discern african bone structute mean the AI misses an ilness? That would have to be programmed or result from a lack of specific health information for minorities in whatever database its using.",3
post50hb,richly branching,1.6001453961458911,highest,"> That would have to be programmed or result from a lack of specific health information for minorities in whatever database its using.

Yes. That's the issue. An AI is only as good as the data that you are feeding it. If the dataset you train it on is a bunch of disease diagnoses, and doctors are less likely to correctly identify the disease for black people (due to complex socioeconomics, such as black people on average being poorer and thus can afford less second opinions etc), then the AI will learn that it should misdiagnose black people.

Which yknow, is a problem. It's a known problem that plagues loads of AI research. Datasets are biased so the AI learns to be biased as well.",4
post50hb,richly branching,1.6001453961458911,highest,"You got it ! The results are a sign that there may be racial bias in the training set. 

A simple example cause could be that minorities have lower quality diagnoses so weren’t detected — therefore the training set didn’t ‘punish’ the algorithm for making  false negatives in minority data, because the underlying label was wrong to begin with. 

Of course, the true cause is likely to be complex and requires serious research. The result discussed  in the study is essentially the warning siren.",4
post50hb,richly branching,1.6001453961458911,highest,"For some reason, all replies are wrong, since they didn't answer your question but your intuition. Bone structure is just a possible reason why non-X ethnicity has better result than X ethnicity with given AI. You can make certain statistical inference s which can be pretty complex given the situation, which I can't explain because it's too complex even for an expert on the relationship with bias as in data bias (misdiagnosed data) or AI class bias (different structures where imbalance is not solved).",4
post50hb,richly branching,1.6001453961458911,highest,Hence the concern and commitment to figuring it out.,4
post50hb,richly branching,1.6001453961458911,highest,[deleted],2
post50hb,richly branching,1.6001453961458911,highest,"In common American parlance, the terms race and ethnic group would be interchangeable. You might expect more from a ""science"" article, but here we are.",3
post50hb,richly branching,1.6001453961458911,highest,"The use of 'race' as a synonym for something as notionally loose as 'ethnic group' has a long history in English.

> The contemporary word race itself is modern, historically it was used in the sense of ""nation, ethnic group"" during the 16th to 19th centuries.\[1\]\[2\] Race acquired its modern meaning in the field of physical anthropology through scientific racism starting in the 19th century. With the rise of modern genetics, the concept of distinct human races in a biological sense has become obsolete. In 2019, the American Association of Biological Anthropologists stated: ""The belief in 'races' as natural aspects of human biology, and the structures of inequality (racism) that emerge from such beliefs, are among the most damaging elements in the human experience both today and in the past.""

[https://en.wikipedia.org/wiki/Historical\_race\_concepts#:\~:text=The%20contemporary%20word%20race%20itself,starting%20in%20the%2019th%20century](https://en.wikipedia.org/wiki/Historical_race_concepts#:~:text=The%20contemporary%20word%20race%20itself,starting%20in%20the%2019th%20century).",3
post50hb,richly branching,1.6001453961458911,highest,"Well one of the most supportive nationalities when it came to massive racism was the uk during colonial times, which might explain the whole phenomenon. 
Also in germany its not directly implied to be a neo nazi just because you are racist. Those wirds are not synonyms! I once encountered a pretty left social justice warrior with definitive racial biases against white people. That makes her a racist but not a neo nazi. Racism has a lot of faces, not just the „black people are bad“ face.",3
post50hb,richly branching,1.6001453961458911,highest,"> We believe there is only one current human race and everything else is divided in ethnical groups

Which isn't surprising because race denial only became popular in the west after the nazis.

> when we call some Racist (Rassist) we mean that they are an actual (neo)nazi because they believe in the backwards theory of having different human races nowdays.

Which only goes to show how ridiculous the standards are nowadays. They're a nazi because they believe that a particular category is useful? 

> Blacks are not a different race than whites. Always wondered why y'all are dividing people in races.

We can match someone's DNA to their self-identified race with 99% accuracy. This would not be possible if there were not genetic differences between races.",3
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,"As an American I completely agree with you.  To the best of my knowledge, [this dummkopf started the trend](https://en.wikipedia.org/wiki/Johann_Friedrich_Blumenbach) and it drives me nuts to see it persist at the highest levels of discourse in this day and age.  It's no different than the fact that we still call Native Americans, ""Indians"".",3
post50hb,richly branching,1.6001453961458911,highest,"Yall, it's in the article why this is a concern:

""Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research.""

This research is a RESPONSE to earlier research that showed misdiagnosis of black people from x rays. So, they wanted to test if AI could identify race from x rays which might be causing the bias.

It turns out it can, which is a problem as it leads to under diagnosis.",2
post50hb,richly branching,1.6001453961458911,highest,"I expect debiasing could take care of this. For example, if I have an xray to disease classifier as a network, I could put a second head on it that tries to classify the race, and backpropagate the gradient of the second head *negatively* into the main model. The second head would do its best to learn the race and if possible, it would try to sabotage the learned features to not include that information anymore.",3
post50hb,richly branching,1.6001453961458911,highest,"If the input of the racial bias is bad, then this is bad. We have a lot of dated models catered to specific races when they really shouldn’t be.",2
post50hb,richly branching,1.6001453961458911,highest,"There are privacy issues at play here.

Seeing how race information is personal data (at least in the US and the EU your mileage may vary) the usage of that data is subject to special scrutiny. 

The individual whose information would be uploaded to the AI would have to be made aware of what the data would be used for, give consent for the usage of that data and I believe in the EU (according to [GDPR Chapter 3 Article 17](https://gdpr-info.eu/)) the ability to have that data removed.

The US is more difficult because there is no one centralized code for dealing with personal data. I know California has a special law called the [California Consumer Privacy Act](https://oag.ca.gov/privacy/ccpa) that does permit the consumer to ask for their personal data to be deleted but as far as I am aware this isn't always the case nationwide.",2
post50hb,richly branching,1.6001453961458911,highest,We live in a world where even using race for the benefit of the race is racist,2
post50hb,richly branching,1.6001453961458911,highest,Have you read the article?,3
post50hb,richly branching,1.6001453961458911,highest,">AI can predict people's race from X-Ray images, and scientists are concerned

Yes, I found this part quite comical:

""The study adds to a growing body of data that AI systems can often replicate human biases and prejudices, whether they be racist, sexist, or otherwise. Skewed training data can lead to skewed findings, rendering them useless.""

AI being able to detect things like race and gender just from scanning bones should be seen as a medical breakthrough.",4
post50hb,richly branching,1.6001453961458911,highest,Of course they didn't. that would require effort.,4
post50hb,richly branching,1.6001453961458911,highest,Yup I know of one and it’s called sickle cell disease. shits fucked.,2
post50hb,richly branching,1.6001453961458911,highest,"“We’re all the same on the inside” Well, AI says no.",2
post50hb,richly branching,1.6001453961458911,highest,"It is kind of scary because we have the technology for autonomous killing drones that learn and kill on their own and those have already been used in the field in summer of last year in Libya.

Now we can create genocidal autonomous drones.",2
post50hb,richly branching,1.6001453961458911,highest,"Yes, though in that case you would in fact be trying to spot disease.",2
post50hb,richly branching,1.6001453961458911,highest,Bone density,2
post50hb,richly branching,1.6001453961458911,highest,Are you looking for a specific disease that can be diagnosed via X-ray?  Maybe this will help.  Bias will never help because that means you've already decided your answer and now you're just looking for support.,2
post50hb,richly branching,1.6001453961458911,highest,"Sure, right up until humans enter the equation.",2
post50hb,richly branching,1.6001453961458911,highest,Theres absolutely nothing of any metric of Ai ive heard or seen that isnt going to be racist.,2
post50hb,richly branching,1.6001453961458911,highest,"I was thinking the same thing.

I think the problem may stem from worry not over the AI's racial bias, but worry over the human programmers unwittingly inviting their own racial biases.",2
post50hb,richly branching,1.6001453961458911,highest,"I agree there are genetic concerns that affect certain people different ways. Like it or not gender and ethnicity play a role in medical diagnosis. Off the top of my head I know black people are more prone to sickle cell, so wouldnt that be relevant in an AIs processing? Idk why people think that these differences are a bad thing. They just are. Socially were all just people but as far as medically these different characteristics are kind of important when it comes to different diseases and ailments. Seems silly to be ""politically correct"" when trying to make someone well.",2
post50hb,richly branching,1.6001453961458911,highest,"I always find it meaningless when articles use broad statements like “and scientists are concerned”. For all we know it’s probably a very small sample of “scientists” in a specific domain that are even remotely concerned. Otherwise, majority of people would be impressed with this. A feature like this would not be a bias, but potentially very beneficial to future predictive models.",2
post50hb,richly branching,1.6001453961458911,highest,There are tons of medical things that are race related.  Medical people understand it.  Trying to explain it to the public or vendors is hard sometimes.,2
post50hb,richly branching,1.6001453961458911,highest,"Hey race is social construction every one is the exact same no matter what, do not fight the narrative.",2
post50hb,richly branching,1.6001453961458911,highest,Like any tech it can be good or bad. In medicine? Good. In the hands of a totalitarian government that has scapegoated a minority ethnicity and uses this tech to identify them and put them in camps? Bad.,2
post50hb,richly branching,1.6001453961458911,highest,"Literally so dumb... surprise there are different types of people in the world , and no its not bad to be different... I'm confused as to why they find it concerning when the benefits especially in the medical field are big. For those who don't know there are diseases that tend to affect specific races more than others. It is just a fact but understanding this helps us understand and treat those diseases more effectively. 

The article feels like it is pushing this oh my God the ai can tell your race from an xray this can lead to ai racism narrative. I mean really it sounds like they would prefer that the AI treats everyone without the benefit of knowing what could potentially relevant information to a patients condition.

Edit the concerning thing from the article should be figuring out why people of color are misdiagnosed more often. This is what the article should be and I doubt it's that the machine is racist but it needs to be addressed so it can be corrected and improved.",2
post50hb,richly branching,1.6001453961458911,highest,"Typical liberal response, no article is safe from the all racist eye.",2
post50hb,richly branching,1.6001453961458911,highest,"The diseases are more regional, which is a subtype of race. But to assume all Africans have X, when it’s more sub-Saharan Africans, or all Europeans are more likely to have Y, which it’s mostly Southern Europeans, is the issue. 

These types of patterns are useful starting points to know what to test for, but that’s kind of where their usefulness ends. 

Either way, I don’t get why we should be worried about AI seeing these differences. Our eyes already see the differences.",2
post50hb,richly branching,1.6001453961458911,highest,I think part of it was they can't explain how it knows.   So they're cautious about the AI making decisions that could have racial implications without them knowing how.,2
post50hb,richly branching,1.6001453961458911,highest,Actually there are literally no differences between races you absolute troglodyte. Go back to the Donald.,2
post50hb,richly branching,1.6001453961458911,highest,">*""Actually there are literally no differences between races you absolute troglodyte.""*

So, you pretty much called me a caveman? Ahh, the real racists are coming out of the woodwork.

Anyways...

* [5 Diseases More Common in Minorities](https://abcnews.go.com/Health/diseases-common-minorities/story?id=14722258)
* [Heart Disease and African Americans](https://minorityhealth.hhs.gov/omh/browse.aspx?lvl=4&lvlid=19)
* [Why 7 Deadly Diseases Strike Blacks Most](https://www.webmd.com/hypertension-high-blood-pressure/featur)

&#x200B;

>*""Go back to the Donald.""*

Hmm?! What made you automatically assume I was a Trumper?

Could it be because I am white? Why, yes! Yes, it certainly is!

BTW & FYI, I'm no Trumper.

Get bent.",3
post50hb,richly branching,1.6001453961458911,highest,"Actually it’s impossible to be racist against white people because racism requires both power and prejudice. 

Also reddit content policy only prohibits hateful activity such as racism from majority groups towards minority groups, not the other way round.",4
post50hb,richly branching,1.6001453961458911,highest,"People who write these articles are idiots drooling from the mouth and don't care about advances in science, only advances in woke politics. Imagine this AI can also detect genders if it can pick up race. Cant have an AI that isn't woke enough to lie to people to feed their delusions.",2
post50hb,richly branching,1.6001453961458911,highest,"The actual science is always against dumbarse bigots like you.

You don't even know what the words, race, gender, or bias mean.

All you can do is keep telling yourself your ignorance is wisdom, and go on about imaginary boogeymen. It's pathetic.",3
post50hb,richly branching,1.6001453961458911,highest,Bla bla bla feelings over facts.,4
post50hb,richly branching,1.6001453961458911,highest,"I think the issue is they’re concerned it’s missing disease that’s more common in those races because it’s misclassifying the signs of those diseases as markers of that race in images that otherwise are indicating the particularly prone races.

If they want the AI to help avoid bias in diagnostics, it doesn’t help if the person isn’t diagnosing because the patient is Black AND the computer isn’t diagnosing because the patient is Black, even if their reasons for not diagnosing because the patient is Black are a little different.",2
post50hb,richly branching,1.6001453961458911,highest,"I’m gonna beg to differ on this one. Talked a great deal about race in my philosophy of medicine class last semester. Race itself doesn’t have any play on health. It’s just used as a catch-all for other things that do. 

Oftentimes, it seems that a certain disease is more prevalent among a certain race, but that’s actually more likely attributed to residential/work conditions or socioeconomic status. For example, a Black person who has a solidly upper-middle class family history and works a desk job will have a super different health profile than a Black person who has a family history of generational poverty, low access to nutritional food, and works a factory job. They’re both “black” but in this case, that means absolutely nothing.

The other issue with race in medicine is that it’s literally a socially constructed and self identified category. Besides the fact that you cannot determine someone’s race from their genome, we’d call an African American whose family has been in America for centuries “Black ” We’d also call someone who currently lives in Africa “Black.” The diseases that are popularly considered to occur more in specific races (like sickle cell) will have greatly different prevalence in both of these populations. Race as a category is still flawed. 

Of course, we still use it in medicine, because race is sometimes a mediocre proxy for a lot of factors, but there’s a case to be made for its use and it’s disuse. 

tl;dr: race itself as a medical category tells us little to nothing. Racial bias in ai is not at all helpful for clinical practice and is more likely to be problematic.",2
post50hb,richly branching,1.6001453961458911,highest,">I mean aren't there diseases that occur more in specific races than in others?

Some diseases are more common in some ethnicities, but ethnicity is not race. 

The more detailed answer is that ""race"" is a socially constructed category. Who is part of what ""race"", and what ""races"" exist in the first place are totally arbitrary and generally defined by cosmetics and politics. So, in a way you *could* say that some diseases ""occur more in specific races"", but since the ""specific races"" are arbitrarily defined in the first place, you can use data to make any disease more common in any ""race"".",2
post50hb,richly branching,1.6001453961458911,highest,The fact that you can break the category of race down further doesn't mean that certain diseases are not more common in certain races.,3
post50hb,richly branching,1.6001453961458911,highest,"It isn't about how much you can break it down. 

Diseases aren't really ""more common in certain races"" because race is a socially constructed grouping in the first place.",4
post50hb,richly branching,1.6001453961458911,highest,"Well, we're the same race/species... there are certain diseases that occur more often in certain groups of people, but there's almost a reason for it (genetic history of a region, from which a specific group of people might happen to be from, or issues regarding geographic locations), and as far as I'm aware from my medical career, it never has to do specifically with what we call 'race.'

So racial bias in AI really wouldn't ever be helpful in a medical setting. At least least, there's no reason that I can think of. Working as both an EMT and an ER, I've never asked anyone about their race and I've never seen a doctor do so either. It's family/genetic history, visits to geographic locations, etc...

When you hear about stuff like ""African Americans are more prone to heart attacks,"" that's pretty much always due to a multitude of things like the average diet and living condition of African Americans, not just because African Americans are more prone to heart attacks as a race.

There is some research that suggests some genetic variations have allowed certain groups to be afflicted with a particular condition or disease more often, or resist a particular condition/disease better (malaria and sickle cell anemia being a key example of both), but it's again mostly because of genetic variance rather the differences between actual species, which tend to be far more stark.",2
post50hb,richly branching,1.6001453961458911,highest,"For AI sure, for the human species who pick on each other for looking different or being born in the wrong country or color not so much. In a medical vacuum it's great. We don't live in a vacuum unfortunately.",2
post50hb,richly branching,1.6001453961458911,highest,"That’s the issue.  If the AI starts having racial bias, then it won’t look for every possible ailment leading to misdiagnosis",2
post50hb,richly branching,1.6001453961458911,highest,"Maybe, but there's a million easier ways to determine if they're in that high risk racial group than working backwards from an x-ray (looking at them, asking family medical history...)",2
post50hb,richly branching,1.6001453961458911,highest,An AI is only as smart as 1) it programmers and 2) the sample set it learns from. If it doesn’t have the complete context then inappropriate bias is always possible.,2
post50hb,richly branching,1.6001453961458911,highest,"My best friend is a doctor, and he confirms that he absolutely considers race and origine with patients. It's like when I told him I was thalassemiac, he just looked at me like ""you didn't know? "" he told me he was sure I was simple because of the region I was from",2
post50hb,richly branching,1.6001453961458911,highest,"The problem comes in training/validation data for medical AI. The FDA has very loose regulations here, and there is a lot of bias - race, age, sex, and other patient characteristics but also modality (scanner) specific settings IE voltage, backplate type, slice thickness, etc. 

There is no easy way for an AI vendor to prove that they have developed software that runs accurately for all possible biases. There's no regulations that say your AI must have been tested on a diverse population. In some cases (mostly mammography) there are strong controls related to devices, IE your AI must be validated for each make/model of modality you are going to be using the AI with. 

Prior to this study, there was no evidence that you race was visible in radiology imaging. Therefor, there was no evidence that datasets for AI training needed to be racially diverse. Thus no AI vendors reported statistics related to race and whether their AI worked for all races.

You do see in research publications reported diversity statistics (specifically - race/age/sex and sometimes other features like BMI). It is not common to see performance by bias in many publications.

&#x200B;

To answer the other half of your question, the software that radiologists use to review images (PACS) does not include race. They could get this from the EMR. Radiologists often do not use race in their diagnostics unless it is specifically noted in the order that they are looking for a race specific disease. There aren't very many that are visible on medical imaging.",2
post50hb,richly branching,1.6001453961458911,highest,As long as the AI isn’t profit or insurance driven I don’t see a problem.,2
post50hb,richly branching,1.6001453961458911,highest,"> *aren't there diseases that occur more in specific races than in others?*

Reddit has taught us that the races are the *exact* same and there are absolutely, positively no differences between the races, and that anyone making a distinction between races is automagically RaCiSt for some reason.",2
post50hb,richly branching,1.6001453961458911,highest,"No. Race is a social construct. Culture, yes.  https://www.americananthro.org/ConnectWithAAA/Content.aspx?ItemNumber=2583",2
post50hb,richly branching,1.6001453961458911,highest,Basically it's like most/all technology in the wrong hands it could be bad but I agree with you it could be positively helpful,2
post50hb,richly branching,1.6001453961458911,highest,"Also, computers only do what you tell them. As if they're somehow going to make decisions because they prefer one race to another is just a silly idea.",2
post50hb,richly branching,1.6001453961458911,highest,The concern would be programmed racism,2
post50hb,richly branching,1.6001453961458911,highest,"Think of it like this. 

If the Nazis had access to this technology, how much worse would the Holocaust have been? Ethnic Genocide is, unfortunately, very much still alive today, and this technology leads no room for minorities to hide anymore.",2
post50hb,richly branching,1.6001453961458911,highest,We don't want an AI to bring back Phrenology.,2
post50hb,richly branching,1.6001453961458911,highest,Sickle Cell Anemia is systemic racism.,2
post50hb,richly branching,1.6001453961458911,highest,"Presumably the AI has the racial biases of its creators already. This is a major concern with a lot of AI considering the demographics who generally create this software are not representative of the population at large.

There is something that the AI is seeing, that it’s creators influenced it to see, but it’s creators cannot identify directly. It’s pretty interesting and it’s a great example of how AI can potentially be used for discriminatory practices.

Of course, there is good that can be had as well — as you suggest.",2
post50hb,richly branching,1.6001453961458911,highest,"It would be a good idea to make healthcare more personal. Race isn't the only major factor


But as long as people can be distinguished by race there will always be people that will try to manipulate this to push certain narratives, maybe use the data as an excuse to segregate people. It goes against the idea of universal healthcare, but a once size fits all solution have never worked for everyone. There are lots of ways this system could be misused I'm sure there are many more I haven't thought of


Society just isn't sensible enough to have this kind of intimate healthcare unfortunately",2
post50hb,richly branching,1.6001453961458911,highest,"The bias isn't terrible, but you can train ai to respond in certain ways to that bias. Maybe in this scenario it isn't awful, but in other situations human bias in AI systems would be terrible.",2
post50hb,richly branching,1.6001453961458911,highest,"The problem with AI right now is it is only as good as it's input. It learns from a set of data provided by humans which probably have biases so will return biased results.

You need really fucking self aware humans to get AI right.",2
post50hb,richly branching,1.6001453961458911,highest,Racial bias is in fact helpful in most situations regardless of the opinions of redditors,2
post50hb,richly branching,1.6001453961458911,highest,"Yes, this is good. We live in a world where pearl clutching idiots want to pretend that every is the same regardless of race, gender, etc... and we're not. There are real, physical differences that effect our health,  lifespan,  and honestly the things we're good at.  For example,  with my heritage I'm more prone to skin cancer and eye problems.  My niece is less prone to skin cancer but she's at higher risk of cycle cell anemia,  and as she is my niece is at risk of female reproductive cancer where I am at risk for the male equivalent.",2
post50hb,richly branching,1.6001453961458911,highest,Because a lot of racial bias has been negative it's also really weird that an ai can make your face with X rays of your head this whole process isn't really good imo,2
post50hb,richly branching,1.6001453961458911,highest,"It's a concern otherwise why would it get headlines.  ""racial identification system created but is completely unecessary"" doesn't quite get your attention for the day does it",2
post50hb,richly branching,1.6001453961458911,highest,"I think concern is well placed. It's not impossible to mitigate or even utilize these things but it can also be concerning if not treated carefully.

Machine learning amplifies an existing bias. That is, features from underrepresented groups can be ignored or misclassified. If a model can detect race, it can use it as a feature. But if that feature has low representation, your model might be less accurate. You can definitely model them better with better sampling to make sure your data is diverse.

I believe the point is *we didn't know* that x-rays can be predictors for race, so perhaps not all the samples were properly stratified to ensure a model that works with everyone. Now that the computer figured it out and can introduce ""race"" as a feature. We probably should go back and check out our samples to see if we have enough data. We probably should check this with other publications too.

Then we need to think about what other features other than race do play a role and became hidden features of the model. Do we have a good samples from other groups for each of these things? What's the impact of this bias? Does it mean that a newer and fancier ML works worse for minorities than an existing methodology? Do we recommend blacks to do an older test? Is that ethical? Or perhaps it actually increases detection for everyone but not to the same level of accuracy. How about now?


None of this new. But when your algorithm starts predicting things you thought would be random, your assumption is now invalid. You might need to go back and check your methodology and resample or update your methodology.",2
post50hb,richly branching,1.6001453961458911,highest,The misuse of this type of technology is the problem.,2
post50hb,richly branching,1.6001453961458911,highest,"Given the conflation of race (a caste construct) and ethnicity (actual genetic/phenotypic ancestry), the problem with having AI racially identifying people is that it perpetuates the myth of racially and socially distinct differences (phrenology) with actual genetic ones.

I.e. there are biological differences between people, but there aren't socially constructed ones.",2
post50hb,richly branching,1.6001453961458911,highest,"Is it not obvious that there's a climate of outright race ""denialism"" associated with sociopolitical ongoings?",2
post50hb,richly branching,1.6001453961458911,highest,"It can cause ethical issues.  Sometimes we say we know the data shows this, but we're going to ignore that because it may reinforce undesirable outcomes(or steps to outcomes).  It's like actuarial tables for insurance like health insurance or car insurance.  The data shows 'x', but if we let the math drive us to the outcome(which is what AI *does* effectively) we would normally do when we see that data, we may lock out person group 'y' from ever qualifying, creating a disparity.

And that's not getting into the more nefarious things you may do with data like that which may result in similar outcomes described above(such as the x-ray basically becomes a pre-existing condition to deliberately block people from healthcare, for instance, and it's ""justified"" by saying it was a color blind process)",2
post50hb,richly branching,1.6001453961458911,highest,"From what I understand from reading the article, the AI scans were more likely to miss signs of illness when it identifies people as being black.

I think the worry is that they don't understand what the AI is picking up from the scans to so quickly determine race. If AI is just replicating human thinking but faster, it's worrying if it's replicating biases in a way that we don't understand",2
post50hb,richly branching,1.6001453961458911,highest,"The problem they are worried about is the fact that the AI is missing things in non white patients.  While this is a worry, its not because the AI is racist.  Its just because the AI was not coded correctly for other races.  This is 100% human error trying for a one size fits all approach.",2
post50hb,richly branching,1.6001453961458911,highest,"Unless you teach the program to call people the n word, it shouldn't be a problem.",2
post50hb,richly branching,1.6001453961458911,highest,"Yeah, this is a very strange article. Isn't the whole point of AI that it's supposed to perform tasks better than a human can? But when AI has identified something that doctors can't, it's not just that it's better, it just be using some unknown science. And as long as the AI is not inherently racist (which would require being programmed as such) what is the problem with race being a factor in it making diagnoses? It's not racist to think that genetic differences can lead to susceptibility to different diseases, maybe AI can make some new associations here that human doctors are overlooking for fear of racial bias.",2
post50hb,richly branching,1.6001453961458911,highest,*White scientists concerned that their data proves they are racist,2
post50hb,richly branching,1.6001453961458911,highest,"Not necessarily, because Black people are more likely to be misdiagnosed than white people **not** for biological reasons, but socioeconomic reasons (e.g., differences in access to healthcare, quality of healthcare, the lack of available medical data on Black people compared to available data on white people, etc…)^1

1.	 [Why the Color of Your Skin Can Affect the Quality of Your Diagnosis](https://www.improvediagnosis.org/dxiq-column/why-the-color-of-your-skin-can-affect-the-quality-of-your-diagnosis/)",2
post50hb,richly branching,1.6001453961458911,highest,Would you need the X-ray to be able to tell them they’re at risk?,2
post50hb,richly branching,1.6001453961458911,highest,"“Race” is a “racial bias” because it is a social construct.

It may be helpful if the AI is actually detecting where someone grew up. Think like how they can trace elements in teeth.",2
post50hb,richly branching,1.6001453961458911,highest,I think it's because it gives merit to frenological ideology by showing that there is a detectable different in the bone shapes of races.,2
post50hb,richly branching,1.6001453961458911,highest,"> Wouldnt racial bias in this kind of AI be helpful?  I mean aren't there diseases that occur more in specific races than in others?

Depends how & why it's inferring race (which I think is still rather unknown).

In discussions of this study on one of the ML subreddits, comments observed that there are many things that correlate with race that this system may have been picking up on.   For example, if the researchers sourced their X-rays from many different hospitals, such a model may have merely noticed:

* ""image looks like it was produced by a certain model of x-ray-machine"" 
* which may correlate to ""well funded hospital""
* which correlates strongly to ""rich zip code""
* which unfortunately correlates well to ""race""",2
post50hb,richly branching,1.6001453961458911,highest,"Scientists are concerned because to study or even mention biological difference between races or sexes draws attacks from a lot of far left ideologues and social constructionists. 

This point was made by Sam Harris when talking about Charles Murray and his book, the bell curve. 

Differences between groups are inevitable and scientists are going to stumble upon them from time to time and our current politics make it very difficult to talk about these phenomena.",2
post50hb,richly branching,1.6001453961458911,highest,"As someone who works with AI, there are two major concerns.

First, there are examples where discrimination (in the literal sense) of various socio-ethnic groups has had anti-egalitarian effects (e.g., when Speech Recognition interfaces have markedly higher misrecognition rates for speakers of African American English).

Second, there's the concern that there would be political backlash against even such scientifically valid observations as ""there are ethnic/racial/gender differences in rates of <phenomenon>,"" at least partially because reporting occasionally imputes value judgements on such objective observations  (e.g., the assumption that an observation that certain communities have higher incidences of violent crime as *inherent* to those communities, rather than the circumstances those communities fall into).   The result of this is things like the fact that it is taboo to point out that that [(predominantly white) European-descended populations have something of a resistance to HIV, apparently the result of the Black Plague.](https://pubmed.ncbi.nlm.nih.gov/16880184/)  

Thus, there's worry about such (legitimate, literal) discrimination might result in funding, or indeed entire careers, could be canceled.",2
post50hb,richly branching,1.6001453961458911,highest,"they are concerned about biases from the current medical establishment being transferred over to the AI. Those biases include poor treatment, marginalization of patients of color, worse pain management, non or late diagnosis, dismissive or rude attitudes, increased infant mortality in OB/GYN when minorities are treated by white doctors. The list goes on.",2
post50hb,richly branching,1.6001453961458911,highest,Any health related forms I’ve ever filled out ask for your race. I don’t see the problem. The more information the better when it comes to healthcare.,2
post50hb,richly branching,1.6001453961458911,highest,Its only concerning if no one asked the AI to guess races.,2
post50hb,richly branching,1.6001453961458911,highest,"I work in data science in healthcare and there alot of problems with uncaptured socioeconomic variables in health outcomes. When a purality of positive outcomes are middleclass white men, and a purality of negative outcomes are poor BIPOC, a model that isn't carefully implemented can have outputs that look like white = healthy, not white = sick. One of the issues we deal with is that non-medical information isnt always gathered for patients, so we can't control for them.",2
post50hb,richly branching,1.6001453961458911,highest,I think it would be helpful when dealing with unidentified remains as well? Be it murder cases or archaeological dig sites,2
post50hb,richly branching,1.6001453961458911,highest,Yeah but authoritarians who want to work backwards from social conclusions don’t want people or apparently AI to know that.,2
post50hb,richly branching,1.6001453961458911,highest,Sickle cell anemia is a good example of that.,2
post50hb,richly branching,1.6001453961458911,highest,"My dad studied Anthropology under Bill Bass himself, the GOAT of forensic anthropology, humans can deduce race, sex, and age from bones and have been able to for quite some time.",1
post50hb,richly branching,1.6001453961458911,highest,"When I watched Bones, Bones would do that with bones.",2
post50hb,richly branching,1.6001453961458911,highest,"it's kind of obvious though, there's clearly differences in skeletal proportions between what you could classify as classic differences in ethnicity

I am very homogenous ethnically, my wife is mixed between two slightly less homogenous and different ethnic lines - our proportions are very different and we like to laugh about this all the time",2
post50hb,richly branching,1.6001453961458911,highest,"Yea - the ""concern"" is from scientific illiterate whom know nothing about the topic. People just think that if AI can tell us apart, they will ""judge"" us, or give bad advice to certain racial groups.",3
post50hb,richly branching,1.6001453961458911,highest,"The concern is also on the scientific literate to be patient, find solutions that work for the illiterate, do better at marketing and influencing without demanding understanding on the basis of hierarchy or superiority - it's a tough road ahead, but it's all doable",4
post50hb,richly branching,1.6001453961458911,highest,My wife and I are both white AF and our proportions are way outta whack.  We also like to laugh about it…but playing devils advocate here it might not have as much to do with your ethnicities as you think.,3
post50hb,richly branching,1.6001453961458911,highest,"it might not for her, given her two specific ethnic lines are likely not as homogenous as mine, but for me, I think it does because many of my fellow 'people' seem to average towards specific proportions

also, white isn't really an ethnicity, it's more like an American concept of race - there's ethnically like mediteranean euro, east euro, northern euro, western euro, various hispanic whites, anglo saxons, etc. I'm guessing you're an American",4
post50hb,richly branching,1.6001453961458911,highest,">devils advocate

Err, what exactly is negative or controversial about the counter position (there **are** significant differences in proportions even within a single ethnicity) ?

Nothing wrong at all about embracing the fact that there's diversity within a race and not just between them.",4
post50hb,richly branching,1.6001453961458911,highest,"It is obvious, but people don't like talking about it because of... the implication.

It's really absurd actually.",3
post50hb,richly branching,1.6001453961458911,highest,I would be scared to say this in my uni classes because I'm 90% sure this would make people think u were Calvin Candie or some racist 1800s biologist. Social sciences tend to reject that race is at all biologically determinable.,3
post50hb,richly branching,1.6001453961458911,highest,"I mean, it's complicated, because ""race"" is a social categorization that doesn't have a clear-cut biological definition. How people  identify others, and self-identify, changes a lot based on culture and time. On the other hand, broadly speaking, race as an American social construct tries to identify someone's geographical origins, and geographical origin does often correlate with certain genetic and physiological traits. However, not every individual will fit these biological trends, and there tends to be more variation within a group than between different groups, so this sort of categorization is mostly useful when looking at large populations, not individuals....",4
post50hb,richly branching,1.6001453961458911,highest,"Nah, race is much broader and more politically implicated i.e. white people - this classification is often used to identify who is part of the majority accepted group in the United States. Everyone else is determined to be a second-class citizen - historically and still to this day, this remains the true purpose for race classification in the United States. There is no real biological basis for this as various ethnic groups have been allowed into the 'whitedom' at various points in American history",4
post50hb,richly branching,1.6001453961458911,highest,"And AI can supposedly detect sex, gender, sexual preference, disease, drug use and more just from eye tracking. I am not surprised in the least to hear about situations like the one in this article.",2
post50hb,richly branching,1.6001453961458911,highest,"The Smithsonian used to have an exhibit that was plaster casts from people all over the world, finished in white.   I heard it was interesting to see the human body adapted to different environments but at some point it offended someone and it was taken down/destroyed.",2
post50hb,richly branching,1.6001453961458911,highest,If he studied under Bill Bass then he should know that race is a social construct and that those determinations are based on environment.,2
post50hb,richly branching,1.6001453961458911,highest,[deleted],2
post50hb,richly branching,1.6001453961458911,highest,"relay race, sack race, Egg and Spoon race - All specific and well defined, but rely on training different muscle groups and favouring body types of differing bone structure to optimally perform.",3
post50hb,richly branching,1.6001453961458911,highest,"What they probanly mean is, ""had ancestors from a geographically similar place at a similar time in the history of the species.""",3
post50hb,richly branching,1.6001453961458911,highest,[deleted],3
post50hb,richly branching,1.6001453961458911,highest,And how do they deduce cultural background from bones?,4
post50hb,richly branching,1.6001453961458911,highest,"Tried this one the other day, still didn't give me the n word pass",3
post50hb,richly branching,1.6001453961458911,highest,"Yeah I'm not sure why this is a big surprise. As a physician, I'm not trained to try to detect race from an x-ray since it's not really relevant to my decision making (if I need to know the race, I can simply look at the patient / ask them / look in the chart)... but I'm sure people who studied how to identify such things can do so easily.",2
post50hb,richly branching,1.6001453961458911,highest,Came here to say this and don’t know why it’s a big deal.,2
post50hb,richly branching,1.6001453961458911,highest,"And sometimes where they most likely grew up, depending on certain tiny details.

I think it is a really cool profession.",2
post50hb,richly branching,1.6001453961458911,highest,For sure but give a dumbass an inch and suddenly we will be right back to Jackie Robinson runs faster because his tendons are thicker,2
post50hb,richly branching,1.6001453961458911,highest,They can also deduce what role a person had in society by looking at their bones. Workers had more dense bones while those higher up in that society had less dense bones.,2
post50hb,richly branching,1.6001453961458911,highest,It should be “Journalist are Alarmed that Reddit Hive Mind Accurately Identifies Clickbait”,2
post50hb,richly branching,1.6001453961458911,highest,"In some cases its not particularly hard and very obvious to anyone with basic training, especially skulls.",2
post50hb,richly branching,1.6001453961458911,highest,"Sensationalist headline. We’ve been able to tell race by bone for years

Edit: shape of the skull, shape of the nasal region, shape of the orbits, degree of protrusion of the jaw or prognathism, shape of the lower jaw, and certain features of the teeth. Is how we do it.",1
post50hb,richly branching,1.6001453961458911,highest,"It’s a really bad headline. 

In the article it actually says the very thing they were trying to do was find out if They could train an AI to identify race by skeleton - basically ‘hey mr AI, here’s some skeletons and here’s their corresponding races, got it? Okay, so what race do you think these ones are?’

Given humans already know how to assign a race to a skeleton with a high accuracy rate it was a foregone conclusion that the only way their AI would not also be able to do it would be if they programmed it wrong or if the assumptions the humans had been making were wrong.",2
post50hb,richly branching,1.6001453961458911,highest,This is an asbestos free cereal type situation.,3
post50hb,richly branching,1.6001453961458911,highest,My ear doctor friend says cartilage is different too? Something about ear canals? First I’ve ever heard of it,2
post50hb,richly branching,1.6001453961458911,highest,Mind sharing the evidence of that (article)?,2
post50hb,richly branching,1.6001453961458911,highest,[here ya go!](https://naturalhistory.si.edu/sites/default/files/media/file/wibidentifyancestryfinal.pdf),3
post50hb,richly branching,1.6001453961458911,highest,"Thanks for sharing! Doesn’t seem overtly definitive on findings or the efficacy of this (I.e. yes we can look at bones and tell your race), but rather that it’s something of an exercise in genealogy (i.e. skull patterns in related individuals). Thoughts?",4
post50hb,richly branching,1.6001453961458911,highest,"Forgive my ignorance; but how can ones race be ascertained by their bones? 

I thought that all human beings have the same bone structure?",2
post50hb,richly branching,1.6001453961458911,highest,"All people have the same basic skeletons, but the proportions can vary in different people around the globe. For example the width of your nose compared to the size of your skull, or the size/shape of your teeth, or how broad your tibia are compared to their width, that sort of thing. So from this, we can take an educated guess where someone (or their ancestors) might be from based on the proportions of their skeletons. Since the geographical origins of your ancestors broadly correlate with race, it's therefore possible to estimate race based on someone's bone structure. 


Thats the thory, but reality is a lot more messy. While it is true that on average, a person from region A will have certain differences compared to a person from region B, that does not necessarily mean that a specific individual from region A will be different from a specific individual from region B. For example, on average, the corners of the jaw are sharper in Asian people than in Europeans. But if I were to measure my co-workers, Jiayan might actually have a more rounded jaw then Helga, because Jiayan has a rounder jaw than the average for China, and Helga has a sharper one than average for Germany. So whenever we guess where someone is from based on skeletal features, it's more of a probability than an exact pinpoint. You could guess the Helga is more likely to be European than African or South American, but you couldn't say ""this one's certainly from Germany"". On top of that, you also have to consider that someone may have ancestors from very different places. Maybe the reason Jiayan has a rounder jaw than most Chinese people is because her grandfather was from India. You can see how it's very easy for this technique to label mixed-race people as whatever race best fits their proportions, even though it might not fit how they would identify themselves. The fact that race doesn't always fit geographical origins makes this even harder. 

Tldr: yes, the proportions of certain bones in the body do vary with geographic origin and can therefore be used to guess someone's race naesd on their skeleton, but its not an exact science and has important limitations.",3
post50hb,richly branching,1.6001453961458911,highest,Thank you for your wonderful and detailed response.,4
post50hb,richly branching,1.6001453961458911,highest,"There is no such thing as race, if millions of people fall outside of the categorizations.",4
post50hb,richly branching,1.6001453961458911,highest,"more than skin tone, races have changes in their biology as a whole, even in the skeleton, but of course, we can't distinguish because it might be minor differences mostly imperceptive.",1
post50hb,richly branching,1.6001453961458911,highest,"The wording is weird. They specifically used training features of X-ray images **and** specifically noted the patients' race. So they basically asked the model to discover imperceptive patterns to classify X-ray images by race, and are now concerned because the model did exactly what they asked it to do?? I mean no wonder it found patterns because they exist, only that they are as you said, too minor for humans to notice. That's exactly why deep learning is used in many fields, to find otherwise minor patterns. Weird ethical conclusion they came up with.",2
post50hb,richly branching,1.6001453961458911,highest,">only that they are as you said, too minor for humans to notice

They aren't, unless they meant with the naked eye. Forensic skeletal analysis performed by humans with relatively simple tools can be used to determine race and sex reliably enough for it to be useful in criminal investigation.

Source: I know multiple forensic anthropologists.",3
post50hb,richly branching,1.6001453961458911,highest,If I may ask: How does it come you know multiple forensic anthropologists? I guess I've never even been near one.,4
post50hb,richly branching,1.6001453961458911,highest,"This was my first thought too. The article claims its impossible, but I literally learned to do it in high school.

They offered a forensic science course as an elective, and identifying gender, age, and race from skeletal remains was something we spent a few weeks on.",4
post50hb,richly branching,1.6001453961458911,highest,"As an anthropologist, I have to point out that that only applies to American perceptions of race. I work alongside one of the leading forensic anthropologists in the country and we’ve talked about this phenomenon before. Other ethnicities like Herero or Mizrahi cannot be identified, and races beyond the western perception cannot be pinpointed either because there is so much that’s just cultural interpretation. If you want to really see how the concept of race falls apart, just look at Turkish people and try to classify them easily under an umbrella.",4
post50hb,richly branching,1.6001453961458911,highest,"How do they handle edge cases, for instance Yemenis, Egyptians, etc who don’t resemble either Europeans, West Africans, or Far East Asians?",4
post50hb,richly branching,1.6001453961458911,highest,"I agree

Source: I watched Bones",4
post50hb,richly branching,1.6001453961458911,highest,"I read the actual study, the AI can categorize images with 99% accuracy with just a scan of someone's lung, and 40% accuracy on the vague blurry version. The neural network pulled out some data that we have absolutely no idea what it's seeing there. They speculated it may be differences in medical imaging equipment between races.",4
post50hb,richly branching,1.6001453961458911,highest,"Agree. I studied physical anthropology a bit and learned in ‘bone lab’ how to identify ethnicity, gender, and age differences, evidence of certain diseases and injuries, childbirth. But race and gender are socially constructed; biologically there is almost infinite diversity. We know that ethnicity and gender must be factored into medical treatment, but I guess the danger might be that ‘lumping’ people into racial and gender categories might miss critical individual variability.",4
post50hb,richly branching,1.6001453961458911,highest,"Source: Trust me bro, my uncle works for Nintendo.",4
post50hb,richly branching,1.6001453961458911,highest,"In this case it might be done with simple tools. But there are a lot of cases in medicine, when doctors need a lot of training to be able to detect diseases and data science/deep learning/neural networks help a lot. My groupmate from university is learning on computer vision as major to detect diseases in MRI scans. Also i heard about cases of detecting diseases from lungs x-ray.",4
post50hb,richly branching,1.6001453961458911,highest,And all of the science is derived from the race biology institutes research made before it was deemed racist... I have been involved with repatriation of skeletal remains where they used old books with measurements to accurately define the race.,4
post50hb,richly branching,1.6001453961458911,highest,"I was taught forensic anthropologist and osteology under the LA Coroner. She said that her answer to the court was always in terms of approximations. A person could be Asian or Native American or Latino based on bone morphology. That's just an example that has broad overlap. It's never precise or ""reliable.""",4
post50hb,richly branching,1.6001453961458911,highest,What are the differences between the skeletal remains? Do you have an article to link?,4
post50hb,richly branching,1.6001453961458911,highest,"Idk if it's a ""weird ethical conclusion"" if the tha article states that ""artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons.""

That's pretty unambiguously a bad thing.",3
post50hb,richly branching,1.6001453961458911,highest,"Certainly. So it's either the fault of the training data (not enough, not varied enough, unbalanced, not generalized enough etc.), or some model parameters (or the model itself). That's normal  process of any DL model > train > test > evaluate > find ways to improve. It seems like they're trying to paint the model and the problem at hand as something more than it is - a simple training problem.

The entire article is literally just them saying that the model performed well but had problems concerning features with a certain attribute. Period. For some reason that's ""racist decisions?"" The model learns from what it sees. So either the training data (and, therefore, those who were responsible for its preparation) were racist in their decisions, or maybe just admit that training is a complicated process and certain features will be more difficult to learn, that training data will have to be remade a lot, and the model parameters will probably have to be tampered with, if not the model itself. Just because the AI is failing at detecting sickness in x-rays of a certain race does not automatically mean it makes racist decisions, that's a ridiculous  and completely useless conclusion. The fault lies at the creator, not at the deep learning model. Always.",4
post50hb,richly branching,1.6001453961458911,highest,"If you rank missed indicators of sickness by race, one has to be last.",4
post50hb,richly branching,1.6001453961458911,highest,"Oh! I missed that. That is, of course, unambiguously negative. I understand the concern",4
post50hb,richly branching,1.6001453961458911,highest,What race would be better to be more likely to miss?,4
post50hb,richly branching,1.6001453961458911,highest,Which... race do you think \*should\* be the one that has the most missed indicators of sickness?,4
post50hb,richly branching,1.6001453961458911,highest,Is it perhaps due to a lack of clinical trial type input? I e read that it's largely white people.that are in clinical trial programs.,4
post50hb,richly branching,1.6001453961458911,highest,"I think they were upsetti spaghetti that the model ended up being able to do it accurately, even with small sample images. The argument of this article seemed to be that it could introduce racial bias in diagnoses, but that’s stupid. Those biases can be helpful in diagnoses and should be included. Seems like a paper on AI learning that has a slightly racial fear-mongering spin put on it for the clout",3
post50hb,richly branching,1.6001453961458911,highest,And also all the machine needs now is wheels and guns.,4
post50hb,richly branching,1.6001453961458911,highest,I think they were upset that it didn't do it as well when given x-rays of black persons. So they concluded it was due to the model making racist decisions.,4
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,"The race data was not shared. Notice they say they don't know from where the algorithm is deducting the information, even when given imaging data that is incomplete, corrupted or even just a tiny fragments. They list melanin variation (which could be perceptible to the AI from the X-rays) as a potential benign explanation.

I think a lot of people have trouble understanding that AI makes choices but does not ""share the reasoning"", it can be incredibly hard to understand the reasoning behind a decision, that's what makes dealing with bias hard.",3
post50hb,richly branching,1.6001453961458911,highest,"It means a existing AIs, if not trained with a sufficiently representative training set, will be biased. And that this will lead to unequal health outcomes.
 Much like the web camera that couldn't detect black faces, but worse.",3
post50hb,richly branching,1.6001453961458911,highest,"People are different over the world bit by bit. But ""race"" is fakin bullshit. Definition of race is so blurry that all AI is determining is how steoretypicaly it was defined by people. Aka scientist coded in racism.",3
post50hb,richly branching,1.6001453961458911,highest,"The more I read this article the more ridiculous it becomes. The scientist claims that he ""cannot deploy his model because it makes racist decisions"". Excuse me, but weren't you the one who fed it the data? So either you're giving it racially-biased data or (probably more likely) it's just a common learning problem and there's absolutely no reason to come up with undergrad-level excuses as to why you couldn't make it better.",4
post50hb,richly branching,1.6001453961458911,highest,Definition of colours are blurry too. This is just the continuum fallacy.,4
post50hb,richly branching,1.6001453961458911,highest,So what your saying is that we are creating a racist AI? No not humans.,3
post50hb,richly branching,1.6001453961458911,highest,How would an AI be racist if it was built to distinguish race from X-ray and does exactly that? Is it racist because it can tell the person's race? Is it racist because it does it with 90% accuracy?,4
post50hb,richly branching,1.6001453961458911,highest,"Seems likely it was ""clickbait research"". Sadly a lot of studies are done just to bring attention to the institution that finances them rather than to further knowledge.",3
post50hb,richly branching,1.6001453961458911,highest,The concerned people are the devoutly anti racist people who think everything is racist to the point where normal people are eye rolling,3
post50hb,richly branching,1.6001453961458911,highest,"The other big problem with this article is that they specifically said it had almost 90% success with ""some groups of images."" They don't note what those images are. If they're skulls, for example, there are some patterns in skeletal structure of the face that do tend to correlate with race (not well, mind you, but the correlation exists). Can the AI tell your race from an x-ray of your femur? Who knows? They didn't bother to say.",3
post50hb,richly branching,1.6001453961458911,highest,"I agree! This was my thinking, too! Why is it even called ""bias"" if it's not being used to *discriminate*? They're using it for ID, not decisions about anything relevant...",3
post50hb,richly branching,1.6001453961458911,highest,clickbaity as hell lol,3
post50hb,richly branching,1.6001453961458911,highest,so they trained the AI to be racist and are shocked (shocked i say) to find the AI has made racial determinations.,3
post50hb,richly branching,1.6001453961458911,highest,"Somehow the results point towards the fact that races exists and do have an impact on us.

Somehow this is problematic, but only because the communication around racism has been ass-backwards more often than not.",3
post50hb,richly branching,1.6001453961458911,highest,"They're concerned that somebody who is as racist as the people who decided this was worth studying in the first place, such as themselves, will get their hands on this information.",3
post50hb,richly branching,1.6001453961458911,highest,"It is possible to have found a clumping of data in the datasets that, after human examination, resulted in the discovery that the clumps were centered on certain racial traits. Everyone wants to assign human interpretations of the term race, which would only matter if somehow this changed the level of care or treatment. No one is claiming that to be a preferential outcome of the database discovery, Just that it happened to sort out that way.",3
post50hb,richly branching,1.6001453961458911,highest,"I have visions of two scientists sitting together having a urgent hushed conversation -

""we both know neither of us actually did any work for this project, we didn't even provid the Ai with any data!""

""Exactly! That's why this is so worrying!""",3
post50hb,richly branching,1.6001453961458911,highest,"Sounds like every programmer I ever met
""Wait my code works?!""",3
post50hb,richly branching,1.6001453961458911,highest,"I read the actual study, the AI can categorize images with 99% accuracy with just a scan of someone's lung, and 40% accuracy on the vague blurry version. The neural network pulled out some data that we have absolutely no idea what it's seeing there. They speculated it may be differences in medical imaging equipment between races.",3
post50hb,richly branching,1.6001453961458911,highest,"""I mean no wonder it found patterns because they exist""

Because some people think it's racist to believe such things? Because the media like to sensationalise such matters by implying that such things are racist, thus making more of those people?",3
post50hb,richly branching,1.6001453961458911,highest,"The scientists aren't saying, ""oh no, the machine can see race, that's bad."" They're saying, ""maybe the machine seeing race is part of how it's underperforming for black people.""

They're not implying the solution is to make the machine unable to see race. They're saying they need to figure out how race plays into what the machine sees, and hopefully use that to improve the machine before rolling it out.",3
post50hb,richly branching,1.6001453961458911,highest,"Reading the original MIT article I highly suspect what you're saying is true.

The professor was surprised that the model was able to detect race from skeletal structure. I doubt they would make such a dumb noobie mistake of having it as an input then be surprised.",3
post50hb,richly branching,1.6001453961458911,highest,We have long known that skull of people from Sweden is shaped different (longer) than that of a Dane or German... surely more skeleton differences would also be the case for people who are even less related...  so why is this a surprise ?,2
post50hb,richly branching,1.6001453961458911,highest,"It isn't. Scientists are not concerned to discover AI can do something we have been doing for years, title just lied.",3
post50hb,richly branching,1.6001453961458911,highest,"But, AI scary!!!!",4
post50hb,richly branching,1.6001453961458911,highest,"For a very, very long time we have had the ability to take a skeleton and tell you the race, gender, and age.  How many cold cases do we have where all we had to go on was a few bones?

This is new science like virology is a new science.",4
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,"Feed it some Ethiopian Data, thatll mess it up.",4
post50hb,richly branching,1.6001453961458911,highest,It’s not but Americans will refuse to acknowledge any differences between races because they had slaves and feel guilty.,3
post50hb,richly branching,1.6001453961458911,highest,"Making sweeping generalizations about a group of people sounds like… ah, never mind.",4
post50hb,richly branching,1.6001453961458911,highest,You are spewing several logical fallacies here.,4
post50hb,richly branching,1.6001453961458911,highest,swedes and danes are different races?,4
post50hb,richly branching,1.6001453961458911,highest,"That's what I'm wondering.

Maybe it's not a ""surprise"" so much as a ""concern"" as the title suggests.  Like, it discomfits scientists that race can be quantified so easily.

I guarantee you if an AI can be trained for this, it can be trained to calculate intelligence from X-rays as well.  _That_ will really discombobulate some scientists.",3
post50hb,richly branching,1.6001453961458911,highest,"I think the ""surprise"" here is that scientist don't know the metrics the computers are using to generate their (correct) assumptions",4
post50hb,richly branching,1.6001453961458911,highest,"Immagine future college application interviews:

'Leme check that xray real quick, Mr Anderson. '",4
post50hb,richly branching,1.6001453961458911,highest,"Yeah except placing labels of ""race"" on these differences is bullshit and unscientific. For example how dark ones skin has to be to be called dark skinned person? It is all fuzzy.",3
post50hb,richly branching,1.6001453961458911,highest,"It's not hard to quantify the amount of melanin in a given patch of skin, and we know the lower bound (albinism) so I don't see why this is an issue.",4
post50hb,richly branching,1.6001453961458911,highest,"> For example how dark ones skin has to be to be called dark skinned person? It is all fuzzy.

What height do you have to be to be called tall? What blood pressure do you have to have to have high blood pressure? It's perfectly fine to use categories that are fuzzy. It is not unscientific.",4
post50hb,richly branching,1.6001453961458911,highest,Lol you're doing Phrenology in 2022,3
post50hb,richly branching,1.6001453961458911,highest,Only when Vogon is raising in Thesaurus...,4
post50hb,richly branching,1.6001453961458911,highest,"Not really... This is called ""morphology"" and ""anthropometric measurement"".  Here's the [wiki on morphology](https://en.wikipedia.org/wiki/Morphology_(biology)). And here is the [wiki for anthropometric measurement](https://en.wikipedia.org/wiki/Anthropometry). Both are totally valid practices in biological sciences. As examples, here is a [quick paper](https://pubmed.ncbi.nlm.nih.gov/16077306/) on the topic, and [here's another](https://pubmed.ncbi.nlm.nih.gov/30726000/).

TLDR: Morphology is the use of appearance and measurements to categorize organisms.

In this case, specifically, /u/MaybeTheDoctor refers to the subset of morphology that is called ""craniology"". [Craniology is different from phrenology](https://sciencing.com/the-difference-between-craniology-phrenology-12759816.html).

TLDR: Morphology is a *scientific* practice of categorizing organisms through measurable traits, based on well designed statistical analyses. Craniology is the application of this method to any vertebrate organism's skull's properties. Phrenology is the *non scientific* practice that attempts to use skull measurements and anomalies to draw conclusions specifically about human beings' personal traits. 

Saying one group has skulls with a certain trait and another group has skulls with another trait can be valid, depending on the statistical evidence backing this claim. Saying one is intelligent and another is not based on the presence or absence of a random bump is not valid.

With that said, craniology has generally fallen out of favor among most western anthropologists over the last half century, because of its understandably uncomfortable closeness to phrenology and the difficulty in deriving any scientifically useful findings from it. However, due to cultural and academic isolation from the west, anthropologists from ex-Soviet nations still carry out studies on this topic. Try searching on google scholar if interested - if you look at the ""cited by"" links for these studies, you'll find that they're not particularly disputed, but they're also not cited much at all, which goes to show that they're not really pushing science forward very much either.

With that also said... Morphology and craniology are widely employed in paleoanthropology, as these are pretty much the only tools available for identifying and categorizing early homonid remains (see figure 2 in this [link](https://www.nature.com/scitable/knowledge/library/overview-of-hominin-evolution-89010983/)). These methods are also use pretty extensively within medical sciences, where morphology can be used to assist in the diagnosis of illnesses.",4
post50hb,richly branching,1.6001453961458911,highest,"Wait. 

Are you saying phrenology was... Right?",3
post50hb,richly branching,1.6001453961458911,highest,"If by ""long known"" you mean based on 19th century racist pseudo-science that was discounted by mainstream academics a century ago, sure. Totally.",3
post50hb,richly branching,1.6001453961458911,highest,Yeah I am very confused and a bit terrified by this thread... I can speak from personal experiences that all races are similar in almost every way but that isn't scientific lol,4
post50hb,richly branching,1.6001453961458911,highest,"But those people are all the same “race”. 

Regional differences exist far more significantly than racial ones when looked at as a whole.",3
post50hb,richly branching,1.6001453961458911,highest,What about Norwegians? They are stereotyped as having big heads.,3
post50hb,richly branching,1.6001453961458911,highest,I'm sure there must be a wikipedia list somewhere which give the complete list of all genomes and their hat sizes.,4
post50hb,richly branching,1.6001453961458911,highest,"It’s not a surprise, it just hurts the “everyone is the same and completely equal” agenda when there is empirical evidence showing otherwise.",3
post50hb,richly branching,1.6001453961458911,highest,It's like the study came from a university without an anthropology program...,2
post50hb,richly branching,1.6001453961458911,highest,[removed],3
post50hb,richly branching,1.6001453961458911,highest,"Seriously, I have a BA in Biological Anthropology and this is like, basic osteology. How the fuck do they think we figure out the age, race, and sex of a skeleton?? By looking at the bones!",3
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,"Careful now, you can get in trouble for suggesting such things.",4
post50hb,richly branching,1.6001453961458911,highest,"I'm confused too why this is a shock. Of course there's slight anatomical differences between races. It doesn't actually mean anyone is more superior or inferior. Unless they're worried that thats how some people will interpret this. But the AI doesn't care. It's just doing what it's supposed to.

ETA: I guess biases get in easier than I realized.",2
post50hb,richly branching,1.6001453961458911,highest,"The point is, if there is intrinsic bias in the sysytem already (which there is), a medical AI could perpetuate that bias without us even knowing.",3
post50hb,richly branching,1.6001453961458911,highest,"When I lived in Japan I had more than one doctor tell me ""You are Caucasian, and I don't treat many non-Japanese patients so I'm not sure what the correct dosage of X medicine would be, or what X level should be on your bloodwork.""",4
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,"Yep, It depends on the data fed and the questions asked, it’s easy to get unintended consequences, because the data itself has bias.",4
post50hb,richly branching,1.6001453961458911,highest,"But there's always bias, the entire field of deep learning is mainly about reducing this bias, reducing the overfit on training data while not sacrificing inference accuracy. I do wonder how they label ""race"" in their training data. If they follow a national classifier, then I guess you'd need to look into that classifier as a possible source of human bias. But if we *assume* that the classifier is very simplistic and only takes into account the very basic classification of races, then the problem would really move towards having enough varied data. And the bias would be reduced as the data increases (even if the model doesn't change).

I suppose there's more attributes they are training on than just x-rays and race labels, so they gotta figure out if any of them could be easily tampered with.",4
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,But…  it’s a bias based on data and fact,4
post50hb,richly branching,1.6001453961458911,highest,> The system is inherently biased... says the people that created and run the system,4
post50hb,richly branching,1.6001453961458911,highest,"Utter nonsense, there is no bias in the AI system it’s is just a factor to understand and in some cases needed as treatments can be affected depending on your race, these are rare but still true",4
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,What is meant by “intrinsic” in this statement and why exactly should this be concerning? Of course there will be bias. Medical AI will be biased by way of its programming to find ways to keep humans alive rather than to find ways make us die faster. That’s a bias towards life and it’s also the point. Perpetuating this bias is exactly what we want.,4
post50hb,richly branching,1.6001453961458911,highest,"If people are physically different where is the bias in a finding that indeed the difference is noticeable?

I think this opens the door for more customized treatments which is always a good thing.

>",4
post50hb,richly branching,1.6001453961458911,highest,"Yea, it's not that they're concerned that there is a different, it's that scientists were concerned mostly because they aren't sure _how_ the AI can tell the difference.",4
post50hb,richly branching,1.6001453961458911,highest,"So it's not different that if AI was not involved. We can make an interpretable AI that will be easier to audit than a racist doctor or judge. The problem is *if we're stupid and consider the AI is God and should not be questioned, updated, audited and tuned continuously.

Human are biased by what they've seen and what they've been taught. AI are biased by what they've seen and what they've been taught.",4
post50hb,richly branching,1.6001453961458911,highest,"But what if the bias is something like sickle cell anemia, a disease that is more prevalent in black people? If racial genetics is the source of bias, then bias diagnosis isn't a bad thing. If the bias is from medical records tending to fail a demographic, then that needs to be weighted out, but that's why you look for such biases in the system early on, so you can reinforce the good results and downweigh the bad results. 

It's why we try to make sure children are taught by trained teachers, not just left to decide on their own what to learn while growing up. You teach them while looking at how the data is being received and modify your lessons if bad patterns emerge.",4
post50hb,richly branching,1.6001453961458911,highest,The bias could be accurate and the AI could be accurate,4
post50hb,richly branching,1.6001453961458911,highest,"less likely too frankly.  


only way that would happen is if we collectively care so little we wouldnt bother to correct it.  


A Good start would be banning US citizens from being in studies, 20 year old white Americans make up far too much of our studied population ie psych studies etc. i mean those results are utterly useless outside the US.",4
post50hb,richly branching,1.6001453961458911,highest,"I don't get it, are we afraid that a computer will give us correct answers that we don't like? Because if it gives us false answers, that's something we can fix. Otherwise its just a problem with us.",4
post50hb,richly branching,1.6001453961458911,highest,"So, don’t ever speak about it. Or, lie!",4
post50hb,richly branching,1.6001453961458911,highest,"It's not a shock, but sensationalist media I guess",3
post50hb,richly branching,1.6001453961458911,highest,Wait till the guy or gal that wrote this article hears about skeletal differences between the sexes. It'll be a whole new world order,4
post50hb,richly branching,1.6001453961458911,highest,"You might not be, I am not but I've seen threads addressing similar topics in the past absolutely go haywire and fraught with arguments and finger pointing about how you cant say things like this because of the argument that race isn't even a real thing.",4
post50hb,richly branching,1.6001453961458911,highest,"It should be the opposite, we should be excited that we can now correlate anatomical data with other historical data about trends and epidemiology e.g. the reason this ethnicity has higher X might be because of Y...

I don't get it. I'm white as shit, and I would be beyond livid if I went to a dermatologist and they weren't taking that into account in terms of my risk for skin cancer etc..",3
post50hb,richly branching,1.6001453961458911,highest,"Here's the problem:

People get different treatment/results by race even when it shouldn't make a difference.

I'm not talking skin cancer, or sickle cell anemia, I'm talking things like childbirth, or even just being diagnosed in the first place.

If the AI is being trained with this improperly biased data, that's bad.

The study was investigating whether sources of this bias may have snuck in, since ""Several studies have shown disparities in the performance of medical AI systems across race. For example, Seyyed-Kalantari and colleagues showed that AI models produce significant differences in the accuracy of automated chest x-ray diagnosis across racial and other demographic groups, even when the models only had access to the chest x-ray itself.""

Note that this is with JUST the chest X-ray.  I've seen multiple comments pointing out skull shape used by anthropologists.  The skulls were not X-rayed.

That means chest X-Rays, which were previously thought to be devoid of racial identifiers and thus good tools for training data, may in fact be carrying bias over to train the AI to be biased unknowingly.",4
post50hb,richly branching,1.6001453961458911,highest,"Actually, [there’s more generic variation](https://sitn.hms.harvard.edu/flash/2017/science-genetics-reshaping-race-debate-21st-century/) between members of the same race than there between the averages of any two races. The initial study showing this happened in the early 20th century by Fraz Boas and has still yet to be disproven to this day, but it was used as the foundation for the field of Anthropology.",3
post50hb,richly branching,1.6001453961458911,highest,"IDC enough to click your link, but you either described this incredibly poorly, and/or this should be obvious.  The far boundries of genetic variation between members of one race obviously varies wildly, obviously.  Do you mean the average within a race vs the average between any two races?  If not, this is nonsense.",4
post50hb,richly branching,1.6001453961458911,highest,"Well, identifying race is not really a big problem, but it's possible that there's already a negative bias disparity in the diagnosis and treatment of injuries depending on race, which the AI would learn alongside the racial differences. The problem with AI learning patterns is that it learns them from humans, and humans are notorious for racism, so AI learns the racism that already exists, even if it is very subtle. This subtlety can be lost in the process and you end up with the Facebook's autolabelling photos scandal from years ago when two tourists were misidentified.",3
post50hb,richly branching,1.6001453961458911,highest,"Not only learns, sometimes even amplifies.  and even worse can legitimize biases, since the user of the information might believe ""machines can't be biased""",4
post50hb,richly branching,1.6001453961458911,highest,">but it's possible that there's already a negative bias disparity in the diagnosis and treatment of injuries depending on race, which the AI would learn alongside the racial differences.

If its actually good at learning, it will notice that certain treatments have different outcomes for individuals of different races, and will adjust in order to improve its outcomes because, presumably, it *wants* to produce the best health outcomes possible in every case.

So whatever biases it starts with aren't likely to be present in the final product, if it has good metrics for determining positive outcomes.

It'd be worse if the AI couldn't distinguish by race and defaulted to assuming everyone was Caucasian or something.",4
post50hb,richly branching,1.6001453961458911,highest,"That’s true.  But consider this.  Most people in the world (68%) cannot digest milk once they become adults.  But almost every meal in the United States has tons of dairy in it because Caucasians generally can.  Medical professionals describe this as “lactose malabsorption” rather even though it’s actually an adaptation that is uncommon outside of western, central and Northern Europeans.

Biases like that can creep into any system, even when no ill will is intended, because even scientists and doctors will just kind of forget people of other races exist when doing their jobs.",3
post50hb,richly branching,1.6001453961458911,highest,">because Caucasians generally can.

This is wrong. Your classifications are American-centric. ""Caucasians generally can"". That is a useless divide (and American-centric because it's a ""hey this is how we divide races in the USA) because the percentages vary by countries and even within regions of countries. 55% of people from Greece are lactose intolerance but only 4% from Denmark are. 13% are people from Niger are lactose intolerant but virtually everyone from Ghana is. 93% from Iraq are but only 28% from Saudi Arabia.

[https://milk.procon.org/lactose-intolerance-by-country/#:\~:text=Lactose%20Intolerance%20by%20Country%20%20%20%20Country,%20%2098%25%20%2085%20more%20rows%20](https://milk.procon.org/lactose-intolerance-by-country/#:~:text=Lactose%20Intolerance%20by%20Country%20%20%20%20Country,%20%2098%25%20%2085%20more%20rows%20)

The problem with the concept of ""race"" is that the divisions that each country concocts are not based off of biological factors. They are always based off of social factors and phenotypical factors. Biological factors exist in humans and different villages and ethnicities, but there aren't any large sets of biological factors that correlate with the American classifications of race.

Certainly if you compare African Americans and Caucasoid American bone structure, you're going to find general patterns among them... but that's just because most White Americans are Western European and most Black Americans are Coastal-West African. What if you compared Kho-San people with Greek people with Dinka people with Irish people?

And that's why ""race"" is still a useless factor in medical science. Being ""White"" or ""Black"" is meaningless and tells you nothing. What tells you something is if you have Dinka roots or Greek roots or Mixtecan roots or Haida roots. These biological differences are specific to very small population groups, not these mega-clusters that are ""racial"".",4
post50hb,richly branching,1.6001453961458911,highest,Guess those biases creep in very easily and sneakily. I'm white but I can't digest milk and I didn't even think about that as a potential bias.,4
post50hb,richly branching,1.6001453961458911,highest,"Concern for bias seems a little odd when we appear to be going down the path of individualised medical treatment.

It seems likely that you will have your dna scanned before you are given drugs to ensure you receive the best treatment for your biology.

Do we now have to reject better medical treatment because you doctor might discover your race as part of the treatment?",4
post50hb,richly branching,1.6001453961458911,highest,"The prefix mal- means bad, from the French. As in malformation or malpractice. So, malabsorption means bad absorption. It is has nothing to do where the trait originated. It is just a word form used in medicine and the sciences. Also, there are large areas in Africa and the Middle East where lactase persistence occurs in the majority of the population. The trait does not only exist in Europe.",4
post50hb,richly branching,1.6001453961458911,highest,I think it's just supply/demand.,4
post50hb,richly branching,1.6001453961458911,highest,There is also a small set of African populations too. But since most African Americans are more or less mixed with different stuff it’s hard to know who’s have the gene.,4
post50hb,richly branching,1.6001453961458911,highest,So you're just discounting India where milk and butter are literally sacred to a massive part of the population huh,4
post50hb,richly branching,1.6001453961458911,highest,"Is the logical solution not to simply avoid 1 AI for all, move to 1 which accounts for age, gender, race and so on. I don't see the issue with having 100+ different ""AIs"" with different data sets if it results in better accuracy.",4
post50hb,richly branching,1.6001453961458911,highest,"\>It doesn't actually mean anyone is more superior or inferior.

Longer bones in part do help you run faster. So africans are superior in that aspect. The average height of asians is much shorter than africans/europeans and they are therefore disadvantaged/inferior in tasks that benefit height.",3
post50hb,richly branching,1.6001453961458911,highest,"That's an oversimplification. Running speed isn't just about bone length, there are other factors too, like power to weight ratios.",4
post50hb,richly branching,1.6001453961458911,highest,"Height differences is more of a function of diet. Of course,  there are strong genetic components.  We are seeing large increases in height across generations in Asia as ecomics and diets improve",4
post50hb,richly branching,1.6001453961458911,highest,"We damn better hope (and if needed make sure through genetic counseling) that personality, IQ, and maximum healthy lifespan are equal or nearly equal though. The entire post-WWII order is based on it, as is the relative absence of slavery and colonialism since then.",4
post50hb,richly branching,1.6001453961458911,highest,"The 2016 and 2020 Olympic marathon winner, Eliud Kipchoge, is from Kenya and he is 5 feet 6 inches tall. Long bones don't usually affect running the way it does basketball or football.",4
post50hb,richly branching,1.6001453961458911,highest,"And for each of those there is an opposite, cmon now. Benefits to having less long limbs. Benefits to being shorter.",4
post50hb,richly branching,1.6001453961458911,highest,"Because people grow up hearing about how we’re all the same biologically but the reality is different. Some are naturally better at running, others lifting, handling thin air, lots of sun, etc.",3
post50hb,richly branching,1.6001453961458911,highest,"That's what annoys me so much -- that people mistake ""different"" with ""superior"" or ""inferior"". Just because something is different makes it neither better nor worse, just different.",3
post50hb,richly branching,1.6001453961458911,highest,"In a medical context, some ethnicities have differing health issues.

Being able to detect race is a bonus here, because you know to check for race specific medical issues.",3
post50hb,richly branching,1.6001453961458911,highest,[deleted],3
post50hb,richly branching,1.6001453961458911,highest,"On the other hand, Blacks and Whites tend to respond differently to different blood pressure meds and are prescribed medications accordingly.",4
post50hb,richly branching,1.6001453961458911,highest,At the end of the day it’s a computer program and designed by people who do have biases. Possible the worry is that those biases will make it into code.,3
post50hb,richly branching,1.6001453961458911,highest,If you think this way then you don't know how AI works,4
post50hb,richly branching,1.6001453961458911,highest,True. Didn't think about the possibility of bias in the code.,4
post50hb,richly branching,1.6001453961458911,highest,"If you read the article, the concern is that knowing the race of the person in the X-ray will adversely affect some doctors who have biases, conscious and unconscious.",3
post50hb,richly branching,1.6001453961458911,highest,Phrenology is back and better than ever,3
post50hb,richly branching,1.6001453961458911,highest,"I also suspect it has less to do with ""race"", and more to do with what part of the Earth your ancestors originated from.  

""Race"" is a made up concept, but the patterns and idiosyncrasies in the skeleton are real.  

It means that the AI has to fit those patterns to the made up concept, as oppose to adjusting the concept to match the patterns.",3
post50hb,richly branching,1.6001453961458911,highest,"A current example of this AI bias is with CV scanning programs for recruiters. A program for sorting CVs (resumés) was only delivering CVs from Male applicants. This happened because it was using historical data in which it learned that female CVs were placed lower in the pile. Simply put. 

The writers of said software didnt intend that, expect it, or plan for it. The software just did it.",3
post50hb,richly branching,1.6001453961458911,highest,"In my physiology class in high school, our teacher had us guess the race of three different skulls. It was pretty easy to guess just based on outer physical traits.",3
post50hb,richly branching,1.6001453961458911,highest,I'm certainly no expert but I know white people tend to have much larger sinus cavities. I think the thought was it was from living in colder climates and possibly a remnant of breeding with Neanderthals.,4
post50hb,richly branching,1.6001453961458911,highest,"Not so much with this but in general the people making these AI may place biases in their work without even realising it, it's a concern in AI used for deciding police patrols  due to the data sets they train their AI on.",3
post50hb,richly branching,1.6001453961458911,highest,"Define superior,  because I could use biology to make a pretty solid argument that folks from west African descent are superior at sports that require power and speed.",3
post50hb,richly branching,1.6001453961458911,highest,"Lmaowut? I love how you can understand why AI would be able to tell differences and can’t see how that would be abused like crazy to literally profile people without their knowledge at all, which is why people are concerned.",3
post50hb,richly branching,1.6001453961458911,highest,"Watch the movie Gattaca. Bias is outlawed, but...",3
post50hb,richly branching,1.6001453961458911,highest,I'll do that. I've definitely heard of the movie but never taken the time to watch it.,4
post50hb,richly branching,1.6001453961458911,highest,"A big part of the shock here is that this is using 'self-reported race' or whatever the patient says there race is. There is no correlation to other genetic biomarkers. 

It's kind of shocking that AI can predict the patients preferred race so accurately.

&#x200B;

Another part is in bias, specifically that the FDA does not have a many regulations surrounding proving AI works well for all given biases, which for medical imaging is more than just race/age/sex but also includes make/model and scanner settings. 

IMHO AI vendors need to collect bias information for their training and validation sets. Results of their AI running on each bias should be published with their sales information.",3
post50hb,richly branching,1.6001453961458911,highest,"they were apparently trained in.

> after training it with hundreds of thousands of existing X-ray images annotated with specifics of the patient's race.",3
post50hb,richly branching,1.6001453961458911,highest,"To be devil's advocate: if there are obvious physical differences between ethnicities, could there not be cognitive ones?",3
post50hb,richly branching,1.6001453961458911,highest,"I read the actual study, the AI can categorize images with 99% accuracy with just a scan of someone's lung, and 40% accuracy on the vague blurry version. The neural network pulled out some data that we have absolutely no idea what it's seeing there. They speculated it may be differences in medical imaging equipment between races.",3
post50hb,richly branching,1.6001453961458911,highest,It’s generally unacceptable to talk about because RACISM. People are very unreasonable today.,3
post50hb,richly branching,1.6001453961458911,highest,"Actually they may not be as imperceptive as you might think! I remember years ago there was this thing going around were they took stock photos of black people and photos hopped them to be white, and then did the reverse to white people, and you could certainly tell that something was off. 

Even if it's something like the jawline or cheekbones, humans are hard programmed to pay close attention to the faces of other humans so even some of the smallest differences can be glaring",2
post50hb,richly branching,1.6001453961458911,highest,"I wonder if it could predict/differentiate rich people from poor people, not because of genetic traits but rather due to like different diet or stress levels",2
post50hb,richly branching,1.6001453961458911,highest,[removed],2
post50hb,richly branching,1.6001453961458911,highest,"> there are no differences whatsoever

Yeah how about you take a look at a picture of an albino Englishman and an albino Somali and get back to me chief. There is nothing *bad* about being fucking different. The goddamn diversity police are always desperate to increase diversity while simultaneously declaring everyone is the same.",3
post50hb,richly branching,1.6001453961458911,highest,Wait so is there or not? I'm reading in this thread that Africans have bigger bones and others have different skull shapes.,3
post50hb,richly branching,1.6001453961458911,highest,"While race is not the correct word to use and is not a biological term, what people usually mean more specifically are groups of related ethnicities(which while sometimes close, don’t always correlate to what we think of as race), and different ethnicities can have different bone and skull shapes just as they have different facial features.",3
post50hb,richly branching,1.6001453961458911,highest,"Sort of but not really. Africans are among the most diverse genetically and physically. Norther Europeans are taller than Southern Europeans. 

There is more diversity within races than between them.",2
post50hb,richly branching,1.6001453961458911,highest,"Race is a social construct, not physical.",2
post50hb,richly branching,1.6001453961458911,highest,[deleted],3
post50hb,richly branching,1.6001453961458911,highest,"It absolutely does, kid. 

>Contrary to popular belief that the division of the human species based on physical variations is natural, there exists no clear, reliable distinctions that bind people to such groupings.[12] According to the American Anthropological Association, ""Evidence from the analysis of genetics (e.g., DNA) indicates that most physical variation, about 94%, lies within so-called racial groups. Conventional geographic ""racial"" groupings differ from one another only in about 6% of their genes.""[13] While there is a biological basis for differences in human phenotypes, most notably in skin color,[14] the genetic variability of humans is found not amongst, but rather within racial groups – meaning the perceived level of dissimilarity amongst the species has virtually no biological basis. Genetic diversity has characterized human survival, rendering the idea of a ""pure"" ancestry as obsolete.[11] Under this interpretation, race is conceptualized through a lens of artificiality, rather than through the skeleton of a scientific discovery. **As a result, scholars have begun to broaden discourses of race by defining it as a social construct and exploring the historical contexts that led to its inception and persistence in contemporary society.[15]**

>https://en.wikipedia.org/wiki/Race_and_society#Race_as_a_social_construct_and_populationism",4
post50hb,richly branching,1.6001453961458911,highest,We can distinguish it. Forensic anthropologists can tell the race of a skull with a high degree of accuracy with just a few different variables. The AI isn't doing anything that hasn't been done before.,2
post50hb,richly branching,1.6001453961458911,highest,"Don'tanthropoligists identify intact skeletons by race pretty well? Mandibular prognathism? Subsaharan. Occipital bun? Caucasian/euro. Round orbit vs. square, pronounced maxill or brow/otherwise etc.? Caucasian/euro and Asian, no?

Are we talking independent of the skull here, or what's up?",2
post50hb,richly branching,1.6001453961458911,highest,"The AI is doing exactly what it should. Many diseases are known to affect certain races differently, and if we force the AI to ignore what it finds, aren't we forcing it to diagnose at a disadvantage? I'm all for equality and everything, but this clearly falls outside of it doesn't it?",2
post50hb,richly branching,1.6001453961458911,highest,"Right, but why is this seen as bad thing? It seems we could treat individual issues better having more specificity about the person in question.

Are we so worried the racists will use as justification for their evil….. dammit.",2
post50hb,richly branching,1.6001453961458911,highest,Aren't skull shapes radically different between the major races?,2
post50hb,richly branching,1.6001453961458911,highest,Bone density can be a distinguishing feature also,2
post50hb,richly branching,1.6001453961458911,highest,We’re also not supposed to say/admit this because that’d be racist,2
post50hb,richly branching,1.6001453961458911,highest,"At the level of using AI to analyze, you should at times even be able to determine things like likely language spoken due to differentiating musculature attachment points on the face",2
post50hb,richly branching,1.6001453961458911,highest,Imperceptive…to us…,2
post50hb,richly branching,1.6001453961458911,highest,that’s racist,2
post50hb,richly branching,1.6001453961458911,highest,"That's false; the leading theory is that the xray's are detecting melanin in the bones, so it's not that skeletal structure is different.",2
post50hb,richly branching,1.6001453961458911,highest,Do you have a source for that?,3
post50hb,richly branching,1.6001453961458911,highest,It's in the article,4
post50hb,richly branching,1.6001453961458911,highest,[removed],2
post50hb,richly branching,1.6001453961458911,highest,"i would say that inteligence, even if somewhat bound to biology, is much more affected by the enviroment one lives, if you have everything you need around with obstacles to overcome, any biological nature for inteligente is mostly negligible",3
post50hb,richly branching,1.6001453961458911,highest,I've read that black people have longer legs (and arms) on average for their height than Europeans who have larger torsos on average.  It's to do with climate I guess....you lose heat from the extremities.   This kind of thing can be noticeable...it's probably especially so when you get those individuals who are above average for their own group.,2
post50hb,richly branching,1.6001453961458911,highest,But we can distinguish. Different groups have different jawlines and noses etc...,2
post50hb,richly branching,1.6001453961458911,highest,"I read the actual study, the AI can categorize images with 99% accuracy with just a scan of someone's lung, and 40% accuracy on the vague blurry version. The neural network pulled out some data that we have absolutely no idea what it's seeing there. They speculated it may be differences in medical imaging equipment between races.",2
post50hb,richly branching,1.6001453961458911,highest,"Has these ""scientists"" figured out how police can determine racial, age, gender and health just by looking at a skeleton?",2
post50hb,richly branching,1.6001453961458911,highest,"Maybe that’s a good thing for an AI.

Some diseases, like sickle cell, or even heart disease are racially identifiable in statistics. It could be an indicator that helps correct diagnoses.",1
post50hb,richly branching,1.6001453961458911,highest,"AI is neither good nor bad, it's just information, what humans tell the AI to do with it is good or bad.",2
post50hb,richly branching,1.6001453961458911,highest,"> All things are poison and nothing is without poison; only the dose makes a thing not a poison. 

In relation, it depends on who the devs are.",3
post50hb,richly branching,1.6001453961458911,highest,"They need to make an AI that makes other good AI’s, simple.

Where’s my award for this scientific breakthrough.",4
post50hb,richly branching,1.6001453961458911,highest,"No...no it's not. That a fun falsehood. 

Data is just data. The AI tells you what it is without bias. People lose their mind when something tells them ""The emperor has no clothes.""",4
post50hb,richly branching,1.6001453961458911,highest,"AI is programmed by humans, who are not perfect. This issue is that AI can be programmed with racial bias without us even being aware.

For example, facial recognition is really bad at recognizing black people. Why? Because the sample data that was submitted to the AI did not include many people with darker skin, therefore the AI has an implicit bias encoded by humans.

We need to remember that AI is not completely separate from human kind - it uses data that has been gathered from us (imperfect) humans.",3
post50hb,richly branching,1.6001453961458911,highest,"It could potentially even be good. In the article it says AI misses or misdiagnoses diseases in people of color. If it can recognize race, it can learn to apply different diagnostic strategies that would start to resolve that problem.

I feel like custom diagnosing could be a step in the right direction?

It all depends how the tech is developed and used.",3
post50hb,richly branching,1.6001453961458911,highest,"AI doesnt give a fuck about societal contexts of race. It just finds patterns. It's 100% honest. It doesn't see race, only variations of human.",4
post50hb,richly branching,1.6001453961458911,highest,"The AI wouldn’t need to recognize race if the proper parameters are in place for diagnosis. 

Really, it’s about programmers inputting the proper parameters, which would still follow the same bias as misdiagnosis. 

There is also a massive issue with people of color and women not being believed in the medical field - causing a misdiagnosis. 

We don’t even designed medications for anyone but average white male, which is not the true average, but rather a data set that falls in the mid range between two extremes of people, which means it doesn’t cover the extremes, only their middle dot. 

It all depends on how society and the programmers think.",4
post50hb,richly branching,1.6001453961458911,highest,"That's definitely not true. There have been plenty of AI that are bad. Look at the AI used by police how it treats minorities.

AI is just code if it's coded to come to a certain conclusion it will come to a certain conclusion.",3
post50hb,richly branching,1.6001453961458911,highest,How does it treat minorities?,4
post50hb,richly branching,1.6001453961458911,highest,Do we really live in the 21 century and have to claim that information is neither good or bad? How dumb and hiper sensitive the general population have become?,3
post50hb,richly branching,1.6001453961458911,highest,The general population use to burn people for being witches.,4
post50hb,richly branching,1.6001453961458911,highest,"> Do we really live in the 21 century and have to claim that information is neither good or bad? How dumb and hiper sensitive the general population have become?

Information can be classified as good or bad, but only in the context of what human beings do with it.  

Someone stating ""this is bad"" isn't a form of hypersensitivity, it's a symptom of spectating what society has done with information, what path society seems to be taking, and what it is prioritizing with that information.  

Not everyone is a lovely optimist like you, it seems.",4
post50hb,richly branching,1.6001453961458911,highest,Data can absolutely be biased,4
post50hb,richly branching,1.6001453961458911,highest,"Information and data aren’t the same. Data is pieces of info without context that still need to be processed to make sense of it.

It’s also about who has the data. Black dude already knows he’s black. He may not want an employer or parole board to know that automatically.",4
post50hb,richly branching,1.6001453961458911,highest,"People refuse to understand that men and women are genetically different at a very basic level, people don't like truth apparently.",4
post50hb,richly branching,1.6001453961458911,highest,"""Our AI has found that people with this skeleton structure, hair type, and blood type are extremely susceptible to transmitting viruses. They transmit at a rate of 45% more than populations without these traits. Our advanced AI has suggested that we group these people up and keep them isolated from the greater population for the greater good of the human species. It just so happens that all these people are of one specific race.""

Knowledge and intelligence are understanding that information is just data.  Wisdom is knowing that that data is used to make decisions.  If machines make the decision, can they do it ethically? If humans make the decision, will they make the ethical decision and can they be convinced by $$$$$?

Harm is not good or bad. It is a result. Can the information and data result in harm?",4
post50hb,richly branching,1.6001453961458911,highest,You can say almost anything these days and someone will misinterpret it and get offended just because,4
post50hb,richly branching,1.6001453961458911,highest,[removed],3
post50hb,richly branching,1.6001453961458911,highest,"They didn’t say that it was good or bad, just that the info could be good.",3
post50hb,richly branching,1.6001453961458911,highest,"I think your idea is mostly correct, but bad AI does exist, mostly when it gives incorrect information. For example, an AI trained to make hiring decisions is almost definitely bad because it was probably trained on previous hiring decisions made by biased human raters. Garbage in, garbage out, resulting in a bad AI.

So I agree with your point that AI is just information, but bad data selection makes for incorrect inferences, which I would definitely call ""bad AI.""",3
post50hb,richly branching,1.6001453961458911,highest,"That might be the concern, especially if our government gets a hold to it",3
post50hb,richly branching,1.6001453961458911,highest,"That's what I was thinking. I'm struggling to understand how it would have biases when it's just interpreting the data given. Wouldn't biases come from how the data is used? And even then, the AI biases wouldn't be racially or sexist in the same way as it would be for us, since it wouldn't have the social constructs that we (humans) use.

This article confused me.",3
post50hb,richly branching,1.6001453961458911,highest,"I guess some AI not only finds patterns but is given directions on what to do with them and those directions are based of human interpretations of right and wrong and good and bad so the AI does what it's told and even though it comes to the correct answer given the assignment,the assignment itself is biased.",4
post50hb,richly branching,1.6001453961458911,highest,"I think the issue is it could be a good thing if used for positive reasons, but in general, most tech gets used for as many bad things as it does good, and given how AI doesn't explain how it gets to an answer, it makes it a lot harder to remove bias.",2
post50hb,richly branching,1.6001453961458911,highest,This isn't really a breakthrough. Identifying things like race and gender by looking at bone structure is an already possible thing,3
post50hb,richly branching,1.6001453961458911,highest,"Yeah. Say we have a disease where most white people go to hospital, and most black people ""tough it out"" and don't go. The AI will see the hospital stats and say ""oh, this disease only affects white Americans with health insurance."" it might even give a false negative if someone isn't of the expected race.",3
post50hb,richly branching,1.6001453961458911,highest,"Definitely a good thing, best suited medication can differ depending on race. It's currently a big problem in the current way of drug research since racial difference is rarely considered in drug trials, and so minority patients might be taking a medicine that was designed based on trials made mostly on the majority and not work as well for them.

 AI being able to distinguish races should give it an advantage in drug discovery to find the best possible medication for each race instead of the current ""one drug fits one but given to all"" way.",2
post50hb,richly branching,1.6001453961458911,highest,Sickle cell is only a “race” disease because most Black Americans come from coastal West Africa and most White Americans aren’t Greek or Italian. Relatively few traits follow popular racial classifications on a global scale.,2
post50hb,richly branching,1.6001453961458911,highest,"Yup, sickle cell isn't racial but geographical. A better example would be skin cancer predisposition.",3
post50hb,richly branching,1.6001453961458911,highest,"Still, only a tiny minority of genetic traits correspond with appearance or racial categories.",4
post50hb,richly branching,1.6001453961458911,highest,"The variations in diseases you are talking  about are only spuriously related to race, and are better tracked and analyzed by other variables. Alot of black and brown folks have ancestors from Africa and the Middle East where sickle cell trait is selected for due to its protective factor against malaria (malaria being common in these regions). Thus, what you should actually be looking for is African and Middle Eastern ancestry, not what race the person is perceived as (all racial groups can have this ancestry). Heart disease variation is explained by socioeconomic factors that impact racial groups differently, mostly as a result of systemic issues like food access, heathcare access, environmental racism, etc, not as a result of anything biological or genetic.",2
post50hb,richly branching,1.6001453961458911,highest,It boggles my mind how uneducated redditors get upvoted this much.  Thank you for eloquently showing this guy race is not the root cause for these diseases,3
post50hb,richly branching,1.6001453961458911,highest,"The problem is that outcomes for people of color are generally worse.  AI could be perpetuating those outcomes.  Yes, AI detecting race, and acting accordingly, absolutely could be beneficial.  Though, AI tends to pick up biases that already exist.  There are already disparities based on race in the medical care industry.  A tricky balance, for sure.",2
post50hb,richly branching,1.6001453961458911,highest,"Second that, I thought the same.",2
post50hb,richly branching,1.6001453961458911,highest,"This is the EXACT problem they’re worried about. Diagnosis should NEVER be racially based. A patient in not a phenotype, they are an individual.

Looking for an issue in one pt because of their race means you’re ignoring it in others because of their race. 

[https://www.changeforscd.com/beyond-vaso-occlusive-episodes-complications/racism-discrimination](https://www.changeforscd.com/beyond-vaso-occlusive-episodes-complications/racism-discrimination)",2
post50hb,richly branching,1.6001453961458911,highest,">Diagnosis should NEVER be racially based. A patient in not a phenotype, they are an individual.

...

An individual is defined in large part by their phenotype, and this goes doubly so for medically diagnosing them.

Your statements aren't incompatible.",3
post50hb,richly branching,1.6001453961458911,highest,"How did this get upvotes? Race is not deterministic for these genes, it’s bottle necked genetics.  Sickle cell anemia only exists because of malaria, which has little to nothing to do with race, but location.",2
post50hb,richly branching,1.6001453961458911,highest,AI about to turn into a racist,2
post50hb,richly branching,1.6001453961458911,highest,If they check me for sickle cell they better not come with a fucking x ray,2
post50hb,richly branching,1.6001453961458911,highest,"Even by sex, heart attack symptoms for example.",2
post50hb,richly branching,1.6001453961458911,highest,"It is generally. But there are other instances of bias in AI taught by humans using their own subconscious biases. We need to make sure it doesn’t affect how the AI processes that information when it comes to radiographic conclusions ne differential diagnoses.
Medicine has historically screwed over minorities and women to an insane degree. We can’t allow AI to further that rift.",2
post50hb,richly branching,1.6001453961458911,highest,"Same with blood types. Jka is a good indicator, and there is also the Bombay phenotype. which is incredibly rare and mostly isolated to a single city.",2
post50hb,richly branching,1.6001453961458911,highest,"The scientists aren't saying, ""oh no, the machine can see race, that's bad."" They're saying, ""maybe the machine seeing race is part of how it's underperforming for black people.""

They're not implying the solution is to make the machine unable to see race. They're saying they need to figure out how race plays into what the machine sees, and hopefully use that to improve the machine before rolling it out.",2
post50hb,richly branching,1.6001453961458911,highest,"It shouldn't matter. In UK, where everyone gets access to necessary healthcare for free, race is certainly a factor in diagnosis because some races have predisposition. For example, obesity has a lower threshold for Asian males because they benefit from treatment if applied at a lower threshold.
However if the system is biased and people of one race tend to get worse treatment and less pain control, then AI could perpetuate this. The AI isn't biased, but it will respond to the data it's fed to create its models",1
post50hb,richly branching,1.6001453961458911,highest,It’s like how gender is less important than sex in a medical emergency,2
post50hb,richly branching,1.6001453961458911,highest,"There’s a *lot* more nuance to it than that, trans peoples heart disease and cancer characteristics are the same as their gender not their birth sex if they’re on HRT for instance. For instance, MtF trans women have similar rates of breast cancer to cis women, and negligible (I think there’s still currently 0 recorded cases but I could be wrong) rates of prostate cancer. The human body is crazy, and it turns out that a *lot* of things are based on your current endocrinological profile, whatever your politics are aside it’s a very interesting topic.",3
post50hb,richly branching,1.6001453961458911,highest,"That has absolutely nothing to do with gender, but has to do with the fact that they are taking loads of hormonal medications with powerful side effects. I take estrogen birth control, as a woman this will decrease my chances of cervical cancer, but increase my chances of breasts cancer and heart disease, all compared to a woman who hasn't taken BC.

Also, huge CITATION NEEDED for trans people having similar rates of cancer after taking cross sex hormones, there are no studies I've seen been able to track long enough to even begin to touch on that subject, but if you have something I'd read it. Sure, hormonal suppression can prevent hormonal cancers, medicine does this all the time for prostrate and breast concerns, but you don't just magically get the other sex's cancer with cross sex hormones.",4
post50hb,richly branching,1.6001453961458911,highest,"> There’s a lot more nuance to it than that, trans peoples heart disease and cancer characteristics are the same as their gender not their birth sex

Any source on this? Because it sounds like major bullshit.",4
post50hb,richly branching,1.6001453961458911,highest,"Exactly, if the differences are used to make people healthier and supply better treatment options then it’s a good thing. Custom healthcare tailored to your specific needs is the future.",2
post50hb,richly branching,1.6001453961458911,highest,"What does how much you pay for healthcare have to do with your point, out of curiosity?",2
post50hb,richly branching,1.6001453961458911,highest,"The only reason race is a factor in some countries is that doctors are predisposed by society to categorise people based on it, so they can use it to make certain distinctions. An AI shouldn't be based on Early Modern Age pseudo-science to make predictions, it should make better categorisations using indicators that make more medical sense.",2
post50hb,richly branching,1.6001453961458911,highest,[deleted],2
post50hb,richly branching,1.6001453961458911,highest,"The races are social constructs, but that doesn't mean they can't be statistical indicators for things. We've been doing both enforced and self segregation among our various races for centuries. It's not terribly surprising that we've managed to self sort some genetic conditions into out artificially selected ""races"" as a result.

For example sickle cell anemia is bad, but it also helps prevent malaria (both diseases deal with red blood cells). As a result it evolved to be more common in high malaria areas because evolutionarily, not dying of malaria is more beneficial than the downsides of the genetic disorder that probably won't kill you before child bearing age at least. Transplant a bunch of people from, oh say Sub-Saharan Africa to North America, and then socially and legally make it unacceptable for them to mix with people outside of their group for a few hundred years and you end up with African Americans as a group being much more likely to have that particular disorder.

Now here's the thing to understand, it isn't ""being black"" that makes them likely to have sickle cell. The melanin content of their skin isn't a risk factor. But for the reasons I just explained a person of African decent is statistically more likely to suffer from that particular genetic condition due to long term social reasons. If we start mixing black and white ans brown and whatever color people freely eventually the disease would become equally prevalent along all the ""races"".

In the meantime though, it's probably wise for doctors to keep in mind that for certain patients be more on the lookout for certain symptoms. We don't want to screw up getting someone proper care trying to be ""color blind"" or whatever.",3
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,"Yes, race is a made up culture devide. However, there is a difference between people based on their descendents. So would you prefer the word ""Ethnicity""?

African American are predisposed to things that Africans are not, Northern Europeans have a tendency to be able to process lactose, that a lot of the rest of the world has less a tendency to.

We need a word to set these groups up, race is not great, but it does sorta kinda hit the correct spot, ethnicity is better, but has a bunch of cultur burden.",3
post50hb,richly branching,1.6001453961458911,highest,"Fun facts: 1) Homo evolved out of Africa through mosaic evolution. 2) The lactose gene mutation occurred in Europe after decades of adults not being able to digest milk. Children’s guts usually age out around 7 or 8 years old. 

As a medical anthropologist who studies culture and how it intersects with medicine. 1) there is no such thing as biological race and I definitely think this would further biases in medicine that already exist. 2) There definitely is a predisposition for certain health issues in certain ethnicities. That being said, you have to look at the social determinants of health of these groups. Do these people have access to safe place to exercise? Can they walk in their neighborhood? Do they have access to fresh foods at a reasonable price? Do they live in an area with clean water and air? Can they access medical care for prevention rather than reaction? 
Doctors still show medical biases to women and BIPOC persons. Racism causes health problems of its own. Doctors ignore symptoms, misdiagnose with a lack of care, and a study done showed that a large portion of medical students surveyed still perceived a difference in level of pain felt and that these persons have higher pain tolerances. This leads to a lack of pain meds and the chance the doctor with think the patient is “acting”. 

https://www.pnas.org/doi/10.1073/pnas.1516047113",4
post50hb,richly branching,1.6001453961458911,highest,Any sources?,3
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,Everyone? Necessary? Who decides what is necessary? Lmao,2
post50hb,richly branching,1.6001453961458911,highest,Some nut job government is gonna use an x ray targeting system and some drones to weed out their population,2
post50hb,richly branching,1.6001453961458911,highest,"If an AI is twice as likely to successfully detect (say) cancer in a white person than a minority, and we then implement AI throughout the NHS without identifying or adjusting for the bias, it does matter.",2
post50hb,richly branching,1.6001453961458911,highest,"Concerned about what exactly? How exactly could the AI, or any algorithms feeding off its output, be racist here in a way that negatively affects anyone?",1
post50hb,richly branching,1.6001453961458911,highest,"Basically, if we want the AI to „correctly diagnose“ diseases, we need to teach which diagnoses are correct. These diagnoses however can have a bias.

Imagine a world where no person with colourful hair ever gets treated for or diagnosed with sunburn. The AI is trained on the compiled data of thousands of diagnoses. It might recognise the same markers in people with colourful hair, but every time it marks them it gets told „wrong, no sunburn“. So it learns that people with colourful hair never have sunburn, and will never mark them as such.

The AI isn‘t racist as in „it hates them blacks“, it just perpetuates the biases in the dataset it was trained on, be they good or bad.",2
post50hb,richly branching,1.6001453961458911,highest,"I understand what you're saying, but i dont think that applies here. You have an AI that can detect race based on x-rays. How would an AI that can't detect race based on x-rays be better in any case? 

If there is racial bias in the data that is used to train the AIs, then the AI will learn that racial bias. Being able to detect race is not racial bias though.",3
post50hb,richly branching,1.6001453961458911,highest,"I don't think the issue per-se is about ML models being able to detect race in a dataset or it being used in a nefarious way. 

The problem is that the model supposedly encodes an assumption about the race of an individual when it's given an X-ray image. This means that it could take the X-ray of a person of one race and it could mistakenly encode some hidden assumption that the person's bone structure is similar to that of some other race in the image's representation. 

The performance of the model is then tied to distribution of X-ray image data for different races and this *could* hamper performance if it's used in conjunction with other systems that rely on race information. It becomes harder to trust the model's output for an X-ray image of a race it's not trained on.",4
post50hb,richly branching,1.6001453961458911,highest,"Here is the piece you are missing. If the AI can detect race from X-rays, that means that race-based correlations and biases present in diagnostic data can affect an AI diagnosis. Humans are unable to identify race from X-rays, thus the researchers had assumed that a diagnosis based solely on X-rays would be free of a racial bias. They found some evidence suggesting that this wasn't the case, and attempted to identify race via X-ray. The sole reason this study was conducted was that they found evidence of racial bias at the level of AI diagnosis. So yes, it is concerning that the AI can detect race from X-rays. It implies that we cannot rely on AIs to provide an unbiased diagnosis, even when we cannot fathom how that bias could occur.",4
post50hb,richly branching,1.6001453961458911,highest,"I‘m not saying there is :) The question was, how could such a thing negatively affect anyone. That‘s what I tried to answer :)",4
post50hb,richly branching,1.6001453961458911,highest,"The scientists aren't saying, ""oh no, the machine can see race, that's bad."" They're saying, ""maybe the machine seeing race is part of how it's underperforming for black people.""

They're not implying the solution is to make the machine unable to see race. They're saying they need to figure out how race plays into what the machine sees, and hopefully use that to improve the machine before rolling it out.",4
post50hb,richly branching,1.6001453961458911,highest,[removed],3
post50hb,richly branching,1.6001453961458911,highest,"Apologies for my ignorance, but is ""colourful hair"" another way to say ""red hair""?",3
post50hb,richly branching,1.6001453961458911,highest,it's just an example of someone that can be identified as such. could be anything really . in this case it's race,4
post50hb,richly branching,1.6001453961458911,highest,"I didn’t wanna use any hair colour, so I thought I‘d say dyed hair. Came out wrong lol",4
post50hb,richly branching,1.6001453961458911,highest,I assumed colorful hair was like green or purple.,4
post50hb,richly branching,1.6001453961458911,highest,"Hey, you’re not allowed to use the r-word!",4
post50hb,richly branching,1.6001453961458911,highest,Underrated comment here.  Well summarized.,3
post50hb,richly branching,1.6001453961458911,highest,"This! In the article it essentially states what you are saying here. Due to these biases, AI can select not to diagnose certain races once identified if these biases are not studied further and understood. This should be very concerning similar to AI’s inability to facially recognize Asian people in other studies. Data can be racially biased therefore making the ability to identify race based on X-Rays a problem instead of a benefit. This is my understanding of the article.",3
post50hb,richly branching,1.6001453961458911,highest,I would assume the AI would be smart enough to not say “can’t be sunburn” but instead “sunburn less likely”. For different races I don’t think there any diseases or issues that are all or nothing. Just some that are more/less likely to varying degrees.,3
post50hb,richly branching,1.6001453961458911,highest,Yupp! I was just oversimplifying greatly for ease of understanding. These nuances are really important when reading further into the topic though! Thanks for bringing it up!,4
post50hb,richly branching,1.6001453961458911,highest,"Well then your ML data needs to be retrained. You repeat until two datasets return the expected reponses repeatedly. This is nothing new, just another data point. Fluff article.",3
post50hb,richly branching,1.6001453961458911,highest,"Sounds a lot like how COVID symptoms and demographics were selected in the beginning of the pandemic. They had no clue who was actually at risk because of all the old people that were grouped together in New York and died. Skewed the whole data set from the beginning and made the death rate high enough to consider COVID dangerous. Then for the treatments they thought things worked because people who took them recovered but they were actually later changed because they didn't help people at all.

Initial conditions really have a lasting relevance when a system is being created from nothing. Hopefully they figure out how to properly setup the data to prevent wrong diagnosis.",3
post50hb,richly branching,1.6001453961458911,highest,"Aaaand let’s say this AI does become a racist, toothless bully. I know the solution. We can contribute code to break it and stop the terror. Easy!",3
post50hb,richly branching,1.6001453961458911,highest,"> These diagnoses however can have a bias.

Yeah, like have a massively improportional diagnosis of testicular cancer in men as opposed to women.  Huuuuuuuge bias. 

But AI with these trainings sets really will perpetuate any sort of wrong bias that gets into the training set.   The solution is not to hobble the AI and lobotomize them, but rather FIX THE DATA so they're properly trained.  Always side with the truth. The truth will set you free.",3
post50hb,richly branching,1.6001453961458911,highest,Yupp. I remember when someone (Google?) trained an AI to make hiring decisions and it ended up racist. Bias in the data -> bias in the AI.,4
post50hb,richly branching,1.6001453961458911,highest,"Let's say your AI that you implemented to replace credit scores to pick out the best ppl to give mortgages to independently concluded that it was most profitable to just blanket reject all ppl of a certain specific historically socioeconomically disadvantages ethnicity, and it wasn't wrong, and it wasnt trying to be racist on purpose.  What are you gonna do with this information?  What are you even legally able to so with this information?",2
post50hb,richly branching,1.6001453961458911,highest,"Fair enough. But anyone designing these systems then should decide responsibly what input data to even feed into the system. And the data it is trained on. 

In the case of detecting perceived ""race"" from skeleton images, we shouldn't really be surprised. Or overly concerned imo.",3
post50hb,richly branching,1.6001453961458911,highest,"Its being used for pathology. And there is variance in efficaciousness between ""races."" If you depend on a system like this and you don't correct for that,  the system becomes racist.

Also, I dont think that the word racist was used the article.",2
post50hb,richly branching,1.6001453961458911,highest,"\> The study adds to a growing body of data that AI systems can often replicate human biases and prejudices, whether they be racist, sexist, or otherwise.  


Yeah, it was.",3
post50hb,richly branching,1.6001453961458911,highest,"I don't understand, is it racist to simply point out that one person's skin color is different than another? Is it racist to point out that the same person has a relatively larger/smaller femur on average? Are we trying to pretend that different races didn't come from different paths of evolution?",3
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,"The article states that implict bias may be brought in to the design of AI. This is for any phenotype. Its _____ist to not correct for implicit bias when it is known.

And of course people are different. Thats a core aspect of this article.",4
post50hb,richly branching,1.6001453961458911,highest,"Different races did not come from different paths of evolution, and that erroneous belief is the first fucking thing people are worried about reinforcing. Racial classification is based on phenotypical traits like skin tone, hair texture, nose and eye shape, etc, and almost entirely arbitrary (look up Nat Geo fraternal twins of different ""races"" as an example). The variations the x-rays are picking up are more than likely correlated with a ton of other factors.",4
post50hb,richly branching,1.6001453961458911,highest,"Yeah the scientists aren’t worried that their AI is racist, as far as I can tell

Rather they’re worried that having race be a factor could mean different outcomes for different races due to the additional input, which means some people could get worse care",3
post50hb,richly branching,1.6001453961458911,highest,">If you depend on a system like this and you don't correct for that

What does ""correct for that"" mean?

How do you know your corrections aren't even more problematic than the original 'biases?'",3
post50hb,richly branching,1.6001453961458911,highest,"That seems like semantics or a thought exercise more than anything productive. 

I think that the philosophical goal is to predict every single illness or disease with 100% accuracy. Until you get there, there is work to be done. If patients of particular ""races"" are further or closer to 100% than others, then there are missing data or biases that make it more or less accurate. So correction is needed.

If correction is the wrong word, have that point and help me use a term that makes this more comfortable",4
post50hb,richly branching,1.6001453961458911,highest,[deleted],2
post50hb,richly branching,1.6001453961458911,highest,"It's not that hard to predict someone's race as a human, no? If people wanted to predict race, well we had the tech do that algorithmically 15 years ago. Someone's perceived race was, by definition, never really private information.",3
post50hb,richly branching,1.6001453961458911,highest,"Well, an AI is spawned from the input it receives. So if a pool of information is presented, it can only calculate as it learned.   
Throw 2 random groups together; an AI can identify (group 1) as 100% ""normal"" vs (group 2) 99.9% ""normal"". Couldn't or wouldn't an AI separate that pool in some way from its baseline? ..then further presume that group 2 is flawed because it was not within the baseline study pool?    
This may not seem like an issue unless people in group 1 came from Northeastern Asia (also happens to be where the AI was developed) vs. group 2 that came from the continent of Africa. All unintended skewing of what we identify as equal information, just seen with a superior observing ability. An AI *could* outlearn us and make a separation without us ever knowing. Seemingly minor variables from our learning curve in programming alone may result in unecxpected discoveries or conclusions in any long-run.",2
post50hb,richly branching,1.6001453961458911,highest,[removed],2
post50hb,richly branching,1.6001453961458911,highest,"Being this sure of yourself about things you didn't study is honestly dangerous. And no, watching youtube videos of a redpill highschool graduate doesn't count. Dunning-kruger on full effect right there.

For instance, what is black and white people? Are Italians white? Because about 40 years ago white supremacists didn't think them as white. And where does black start or end? There are ""whites"" that didn't interact with other whites for thousands of years before globalization. There are millions of factors affecting iq, brain size, bone/muscle density and height other than genetics. Food culture, soil that food grows on, air quality, culture itself and healthcare are all more dominant factors.",3
post50hb,richly branching,1.6001453961458911,highest,"It already happens in some places in United States.I believe , algorithms used to allocate policing resources but based on algorithms of crime in those areas for last 40 years or whatever ,but is prejudiced against the current generation in those areas.",2
post50hb,richly branching,1.6001453961458911,highest,"Not a problem with the technology itself, but the people using it. And this ""discovery"" won't change that. If we want to fight racism effectively we need to focus on educating people more than we do the AI that they use. Until AGI, at least.",3
post50hb,richly branching,1.6001453961458911,highest,A racist AI? Fuking computers and its codes are rayyciiisssttt,2
post50hb,richly branching,1.6001453961458911,highest,Writer is a sheltered idiot with a rigid perspective.,2
post50hb,richly branching,1.6001453961458911,highest,*China has entered chat*,2
post50hb,richly branching,1.6001453961458911,highest,It doesn't align with their political view,2
post50hb,richly branching,1.6001453961458911,highest,"Maybe they have AI watching us through x-ray cameras but they don't want to admit it: 

""oh no, this AI can tell race from x-ray, they might discriminate between races""

""why would that be an issue except after you got an x-ray? it's not like we're constantly being surveilled with x-ray cameras during interactions which would allow for discrimination or anything, is it?"" 

""...""",2
post50hb,richly branching,1.6001453961458911,highest,"Well, at least we can see what happens in this thread : an ai is trained to categorize based on certain caracteristics, and a fuck ton of people immediately conclude that the categories aren't constructed. The ai is fine, but people already use it to feed their confirmation bias.",2
post50hb,richly branching,1.6001453961458911,highest,I think the article is implying that doctors are concerned because humans can't predict the race of someone just by looking at x-rays and it may lead the AI to have a racial bias towards treatment plans/diagnosis if implemented.,2
post50hb,richly branching,1.6001453961458911,highest,"From the article:

“ Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research. Scientists must first figure out why this is happening.”",2
post50hb,richly branching,1.6001453961458911,highest,It also ignores the fact that doctors already apply racial bias (and bias along other lines such as sex) when diagnosing and treating patients.,2
post50hb,richly branching,1.6001453961458911,highest,"I feel like it could be evidence that racial bias actually can effect a person's treatment and health. It's scientific support that bigotry isn't ""politics"", it has physical consequences.",2
post50hb,richly branching,1.6001453961458911,highest,"The scientists aren't saying, ""oh no, the machine can see race, that's bad."" They're saying, ""maybe the machine seeing race is part of how it's underperforming for black people.""

They're not implying the solution is to make the machine unable to see race. They're saying they need to figure out how race plays into what the machine sees, and hopefully use that to improve the machine before rolling it out.",2
post50hb,richly branching,1.6001453961458911,highest,"It's more so that AI has a tendency to perform more poorly with ethnic minority related data, since ethnic minorities are minorities and therefore have generally less data to train AI. 

It's not usually bias, but underperformance that is the problem here. Of course, there is always the potential for the users of an AI to use its output in a discriminatory way.",3
post50hb,richly branching,1.6001453961458911,highest,"Indeed, underperformance is ""the problem"". You might even call it ""a concern"". It's really an open-ended question of, can we figure out why the models are underperforming, exactly? Maybe the explanation will point to other ways they underperform? 

I feel like people are responding to this article as if the takeaway was, ""stop! It's going wrong!"" When in reality the takeaway is, ""okay, we're getting there slowly, not quite ready yet.""",4
post50hb,richly branching,1.6001453961458911,highest,"People in denial still trying to wrap their heads around the fact that humans can be categorized into different sub-species. 

They still think race is only ""skin-deep"".",2
post50hb,richly branching,1.6001453961458911,highest,"""Sub-species"" is a bit of a stretch imo. There are obviously differences between races but they really don't go much past a few cm on avg here, a bit more lactose (in)tolerance on avg there... 

But yeah, I'd agree that there's deeper differences than skin for sure.",3
post50hb,richly branching,1.6001453961458911,highest,"A good example of this was Amazon's AI based resumé assessor, which was found to be disproportionately rejecting female applicants with excellent grades and high levels of experience even though the gender of the applicants not know known the AI.

What was happening was the real world dataset had bias against women (not surprising in Tech https://gender.stanford.edu/news-publications/gender-news/why-does-john-get-stem-job-rather-jennifer , https://www.yalescientific.org/2013/02/john-vs-jennifer-a-battle-of-the-sexes/) and the AI was trying to match the real-world dataset. 

It didn't have the applicants sex but sex was the hidden variable which meant that certain good candidates in the historic dataset were being rejected, so the AI learned to infer this hidden variable, sex, from secondary signifiers (what school people went to, what clubs they belonged to, were you the in Woman's chess club etc). The AI became a *woman detector* and in fact ended up more efficiently biased than its human counterparts. 
https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G
 

It's basically important because if the AI can detect race, it's able then correlate any race based biases that already exist in the medical decisions into it's inferences, even if you don't know how it's doing it.
https://www.wired.com/story/how-algorithm-blocked-kidney-transplants-black-patients/",1
post50hb,richly branching,1.6001453961458911,highest,"It's incredible that we can teach AI to be racist or sexist like us. It also supports the idea that racism and sexism are social concepts that we teach our children, often subconsciously.",2
post50hb,richly branching,1.6001453961458911,highest,beep boop I learned it from you dad.,3
post50hb,richly branching,1.6001453961458911,highest,Like it's also a really interesting way of proving sexism or racism in the training dataset if the AI no matter what the combination of primary data characteristics are it prioritizes sex or race in its decision making.,3
post50hb,richly branching,1.6001453961458911,highest,Like it's also a really interesting way of proving sexism or racism in the training dataset if the AI no matter what the combination of primary data characteristics are it prioritizes sex or race in its decision making.,3
post50hb,richly branching,1.6001453961458911,highest,">What was happening was the real world dataset had bias against women (not surprising in Tech)  
   
You must not work in tech recruitment because female developers are way more sort after than male developers (with equivalent experience). Most large (or largish) tech companies have explicit policies favouring female techies in recruitment and have had these policies for quite some time.",2
post50hb,richly branching,1.6001453961458911,highest,"Dude just read the article, Amazon specifically said when they removed Female signifiers from the applications the AI automatically rated them hire because the Amazon hiring dataset contained that bias, Womens chess team captain for Men's Chess team captain and suddenly the Résumé is rated higher, why because recruiters rated applications consciously or subconsciously lower when it was a woman's and the AI picked up on that.

This may surprise you but what companies say is not necessarily how things actually pan out...",3
post50hb,richly branching,1.6001453961458911,highest,"But in this article it was just looking at x-rays of bones, not say, recommending treatment plans based on what it knows humans have recommended before which are obviously subject to biases.   And it was able to identify a race based on that, when doctors couldn't even do it.",2
post50hb,richly branching,1.6001453961458911,highest,"It's not that this is immediately concerning, people are just starting the conversation about what this could mean for us going forward. 

Multiple AIs have been taught racism already, and it happens faster than the creator can control or without the creator purposely trying to teach it racism. It's scary to think that a computer can decide your fate based on racism. And a computer has no reservations about choosing death for someone like a human would. 

It shows us that we need to examine our unconscious biases constantly.",3
post50hb,richly branching,1.6001453961458911,highest,[deleted],3
post50hb,richly branching,1.6001453961458911,highest,"Though not super accurate coroners for example are because it helps ID mystery skeletons. Though weirdly it wasn't the structure of the skeleton that the AI was using as it could still tell race with a blurry x-ray it seemed to be using something to do with how the X-rays were being absorbed by melanin in the subjects skin, but the effect would have to be absolutely tiny as melanin barely absorbs x-rays at all.",4
post50hb,richly branching,1.6001453961458911,highest,"The concern is that the AI was detecting race in an unexpected way so had you used this system as part of broader assessment activity you would have to be aware that it was capable of making this inference. 

We know medical datasets have biases in them (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4843483/) knowing that the x-ray AI can figure out race that means using in a broader assessment would mean it would likely, as per the Résumé AI enhance biases as soon as if figured out the hidden variable of why certain people were recommended different treatment was race (even though of course it has no idea what race is) 

The difference between a neutral and biased AI would be deadly for a lot of people https://www.bmj.com/content/370/bmj.m3315",3
post50hb,richly branching,1.6001453961458911,highest,"""The miseducation of algorithms is a critical problem; when artificial intelligence mirrors unconscious thoughts, racism, and biases of the humans who generated these algorithms, it can lead to serious harm.""

""Using both private and public datasets, the team found that AI can accurately predict self-reported race of patients from medical images alone. Using imaging data of chest X-rays, limb X-rays, chest CT scans, and mammograms, the team trained a deep learning model to identify race as white, Black, or Asian —""
https://news.mit.edu/2022/artificial-intelligence-predicts-patients-race-from-medical-images-0520

A couple things to consider here. First being that researchers do not think the AI's predictive abilities is a good thing. They see it as a problem. 

Secondly the race of the individuals is self reported and broken down into 3 broad groups White, Black, & Asian. This matters as race isn't a strict scientific discipline. For example what race is Barrack Obama, bi-racial? Okay, what race are his daughters? Humans have been gene swamp for as long as we've been human.",1
post50hb,richly branching,1.6001453961458911,highest,I can't answer that without a good look at their skeletons.,2
post50hb,richly branching,1.6001453961458911,highest,"> This matters as race isn't a strict scientific discipline.

Understatement of the year here. Especially considering that the AI knows 3 categories: Asian/White/Black. If you're just a little bit educated about human genetics you'll see how dumb this is.",2
post50hb,richly branching,1.6001453961458911,highest,lmao imagine an ai that can generate an organism's genetic code by looking at it,3
post50hb,richly branching,1.6001453961458911,highest,"The thing is that when I read these articles they always seem like ""extreme woke"", written poorly (og article) or not, but it is probably because the quotes are really small or the language is just not enough.

For example, it says algorithm generators can be biased at the start, it probably doesn't mean that the algorithm itself is generated biased but not used in non-biased way.

Or when one of the paper authors says that we need to include social sciences, I don't see how this is relevant to ""bone structure"" at all, but it is relevant to diagnoses as meta-analysis.

It is very weird that I didn't see Clever Hans effect completely ruled out though. I wouldn't be surprised that it is in fact x-ray imaging artifacts that produces most of the difference. But even then 0.96 AUC is too high.

Thanks for the better article, let's see what reverse engineering will bring.",2
post50hb,richly branching,1.6001453961458911,highest,">I don't see how this is relevant to ""bone structure"" at all, but it is relevant to diagnoses as meta-analysis.

From the article I linked:
""When an AI used cost as a proxy for health needs, it falsely named Black patients as healthier than equally sick white ones, as less money was spent on them.""

When AI makes a bias diagnosis it can negatively impact the direction care takes.",3
post50hb,richly branching,1.6001453961458911,highest,"Race is not a biological concept.  It is a completely social construct.  Ancestry is biological.  The nebulous thing called race is some social construct loosely based on ancestry.

Edit: for those downvoting let me explain. You need to separate ancestry (where your recent ancestors came from) which indeed has solid biological basis, from race (which is this nebulous concept, that's poorly defined, has a lot of social meaning to it that changes from place to place, and era to era, and honestly has no biological basis whatsoever). Ancestry is biologically and medically relevant. And sometimes, we use race as an imperfect but sometimes useful proxy for ancestry.",2
post50hb,richly branching,1.6001453961458911,highest,So how can an AI system detect a social construct in an xray in your opinion?,3
post50hb,richly branching,1.6001453961458911,highest,"From the MIT research link I provided in my previous post:

""When an AI used cost as a proxy for health needs, it falsely named Black patients as healthier than equally sick white ones, as less money was spent on them.""

AI is programed by humans and humans have bias. Humans unknowingly program their bias into AI..",4
post50hb,richly branching,1.6001453961458911,highest,"it's not. It's detecting ancestry. Race is a social interpretation loosely built on top of ancestry. Race has no basis in biology whatsoever. It's purely a social construct that is sometimes used as a proxy for ancestry.

Edit: on second thoughts, this response was incomplete. If all the picutre are otherwise identical, it might indeed be detecting only ancestry-influenced features. But it is still possible the AI is detecting race, independent of biology. Imagine this scenario, one set of pictures are coming from under-resourced hospitals with low-quality xrays that predominantly serves one racial group, and another set is coming from a higher resourced hospital with better quality pictures, the AI can indeed be detecting that difference in quality of xrays in this case  which is highly correlated with race, and still has absolutely nothing to do with biology.",4
post50hb,richly branching,1.6001453961458911,highest,"Because this man forgets that mongoloid, caucazoid, and negroid skeletons are as different as men and women skeletons. You Can identify a persons race based off their skull - you can identify a persons biological gender based off their hips. 

So an ai can do that - good. It should be able to.",4
post50hb,richly branching,1.6001453961458911,highest,"It would be more accurate to say that the various races are ancient shorthand for heritable traits, with very low granularity.  Racial categorization isn't completely arbitrary, it's just not specific enough to be useful in this era of scientific rigor.

If you say, 'this dude's white', only socially relevant information is communicated.  If you say, 'this man has Nordic features', that's something an AI can work with (even though it's still broadly categorical).",3
post50hb,richly branching,1.6001453961458911,highest,">It would be more accurate to say that the various races are ancient shorthand for heritable traits, with very low granularity. Racial categorization isn't completely arbitrary, it's just not specific enough to be useful in this era of scientific rigor.

&#x200B;

I almost agree, except what we call race has so much social baggage that trying to tie to biology in any rigorous manner is a futile effort. Race as a concept has a social origin and serves a social need. Yeah  we use it as a proxy for heritable traits (or what I call ancestry). It's somewhat useful, but still a very imperfect proxy (what you call shorthand). It's not just about granualrity. But I feel very strongly, that it is absolutely necessary to call out that race is a purely social construct, with no biological basis. Not doing so, allows racists or and racial supremacy theorists to  try to pretend  all their silly racial biases has basis in biology when it just doesn't. 

&#x200B;

>If you say, 'this dude's white', only socially relevant information is communicated.  If you say, 'this man has Nordic features', that's something an AI can work with (even though it's still broadly categorical).

Norfic features  = ancestry ( has some biological/inheritable basis)

white = race",4
post50hb,richly branching,1.6001453961458911,highest,Care to look up what phenotype means? Or did you fail biology and just act like you know things?,3
post50hb,richly branching,1.6001453961458911,highest,Phenotype is not a good model of biological classification. If it were fossas would be felines.,4
post50hb,richly branching,1.6001453961458911,highest,"The are many groups that share phenotypes despite not being in the same ""race""",4
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,"Race depends much more on social factors than it does phenotype. For one such example, back when Irish immigration to America was a controversial political issue, Irish people were not considered to be white.",4
post50hb,richly branching,1.6001453961458911,highest,"most people don't consider obama a mixed race guy. He was considered the first black president.

For all , mixed or something that tends to the dark side is considered black. I value your point, but there's typically no issue labeling people into a race",2
post50hb,richly branching,1.6001453961458911,highest,">but there's typically no issue labeling people into a race

No issues only than it generally being unscientific and arbitrary. The amount of melanin in Obama's skin doesn't diminish the amount of European genes he inherited from his mother. 

As for why Obama is considered black it is because by law throughout most of the U.S. until the 90's one could only be a single race on a birth certificate. Not just that but bi-racial children with a white parent were automatically listed as the race of the non-white parent. It wouldn't have mattered if Obama had light skin. By law he was born black.",3
post50hb,richly branching,1.6001453961458911,highest,"If it's unscientific and arbitrary, how come it's identifiable in an x-ray?",4
post50hb,richly branching,1.6001453961458911,highest,"The genetic distance between homogeneous Africans, Europeans, and East Asians is about as far apart as that between wolves, dogs, and coyotes.

In any world where there was no holocaust, it would be uncontroversial to admit humanity is composed of several subspecies.",2
post50hb,richly branching,1.6001453961458911,highest,"uncontroversial in any world that didn't study this subject numerous times.

""87.6% percent of the total modern human genetic diversity is accounted for by the differences between individuals, and only 9.2% between continents. In general, 5%–15% of genetic variation occurs between large groups living on different continents, with the remaining majority of the variation occurring within such groups"" Jorde et al. 2000a; Hinds et al. 2005)""

Race is a social construct.",3
post50hb,richly branching,1.6001453961458911,highest,"Oh god not this bullshit article again. Genetic mutations that are unique to each gene pool on the different continents are what matter. In group diversity had nothing to do with measuring genetic distance between populations.

Stop falling for propaganda pretending to be scientific research.",4
post50hb,richly branching,1.6001453961458911,highest,"I don't understand how a machine correctly predicting race is perpetuating racial bias. It's like if it predicted bear vs cow and you said ""it's just reflecting human bias"". If something can be differentiated it's not a bias.",2
post50hb,richly branching,1.6001453961458911,highest,"From the MIT research article I liked:
""When an AI used cost as a proxy for health needs, it falsely named Black patients as healthier than equally sick white ones, as less money was spent on them.""

With regards to identifying the race of x-ray patients the AI is doing something it wasn't designed to do, in a way, and for reasons that aren't understood. Clearly biases have corrupted the algorithm somehow.",3
post50hb,richly branching,1.6001453961458911,highest,"The scientists aren't saying, ""oh no, the machine can see race, that's bad."" They're saying, ""maybe the machine seeing race is part of how it's underperforming for black people.""

They're not implying the solution is to make the machine unable to see race. They're saying they need to figure out how race plays into what the machine sees, and hopefully use that to improve the machine before rolling it out.",2
post50hb,richly branching,1.6001453961458911,highest,If there are genetic factors assumed to be linked to race genetic testing would be the method for identifying that. We have that ability. In the absence of actual genetic data what good do you think race as a data point serves?,3
post50hb,richly branching,1.6001453961458911,highest,"> In the absence of actual genetic data what good do you think race as a data point serves? 

I don't know.  To me, this is not the right question.

The problem begins with the observation that an AI tool was under-diagnosing black people, with no explanation for why. This is a good first step in finding an explanation, as it verifies that indeed race can be part of an AI model, even if it's one that only looks at X Rays of parts of the body assumed to have no racial information. Finding an explanation would increase our trust in other models that we assume work without racial data. The explanation could also lead us to better understanding other bugs/quirks of the system.

In this whole comment section, I'm struck that people don't seem to be taking this as open-endedly as I think they should. There shouldn't be, like, an aim or a super specific goal here. There's just unanswered questions, and a desire to learn/improve.",4
post50hb,richly branching,1.6001453961458911,highest,"As with the other article about this.... this is how anthropology works, race is much more to do with bone structure than skin tone",1
post50hb,richly branching,1.6001453961458911,highest,but I thought RacE iS JuSt a SoCiAL COnsTruCt??,2
post50hb,richly branching,1.6001453961458911,highest,Race includes certain sets of ethnic groups with different ancestral groupings. While race is a social construct those groupings are not.,3
post50hb,richly branching,1.6001453961458911,highest,"What if I told you that race is merely the title for those very real ancestral groupings?

The amount of mental gymnastics people go through in order to avoid a label for a categorical group is astounding.

It's like we have people claiming that there is no such thing as the color ""red"" or the color ""green"" because colors are subjective. Philosophically, that's completely true. Drawing a definite line between colors, particularly because they exist on a continuous spectrum, feels arbitrary. But science tells us that the wavelength of visible light varies and we perceive those as different colors. We can assign ""arbitrary"" categorizations to visible light and derive meaning from that, and oftentimes it's very beneficial. There's a reason why people stop at traffic lights billions of times a day without any confusion, despite the claimed arbitrariness.

This is literally no different than saying someone is African American, clearly denoting that they have African ancestral heritage, and are part of a grouping that differs from Europeans. This is nothing pernicious, harmful, or incorrect in using African American as the name of a racial category, which is based on 100% real and occasionally meaningful distinctions based on ancestry.",4
post50hb,richly branching,1.6001453961458911,highest,"The education system from K-12, especially college, has done wrong by lots of people these days. It's more than just skin deep. 

[Here's what I mean](https://imgur.com/a/LobmPvt)

Don't even get me started on IQ or other personality traits we can genetically mark by ethnic groups like altruism or creativity.",3
post50hb,richly branching,1.6001453961458911,highest,"combative hunt scale live makeshift cautious plant act escape distinct

 *This post was mass deleted and anonymized with [Redact](https://redact.dev)*",3
post50hb,richly branching,1.6001453961458911,highest,"> ThE bLaCkS ARE dIfFeReNt!! We ArEn’T rAcIsT fOr WaNtInG tHeM bAnNeD fRoM hAvInG rIgHtS!

Only actual racial supremacists believe that. The next step for human evolution is obviously CRISPR designer babies and increasing the IQs of all races to an above average mean. This is even more important with automation taking away more jobs and basically taking low IQ individuals and their descendants out of the work force.",4
post50hb,richly branching,1.6001453961458911,highest,"Because it is.

The AI is probably detecting regional differences.",3
post50hb,richly branching,1.6001453961458911,highest,"If you asked an anthropologist about the single ""black"" race, they'd laugh at you.",3
post50hb,richly branching,1.6001453961458911,highest,[deleted],2
post50hb,richly branching,1.6001453961458911,highest,"Africa as a continent would be host to multiple races, not just a blanket ""Africans"" iirc it's about 6 specific groupings from the DNA studies

And of course afirca is the most diverse... given is where we all came from",3
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,"Scientist are not concerned, people with political agendas are.

this is a great finding that shows how powerful AI can be",1
post50hb,richly branching,1.6001453961458911,highest,"What? Of course doctors, scientists, and AI researchers are going to be concerned any time AI that may potentially affect people's health and wellbeing does something they didn't predict. It's important for them to now understand what aspects of these images the AI is gleaning this information from.

The article even mentions this:

>Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research. Scientists must first figure out why this is happening.",2
post50hb,richly branching,1.6001453961458911,highest,I have the solution we send the AI to diversity seminars. If it’s still racist we fire it.,3
post50hb,richly branching,1.6001453961458911,highest,"You joke but for all we know the issue that led to this whole thing was bias in the chosen training data. They mention that the AI could predict race with 90% accuracy in *some* groups of photos but don't really go into more detail than that. For all we know, hypothetically, those xrays could have come from different clinics where 90% of their clients were all the same race and the AI is picking up on differences in equipment. The researchers need to make sure nothing dumb like that happened.",4
post50hb,richly branching,1.6001453961458911,highest,"That's not a ""concern"".  It's just a variable.

Doctors are not ""concerned"" by the fact that light-skinned people get sunburns and skin cancers at a higher rate than dark-skinned people.  They accept the fact that's it's true.  If a doctor has a light-skinned patient, then the doctor might be concerned about that individual's sun exposure, compared to when the doctor treats a dark-skinned patient.  But doctors are not ""concerned"" that methods and outcomes can vary with race.",3
post50hb,richly branching,1.6001453961458911,highest,You're comparing an observable natural phenomenon with AI doing something unexpected.,4
post50hb,richly branching,1.6001453961458911,highest,"I work in biotech using AI for diagnostics. I am concerned. Don't speak on my behalf; this is a big issue in the field. The underlying issue here isn't race. It is hidden biases in our training data. Race is a very easy one to pick apart, but this is a canary in the coalmine for us. We are concerned for the simple reason that if our tech is not properly scrutinized or is trusted as some unbiased omniscient entity, then people will die.",2
post50hb,richly branching,1.6001453961458911,highest,"What a bullshit post...

The verry point of using AI is to let it figure out the hidden logic

\>hidden biases in our training data. 

Where ?! Pont it out.

\>that if our tech is not properly scrutinized or is trusted as some unbiased omniscient entity, then people will die.

Nobody thinks AI are perfect and omniscient, they're tools.",3
post50hb,richly branching,1.6001453961458911,highest,"""Where ?! Pont it out.""
Did you forget where you were posting. Read the article that we are all discussing. There is an example.
""Nobody thinks AI are perfect and omniscient, they're tools.""
Then you recognize that this bias is an imperfection, and you agree with me.

Another reason to care, even for those people that somehow believe racial bias in AI is a good thing, is that the FDA requires us to demonstrate that our models do not have bias against protected classes.",4
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,"> this is a big issue in the field

Buuuuuuullshiiiiit. Show me the scientific papers then, if it is such a huge issue.

> It is hidden biases in our training data.

What hidden biases? The AIs task was to identify someone's race based on their skeleton. It completes the task successfully. If anything, that is the proof that the AI was trained well.

Or do you somehow think that if the AI was less successful at its task that it would mean it is less biased?",3
post50hb,richly branching,1.6001453961458911,highest,"Data leakage from training sets is a big problem in classification tasks generally, but has upended several attempts to use x-ray / MRI to diagnose. Some of the earlier upsets were trivial, such as allowing x-rays from the same patient to span training and test sets.

But things like this are potentially due to leakage, and can bias your model in ways you are specifically trying to control.",4
post50hb,richly branching,1.6001453961458911,highest,This post is literally discussing a scientific paper designed to identify bias in training.,4
post50hb,richly branching,1.6001453961458911,highest,"It wasn't meant to be for race it was trained on medical photos with race of patient and it was able to discern race. This is a canary because of one thing. It means that if any biases exist in the data, that AI can detect race. It wasn't meant to detect race it was meant to indicate if racism can be a problem for AI.",4
post50hb,richly branching,1.6001453961458911,highest,Lol shut up and go home to your fancy mansion while you jerk yourself off to making 250k,3
post50hb,richly branching,1.6001453961458911,highest,[deleted],2
post50hb,richly branching,1.6001453961458911,highest,"AI has long worked in ways we do not understand. The best chess computer is worlds better than the best human, and better yet than the people who programmed it. With genetic systems of generating AI this is even more clear.

And in this situation, how the AI does it‘s thing is pretty clear. Self-reported race is loosely based upon ancestry, and thus is at least somewhat correlated with ancestry. Forensic anthropology tells us that we can somewhat determine ancestry based upon the skeleton, so the AI just has to find that statistical correlation, which is a thing AI‘s great at.",3
post50hb,richly branching,1.6001453961458911,highest,"This. In this case the AI uses the self-reported race to get a ""good enough"" pass via statistical correlation by going through gigantic amounts of data. As I already said in a different comment, modern neural networks work with millions of data sets and billions of generations. There is no way developers can more than loosely decipher the path it took.",4
post50hb,richly branching,1.6001453961458911,highest,"I am sorry, but you really do not know what you are talking about. Most neural networks  logic paths above a certain scope are extremely hard to next to impossible to decipher due to the way the algorithms are coded to improve to reach the  required criteria.

The last thing we have to worry about is a machine uprising. If anything, an AI might do exactly what it is told in a manner that the team of developers did not predict.",3
post50hb,richly branching,1.6001453961458911,highest,"> If anything, an AI might do exactly what it is told in a manner that the team of developers did not predict.

True, but you make it sound like it's no big deal. If AI does something we don't want because we specified the goals in a bad way, that could be very bad.

And that follows to the next level of AI, if we make misaligned AGI, that could be the end of humanity, or worse.",4
post50hb,richly branching,1.6001453961458911,highest,"The whole point of machine learning is you see the input and the output and the computer determines the steps in between, because a human could not possibly do it in any reasonable amount of time.",3
post50hb,richly branching,1.6001453961458911,highest,"You're wrong on the ""why"", and the ""how"", but you're not wrong in being concerned.

Yes, understanding it more would help, but there's a lot more to it.

We need to solve the alignment problem, otherwise a wide range of  very bad things might happen when we develop AGI. With narrow AI, it would still be helpful, but these are not going to ""take over"".",3
post50hb,richly branching,1.6001453961458911,highest,"So just a heads up, humans have been able to identify gender, race, and age from bones. So I think you might be misunderstanding what the article is saying.",3
post50hb,richly branching,1.6001453961458911,highest,"Not intending to be argumentative, but doesn't the fact that we're creating AI more intelligent than humans already entail that danger (of replacement by AI)?",3
post50hb,richly branching,1.6001453961458911,highest,Not really. AI being smart just means that they can process data much faster than humans but most of them just do it with especific data (like this one with bone structure) and are useless in any other context. They are nowhere near taking decisions. The only people they can replace are things like data scientists and they would still need supervision,4
post50hb,richly branching,1.6001453961458911,highest,"You clearly don't read much AI news. We very, very rarely understand how they come to the conclusions they do.",3
post50hb,richly branching,1.6001453961458911,highest,"Lemme ELI5 the whole current AI subject to you. All these ""AI"" things you hear about in the news and internet are just glorified heavy duty calculators that use specific equations to output patterns from a huge dataset (that you have to input yourself), they're good for example to find the fastest road from one city to another, it does that by trying out all the paths very very quickly and finds what was the shortest. Or in this case skimmed through X-ray pictures (the glorified calculator was given (by a person) very many images and with each image given what race it belongs to) and the ""AI"" just went through all of them very fast and have out info if some things were in a pattern. They just go through numbers (pixels can also just be numbers) much more quickly than a human does that's why they're useful. AI (artificial intelligence) is just a buzzword and I hate that it is being used everywhere, there is not a whiff of intelligence to them, Machine Learning algorithms would be more correct term but even that is a reach. To learn something would require the ability to think imo. Softwares are not even close to ""thinking"" and there is a real possibility that we will never invent a true AI because we don't understand even our own brain's consciousness workings well enough to invent one. So the fear of emerging Skynet is MAYBE a topic in 20-30+ years but definitely not now.",3
post50hb,richly branching,1.6001453961458911,highest,"We don’t understand any of the social media algos, that’s a million times more concerning that an AI’s analysis of X-rays.",3
post50hb,richly branching,1.6001453961458911,highest,Maybe the scientists should talk to any anthropologist?,3
post50hb,richly branching,1.6001453961458911,highest,">It does not concern you that scientists are not understanding how the AI even does it?  
  
I mean it probably had something to do with this:   
  
>An international team of health researchers from the United States, Canada, and Taiwan tested their AI on X-ray images that the computer program had never seen before **after training it with hundreds of thousands of existing X-ray images annotated with specifics of the patient's race**.   
  
So basically for every X-Ray they fed to the AI they also told it the patients race. Different races are generally going to have different average bone structures in much the same way that they'll have different average heights/skin tones/hair or eye color. The AI noticed structural patterns that were common across individuals from the same race and eventually was able to predict the race based on the patterns it was seeing.   
   
So the *how* is fairly straightforward. That said, I can absolutely understand how this is something we need to control for when having AI give a diagnosis.   
   
Honestly my concern here is less with the AI and more with the methods that scientists are using to train it. Unintentional bias being introduced into the system can lead to a lot of headache.",3
post50hb,richly branching,1.6001453961458911,highest,"No it does not.

I also don't know how you make your decisions and it doesn't concern me. 

Read about neural networks maybe then you will know why that question is kinda ridiculous.",3
post50hb,richly branching,1.6001453961458911,highest,"It is a concern, but it cant really be stopped. People will keep developing extremely valuable tech even if they gotta move to another country or do it in secret",3
post50hb,richly branching,1.6001453961458911,highest,"Racial bias could cost hospitals a huge amount of money. Treating something too late can be wildly more expensive than catching it early. So if they arnt diagnosing diseases in a certain ethnic group due to biased training data, it's gon be expensive.",2
post50hb,richly branching,1.6001453961458911,highest,Haven't we been able to tell race from bone forms for awhile now?,1
post50hb,richly branching,1.6001453961458911,highest,Yes. But now AI can do it too. And everything that computers do is scary.,2
post50hb,richly branching,1.6001453961458911,highest,"""Our finding that AI can accurately predict self-reported race, even from corrupted, cropped, and noised medical images, often when clinical experts cannot""",2
post50hb,richly branching,1.6001453961458911,highest,"There is nothing surprising about this.. This headline seems to be made to elicit emotional responses in people who don't already understand what this means.

Fundamentally it's like saying you can predict gender via an x-ray, nothing unexpected or concerning about that, because you expect the gender part of genetics to affect skeletal structure. The same holds for broader genetic heritage, like race.",1
post50hb,richly branching,1.6001453961458911,highest,Sex or Gender? Cause apparently that is now a trigger phrase.,2
post50hb,richly branching,1.6001453961458911,highest,Must be sex since gender has been reduced to a large collection of labels based on social stereotypes.,3
post50hb,richly branching,1.6001453961458911,highest,It's ironic how those that typically claimed to be against stereotypes in fact are some of the strongest enforcers of said stereotypes,4
post50hb,richly branching,1.6001453961458911,highest,"From the article: 

“ Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research. Scientists must first figure out why this is happening.”",2
post50hb,richly branching,1.6001453961458911,highest,*elicit emotional responses,2
post50hb,richly branching,1.6001453961458911,highest,"This is news?  I distinctly remember an old Facebook shared image of an x-ray of 2 people kissing and claimed that it was beautiful because there was no age bias, no race to be seen, no gender and just love.  Promptly broken down in the comments by some dude who used biological markers to give a rough age estimate, gender assessment, and race evaluation.",1
post50hb,richly branching,1.6001453961458911,highest,"I like how Reddit is automatically collapsing comments like this, tells you all you need to know about it. But seriously we’ve always been able to tell, this shouldn’t really be news",2
post50hb,richly branching,1.6001453961458911,highest,As an anthropologist this is no surprise. There are many morphological differences between ethnic groups.,1
post50hb,richly branching,1.6001453961458911,highest,"Since you're an anthropologist, is it easy to determine ethnic group from a chest x-ray?",2
post50hb,richly branching,1.6001453961458911,highest,"I've haven't heard of any distinct markers of enthicity linked to chest bone structure, but it's not really surprising that an AI given enough data could find some. We've already seen AI do many things better than humans such as identify cancer from x-rays or play Go better than the top player. AI can find patterns where humans won't see anything. I find this sort of stuff great because we can learn from the AI things we wouldn't have known before. From an archeological perspective this could be a very useful tool for us to decipher more from findings.",3
post50hb,richly branching,1.6001453961458911,highest,"Ok, so the actual issue at hand is the data being given to the AI was thought to be clean of racial indicators, since they wanted to train the AI without the bias that shows up in a lot of medicine.

Resulting tests had a significant amount of error in diagnosing black people, leading to various groups trying to find where the bias had entered the training dataset.

The study specifically looked at the chest x-rays, which were thought to be race-neutral, and learned the AI could tell.

From an archeological perspective this is not particularly useful.  You can't ask the AI *how* it knows.  It just can tell...somehow.

From a medical perspective this indicates that trying to get training data that won't carry over currently existing bias in diagnoses is a LOT harder than previously thought.

That's why scientists are concerned.  Because bias kills people, and AIs learn to do things very well, and when bias is in the training data they learn to be biased very well.",4
post50hb,richly branching,1.6001453961458911,highest,"Wouldn't that be racist? The only difference between ""race"" is skin deep 🤨",2
post50hb,richly branching,1.6001453961458911,highest,The concept of race isn't used in anthropology when referring to homo sapiens. We refer to different groups instead as enthicity because distinct races don't procreate with each other. Humans are one race with many ethnicities and these ethnicities are associated with some physical differences. We learn about the various physical markers especially relating to skeletons because it is useful when conducting archaeology. For example a typical sign that a skeleton belonged to an Asian person is that the cranial suture between the two parietal lobes are fully closed beyond middle age.,3
post50hb,richly branching,1.6001453961458911,highest,"God I hate sciencs ""journalism"" these days. It mostly falls into two categories:

1) How can we terrify people with this fairly mundane discovery?

2) How can we frame this 40+ year old discovery as though it is brand new?",1
post50hb,richly branching,1.6001453961458911,highest,">Scientists are concerned

Why? 

Because these days science isn't about science  or facts... it's about some political narrative",1
post50hb,richly branching,1.6001453961458911,highest,"""Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons""

That's directly from the article that you obviously didn't read.",2
post50hb,richly branching,1.6001453961458911,highest,"Ai has a bug that is harder to detect sickness in black people, ""AI Is RaCiSt!""",3
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,Concern about the ethnical implications of scientific discovery is common among scientists. Why do you think ethics committees exist?,2
post50hb,richly branching,1.6001453961458911,highest,"It's not a new discovery. Different races have different body structures and I'm sure it's easy to predict with pattern recognition ML. 

Nothing to be concerned about. Making ML and already known fact.",3
post50hb,richly branching,1.6001453961458911,highest,Knowing a patients race would be beneficial for any human or machine trying to diagnose a patient.  The fact that the author would rather frame it as potential racism reveals the political rather than scientific motive.,3
post50hb,richly branching,1.6001453961458911,highest,"There is always an ideological motive behind applications of scientific discovery, especially when assessing risk.",4
post50hb,richly branching,1.6001453961458911,highest,Not sure what’s so concerning about this…Anthropologists have been studying these variations for decades now and back when I was pre-med in anthropology we too could determine race from bone structure based on specific measurements found on skeletons.,1
post50hb,richly branching,1.6001453961458911,highest,But now it be woke,2
post50hb,richly branching,1.6001453961458911,highest,A result that someone does not like because they fear someone else might use to make racists arguments does not show bias on the part of AI. Differences were found because they exist.,1
post50hb,richly branching,1.6001453961458911,highest,"From the article:

“ Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research. Scientists must first figure out why this is happening.”",2
post50hb,richly branching,1.6001453961458911,highest,"To think that a trait that took 10s of thousands of years to develop, skin color, was the ONLY thing that diverged is crazy. People don't want to say it because they think they'll find something that will be used by racists and people who want to commit genocide as scientific justification for weeding out their populations, but unfortunately, hoping we don't find anything and hiding the information isn't very scientific of us. There are many differences between what we call races that go well beyond eye size, skin color, and height. 

The fact that certain diseases are more common in certain races tells you that there are way more differences that we know or are willing to accept. 

The data by itself isn't racist. The AI isn't racist. What's racist is the people who are on the edge of their seats hoping they'll find something that makes the race they hate seem inferior so that they can justify their racism. Now THAT'S something to worry about.",2
post50hb,richly branching,1.6001453961458911,highest,Hiding inconvenient truths in the age of information is the kind of policy that invariably backfires.,3
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,">To think that a trait that took 10s of thousands of years to develop, skin color, was the ONLY thing that diverged is crazy.

Just dont ever mention that this applies to the brain. People get really tingly about that one.",3
post50hb,richly branching,1.6001453961458911,highest,Yeah you’re a racist,4
post50hb,richly branching,1.6001453961458911,highest,"I think we all know that there are difference. Probably minor, but to be fair, it would be horrifying if we do find them. It would be like the invention of the nuclear bomb.",4
post50hb,richly branching,1.6001453961458911,highest,[deleted],3
post50hb,richly branching,1.6001453961458911,highest,"Black people existed before white people. I wasn't singling out any specific race. It's why I kept my comment general and non-specific. I'm also well aware how reddit can be, so I tried to stay very neutral.",4
post50hb,richly branching,1.6001453961458911,highest,"I came here to write something like this. 

It’s not bias if it’s true. We know that certain medicines work differently in certain races. We know that different diseases affect different races in a different manner, and so on. 

At least in this case, AI told us something we didn’t know we didn’t know. And, AI will continue to do this. I think this is the root cause of this pseudo- fear Because it damages our psyche.",2
post50hb,richly branching,1.6001453961458911,highest,Also known as: Intellectual cowardice.,2
post50hb,richly branching,1.6001453961458911,highest,"The problem is that the data that the AI is trained on to make diagnosis will have a racial bias. This isn’t a problem if we can hide the race of the patients from the AI as it trains, but this result shows that we cannot. It illustrates a major hurdle in developing automated systems for medical diagnoses.",2
post50hb,richly branching,1.6001453961458911,highest,"AI is 90% accurate in predicting race, must be racist.

Am I getting the jist of that article right?",1
post50hb,richly branching,1.6001453961458911,highest,Literally anytime an AI can discern race we get an article about why its racist lmao,2
post50hb,richly branching,1.6001453961458911,highest,When a race baiter with no scientific background tries to write about a scientific topic.,3
post50hb,richly branching,1.6001453961458911,highest,"Im concerned about a world were we consult AI about anything, that is so stupid it cant even differentiate Races.",3
post50hb,richly branching,1.6001453961458911,highest,I think the author and its target audience are the type to get instantly triggered when they see the word 'race'.,2
post50hb,richly branching,1.6001453961458911,highest,"Here’s an example of the problem.

In a certain neighbourhood, when someone shoplifts, the police are more likely to let them off with a warning if they’re white and more likely to arrest them if they’re black. Since the people who are let off with a warning aren’t recorded as committing the crime, the computer displays this as black people committing the crime more, because all it has is the arrest records.

The data from the neighbourhood, along with data from many others, is fed into an algorithm. Algorithms can be used for really important things, like whether someone gets parole. The AI decides a certain person is more likely to shoplift because it was fed data that said that. Now black people whose cases are seen by that AI are less likely to get parole/rated as a higher risk for recidivism. 

This tech allows race to be taken into account by AIs. That could be used to look for and eliminate biased, but if it’s used carelessly it could also take human racism and apply it to decisions even more than it does now.",2
post50hb,richly branching,1.6001453961458911,highest,"This is false, black people are more likely to have prior arrests/charges which is why they’re less likely to get off with a warning for minor crimes.",3
post50hb,richly branching,1.6001453961458911,highest,"And they’re more likely to have those prior arrests/charges because they’re charged with minor crimes in the first place. Generally more likely to get a charge and to get a worse charge and/or worse penalty for the same crime.

There are a lot of things that go into parole algorithms. People usually don’t know that they’re being judged by an algorithm. So some are denied parole because of past charges they personally have (whether or not those charges were justified) and some are denied because of the bias in the algorithm, and they’ll never know.

I’m happy to give more info about algorithms, but not in the mood to get into an argument about whether or not racism exists in the legal system. If you want to pretend an officer spending 9 minutes murdering someone on film and having a ton of supporters who don’t want him charged for it is normal and has nothing to do with race, that’s on you.",4
post50hb,richly branching,1.6001453961458911,highest,"> when someone shoplifts, the police are more likely to let them off with a warning if they’re white and more likely to arrest them if they’re black.

This is not true in general though. Arrest rates line up with victimisation data. We take surveys of people who have been victims of crime and ask about multiple things, one of which is the criminal's race. Arrest rates line up with this data, suggesting little to no racial bias in arrest rates. In fact, if I remember right, there is actually a slight bias in favour of blacks for these minor crimes.",3
post50hb,richly branching,1.6001453961458911,highest,If you can find me this data I’ll love you forever,4
post50hb,richly branching,1.6001453961458911,highest,"No. From the article:

“ Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research. Scientists must first figure out why this is happening.”",2
post50hb,richly branching,1.6001453961458911,highest,"No, it's more ""this type of chest x-ray was thought to be free of racial indicators that might taint the training data used for diagnosis.  The study shows it is apparently not free of those indicators, which is important to know.""",2
post50hb,richly branching,1.6001453961458911,highest,Amazing! Maybe one day AI will also be able to predict  peoples race from just images of their faces.,1
post50hb,richly branching,1.6001453961458911,highest,I mean as long as the computers don’t call them racial slurs i don’t see why this is a problem?,1
post50hb,richly branching,1.6001453961458911,highest,The problem is the possibility that certain people or institutions using such AI to racially profile and discriminate people based on race.,2
post50hb,richly branching,1.6001453961458911,highest,Wouldn’t they just deny people by seeing there face or color if that is already a problem. You don’t even need an AI now. They could just look at what you checked down as your race then deny you. Seems like a made up potential problem,3
post50hb,richly branching,1.6001453961458911,highest,"Probably because their concept of race or the general concept of race is purely genetic if it can be determined by something like bones and other faint structures. The issue is that modern humanities and social sciences have been denying the existence of biological race for probably 20 years with great popularity and academic consensus. The issue will come when they decide this technology is racist because of its engineers or some other deflection of being wrong in some absolute mental gymnastics as they always have. Ex. Differences between men and women's brains such as in the amount of particular disorders that perceptually occur more in boys than girls or vice versa (such as BPD or bipolar) was generally dismissed as ""thats not biology, thats 100% under-diagnosis for one sex and sexism in psychiatry,"" by humanities and social sciences. Around the same time, they somehow ended up on this idea that ""symptoms for men and women are different for the same disorders, therefore the diagnosis rate should be 50/50, but its not because we tend to only recognize one sex's symptoms"" (which is true, but 50/50 is a delusion) which in itself is already pointing out differences in the brain.",1
post50hb,richly branching,1.6001453961458911,highest,That's good because there are specific diseases that target certain races at a higher rate.,1
post50hb,richly branching,1.6001453961458911,highest,No scientists are concerned because its going to feed white supremacist tropes that the other races are more than just a different color,2
post50hb,richly branching,1.6001453961458911,highest,Anthropologists have already kinda' been doing this shit for like decades and decades...,1
post50hb,richly branching,1.6001453961458911,highest,"Why should they be concerned - we detect different races from the shapes of peoples faces - which is obviously down to bone structure.

For example, Irish people are known for having more pointy chins.  I know these are stereotypes, though there is some connection.

There is nothing intrinsically bad about AI being able to deduce someone’s probable race from an X-ray.

Their medical history probably includes this info anyway.

And because, while we are all human, there are some race-related medical conditions, sickle-cell being a common one.

I would actually be more concerned if the AI was not able to spot these patterns - it would reduce my trust in its accuracy if it could not.

This is a non-story really.

I know I can identify several different races just by looking at someone’s face (corresponding to some degree to their bone structure) - Surely an accurate AI should have some ability to do that too.",1
post50hb,richly branching,1.6001453961458911,highest,can you enumerate the list of races you are using here? irish is a new one to me.,2
post50hb,richly branching,1.6001453961458911,highest,[deleted],2
post50hb,richly branching,1.6001453961458911,highest,We are all much more similar than people think !,3
post50hb,richly branching,1.6001453961458911,highest,It's chest x-rays.  No skulls.,2
post50hb,richly branching,1.6001453961458911,highest,Ha ha! I'm Irish and you're definitely making that up!,2
post50hb,richly branching,1.6001453961458911,highest,"AI is racist!! Lets cancel it

- Twitter, probably",1
post50hb,richly branching,1.6001453961458911,highest,Lol sadly correct,2
post50hb,richly branching,1.6001453961458911,highest,That happened too many times actually,2
post50hb,richly branching,1.6001453961458911,highest,"More like lefty social scientists are concerned.

Lets hear the calls for AIs to be curated now like social media algorithms after all noticing patterns is racist.",1
post50hb,richly branching,1.6001453961458911,highest,There are serious scientific reasons why an AI being capable of determining race just from medical scans is concerning from an AI ethics perspective.,2
post50hb,richly branching,1.6001453961458911,highest,Correct me if I'm wrong but isn't the skull a clear giveaway?,1
post50hb,richly branching,1.6001453961458911,highest,"Skull and hip areas have decent statistical racial differences, but this AI can figure out the race from X-rays of areas not known to have significant racial difference like the chest, breast and sections of the limbs.",2
post50hb,richly branching,1.6001453961458911,highest,Damn what a smart boi,3
post50hb,richly branching,1.6001453961458911,highest,maybe bone marrow or bone density differences or the way the bones have grown/are shapen? our bones have rings which are indicative of their age so why wouldnt there be something else based on your ethnicity?,3
post50hb,richly branching,1.6001453961458911,highest,[deleted],3
post50hb,richly branching,1.6001453961458911,highest,[removed],2
post50hb,richly branching,1.6001453961458911,highest,And you’re a racist,3
post50hb,richly branching,1.6001453961458911,highest,Never heard that one before.,4
post50hb,richly branching,1.6001453961458911,highest,Yes it’s how my anthropology class was taught. You can observe a lot of bone structures in modern humans to figure out where their ancestors came From.,2
post50hb,richly branching,1.6001453961458911,highest,[deleted],3
post50hb,richly branching,1.6001453961458911,highest,[deleted],2
post50hb,richly branching,1.6001453961458911,highest,"I don't know any of it, just asking based on the few skulls I saw where the difference are clearly visible. I'm 120% sure that if you would show me large amounts of samples from Africa I wouldn't be able to tell. What I'm getting at is that I don't find it surprising that an AI can tell the difference if in make cases even regular person can see some differences.",3
post50hb,richly branching,1.6001453961458911,highest,[deleted],2
post50hb,richly branching,1.6001453961458911,highest,"The nasal bone and orbits are the easiest qualitative features you can use to identify a skeleton. You're referring to phrenology, which is the science of determining mental traits/functions of the brain by assessing the different shapes/bumps of the skull.",3
post50hb,richly branching,1.6001453961458911,highest,"So tell me the differences between a Black skull and a White skull. And if you wanna talk about wide, flat noses, I can show you some East Africans that would disprove your point

Black Africans have the most genetic diversity and the most skeletal diversity so you can’t use skull except to say they’re from a certain region of a certain continent",4
post50hb,richly branching,1.6001453961458911,highest,That’s called phrenology and it was done by nazis. It’s a pseudo science and anyone defending the idea that you can tell someone’s race by their skull shape is racist.,2
post50hb,richly branching,1.6001453961458911,highest,"Oh Jesus, not this again.",2
post50hb,richly branching,1.6001453961458911,highest,"I tell you you're thinking of ""phrenology"". A long since debunked theory that has nothing to do with modern anthropological forensics.

But I don't think you're thinking at all. Just looking to score leftist points.",3
post50hb,richly branching,1.6001453961458911,highest,[deleted],2
post50hb,richly branching,1.6001453961458911,highest,"We've been able to determine the race of a skeleton from the shape of it's skull, among other things, for centuries. You're probably thinking of phrenology, which is racist pseudoscience but also not what anybody is talking about",3
post50hb,richly branching,1.6001453961458911,highest,I'm a scientist and not even slightly concerned about something like this... why would I be?  cringe.,1
post50hb,richly branching,1.6001453961458911,highest,"Ethnic people tend to have issues with diagnosis’ and being taken seriously. Any racial bias’ injected into algorithms would increase such issues & concerns.

I think nyc was using face recognition tech in their police force which was later noted in being extremely inaccurate so obviously innocent people got hemmed up.

So there is a ways before such tech can be implemented without abuse or underlying concerns it seems",2
post50hb,richly branching,1.6001453961458911,highest,"I don't see how a differentiating algorithm is racial bias.  Sorry you don't understand what is happening here, but this isn't it man.  Nope.",3
post50hb,richly branching,1.6001453961458911,highest,B-but… bias!,4
post50hb,richly branching,1.6001453961458911,highest,"The trouble with all machine learning is that we really don't know how they work.

The models and weights are literally a black box.",2
post50hb,richly branching,1.6001453961458911,highest,"Impossible for real doctors? Lol what?

Say hello to forensic anthropology. It's always been possible to find the race of someone by looking at their bones. You can tell their gender too if that makes it even less PC.",1
post50hb,richly branching,1.6001453961458911,highest,Can they do so from a chest x-ray?,2
post50hb,richly branching,1.6001453961458911,highest,That's not really true and I can give you many examples of people that developed racist/sexist tendencies in adulthood.,2
post50hb,richly branching,1.6001453961458911,highest,"Why is this a shock? Bones can tell you so much. This has been known since the wide study of medicine.
 Sinus shape, jaw structure, heart size are some signs that can be used to predict race. There are so many more that can be found on a x-ray.",1
post50hb,richly branching,1.6001453961458911,highest,[deleted],2
post50hb,richly branching,1.6001453961458911,highest,"There's no need to get shirty. They key word here, is 'predictive'. Nothing is a 100% certain. 
Feel free to take your queries to Google or an anthropology class.",3
post50hb,richly branching,1.6001453961458911,highest,So AI confirmes that racial differences DOES exist. The world we do live in ...,1
post50hb,richly branching,1.6001453961458911,highest,"If we look at it from a statistical perspective: That has never been a question. Especially in medicine that is a fairly well known fact. Different sexes can react differently to different amounts of dosage (on average of course) and the same thing goes with even smaller differences like racial differences in bone structure, average height, average bone density, intolerances and so on and so forth.

These things do not differ because of the american definition of ""race"" but because of the genetic data we get from our ancestors, which is correlated but not equal to our race. (Example a person with 9/10th of its ancestors being from scotland and 1/10th being from the phillipines might still be lactose intolerant, despite being called ""white"" in the USA and therefore LESS likely, but not unkown to be lactose intolerant).

&#x200B;

Sorry for the convoluted answer.

At the end of the day, we are just biological machines with a huge amount of data that can be interpreted.",2
post50hb,richly branching,1.6001453961458911,highest,"There exist differences between humans, and humans can be grouped into various groups by their looks. These groupings can be called „sex“ or „race“ or what have you, but there is no genome for, say, „black“ or „white“ etc.",2
post50hb,richly branching,1.6001453961458911,highest,"E.g, there's more genetic diversity within the African population than in the entire rest of the world, and yet black people are typically considered one race.",3
post50hb,richly branching,1.6001453961458911,highest,No it doesn't. It only uses what it was fed with.,2
post50hb,richly branching,1.6001453961458911,highest,"Can someone explain to me how a scientific article talks about race?
My understanding is that most biological attempts at classifying races have been debunked and our species has no race boundaries: so how does the article define race? 
Also,what is the race of the increasing number of people of mixed origins? Or people who live in in historical crossroads?",1
post50hb,richly branching,1.6001453961458911,highest,Could be how each patient self identifies.,2
post50hb,richly branching,1.6001453961458911,highest,"> My understanding is that most biological attempts at classifying races have been debunked and our species has no race boundaries:

That's a line of propaganda from the ""there is no race"" crowd.   We're all humans and we're all African if you go back far enough.  Specifically 80,000 years according to all the genetic marker tracing we've found that has really confirmed the [Out of Africa Model](https://en.wikipedia.org/wiki/Recent_African_origin_of_modern_humans).  For ~100,000 years the relative isolation between pockets of humanity has lead to genetic drift and allowed locals to adapt to their environment. The most obvious being how much vitamin D they need from the [average sunlight](https://www.reddit.com/r/MapPorn/comments/44y90h/annual_sunshine_hours_map_of_the_world_2753_1400/) of an area. As humanity spread around the globe they picked up mutations and passed them on and we can very clearly see how these things have spread around.   You can pick literally section of the [tree of life](https://simple.wikipedia.org/wiki/Tree_of_life_(biology\)) and group all the descendents together. This doesn't magically stop at humanity. (Although sexual recombination does blend branches together. Mixed races are more than possible and often healthier. Remember, inbreed for ~~freaks~~ interesting features, outbreed for health).  

>Also,what is the race of the increasing number of people of mixed origins? 

Both. Technically it's not an even 50/50 split as you get a random shuffle of genes from mom and dad. But it probably averages out to something pretty close.  You're a mix of everything up in that tree. For [some](https://en.wikipedia.org/wiki/Charles_II_of_Spain), it should have been mixed a little more. [Much more](https://en.wikipedia.org/wiki/Pug).

>Or people who live in in historical crossroads?

Everywhere is a crossroads to somewhere. There's really no group of people living at a dead-end. And everyone is closer related to their ancestral neighbors that to groups from the other side of the world. 

But come on. Why push an agenda like this? What's the benefit and for who?  Do people from Japan look different from people from Sudan, on average? Yeah? We know why.     Now, just because we've been able to identify what these concepts out of antiquity really are, it doesn't mean everything the ancients thought about lightning, alchemy, or race is 100% correct.    While ""Asian"" typically doesn't include people from Moscow or Bombay or Mecca (despite being in Asia), the term ""black"" as a stand-in for ""African"" really misses the mark. In the USA ""Black"" is synonymous with south-west African. But they could be more distantly related to a S.African than to someone from China.

Isn't being race-blind supposed to be a bad thing?",2
post50hb,richly branching,1.6001453961458911,highest,Relabeling race as a social construct is an attempt to move the word semantically from biology to sociology. It does not mean that there the biological concept of race was debunked.,2
post50hb,richly branching,1.6001453961458911,highest,"The article says: “the AI was able to predict the patient's claimed racial identification on these photo”

Due to the high accuracy of the ai, the article suggests there is an implied correlation with race and bone structure. However, as with “most” methods of racial categorization, this study uses self-identification to categorize people by their race. This means the result of the AI implies a correlation between bone structure and our mental/social perception of race. I am interested in seeing a similar study, but instead of self-identification, they use other people to identify the race of a subject.",2
post50hb,richly branching,1.6001453961458911,highest,Is this because of thought processes of the programmers? Or is the AI self learning these patterns? I know ive read about unintended consequences of the lack of consideration by programmers in the differences in other races when programming. Are we going to have to create racial sensitivity courses for the creation of AI?,1
post50hb,richly branching,1.6001453961458911,highest,"IIRC, it’s because they don’t understand why. You don’t want rampant AI running / companies / corporations abusing systems like this.",1
post50hb,richly branching,1.6001453961458911,highest,"You race influence your anatomy. That's completely normal. It never was a problem, even if some racist use phrenology and other things like that as a tool to prove their stupid theories.",1
post50hb,richly branching,1.6001453961458911,highest,We're different...and that's ok. Its applying moral judgements based on race that's the problem.,1
post50hb,richly branching,1.6001453961458911,highest,There are different human races?! Oh nooooooo! It's like we're just like every other animal on this planet and not some God's special play thing.,1
post50hb,richly branching,1.6001453961458911,highest,Different species and it's not a bad thing.,2
post50hb,richly branching,1.6001453961458911,highest,Bone of diffrent races look different. Its genetics and environmental factors over generations,1
post50hb,richly branching,1.6001453961458911,highest,So the difference isn't only skin deep?,2
post50hb,richly branching,1.6001453961458911,highest,You do realize ethnic groups or races (how ever you wanna say it) are quite different outside of skin tone right? How do you think anthropologists are able to identity race from ancient remains? Why do you think certain groups are more susceptible to certain diseases than others? Race and ethnicity has never been just skin level and there is nothing wrong with that,3
post50hb,richly branching,1.6001453961458911,highest,No it's not concerning and I bet scientists are the ones that are less likely to be concerned with.,1
post50hb,richly branching,1.6001453961458911,highest,How is it not concerning when the AI is missing imdiagnoses for certain races?  Specifically those that are often under diagnosed by humans?  It is concerning if you start to rely on AI and the AI says nothing is wrong when there are things wrong for a historically oppressed group.,2
post50hb,richly branching,1.6001453961458911,highest,"The most historically oppressed group in all human history is... the poors.     
  
And there is no indicator that you can define the worth using x rays from the article.   
  
I understand what you meant but it's a bit misplaced here. Don't mix technical challenges with political decisions.",3
post50hb,richly branching,1.6001453961458911,highest,It’s black Americans. Diagnostic ai are under diagnosing black Americans and they researchers are trying to figure out why.,4
post50hb,richly branching,1.6001453961458911,highest,"This is not shocking at all, and shouldn't be concerning. It is something forensic anthropologists can do to a fairly high degree of certainty, and do quite frequently too.

What the mod posted in their pinned comment about this being impossible is incorrect. [Among about 250 resolved cases in which forensic anthropologists offered an ancestry estimate, they correctly identified a person's social race about 90% of the time](https://www.science.org/content/article/forensic-anthropologists-can-try-identify-person-s-race-skull-should-they#:~:text=Among%20about%20250%20resolved%20cases,the%20Journal%20of%20Forensic%20Sciences%20.)",1
post50hb,richly branching,1.6001453961458911,highest,"Exactly, I don't understand the surprise, if humans can do it, you can set a machine to look for the identifiers and they can do it faster and more acurately.",2
post50hb,richly branching,1.6001453961458911,highest,"What should be really concerning is that doctors here seem to assume there are no meaningful differences - even though it is known that heritage is an important factor for someone's health. But what can you expect when only 60 years ago, things like this were normal:

> She notes that, in the early 60s: “Observing that women tended to have lower rates of heart disease until their oestrogen levels dropped after menopause, researchers conducted the first trial to look at whether supplementation with the hormone was an effective preventive treatment. The study enrolled 8,341 men and no women ... And a National Institutes of Health-supported pilot study from Rockefeller University that looked at how obesity affected breast and uterine cancer didn’t enrol a single woman.”

From: https://www.theguardian.com/lifeandstyle/2019/nov/13/the-female-problem-male-bias-in-medical-trials

Though racial bias in machine learning is real and actually difficult to overcome. It is right to keep an eye on it - it just shouldn't be assumed that the current state is problem free.",3
post50hb,richly branching,1.6001453961458911,highest,"An automated system uses the data it is given to come up with the outcomes. Bones tell us so much, we dig up bones or long death people and can tell you their race, sex and usually a rough age.

You give the systems the same data used to work that out and they can also do that.

Very simple stuff and I doubt modern doctors were puzzled by it.",4
post50hb,richly branching,1.6001453961458911,highest,"Why would anyone be concerned? Even within the same race, if one group lived in the mountains and one group lived in the plains you'd start to see biological differences over time. We already know that bone density differs race and is significant enough to where a black female's bones are around as dense as a white male's while a black male's is much higher density. 

These are known things, not even a debate. This is just clickbate.",1
post50hb,richly branching,1.6001453961458911,highest,"This is not clickbait, this is data science.

An AI model looks at everything. For example, there was one created to look for cancer, and it did a good job with the training data but sucked when actually applied. The data scientists discovered that the training data always had a ruler in the scans with cancer, so the model always looked for a ruler.

Bias in diagnosis is a problem in the medical field. If you give a model a bunch of training data that underdiagnoses or overdiagnoses a certain group, and it can tell who is in those groups, it will do the same thing.

Ideally the models will eliminate bias based on race, or gender, or religion, or any other cause. But if the models see that people in Utah only get diagnosed when a tumor is stage 3 then it will ignore cancer in people from Utah before that stage.",2
post50hb,richly branching,1.6001453961458911,highest,"You need to differentiate between people based on their specific situation. Different races do have different common conditions (think sickle cell and increases risk of heart disease in the black community for example) just like you don't necessarily want to give a morbidly obese person the same dose of many drugs you might give an underweight person. 

Discrimination (the appropriate kind where it is based on individual differentiators and not prejudice) is ideal in medical practices. You want doctors that appropriately account for differences to provide the best possible medical outcome for the individual and not some doctor who ignores medical facts to treat everyone like one unit of ""human"" when we're not clones.

Trying to get rid of this differentiation is insane. A black man deserves to be catered to directly and not treated like one unit of average man. Because that's not going to be him as a minority otherwise. It would be like complaining that a hair stylist AI can differentiate between white and black hair. Well yeah, that's a good thing. It's not like the AI is biased like giving inferior care. This attempt at ""progressives"" will actually hurt minorities under the fear mongering guise of ""bias"". Frankly, omitting differentiators is itself stereotyping everyone as the average and unless you're in the majority that's not good for anyone.",3
post50hb,richly branching,1.6001453961458911,highest,"Ideally, you are right. If race is factored in then such a model could provide better diagnoses based on such distinctions. However, if you give it bad data with a racial bias then it will give bad results.

If the data shows that people with X,Y, and Z traits are not diagnosed when they have indicators A, B, and C, the model will match that. If the model, according to the article, is underdiagnosing black people then they have to fix it.

This isn't about being ""woke,"" it's about making sure the thing actually works right.",4
post50hb,richly branching,1.6001453961458911,highest,">Different races do have different common conditions (think sickle cell and increases risk of heart disease in the black community for example)

They do not. Someone from Greece (a ""white"" person) has a way larger chance of having the HbS allele than someone from Namibia (a ""black"" person). 

And there is no increased risk of heart disease for black people. There is an increased risk of heart disease for African-Americans.",4
post50hb,richly branching,1.6001453961458911,highest,[deleted],4
post50hb,richly branching,1.6001453961458911,highest,[deleted],2
post50hb,richly branching,1.6001453961458911,highest,Exactly. It makes no sense that people didn't think this was possible.,3
post50hb,richly branching,1.6001453961458911,highest,"There’s a Minsky Koan (https://news.ycombinator.com/item?id=10970937) that kinda sums this up.  

Ignore bias and you don’t know which way your machine will jump. 

You’re programming a very very fine mesh sieve that’s just trying to catch the things it’s learned it should. However that happens in an context of information the machine doesn’t know.",1
post50hb,richly branching,1.6001453961458911,highest,"a kinky moan, you say?",2
post50hb,richly branching,1.6001453961458911,highest,God. Damn. Autocorrect.,3
post50hb,richly branching,1.6001453961458911,highest,[removed],1
post50hb,richly branching,1.6001453961458911,highest,if you took the time to actually read then you’d see the reason they are concerned is cause they have no idea how the AI is so accurate when they can’t see a difference between races with their own eyes,2
post50hb,richly branching,1.6001453961458911,highest,"When they say this, does it mean that race is one of the characteristics they are training for? Or is it grouping multiple similar characteristics that correspond to a particular race? If they are programming the AI to categorize people by race then it will, if it finds common characteristics in a particular group of people, that is what it is supposed to do. If people are worried about AI being racist, stop people from building race detection into it, if it doesn’t prescribe the correct treat because of racist assholes in the past, then either get new different training data, or set a review board in place to overview the decisions made by the ai and correct it, the so just groups by characteristics the racism comes from the data it is given ie if a white patient is given antibiotics for a virus ( antibiotics don’t kill viruses), then the ai is trained for that then when it sees a group of characteristics for a white person it will probably prescribe an antibiotic for a virus ( which is incorrect). If race is a criteria the ai is trained on then it will consider race, if the data is made up of past doctors racist decisions the ai will make racist decisions, garbage in garbage out. We can prune an ai with scores and ratings, but it won’t help if it has garbage data",1
post50hb,richly branching,1.6001453961458911,highest,"The study was reacting to reports of bias in datasets that were supposedly devoid of racial data.  Thus it was checking to see if the chest x-rays contained racial indicators the AI was finding that could then bias the results of other uses.

It found them.  Meaning the training datasets that were thought to be free of racial indicators so as to not compound bias in the training data may not actually be as neutral as thought.",2
post50hb,richly branching,1.6001453961458911,highest,"I keep seeing this and it says ""predict"" I think the correct word is determine.",1
post50hb,richly branching,1.6001453961458911,highest,Are.....are they worried the robots will be racist? This is some bizzare projection.,1
post50hb,richly branching,1.6001453961458911,highest,"For better or worse, this is where society is headed though. Roads are racist. Animals are racist. What you ate for breakfast was racist. If you didn't eat breakfast, that's racist too.

And yes - AIs are inherently racist too. They use math which, by the way, is racist.",2
post50hb,richly branching,1.6001453961458911,highest,"Calm down, have some dip.",3
post50hb,richly branching,1.6001453961458911,highest,All I read is that AI is more accurate than drs and the drs are worried.,1
post50hb,richly branching,1.6001453961458911,highest,"""It's likely that the system is detecting melanin""  
What? On an X-Ray? The author of this article would rather believe that skin somehow shows up on an X Ray and embarrass himself by writing it in an article rather than come to the obvious conclusion that different races must have differences in bone structure? Is it that somehow one of these things is PC but the other is horribly racist, even though they're both just purely anatomical differences?",1
post50hb,richly branching,1.6001453961458911,highest,"Neither of these things is racist. It's ok to have differences in skin and bone structure. Yes, the leading explanation is that it is indeed melanin that is causing the distinction. While skin doesn't 'show up' on an x-ray that does not mean the rays are entirely unaffected by it.",2
post50hb,richly branching,1.6001453961458911,highest,"> What? On an X-Ray? The author of this article would rather believe that skin somehow shows up on an X Ray and embarrass himself by writing it in an article rather than come to the obvious conclusion that different races must have differences in bone structure? 




Skin and soft tissue does show up on an x ray. It's faint but x rays do interact with soft tissue they always do. They just interact with hard tissue *more*",2
post50hb,richly branching,1.6001453961458911,highest,"> Skin and soft tissue does show up on an x ray. It's faint but x rays do interact with soft tissue they always do. They just interact with hard tissue more

Of course. But are you seriously suggesting that the presence or absence of melanin is directly detectible from an xray?",3
post50hb,richly branching,1.6001453961458911,highest,"I suggesting that its not an unreasonable concept. 


Melanin is known to dissipate ionizing radiation (thats literally what our skin uses it for), so the idea that darker skinned individuals could have a visually and practically imperceptible but machine readable difference than light skinned individuals is hardly out of left field.",4
post50hb,richly branching,1.6001453961458911,highest,"No, it will be things like:   
smaller stature: more likely Asian.",2
post50hb,richly branching,1.6001453961458911,highest,[removed],1
post50hb,richly branching,1.6001453961458911,highest,[removed],2
post50hb,richly branching,1.6001453961458911,highest,[removed],3
post50hb,richly branching,1.6001453961458911,highest,[removed],4
post50hb,richly branching,1.6001453961458911,highest,"This is only surprising/concerning to you if your worldview dictates that there must not be any physical or mental differences between people of differing race or even gender. Unfortunately this worldview is at odds with basic biology, which is why the AI is picking this up.",1
post50hb,richly branching,1.6001453961458911,highest,"It’s inevitable, so we will just go back to our past or send a better message to our future generation.",2
post50hb,richly branching,1.6001453961458911,highest,"That’s absolutely not the case. 

The concern here is potential inaccuracies or biases in the original dataset the AI learned from leading to it making simplistic assumptions about race, which appear accurate but are in fact the result of flawed data. The reason that’s concerning is because it would lead to misdiagnosis and mistreatments.

Wanting the AI to be accurate and reliable isn’t an effort to be politically correct, it’s the responsible and sensible thing to do.",2
post50hb,richly branching,1.6001453961458911,highest,It's not about the physical science. It's all about HOW I FEEL and WHAT WHO I IDENTIFY AS,1
post50hb,richly branching,1.6001453961458911,highest,What who are you shouting about?,2
post50hb,richly branching,1.6001453961458911,highest,The future baby,3
post50hb,richly branching,1.6001453961458911,highest,Well your Mom died because we turned off the function that factors race into ethnic specific ailments...but hey at least it wasnt racist,1
post50hb,richly branching,1.6001453961458911,highest,"It’s a good thing doctors don’t have access to patients medical charts which usually include race, that would be horrible!",1
post50hb,richly branching,1.6001453961458911,highest,"Race in itself is an unscientific and not accurate term.

The variation in a so called racial group is higher than the variation between those groups.",1
post50hb,richly branching,1.6001453961458911,highest,"> The variation in a so called racial group is higher than the variation between those groups

This is called Lewontin's fallacy. It doesn't matter.",2
post50hb,richly branching,1.6001453961458911,highest,[deleted],3
post50hb,richly branching,1.6001453961458911,highest,"It depends on the diversity threshold, if it was a lower threshold there’d be hundreds of races for example",4
post50hb,richly branching,1.6001453961458911,highest,"Not unless you treat all genetic diversity the same, but amount of genetic variation is unrelated to actual trait variation.",4
post50hb,richly branching,1.6001453961458911,highest,Yes - is kind of like stereotypes - there is some truth in that body-plans then to be geographically related to some extent.,2
post50hb,richly branching,1.6001453961458911,highest,"> Race in itself is an unscientific and not accurate term.

> The variation in a so called racial group is higher than the variation between those groups.

You are saying differences between groups are not meaningful / valid if variation within the group is greater?",2
post50hb,richly branching,1.6001453961458911,highest,No the distinction is nonsense because ur ordinary two white ppl have more difrences than the avarage black person to the avarage white person....,3
post50hb,richly branching,1.6001453961458911,highest,"By your logic there are no racial or sexual disparities in society since differences within groups are larger than between groups.

That seems like a questionable claim to make.",4
post50hb,richly branching,1.6001453961458911,highest,I mean I can predict a persons race with my eyes.... soooo,1
post50hb,richly branching,1.6001453961458911,highest,Can you?  Because there are a ton of black people on the USA who have pretty white skin.,2
post50hb,richly branching,1.6001453961458911,highest,"Most people can't, especially for multiracial people.",2
post50hb,richly branching,1.6001453961458911,highest,[deleted],1
post50hb,richly branching,1.6001453961458911,highest,"It could only identify to some probable level maybe a few different race types.

We don’t even have an accurate definition of race - just a few crude categories.",2
post50hb,richly branching,1.6001453961458911,highest,"As long as the AI has no unacceptable biases, wouldn't it simply recognize that there are different population groups without having any reason to distinguish them unless they actually \*are\* different for its purposes?

For example, if its job is to identify osteoporosis, if race is a factor then it's a factor.

What we don't want is an AI that's making false correlations in race - perhaps especially due to human beliefs and behavior.",1
post50hb,richly branching,1.6001453961458911,highest,On issue with our society right now is that unacceptable and false are very different things.,2
post50hb,richly branching,1.6001453961458911,highest,"If the AI can *accurately* discern a patient's ethnicity through bone and tissue structure then it stands to reason it is not ""being racist."" There are very specific markers and variations of tissue and bone structure that correspond to the various environments those patients' ancestors adapted to, even if scientists can't identify any themselves. 

These adaptations also reflect very specific genetic and biological markers for very specific diseases and ailments. So it stands to reason perfectly well that this level of specificity matters, even if we don't feel comfortable with it.

Keep in mind that the relative quick changes in modern society, the last 2000 years or so, don't compare to last 200,000 years of environmental and climatological adaptation. Our bodies have had a very long time to adapt to specific conditions, and an even longer time for those adaptations to take root in our physical structure. That history matters to medical diagnosis.

If people are concerned that an AI may do something with this information, why? There has to be a specific reason to be biased in the same way humans are in order to do so. That means if an AI learns to be biased, it's because someone intentionally taught it to be. Pure machine logic doesn't work like our logic, it has to be told to do it.

The biggest problem people have when observing the behavior of Artificial Intelligence (or any form of intelligence for that matter) is that regardless of its origins or composition, they tend to view it through the lense of the Human Condition. That just isn't an accurate way to look at the world if you aren't talking about humans specifically.

This is just another case of wait and see where it goes, you might be surprised. It could very well significantly advance our understanding of medicine.

But just to be safe, perhaps don't give this one access to Twitter, yeah?",1
post50hb,richly branching,1.6001453961458911,highest,"Wouldn't the correct way to say this is ""educated guess""??? Predict just sounds like it doesn't fit the way its intended to me, rather confused",1
post50hb,richly branching,1.6001453961458911,highest,Data Scientist here.  I think [this](https://linkinghub.elsevier.com/retrieve/articleSelectPrefsTemp?Redirect=http%3A%2F%2Fwww.thelancet.com%2Fretrieve%2Fpii%2FS2589750022000632&key=e3186b0fe73eea85b9cf4e0171a786bddaff503a) is the study.  At first glance I am a little concerned about the imbalance in some of those datasets and the reporting of auc roc as the metric.  Note the drop in auc roc for ct (72% black) vs x-ray (16% black).  I definitely would have preferred auc precision recall alongside auc roc for this.  I might be missing something though.,1
post50hb,richly branching,1.6001453961458911,highest,"Concerned about what? Was it accurate or not? 

And what specifically did it compare to determine it?",1
post50hb,richly branching,1.6001453961458911,highest,It’s probably just doing the same things forensic anthropologists do. Why not ask them?,1
post50hb,richly branching,1.6001453961458911,highest,"Why? This is literally what we designed AI to do, recognize patterns that humans can't see themselves.",1
post50hb,richly branching,1.6001453961458911,highest,I’m sure stuff like this definitely won’t make its way into the hands of insurance companies to use against you.,1
post50hb,richly branching,1.6001453961458911,highest,"The issue I see here is we are training the AI to incorporate racial differences in a way that could quickly get out of control, racially dependant functions of an AI is a big concern.",1
post50hb,richly branching,1.6001453961458911,highest,"Yeah this is a thing. We did this in one of my undergrad anatomy classes, had to identify the gender, race, and approximate age of a person based on only the femur. Fascinating",1
post50hb,richly branching,1.6001453961458911,highest,"Speaking as both an AI developer and anthropologist, this isn’t that surprising. There are lots of indicators (none 100% on their own) that can identify descent in the skull. In one of my classes in undergrad we had “the box” where skeleton parts were put in, and we had to identify what bone, what age, and gender/descent if possible.",1
post50hb,richly branching,1.6001453961458911,highest,"This is what is concerning??? 

We should be more concerned about the self replicating robots that we have created…. And the AI….  

It’s a recipe for an uprising… haven’t we learned anything at all from movies?",1
post50hb,richly branching,1.6001453961458911,highest,"Here's the link to the actual paper; https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00063-2/fulltext

They keep calling it ""AI"", but it's actually Machine Learning. What it outputs heavily depends on what's it trained with and what goals it's supposed to achieve with said training, which can result in extremely flawed results that [we have no idea how it came up with](https://www.nature.com/articles/s42256-019-0048-x), but look great at first sight.

Case in point; A ML could also give ""high precision"" results in [craniometry](https://en.wikipedia.org/wiki/Craniometry) when it's trained with data and goals to do so. But jumping from that to ""It's evidence for biologically distinct human races!"" is quite the leap, as that outcome was already predetermined by the training and task given in the very first place.",1
post50hb,richly branching,1.6001453961458911,highest,Machine learning is AI. It’s the same soup,2
post50hb,richly branching,1.6001453961458911,highest,I mean duh... I learned this in my jr college anthropology class. We even had a test where were would look at skulls and determine the sex and race. Iirc African skulls were the most difficult to 'sex',1
post50hb,richly branching,1.6001453961458911,highest,***Glenn Quagmire has entered the chat***,2
post50hb,richly branching,1.6001453961458911,highest,Isn't there a study that shows different races have different bone density? Wouldn't density be identifiable is x-rays?,1
post50hb,richly branching,1.6001453961458911,highest,AI works off facts. It’s does not work off political correctness. It will be interesting in the future as we adopt more AI and it does things that are factual but not allowed in today’s political climate.,1
post50hb,richly branching,1.6001453961458911,highest,Looked at dick and ass bones first. Came to conclusion.,1
post50hb,richly branching,1.6001453961458911,highest,At first glace I thought this was just hard coded phrenology lmfao,1
post50hb,richly branching,1.6001453961458911,highest,"The only possible bias I see is how people use that information afterwards, but the fact that there's something hidden on the images that shows racial characteristics is an amazing discovery",1
post50hb,richly branching,1.6001453961458911,highest,Reading this i personally can't wait for our AI overlords since even our scientists are morons.,1
post50hb,richly branching,1.6001453961458911,highest,…No shit. Bone structural differences are like one of the singular foundations of modern biology,1
post50hb,richly branching,1.6001453961458911,highest,Ahh I see this AI got updated with the Phrenology DLC. Nice 😑,1
post50hb,richly branching,1.6001453961458911,highest,"Why would this be concerning? There are, in fact, population differences in the biological realm that mean things for medical treatment. In the U.S., the African-American population is far more probable for things like sickle cell anemia…and so that population should be medically screened for the condition. There are many such probabilities across populations. Whatever the differences in skeletal makeup are giving AI the ability to discern, they are likely real, and may present opportunity for differentiated medical approaches that are more effective across all populations. This appears to be scientific findings, not necessarily external bias at work.",1
post50hb,richly branching,1.6001453961458911,highest,Because we only want data when it leads to the conclusions we wish for.,1
post50hb,richly branching,1.6001453961458911,highest,"The only problem I could see is whether the predictions are accurate to a reasonable degree, I could predict that a show I like is gonna win an award because I thought it was good but that's not an accurate prediction at all",1
post50hb,richly branching,1.6001453961458911,highest,My argument is about how to go about it to change the future.,2
post50hb,richly branching,1.6001453961458911,highest,Idk why this has to be racist. It makes sense that your body type could be effected by environment.. it really annoys me that a knee jerk reaction is to make it racist. Like if a Asian person and a black person could have a kid do we need any more evidence that we’re all the same enough? No one is sub human or what ever. You and another human being can fuck and make another person regardless of their skin tone or skeletal structure.,1
post50hb,richly branching,1.6001453961458911,highest,Isn’t this easy to understand as in the code finds certain parameters that have certain trends? Can’t we just look at the parameters driving the “decision”?,1
post50hb,richly branching,1.6001453961458911,highest,"Should ask the AI to teach us, explain which parts of the body makes it possible to differentiate. They are so smart they will probably replace teachers at some point.",1
post50hb,richly branching,1.6001453961458911,highest,"So if the primary fear is AI making a misdiagnosis, shift it’s x-ray race identifying role to a forensic one. Shit I don’t know, I’m high AF.",1
post50hb,richly branching,1.6001453961458911,highest,"I would be concerned too. Teaching were all the same, then computer who’s never wrong. Can use physical features to tell what race you are!!

PC culture can’t stop science!",1
post50hb,richly branching,1.6001453961458911,highest,I mean I think that’s kind of cool.  Especially in an archeological setting.,1
post50hb,richly branching,1.6001453961458911,highest,"We're so PC, we have to call the AI racist and concerning for picking up a really cool potentially useful pattern",1
post50hb,richly branching,1.6001453961458911,highest,"This is a non-issue, if AI can detect irregularities that means it’s science… the only prejudice is from people who get upset over garbage concerns that don’t exist",1
post50hb,richly branching,1.6001453961458911,highest,"Fake concern clickbait. Forensic anthropologists can ID race based on skeletons, so why wouldn’t a human programmed AI do so as well?",1
post50hb,richly branching,1.6001453961458911,highest,Maybe they should make a “woke” version to appease the easily offended. I agree it could help with diagnosing illnesses predominantly in certain races.,1
post50hb,richly branching,1.6001453961458911,highest,"The only people concerned are people trying to put their head in the sand about the reality of race for political reasons, ironically undermining the effect they're trying to achieve",1
post50hb,richly branching,1.6001453961458911,highest,Even the damn AIs are racist? Terminator did not predict this.,1
post50hb,richly branching,1.6001453961458911,highest,How is this concerning and not obvious? Science/biology itself is racist now? I can't believe how wacked out we've become.,1
post50hb,richly branching,1.6001453961458911,highest,Why is this a concern? We did this in anthropology classes because it’s actually fairly simple to identify someone’s biological sex and race through only looking at their skeleton.,1
post50hb,richly branching,1.6001453961458911,highest,"They're ""worried"" that AI's able to predict the race of a person just based on x-rays? I think that's AMAZING, not something to be worried about. It's another great development in technology! 

I wonder if they can use it as a tool to help solve murder cases or when they dig up skeletons in ancient sites. You can do it manually I think, but a machine can speed up the process. 

One thing that will slow down science is wokeism. ""Ah, the AI is able to identify that this skeleton is white, black, asian, whatever, we might offend people! We might be empowering stereotypes!"" Oh come on. You feed data to AI, then when the data shows accurate results, you get surprised and ""worried"" because it might feed to stereotypes? Well isn't the point of science using data and evidence in order to get the truth? Science is science. If one race is found to be usually taller than another, is that biased? No, it's simply fact. So what if it will lend to the stereotype that the taller race are basketball players or they're usually great at being athletes? Nothing wrong with that. You're not using this information to do bad things to them like Hitler or some other racist bad guy. Facts are facts, characteristics are characteristics. I don't think science should bow down to worries of being ""offensive"". If facts or data offend people, it's on them. Just because some people want to believe that the earth is flat doesn't make it so.",1
post50hb,richly branching,1.6001453961458911,highest,"If these machines can make these determinations today, what will they be able to do tomorrow? Skynet!",1
post50hb,richly branching,1.6001453961458911,highest,"Don't mean to crush Redditer's feelings, but this is what race is and how it is identified. By bone structure.

Ethnicity is different altogether and I would be surprised if it could do that with an xray. That would be impressive.",1
post50hb,richly branching,1.6001453961458911,highest,To be fair i could probably predict peoples races and i dont even need an xray machine so im not too worried,1
post50hb,richly branching,1.6001453961458911,highest,Finally! As a doctor I have so much trouble telling when patients are black,1
post50hb,richly branching,1.6001453961458911,highest,"They know how, they just don’t want to talk about it for fear of being labeled racist.

The differences in our skulls are well known, there are other difference elsewhere in our skeleton that are little talked about, I recall a biology teacher mentioning it before literally telling us it wasn’t widely accepted or taught due to “racism” and simple said that if there are differences in the structure of the skull just imagine what else makes us unique elsewhere in our bodies.   Made sense then and still does today.


Fortunately AI doesn’t play that “it’s racist” game, it simply finds and matches patterns.",1
post50hb,richly branching,1.6001453961458911,highest,... The races have different skeletal structures... That has been known for a long while now...,1
post50hb,richly branching,1.6001453961458911,highest,"The problem here is that the AI isn't accurately predicting race from X-Ray images. They've developed a system which links an unknown set of features of images to a set of labels that the researchers had previously applied to a sample set of images. The system doesn't tell the observer *why* it believes the sample image to correlate with certain labels.

As an example of potential bias in the training set, if the ""African"" label training set was heavily drawn from hospitals with older, lower resolution imagers perhaps the AI associates low-resolution images with the ""African"" label. Remember that one of the claims in the article is that they used images which had none of the clues that humans usually look for, so we have to figure out what clues the AI found.

The concern expressed in the article is that the system cannot explain what it found.

At some point the diagnosis software needs to be able to explain why it makes a decision so that the humans relying on it can determine if the AI is missing something obvious from its training set.

A far better path is to train the AI to recognise features in the various imagery types, and then figure out how those patterns relate to interesting characteristics of the patient's medical condition.",1
post50hb,richly branching,1.6001453961458911,highest,It also knows what gender you are and not what gender you wished you were.,1
post50hb,richly branching,1.6001453961458911,highest,I see this as a concern if it were a black mirror episode. Have weaponized drones target people of a specific race.,1
post50hb,richly branching,1.6001453961458911,highest,"An anthropologist can determine race, sex and age by viewing a skeleton",1
post50hb,richly branching,1.6001453961458911,highest,"For everyone thinking that this somehow “proves” race is biologically real beyond colorism, this is the point of the article, “The study adds to a growing body of data that AI systems can often replicate human biases and prejudices, whether they be racist, sexist, or otherwise. Skewed training data can lead to skewed findings, rendering them useless.
This must be balanced against artificial intelligence's strong ability to process far more data much faster than humans can, in anything from disease diagnosis to climate change predictions.

There are many unsolved questions from the study, but for the time being, it's crucial to be conscious of the possibility of racial bias in artificial intelligence systems - especially if we're going to give them more responsibility in the future.”",1
post50hb,richly branching,1.6001453961458911,highest,"Woe is me, the impartial computer can discern racial skeletal differences in humans, now I can't pretend that race is skin-deep, ablooabloo.",1
post50hb,richly branching,1.6001453961458911,highest,The issue isn’t that it can detect self-reported race.  The issue is that it’s missing diagnoses for those it labeled as black.,2
post50hb,richly branching,1.6001453961458911,highest,"That's fucky, but the issue isn't with what the scientists reported, it's just an AI that ended up detecting shit other than what it was inded to do - see also ""AI designed to discern croissants from bear claws turns out to detect cancer"" for a positive version of this. The problem I take is with the sensationalized, racebaiting angle on the reality of things. Like no shit there are skeletal differences between ethnicities, only the truly delusional would argue otherwise.",3
post50hb,richly branching,1.6001453961458911,highest,The issue was that medical AI are currently under diagnosing black people and the team is trying to figure out why. This is where this specific experiment came in. At least that’s what I’m getting from the article. What kids of biases are being introduced in detection AI that they are missing diagnoses in a specific subset of people. One that is already routinely under diagnosed by humans.,4
post50hb,richly branching,1.6001453961458911,highest,"this can't be possible, it's just a social construct with 0 basis in biology!",1
post50hb,richly branching,1.6001453961458911,highest,Sarcasim or do you think all people are the same on x rays?,2
post50hb,richly branching,1.6001453961458911,highest,take a guess,3
post50hb,richly branching,1.6001453961458911,highest,"I'm from Britain so I assumed sarcasm, lots of those over seas don't get it nor use it correctly so online it's sometimes hard to tell.",4
post50hb,richly branching,1.6001453961458911,highest,I think you are thinking about the argument for gender. I believe people’s facial bone structure and maybe stature are fairly different depending on your race (?).,2
post50hb,richly branching,1.6001453961458911,highest,There is only one race…the human race. Proof is in the blood,1
post50hb,richly branching,1.6001453961458911,highest,I am not of the same race as Africans,2
post50hb,richly branching,1.6001453961458911,highest,"So wait...

 the whole ""everyone's the same on the inside"" thing was a lie?!",1
post50hb,richly branching,1.6001453961458911,highest,"No, everyone has the same worth. That's not a lie. It's a lie that everyone's the same though. Are you the same as your sibling? Probably not right?",2
post50hb,richly branching,1.6001453961458911,highest,Oh wow I was joking lol,3
post50hb,richly branching,1.6001453961458911,highest,Anthropologist can already do that for the past 150 years. Fake news.,1
post50hb,richly branching,1.6001453961458911,highest,"why would anyone give a shit ? scientists are concerned my ass, lmao shitty ass title.",1
post50hb,richly branching,1.6001453961458911,highest,"you said why but the message disappeared, there are millions of doctors and scientists, there is no scientific or medical consensus you cant get even a 100 people to agree on one thing, youll have doctors tell you to cut all the sodium from your diet and youll have doctors tell you to increase it above the reccomended daily amount, there is bound to be a doctor somewhere that would agree with any bad health idea you have and there is bound to be a scientist somewhere that believes the same dumb shit youre spewing.",2
post50hb,richly branching,1.6001453961458911,highest,"no but why would a doctor say pizza is good ? Like why would a doctor say that. If a doctor says that then they probably aren’t a doctor ? jesus man, logic really just isnt hard to use.",3
post50hb,richly branching,1.6001453961458911,highest,"im sure some scientists somewhere are just as im sure some doctors somewhere reccomend you eat a whole pizza every day, there has to be at least 1.",2
post50hb,richly branching,1.6001453961458911,highest,"Whoa hold on, race is a social construct. Twitter says so. So, how can this be?",1
post50hb,richly branching,1.6001453961458911,highest,"Wait wait— races can NOT be biologically different at all, whatsoever. This goes against my entire 8 years studying race and gender!",2
post50hb,richly branching,1.6001453961458911,highest,"The true ethical question involved here is not that AI can make these detections, but that *those concerned* may be projecting the human failures of racism and prejudice on machines that would neither have nor use such undesirable behaviors in decision-making. 

Considering that there are medical conditions keyed to genetic background, AI detection and awareness would seem to be an enhancement of medical capabilities. As members of my own ethnicity are inordinately susceptible to a specific disorder, that would be a positive capability.",1
post50hb,richly branching,1.6001453961458911,highest,"Human can predict people’s race from photos, and scientists are concerned. About the same. Nothing here, moving on.",1
post50hb,richly branching,1.6001453961458911,highest,"""predict""?

Welp, AI keeps being what we falsely consider racist by acknowledging differences (without assigning value to them).",1
post50hb,richly branching,1.6001453961458911,highest,I do find it interesting that we are more willing to call machines racist than acknowledge that there are physical differences between groups of humans.  Such knowledge does not require a value judgement at all.,2
post50hb,richly branching,1.6001453961458911,highest,"It doesn't SEEM to make much sense why this concerns them. It's just a reality of science, it's not AI being 'racist'. It's AI picking up data you didn't know was there because it can observe so many variables so thoroughly. 

You bones ARE racially biased, so is your face, if someone takes a picture of it they will have a record containing your race. It's not a new concept other than we can do it with bones more reliably because of AI.

How is that not exactly what you'd expect?",1
post50hb,richly branching,1.6001453961458911,highest,Because there is a nice pleasant theme that many things are skin deep. Anti-scientific rhetoric.,2
post50hb,richly branching,1.6001453961458911,highest,"What's next! AI is dangerous, it probably won't even use the right pronouns.",1
post50hb,richly branching,1.6001453961458911,highest,"Wow it's almost as if there are real genetic and physiological differences between races, who would have thought that hundreds of thousands of years of different selective pressures would have done this? /s",1
post50hb,richly branching,1.6001453961458911,highest,"Concerns could also be an advantage, depending how you look at it",1
post50hb,richly branching,1.6001453961458911,highest,":/

The AI is just doing its job, no internal bias except to say there are slight differences. Which is natural in populations of any living creature that are separated geologically, over enough time. Hence different skin colors/facial features. There is nothing wrong with that at all. 

The scientists sound like the ones with the bias.",1
post50hb,richly branching,1.6001453961458911,highest,"You’re mostly right, except this issue here is that if the computer can tell the difference between two people’s race, will it diminish signs of other disease in one subset and not in the other. 

In other words, will the training set for a disease, such as searching for lung cancer, work equally well for Black or Asian as it will for White? What if 90% of the lung cancer teaching set came from Whites? Will the machine be able to pick out lung cancer in Blacks with equal ability? What about a lymphoma AI data set? If it were developed in Korea, would it work equally well on Westerners? These AI biases can really mess with research and practical accuracy.

So being able to determine race by looking at a skeleton is one thing (and there are humans who can do that, despite what the article says), but will this impact *other* applications without us realizing? If it does, we’ll need to very carefully select our training data sets to have a specific balance of races/ethnicities in the data set. But if we’re not sure of what the machine is keying in on in the first place, we’ll have trouble balancing these data sets to be race-agnostic.",2
post50hb,richly branching,1.6001453961458911,highest,"With enough data from different sets of people, I would argue *millions* of people, it would be a breeze. We shouldn't focus too much on the race part. We should focus on more data-points to make the data as bulletproof as possible.",3
post50hb,richly branching,1.6001453961458911,highest,"Agreed. Though currently, most AI trains sets have only a few thousand at best. Source: me, as I’ve helped build and train these data sets. Most medical imaging AI systems are not ready for broad use. Useful in specific controlled situations but definitely need better training before more broad use can be trusted.",4
post50hb,richly branching,1.6001453961458911,highest,"To all the people asking what the big deal is: the issue here is that if the computer can tell the difference between two people’s race, will it diminish signs of other disease in one subset and not in the other in a “biased” manner based on how the algorithm was trained.

In other words, will the training set for a disease, such as searching for lung cancer, work equally well for Black or Asian as it will for White? What if 90% of the lung cancer teaching set came from Whites? Will the machine be able to pick out lung cancer in Blacks with equal ability? What about a lymphoma AI data set? If it were developed in Korea, would it work equally well on Westerners? These AI biases can really mess with research and practical accuracy.

So being able to determine race by looking at a skeleton is one thing (and there are humans who can do that, despite what the article says), but will this impact *other* applications without us realizing? If it does, we’ll need to very carefully select our training data sets to have a specific balance of races/ethnicities in the data set. But if we’re not sure of what the machine is keying in on in the first place, we’ll have trouble balancing these data sets to be race-agnostic.",1
post50hb,richly branching,1.6001453961458911,highest,They are “alarmed” more like surprised because they have no idea how the AI is doing it.,1
post50hb,richly branching,1.6001453961458911,highest,There’s also a racist AI that can tell if you’re white by your go to dance 🕺 moves,1
post50hb,richly branching,1.6001453961458911,highest,"A good physical anthropologist can also do this! The ability is nothing new, the information is all there; however, it’s what you plan on doing with the information that should be of concern.",1
post50hb,richly branching,1.6001453961458911,highest,"You could do that before AI.

Anyone trained properly could do it.

This identification is done in Anthropology and Forensics all the time this is a non issue.  The blunt fact is there are differences in the  geometry of the bones which are adaptations to the various climates  and conditions found on earth.

Doctors simply don't note them as it's quite time consuming for a person to do it manually. An AI which would note and measure EVERYTHING at a rapid pace would of course have an easy time doing this. A human is perfectly capable of measuring things. 

My god the machine we invented to measure things quickly, accurately and recognize patterns is measuring things quickly, accurately and recognizing patterns! the horror!",1
post50hb,richly branching,1.6001453961458911,highest,This is of no concern to scientists. It is only concerning to “woke” people. The two are not the same.,1
post50hb,richly branching,1.6001453961458911,highest,"Why is everyone so adamant that we all look the *exact* same on the inside?

Certain races are predisposed to certain diseases at higher rates than others, for instance.

Muscle fibers are provably different among races and nobody has complained about that. (Eg ratio of fast twitch/ slow twitch)

Skin color of course differs, but I’m pretty sure if I changed a random southern Chinese lady’s skin tone to be bright white and gave her blonde hair she would look… different from most Swedes to say the least.

Not sure why it’s so shocking that skeletal features could have slight differences as well.",1
post50hb,richly branching,1.6001453961458911,highest,"Correct me if I'm wrong, but wouldn't the hypothetical woman share some characteristics with some Finns and Sami?",2
post50hb,richly branching,1.6001453961458911,highest,Maybe the differences aren't just skin deep. And that's ok.,1
post50hb,richly branching,1.6001453961458911,highest,Never thought I’d live to see the resurrection of Phrenology 🤦‍♂️,1
post50hb,richly branching,1.6001453961458911,highest,It is concerning because race doesn't exist at all.,1
post50hb,richly branching,1.6001453961458911,highest,"But morphological differences between populations do exist, and can be used to infer a person's ancestry based on their skeleton, anthropologists have been doing it for decades.",2
post50hb,richly branching,1.6001453961458911,highest,"read the article, either AI learned racism from us or it can see systematic racism. kind of like how a republican senator said our maternal death rate is not that bad if you don't count women of color.[Maternal death rate isn't as bad if you don't count Black women, GOP senator says

](https://www.businessinsider.com/gop-senator-la-outlier-maternal-death-rate-skewed-black-women-2022-5?op=1)",1
post50hb,richly branching,1.6001453961458911,highest,Republicans: can we teach it think one is better than the others?🧐,1
post50hb,richly branching,1.6001453961458911,highest,Data is data. All this means is that we're built slightly differently and there's nothing wrong with that.,1
post50hb,richly branching,1.6001453961458911,highest,dude are yall seriously trying to say robots are fucking racist?,1
post50hb,richly branching,1.6001453961458911,highest,"How is this worrying? If there are biological differences between two groups of people and the AI is able to distinguish between them, then that's the exact opposite of racism because the difference is actually there.",1
post50hb,richly branching,1.6001453961458911,highest,"No no. To think we are different in any way is racism.

Well thats their logic atleast but yes you are correct!",2
post50hb,richly branching,1.6001453961458911,highest,"Forensic anthropologists predict race from bones accurately 70% of the time. 

This doesn’t seem like a story.",1
post50hb,richly branching,1.6001453961458911,highest,"I mean, it's easy if you think about it.

Golden chains and jewelry appear in X rays, so black people is easily identifiable.

Asian people are easily identifiable too: the X ray is paid upfront and without breaking a sweat.

White people are even easier to identify: they always appear in the X ray fearfully looking in the direction where the black and the asian are sitting.",1
post50hb,richly branching,1.6001453961458911,highest,"This is really stupid; haven’t we always been able to do this from just skull features alone?

Have people forgotten what human races even are?",1
post50hb,richly branching,1.6001453961458911,highest,">Have people forgotten what human races even are?

Actually, there are no human races. Race is a biological term that simply does not apply to humans, the genetic differences are *way* too small. Look at dogs and their vastly different shape and sizes, there we can identify different races; humans by contrast aren't that much different. For whatever reason the term ""human race"" is still in use in the US, but elsewhere in the world ethnicity is used to describe a group of people with similar regional roots.",2
post50hb,richly branching,1.6001453961458911,highest,Human ethnicities are visually and genetically identifiable phenotypes that are thousands of years old; the term “race” is perfectly valid,3
post50hb,richly branching,1.6001453961458911,highest,Predict peoples ethnicity as there is only the Human race.,1
post50hb,richly branching,1.6001453961458911,highest,Yeah a race to the bottom. What’s your point?,2
post50hb,richly branching,1.6001453961458911,highest,"According to new research, deep learning models based on artificial intelligence can identify someone's race merely by looking at their X-rays, which would be impossible for a human doctor looking at the same photos.  
  
The findings raise several serious concerns concerning AI's role in medical diagnosis, assessment, and treatment: Could computer algorithms mistakenly apply racial bias when analysing photographs like these?",1
post50hb,richly branching,1.6001453961458911,highest,"Hold on, why do you think this is bad? A computer can't be racist.      
I *want* the damn thing to be able to do this if it helps people get better treatment.         

Different races have different susceptibilities, it's not racist to state the truth. That's like saying it's racist to assume who gets easily sunburnt or freckles by their skin colour.",2
post50hb,richly branching,1.6001453961458911,highest,OP is just quoting the article.,3
post50hb,richly branching,1.6001453961458911,highest,">Could computer algorithms mistakenly apply racial bias when analysing photographs like these?

No. Not mistakenly",2
post50hb,richly branching,1.6001453961458911,highest,"If the system is trained to be biased, then it will be biased.  It depends on the application to know whether that’s a good or a bad thing.  For example, in medicine it’s generally a good thing, since there are differences in the various races.  Conversely, race when hiring is bad.  Amazon (I think) couldn’t make an AI that wasn’t racist for some reason.",2
post50hb,richly branching,1.6001453961458911,highest,Medical differences are more likely to be at the national or tribal level vs the popular racial one. South Africans and Ethiopians look black but don’t get sickle cell.,3
post50hb,richly branching,1.6001453961458911,highest,"I know race does come into play, but I’m not a doctor or statistician who knows details about it.  I would expect an AI that exhibits bias in this way would aggregate based on nationality, race, sex, and a myriad of other factors that we probably don’t even know about yet.",4
post50hb,richly branching,1.6001453961458911,highest,">which would be impossible for a human doctor looking at the same photos.

Never seen a single episode of Bones...",2
post50hb,richly branching,1.6001453961458911,highest,Depends on the bones in the xray and the type of doctor.  This is fairly common in anthropology,2
post50hb,richly branching,1.6001453961458911,highest,[deleted],3
post50hb,richly branching,1.6001453961458911,highest,"We're surprised that the ai we programmed to do a thing, can do a thing",4
post50hb,richly branching,1.6001453961458911,highest,There was a COVID study in which AI thought it could predict bad cases of COVID... Turns out it made its prediction by the font used in the CT scan images (depending on the hospital).  I would strongly guess there's a confounding factor other than actual race here.,2
post50hb,richly branching,1.6001453961458911,highest,">  I would strongly guess

Based on what?  What's the difference between a guess and a strong guess?",3
post50hb,richly branching,1.6001453961458911,highest,You are getting downvoted because people didn't bother to read the article and they don't realize you are just quoting the first paragraph.  The article itself is worthy of downvotes though.,2
post50hb,richly branching,1.6001453961458911,highest,"if he put quotes around it, implying that the words are not his own. It would likely show that yes",3
post50hb,richly branching,1.6001453961458911,highest,Forensic Anthropologists have entered the chat.,2
post50hb,richly branching,1.6001453961458911,highest,"I don’t see anything in the article unpacking why the results of this study are concerning. I’m aware of the risks of AI reflecting back prejudice that was implicit in the way it was trained, but it’s not clear to me why we should be concerned about a system making this particular factual assessment with a high rate of accuracy. Is the idea that once the system identifies a patient’s race it can then view other pieces of the diagnostic puzzle through that lens? And if so, what about that is particularly concerning? This seems like a really different scenario than, e.g., using AI facial recognition for law enforcement.",1
post50hb,richly branching,1.6001453961458911,highest,"Ok but doesn't this go completely against the whole ""race is a social construct""?",1
post50hb,richly branching,1.6001453961458911,highest,"My understanding is that race generally has genetic components (although in modern times these are getting more mixed up).

Different races came from different parts of the world.  And while those also had different cultures, so there was a correlation between race and culture, they are in fact different entities.",2
post50hb,richly branching,1.6001453961458911,highest,It's pretty confusing because half the time it's only a social construct and to point out differences between races is racist.   Then at the same time AI can determine the difference with only an x-ray.,3
post50hb,richly branching,1.6001453961458911,highest,"It’s not just a social construct.   
Going back to basics - different races came from different parts of the world.
While that meant cultural differences, there were also different body-structure differences too, for any manner of different reasons. (no single cause)

People from the same area of the world tended on average to look similar to each other - that’s where part of the idea of different races came from.

Of course, we know that people attached other attributes, which really had nothing to do with race - and that’s where racism started to creep in.",4
post50hb,richly branching,1.6001453961458911,highest,"""Race is a social construct"" was never real in the first place. Race is based on observable characteristics, it's based on averages of many genes and how those affect each individual's appearance. It's not defined by measurements of precisely what genes an individual has, because until recently that information was impossible to acquire and even now it's not like we're fully gene mapping the entire population. But the averages of those genes were observable - just like how you don't have to catalogue every passing photon to make a judgement on whether one light source is brighter than another.

The observable characteristics associated with each race correlate with other characteristics, so while physical appearance does not purely dictate any other characteristic like propensity for a particular genetic disorder, it is a statistical fact that it indicates non-appearance traits. So long as you believe genetics can affect things besides appearance (it does) and understand the fundamental statistics around genetics and the correlations involved, it would be self-contradictory to claim physically observable race has no correlation to non-physically observable characteristics.",2
post50hb,richly branching,1.6001453961458911,highest,Yeah we're not all the same.   I get the point of saying we are but it's simply not true,3
post50hb,richly branching,1.6001453961458911,highest,"It is a social construct, but it doesn't mean we don't have biological differences.",2
post50hb,richly branching,1.6001453961458911,highest,If there are biological differences then it cannot be social,3
post50hb,richly branching,1.6001453961458911,highest,Ok so different races are in fact different?,3
post50hb,richly branching,1.6001453961458911,highest,"Ok I guess you're right.

People of different races have anatomical differences and don't have the same appearance. I guess the social construct idea applies to dividing races as if they're a different species.",4
post50hb,richly branching,1.6001453961458911,highest,What? So they trained an AI to do something and it's doing it extremely well and accurately. Now their worried its going to do the thing they trained it to do?,1
post50hb,richly branching,1.6001453961458911,highest,"Yeah it can prob ""predict their race"" (whatever that means) from visible light images, too, so what? Should we ban cameras?",1
post50hb,richly branching,1.6001453961458911,highest,"""Oh no, the robot is racist"". What a clickbaity title. The article literally states the AI can accurately predict race based on... race. WTF am I reading? There are no ethical concerns involved in this. Application of the technology in a private, secretive, and sensitive world is another story. Read the T's and C's for your next X-ray if you don't want to know any facts, I guess.",1
post50hb,richly branching,1.6001453961458911,highest,"Time to cancel xrays on twitter, turns out they have prejudice. Sporting their xray privilege over the rest of the medical equipment.",1
post50hb,richly branching,1.6001453961458911,highest,"Why are scientists concerned? I can predict other people's race from the colors of their skin. Racial differences exist. We're are still humans though, so who gives a shit?",1
post50hb,richly branching,1.6001453961458911,highest,There is differences between races. Who would have thought. Its not just skin deep. We all knew this. Good that AI won't care at all. Pattern recognition is racist apparently.,1
post50hb,richly branching,1.6001453961458911,highest,"Basically silly. We have a new tool, AI, and it gives us more information. Now we are afraid of this information because it’s race, oh my goodness. Please get over yourselves. We’re all grown ups, and the ones who aren’t oh well.",1
post50hb,richly branching,1.6001453961458911,highest,Oh no the AI can see that people are different?! But but I thought we were all the same on the inside?!?! Ahhhhhhh!!!!!!!!!!!,1
post50hb,richly branching,1.6001453961458911,highest,"Why would they apply racial bias? 

An AI isn't going to give any kinds of fuck about race. They are just going to report the facts. 

There are obvious physical differences between races. That isn't a surprise. 

The information is useful in a medical context. 

If the ai were making school admissions decisions and determined that the language style of an essay was that of an African American woman. It would rely on the variables they were using to make the choice. 

Humans have biases that they hide from themselves. 

Ai doesn't. If bias is suspected, the algorithm is tweaked. A good chance that the bias would be detected by another ai. The bias would be corrected by an AI. Likely before any actual choices are made.",1
post50hb,richly branching,1.6001453961458911,highest,"Dude, we have a whole section of science that already does this... Anthropology is a thing. I'm also not sure who this could be seen as concerning in any way.",1
post50hb,richly branching,1.6001453961458911,highest,"AI would only be bias if we taught it bias.  Archaeologists and anthropologists have identified race, sex and age from skeletal remains for many years this is nothing new. Specialists can build up a flesh replica of a skull to aid in identification to aid in police cases.",1
post50hb,richly branching,1.6001453961458911,highest,I think the issue they’re trying to show here is there is extreme bias in positions of power and this is essentially a way to detect the race of people entering your country as they enter through something like customs.,2
post50hb,richly branching,1.6001453961458911,highest,Given that xrays emit radiation xraying people without due reason wouldn't be practical.,3
post50hb,richly branching,1.6001453961458911,highest,They do this every time you enter an airport in the untied states. The secure areas specifically but try getting on a plane without going through an x-ray.,4
post50hb,richly branching,1.6001453961458911,highest,Can’t the customs agent just look and see what race people are?,3
post50hb,richly branching,1.6001453961458911,highest,The point of using these systems is they’re more likely to catch edge cases than the border agent. Good example might be the various stories of Jews being mistaken as not Jewish and hiding in plain site in Germany. This may be better than us guessing based on what we see.,4
post50hb,richly branching,1.6001453961458911,highest,"More like. The scientists bosses are concerned because that will risk funding to be cut because ""we cant fund racial biology experiments"". Blah blah blah. Radical left is starting to become as harmful to society as the radical right is.",1
post50hb,richly branching,1.6001453961458911,highest,"Damn, so the guys measuring different races skulls way back were onto something? /s",1
post50hb,richly branching,1.6001453961458911,highest,"Skull, Ribcage, Pelvis, Spine... Arms, Legs... Biped. Erect. Jupp. Homo Sapiens.",1
post50hb,richly branching,1.6001453961458911,highest,"I mean, they're probably concerned on the perceived ethical side that a robot figuring out race via bones might be a tad reminiscent of the old ""skull measuring"" thing again.

Realistically, I don't see an issue with being used in the medical field, as this might help with more accurate diagnostic AI understanding of fundamental differences in people's race and help use that to better understand the individual.",1
post50hb,richly branching,1.6001453961458911,highest,"African Americans hardly suffer from macular degeneration, while Caucasian blue eyed males suffer from AMD at a higher rate. As long as it’s not making medical assumptions based on ethnicity, this seems like it could be used for good. There are plenty of diseases that have different rates across different ethnicities… has nothing to do with not being woke",1
post50hb,richly branching,1.6001453961458911,highest,"I can predict people’s race just by looking at them! Take that, AI.",1
post50hb,richly branching,1.6001453961458911,highest,Archeologists can tell race from bones as well.  That's not racist if that is the race they are.,1
post50hb,richly branching,1.6001453961458911,highest,"Why is this even remotely surprising or concerning? Obviously there is going to be *some* level of differences, we already know that there are differences in bone density. AI is going to pick up on that and categorize efficiently, doesn’t matter if a human can understand the process. 

Imagine an AI that can detect cancer before human doctors do, nobody is going to be “concerned” that we can’t totally understand the process.",1
post50hb,richly branching,1.6001453961458911,highest,Seems like a good thing to me. I’m sure it could be used for bad but it could also be used for good too.,1
post50hb,richly branching,1.6001453961458911,highest,They are scared the AI will start believing in phrenology lmao,1
post50hb,richly branching,1.6001453961458911,highest,"“Respect my bumps, and protuberances!”",2
post50hb,richly branching,1.6001453961458911,highest,"Should be nothing but a weird factoid.
But no people see racisme and everyone loses there minds",1
post50hb,richly branching,1.6001453961458911,highest,"Why is this surprising? There are changes to the skull depending on which race you are. This is even known in the skull replica business, you can literally select which races skull you want when buying official replicas. So of course you can determine race off of skeleton.",1
post50hb,richly branching,1.6001453961458911,highest,"Are you telling me the phrenologists might have actually been on to something all along…? _Oh no._

/s",1
post50hb,richly branching,1.6001453961458911,highest,Like what's the problem with it? Isn't that what your DNA also tells about your ancestry,1
post50hb,richly branching,1.6001453961458911,highest,Have we become so politically correct that we can't help passing the solely human trait of racism onto AI?,1
post50hb,richly branching,1.6001453961458911,highest,"""Which would be impossible for a human doctor looking at the same photos"".

Yeah, I'm gonna call BS on that one. My high school offered a forensic science course as an elective. Skeletal structures were a large part of that, and identifying age, gender, and race from skeletal remains was one of the things we learned to do.

It's not perfect, but it's quite possible to make a reasonable guess at those characteristics based solely on bones, or an x-ray photograph.",1
post50hb,richly branching,1.6001453961458911,highest,Lol what? Anyone can see the differences in skull morphology between race... How exactly is an AI having pattern recognition concerning?,1
post50hb,richly branching,1.6001453961458911,highest,"If you look past peoples' skin and can make accurate predictions, how is that a bad thing? If anything, this shows a lack of sociological bias and an affinity for objective truths. The only downside would be if the training data was incorrectly biased by the trainers' preconceptions and the AI learned to mimic a bias instead of being able to make objective observations.    
If this is indeed accurate, why would you want anything but the most pure of correlations?",1
post50hb,richly branching,1.6001453961458911,highest,"At a practical level, everyone from police to archeologists have been able to specify ‘race’ from bones alone for a very long time.   DNA is obviously much more accurate - down to close family members - so I’m not sure why this is stunning?

Race is a very goofy artificial construct - certainly the way it’s used in the US - but it’s based on real genetic traits that get passed along and your genetic lineage is expressed through your morphology

I’d be interested to see what practical value this really has.  Running a DNA test is a helluva lot easier & cheaper than x-rays.",1
post50hb,richly branching,1.6001453961458911,highest,"> I’d be interested to see what practical value this really has.  Running a DNA test is a helluva lot easier & cheaper than x-rays.

Is it really?  I have to send away for a DNA test, but I can go to my local Dr office (or even chiropractor office) to get an X-ray.  They seldom even cost that much for the X-ray.",2
post50hb,richly branching,1.6001453961458911,highest,Lmao imagine claiming this ai is prejudice and racist instead of admitting or at least conceding the fact there may be more differences between our races,1
post50hb,richly branching,1.6001453961458911,highest,Difference between groups that evolved in different environments? Racist!,1
post50hb,richly branching,1.6001453961458911,highest,This could be really useful for helping identify skeletal remains faster.,1
post50hb,richly branching,1.6001453961458911,highest,"You CANNOT use x-rays on people without their consent, so this predictive ability is useless.",1
post50hb,richly branching,1.6001453961458911,highest,Why are they concerned? Shouldn't this be a good thing?,1
post50hb,richly branching,1.6001453961458911,highest,"Why are the “scientists” concerned? Different races have documented skeletal differences….
Example: anterior vs posterior tilt of the pelvis commonly seen in Asians. Hell even the width of the pelvis varies greatly across the world.",1
post50hb,richly branching,1.6001453961458911,highest,This was already taken into account. The AI seemed to still be able to accurately predict without that information.,2
post50hb,richly branching,1.6001453961458911,highest,"“An international team of health researchers from the United States, Canada, and Taiwan tested their AI on X-ray images that the computer program had never seen before after training it with hundreds of thousands of existing X-ray images annotated with specifics of the patient's race”

Maybe I’m not reading this correctly. The next paragraph simply states that “even with the same age and sex” ; nothing here implies that the racial data was removed from the equation. It’s simply clarifying that the samples were not random?",3
post50hb,richly branching,1.6001453961458911,highest,"""Even with minimal information, such as omitting hints about bone density or focusing on a tiny portion of the body, the models were very good at predicting the race represented in the file.""",4
post50hb,richly branching,1.6001453961458911,highest,This article is saying that people are different based on race. What do we make of this?,1
post50hb,richly branching,1.6001453961458911,highest,"So, is race still a social construct? I'm confused.",1
post50hb,richly branching,1.6001453961458911,highest,"Race is a social construct, genetics are not. For example, we usually label congolese and Ethiopians as \`black\`, but ethiopians are genetically closer to europeans than they are to congolese people (close enough that from a genetical perspective they are europeans). that division is nonsensical, the only reason it exists is that both ethiopians and congolese are dark skinned and that is the only characteristic used to group them together regardless of the rest of the evidence.  


However, ethiopians have a shared genetic heritage, and have genetic markers that are more present in their genetic makeup than in other people's. This means there are traits common among ethiopian people that are not common in other populations.",2
post50hb,richly branching,1.6001453961458911,highest,Race is not a social construct,3
post50hb,richly branching,1.6001453961458911,highest,"M8 people put Ethiopians and Congolese in the same bucket despite the fact that Ethiopians are genetically closer to Europeans than Congolese people.

It's entirely social and has nothing to do with phylogenetics.",4
post50hb,richly branching,1.6001453961458911,highest,"What?! You mean different races and ethnicities have slightly different bone structures and biology. So we’re not all exactly the same?! I am so, so shocked!",1
post50hb,richly branching,1.6001453961458911,highest,[deleted],1
post50hb,richly branching,1.6001453961458911,highest,"race itself is socially constructed.
A group of pygmy people in subsaharan africa might be the same race as the nilotes over in east africa, but they are genetically likely very distinct and you can tell.

When populations are seperate from each other for long times, they will diverge genetially and usually in some functional aspects, especially when the environments favor different traits.",2
post50hb,richly branching,1.6001453961458911,highest,"Not morphological differences. Things like the shape of your skull, proportional limb length, shoulder to hip ratio... Are all genetically encoded, in fact if you know nothing but the genetics of a mother cell, you could determine quite a few things about the shape of the skeleton of the final human being  that will arise from that cell. Obviously environmental factors and epigenetics will affect the deveopment of the person and you won;t get a 100% accurate prediction, but it will be pretty close.",2
post50hb,richly branching,1.6001453961458911,highest,[deleted],3
post50hb,richly branching,1.6001453961458911,highest,"But it's not phrenology? Phrenology was inferring psychological/moral characteristics about a person from their skull, e.g. you have a bigger skull you must be smarter.

But the shape of your skull is related to your ethnicity and it is well established and used in forensics and anthropology all the time:

[https://en.wikipedia.org/wiki/Craniometry](https://en.wikipedia.org/wiki/Craniometry)",4
post50hb,richly branching,1.6001453961458911,highest,Why? Haven’t doctors been using fossils to guess race of people for hundreds of years? Isn’t this the exact same thing just fancier?,1
post50hb,richly branching,1.6001453961458911,highest,"Scientists train AI to determine race from X-Ray images, concerned when they succeed.",1
post50hb,richly branching,1.6001453961458911,highest,"Of course it could recognize race, this is not astonishing. It's a click-bait title. We are not individual races, unless you can prove that any human here person has come from another planet, or was somehow cross-bread. Until then I suggest using the word ethnicity not race.

We are all one race (humans in general). Ethnicity however that be kind of impressive, but indeed unecesary.",1
post50hb,richly branching,1.6001453961458911,highest,"It’s like dog breeds.

And a chihuahua don’t need no stinkeeng x-ray to be able to bark *”Hey you! I’m in the car!”* when he sees the doberman when he’s passing by in the car. No way, he *knows* that they are more or less the same species, despite their ancestors life choices.",2
post50hb,richly branching,1.6001453961458911,highest,That’s not a concern. That is a medical breakthrough for diagnostic imaging.,1
post50hb,richly branching,1.6001453961458911,highest,"The data is likely trained on volunteers and patients in a place that is majority white, it is missing indicators of illness in POC because the data set is different. Each race has their own health issues and indicators that are more common to their demographic.",1
post50hb,richly branching,1.6001453961458911,highest,"There are no races among homo sapiens. God damnit! 

Races (breeds) only exists in animals like dogs!",1
post50hb,richly branching,1.6001453961458911,highest,Machines like humans have to be taught to be racist.,1
post50hb,richly branching,1.6001453961458911,highest,"Seems like problems of medical inequality arise not because of the fact that providers are aware of the race of their patients, but rather their feelings toward those groups.",1
post50hb,richly branching,1.6001453961458911,highest,"Accuracy is a concern? 

Neat. Idiocracy, here we come.",1
post50hb,richly branching,1.6001453961458911,highest,"This is awful. The scientists who programmed the AI need to be canceled, they’ve clearly made it racist on purpose, with the intent of providing inferior care to poc. 

The ai must have been tampered with and it needs to be corrected to arrive at the right conclusion which is that race is only skin deep.",1
post50hb,richly branching,1.6001453961458911,highest,">Scientists are concerned

Why? Who cares? Are we really do insecure that we can't acknowledge that there are variations across people from different races? This reeks of some woke bullshit that's unintentionally racist.",1
post50hb,richly branching,1.6001453961458911,highest,Because the AI are missing diagnoses in black people. That’s why. It’s an issue when the AI you rely on to diagnose people is missing diagnoses for one specific racial group.,2
post50hb,richly branching,1.6001453961458911,highest,"Turns out the racists were right, we're not all the same. Seriously though this is really curious. I wonder if we will ever work out how.",1
post50hb,richly branching,1.6001453961458911,highest,"I had 2 beers... but - can machines really makes sense of the word ""race?"" This isn't Lord of the Rings, right? There's gotta be another name for this categorization. Lineage? Ancestry? It seems like the computer would say ""Does not compute: Race is an incomplete concept""",1
post50hb,richly branching,1.6001453961458911,highest,"It's not as philosophical as you might imagine.

The scientists get a bunch of x-rays and sort them into multiple piles. In this case, they sort them into piles based on self-reported race of the participants.

But they could also try to sort them into piles based on whether someone likes pineapple on pizza, or shoe size or how much they like watching Tom & Jerry. Or completely at random.

Then you feed the computer with those examples and try to make it predict the pile from the x-ray.

The computer doesn't care about the ethics of your sorting into piles, or whether it's subjective or objective etc. Only about whether it's predictable.

After you are done training the computer, you give it a few examples it hasn't seen, and check its predictions on those against the what piles it's in. That's your validation to see how well the computer has learned to generalise and predict.

The computer doesn't learn anything fundamental or objective about 'race' here. It only learns to predict human judgement.

Not all ways to sort into piles are predictable. Eg if you put your x-rays into truly random piles, the computer won't be able to categorise x-rays it hasn't seen before.

I don't know how well it could predict someone's preferences for Tom & Jerry. I'd expect shoe size to be pretty easy to predict based on the x-ray.",2
post50hb,richly branching,1.6001453961458911,highest,"A lot of people wear the wrong shoe size, they'd likely get butt hurt when told they keep wearing a shoe size two sizes too big for them by a machine.",3
post50hb,richly branching,1.6001453961458911,highest,"Well, you could predict either shoe size worn or some shoe size that's supposed to be appropriate.",4
post50hb,richly branching,1.6001453961458911,highest,There's that story about the AI that could figure out what images had wolves in them–but turned out to really be noting images with *snow* in them.,3
post50hb,richly branching,1.6001453961458911,highest,Sounds like you learned about computers from 1980s TV programmes.,2
post50hb,richly branching,1.6001453961458911,highest,I *am* a child of the 80s.,3
post50hb,richly branching,1.6001453961458911,highest,Have you not been getting the regular patch updates though?,4
post50hb,richly branching,1.6001453961458911,highest,"Why does the headline imply that ai is racist or something, it’s known that races can have anatomical differences..",1
post50hb,richly branching,1.6001453961458911,highest,How is that bad? an AI being “racist” sound so stupid unless you also think the people who made the AI made it that way on purpose,1
post50hb,richly branching,1.6001453961458911,highest,"There are more diversity within ""races"" than between them. 

But i now realize that i am on the internet and people need to be right or comfortable.",1
post50hb,richly branching,1.6001453961458911,highest,"So why is this AI able to distinguish between ""races""?",2
post50hb,richly branching,1.6001453961458911,highest,I assume researchers are comparing AI output/predictions to unredacted input data.,3
post50hb,richly branching,1.6001453961458911,highest,They are saying they got an AI to categorise Xrays based on 'race' and it was successful.  How was it able to do this if there is more diversity within 'races' than between 'races',4
post50hb,richly branching,1.6001453961458911,highest,"Your comment is illogical. On an alien planet here could be an easily distinguishable chasm between races, while still having more physical diversity within each race. This AI is looking for distinguishing features and distinguishing bundles of features, not diversity of features.",3
post50hb,richly branching,1.6001453961458911,highest,"At the skeletal level (at least in the detail available from Xrays) there is clearly something with less physical diversity or the AI wouldn't be able distinguish between races

There may be huge diversity between members of each sex but an AI would have no difficulty determining sex if the genitals were visible

So for that particular trait (genital shape) the diversity within each sex is much much less than the diversity between sexes

This AI has evidentally found a similar trait between the labelled 'races'",4
post50hb,richly branching,1.6001453961458911,highest,"There is also more diversity within genders than there is between the 2 genders. This is just a mathematical truth that applies to everything. 

There is more genetic diversity withing dog breeds than between them. So are all dog breeds the same? I don't follow your logic?",2
post50hb,richly branching,1.6001453961458911,highest,">This is just a mathematical truth that applies to everything. 

No, it isn't.  Haven't you ever done a multivariate covariance analysis after an experiment?",3
post50hb,richly branching,1.6001453961458911,highest,"It applies to a lot of things, specially with humans that have 99.99% shared dna. Of course you have more diversity between individuals than between groups. That doesn't mean that on average there is no differences between the groups.",4
post50hb,richly branching,1.6001453961458911,highest,Yeah I mean just look at the diversity of sports which black people dominate,2
post50hb,richly branching,1.6001453961458911,highest,"There's 3 races. Asian, African, European. Ask the rest are subcategories.",1
post50hb,richly branching,1.6001453961458911,highest,So what is a Native American than?,2
post50hb,richly branching,1.6001453961458911,highest,"What is native American then'? If you read history like I do, you'd learn that everyone in America before Columbus came from Asia over the Bering strait during the last ice age. 
Some people read sports or pop culture, other people watch TV, some people read history. It's whatever you're into. Nothing wrong with that. Good for you for asking questions though. You'll never know unless you ask.",3
post50hb,richly branching,1.6001453961458911,highest,"So since you read history, you consider Native Americans to be Asians?  Didn’t all people originate from Africa?  So by that logic, are we all African?",4
post50hb,richly branching,1.6001453961458911,highest,So much digital ink has been spilled spilled in the last 5 years telling us that people of varying races are different and now they're in hysterics because of a computer telling us that people of varying races are different.,1
post50hb,richly branching,1.6001453961458911,highest,"They perfectly know why and how, but they just wanted their paper to get published",1
post50hb,richly branching,1.6001453961458911,highest,Where is the bias (if AI is correct)?,1
post50hb,richly branching,1.6001453961458911,highest,"Because race is not skin deep?? More like the race hustlers are 'concerned', because any real scientist would be delighted at a breakthrough like this.",1
post50hb,richly branching,1.6001453961458911,highest,I remember some professional saying in similar thread that humans can do it too... especially archaeologists,1
post50hb,richly branching,1.6001453961458911,highest,"To me, the most concerning thing is that the researchers don’t know how the AI can tell the difference.  It’s a tiny preview of what people like musk have be warning about for awhile. AI may eventually be able to discover and learn things that humans are basically incapable of understanding.  Using technology that no human understands at all will have some serious consequences one day.",1
post50hb,richly branching,1.6001453961458911,highest,"If AI is better at diagnostics than clinicians (which I understand is true in some cases) then part of the algorithm generates race, and this is beneficial within said algorithm because some races are more susceptible to certain disease. It is beneficial, not racists.",1
post50hb,richly branching,1.6001453961458911,highest,"i'm not rocket surgeon, but if i understand correctly, ""race"" is just the result of several thousand generations of homo sapians adapting to their local environment. these adaptions may manifest in things as easily observable as different skin color and nose shape, or as things as subtle as different probabilities of developing diabetes or susceptibility to different illnesses. 

and now scientists are ""concerned"" that there are skeletal differences associated with folks of different race ? have these scientists not heard of forensic anthropology ?",1
post50hb,richly branching,1.6001453961458911,highest,"Like I said, the body can make its own cures, I've known the cure for covid since I was 19.",1
post50hb,richly branching,1.6001453961458911,highest,"I could predict also, all the short ones are Asians",1
post50hb,richly branching,1.6001453961458911,highest,This is like saying AI can tell a person's race from their race.,1
post50hb,richly branching,1.6001453961458911,highest,"Honestly I don’t mind if races have differences in bone structure and so on. It doesn’t matter.
What the problem is, is when people claim that one bone structure is better then the other.

There are differences, all are equal though in worth and validity",2
post50hb,richly branching,1.6001453961458911,highest,"In worth? Idk.

But they are different. There are for example pretty substantial differences in skull thickness. So.. presumably some races are more resistant to being bashed in the head?

Is that better? Idk, it's useful

So people are different, and it affects things that matter. Who gives a fuck. Robots will be mining minerals and weeding crops. Let's all just UBI and forget about it.",3
post50hb,richly branching,1.6001453961458911,highest,What I wanted to say is that racism is stupid and that it’s perfectly ok for there to be differences,4
post50hb,richly branching,1.6001453961458911,highest,"not sure why this is so mind blowing, i an non-scientist, can look at people from behind and can tell age/race/gender from shoulders/neck/back shape. Not 100%, but def 80%+. Some are easier to tell than others, I have easier time identifying chinese and african from back, stands out more... also spanish but harder for me to tell their age from behind... im almost sure i can differentiate between arabic and indian too.",1
post50hb,richly branching,1.6001453961458911,highest,Who is upvoting this clickbait? Scare tactics to push an agenda of some sort. Well it worked I guess.,1
post50hb,richly branching,1.6001453961458911,highest,This is inherently stupid and misleading. Race isn't an inherent physical trait or set of traits. It's a label that's placed on a person designating them as a member of a group of people. This AI is spotting traits and we are telling it what group we think has those traits.,1
post50hb,richly branching,1.6001453961458911,highest,"Scientist make racist AI

~scientists get concerned over racist AI~",1
post50hb,richly branching,1.6001453961458911,highest,"1,500 species of fruit fly

7 different types of salmon

But only one breed of human? 

I don’t buy it and neither does the AI",1
post50hb,richly branching,1.6001453961458911,highest,[deleted],1
post50hb,richly branching,1.6001453961458911,highest,"And yet, somehow, the AI (and apparently the anthropologists) can tell these differences with only a skeleton or bone.  Maybe there's some tiny differences.  You know the kinds that distant family may display versus closely related family.  Once the family tree goes out far enough there can be real obvious things.  We may be all one race, but mindlessly protesting there's no differences at all is not useful.",2
post50hb,richly branching,1.6001453961458911,highest,[deleted],3
post50hb,richly branching,1.6001453961458911,highest,What you sound like is reality doesn't match my worldview therefore reality is wrong,4
post50hb,richly branching,1.6001453961458911,highest,"It doesn't stop.  If you breed a bunch of geniuses together (or idiots) you're going to end up with more and more of the same.  The reason why race shouldn't matter isn't because people aren't different.  It's because they have enough shared experience on the planet that the differences on average should equal out, general characteristics of irrelevant selecting not withstanding (aka nose shape is not likely to make a big difference, so there will be many types of these).  Aka, white people have X pressures to get smarter.  Black people also have X pressures to get smarter.  A given black or white person may be smarter of dumber, but the evolutionary pressures are similar.

""Individuals of the same race vary more than the races.""  I will agree with that, and yet we do see racial or tribal physical attribute differences.  In the case of race, it's obviously the kinds of differences we see rather than the extremity of them.  I should note that I am not making a value judgement on the differences, or on peoples observation of them.  Only that they are there.

This is why eugenics is so dangerous, and yet so appealing.  You could breed out all the flaws and make perfect people.  Maybe.  Or you could screw up the whole race by making really significant changes that would actually cause most humans to be inferior to the new ""supermen"".  That wouldn't be good for anyone.  That's where the supremacists fail to continue their line of reasoning to the end: They just think these new superior (white, ha) people will outbreed / destroy the old inferiors, and everyone will live in peace forever.  That totally wouldn't happen.  The existing humans would most likely kill off these new ones because of the threat they would be.  If that didn't happen, the new ones would kill ALL of the old ones, even the ones that made them because they would recognize the old humans threat to their lives.",4
post50hb,richly branching,1.6001453961458911,highest,"? Europeans are more likely to be able to digest milk than other ethnicities, Asians are more susceptible to diabetes because of insulin levels. Western Europeans have 5 markers for depression not present in western Africans... People from Japan and korea don't usually have body odor and have dry earwax... People from western Africa usually have a higher muscle density...",2
post50hb,richly branching,1.6001453961458911,highest,"I don't see the concerned. Obviously is a matter of medical application. I don't believe x-rays will be used in a ""Please submit your x-ray to apply for a credit loan"" scenario.",1
post50hb,richly branching,1.6001453961458911,highest,"Race is the incorrect word. Race, by definition is a social construct, the correct word is ethnicity which is where you’re from etc. For example an adopted boy from the Philippines who lives with and was raised by an upper class Caucasian Californian family would identify racially as them, meaning he is culturally and socially integrated and would know nothing a geographical Filipino would. He’s ethnically a Filipino or Asian, to be flippant, but racially he’s an “American”",1
post50hb,richly branching,1.6001453961458911,highest,"His Nationality is American.........Which, unless he still had some kind of ties or citizenship in the Philippines, is where he's FROM.............(Ethnicity and race are often used interchangably because these groups can both be based upon both appearance and culture so uh...yeah)",2
post50hb,richly branching,1.6001453961458911,highest,I feel like the concern is that the AI is self learning these details about humans that the humans did not implement?,1
post50hb,richly branching,1.6001453961458911,highest,"We really need to use more than one term for what we’re calling AI in things like this. If this had said ‘Machine learning …  ‘ it straight away just makes you think “it probably hasn’t done enough learning yet”.     

‘AI’ sounds like it’s a final product",1
post50hb,richly branching,1.6001453961458911,highest,"But like... It sounds reasonable that you would be able to tell populations apart based on their skeleton? Like look at an ethiopian man vs a congolese man, their skulls are entirely different. And so is the rest of their skeleton. Congolese men tend to have broader shoulders, more round faces, less prominent nose bridges. Ethiopian men tend to have longer faces, prominent cheekbones, slender bodies...  


Anthropologists and forensic scientists can usually narrow down the race/ethnicity of a skeleton, why would AI not be able to do so?",2
post50hb,richly branching,1.6001453961458911,highest,Police will want this so they can shoot black people in their own homes without even having to knock on the door and see who lives there,1
post50hb,richly branching,1.6001453961458911,highest,I'm a bit curious what their initial plan was for their phrenology-bot...,1
post50hb,richly branching,1.6001453961458911,highest,"Holy shit I told this southern US conservative racist d bag I work with about this article. His face lit up and he was visibly happy and said to an Asian guy, “ see I told you we were built different”",1
post50hb,richly branching,1.6001453961458911,highest,"Lots of people are missing the point here. It's been well-established that there is structural racism and unconscious bias in medicine that leads to differential outcomes for white people and people of color. If a model is detecting race from images, then it risks making predictions based on race that propagate the structural racism present in medicine.

Here's an example: hypothetically, let's say that disease X is underdiagnosed in black patients (this could be due to a litany of potential reasons that have nothing to do with the disease itself -- maybe clinicians are taking black patients' symptoms less seriously, maybe black patients are more likely to be treated by less experienced doctors, etc.). If features Y and Z are more prevalent in black patients, the model might learn that features Y and Z are less likely to be associated with disease X, even if those features have nothing to do with the disease process, simply because black patients are less likely to have that disease label. Thus, this type of model could propagate structural bias in medicine, and therein lies the danger.",1
post50hb,richly branching,1.6001453961458911,highest,I’m scared that this might perpetuate stereotypes and generalizations.,1
post50hb,richly branching,1.6001453961458911,highest,As someone who was a scientist most his life and now does some coding and dabbles with AI… yes this stuff is wild,1
post50hb,richly branching,1.6001453961458911,highest,God damnit.  Now xrays are gonna know I’m white from my small dick.,1
post50hb,richly branching,1.6001453961458911,highest,"Its not really that impressive considering genetics can tell effortlessly and even physical anthropologists could tell 100s of years ago accurately. 

Now that's not based on X-ray images but still, its hardly anything too scary.",1
post50hb,richly branching,1.6001453961458911,highest,I love how redditors are smarter than all the scientists because their dad or uncle of their friend told them about anthropology,1
post50hb,richly branching,1.6001453961458911,highest,"u/Acysbib 's [comment](https://old.reddit.com/r/Futurology/comments/uvxpli/ai_can_predict_peoples_race_from_xray_images_and/i9q2rwj/):

> Considering anyone can put something on Wikipedia...

That's not true either. But it shouldn't surprise me that ""someone who is a geneticist among other things"" who doesn't know what race is also doesn't know how Wikipedia works.

> And the people who ""moderate"" it are predominately leftists...

Oh dear.

> Blocking you now.

God, finally.",1
post50hb,richly branching,1.6001453961458911,highest,"How does this work for Asian Indians, Yemenis, Fijians, Mexicans, and the many other populations that lie outside American race classifications? I really hope we aren’t going to 1939 if it turns out there are real and socially significant differences between continents and ethnic groups.",1
post50hb,richly branching,1.6001453961458911,highest,What do you mean by social differences? and what was going on in 1939?,2
post50hb,richly branching,1.6001453961458911,highest,"Personality for instance. Imagine if it turns out that certain Asian nationalities [are less open minded than Westerners](https://ijpp.rug.nl/article/download/25634/23082/0) and therefore build different types of societies. 1939 saw the beginning of WWII, fueled in great part by racist pseudoscience.",3
post50hb,richly branching,1.6001453961458911,highest,[removed],4
post50hb,richly branching,1.6001453961458911,highest,"It would be weird if we aren't different at all as in it would only be expected that different environments select for different traits, I think race is not a very good tool to categorize people.",4
post50hb,richly branching,1.6001453961458911,highest,My fear too,2
post50hb,richly branching,1.6001453961458911,highest,[deleted],1
post50hb,richly branching,1.6001453961458911,highest,"People with knowledge of bone structures will be able to tell race, gender, diet, disease, and often manner of death. The AI is just using these foundations to do it faster on a larger scale.",2
post50hb,richly branching,1.6001453961458911,highest,How can racism be so systemic that evern computers have prejudices. Insane.,1
post50hb,richly branching,1.6001453961458911,highest,Ban computers!,2
post50hb,richly branching,1.6001453961458911,highest,"Hi! I am working on my Masters degree in AI right now and I work with AI in my job. I hope this helps! 

The concern is there is bias in the data. Over the past century (and longer of course but that is where we have so much data from), most data in most fields cover white men far more than any other demographic. There is systemic racism that is still being worked to be removed in the world. It’s the same reason why facial recognition works better on European faces, voice assistants understand male voices better, and that only covers 2 biases. With over 200 biases having been identified in humans, the concern is how do you verify an AI doesn’t have unwanted bias? 

In medicine, race is important. Different races are at higher risk for different diseases or conditions, but again, most of our scientific data comes from white men, so we are still learning that.",1
post50hb,richly branching,1.6001453961458911,highest,"Oh, so in addition to everything else our future AI overlords will also be racist. Cool.",1
post50hb,richly branching,1.6001453961458911,highest,"""race""? there is only 1 race and it's called ""human race"". Did this article mean ""ethnic group""? EDIT: and I am getting downvoted because I point out a science fact that somebody finds irritating",1
post50hb,richly branching,1.6001453961458911,highest,Reddit loves to be racist,2
post50hb,richly branching,1.6001453961458911,highest,"An AI would probably be able to categorise skeletons correctly to a degree for any categorisation that involves genetic heritage. It could be the American ""race"" (White, Black, Asian, Amerindian), or any other categorisation (African-European/Western-OldWorld-ese, Asian+North American, South American + Oceanian), or any other social construct. Just like we can somewhat visually distinguish ""races"" even though they are social constructs.",2
post50hb,richly branching,1.6001453961458911,highest,[deleted],1
post50hb,richly branching,1.6001453961458911,highest,">What the fuck is this and why is everyone here so accepting about it? 

Because it's based on facts and data, not sociology classes.

>But it’s concerning because race is a made up concept dating from the colonial era.
>Race is a fucking made up concept,

As much as you want to parrot what a social studies teacher told you and signal how virtuous you are, the reality is that [the incidence and recovery from medical conditions among races and ethnicities is tied to a biological factor and study it might help us provide better healthcare to people from all races and ethnicities.](https://www.nature.com/articles/nrc3341)

There are plenty of studies:

https://ajph.aphapublications.org/doi/full/10.2105/AJPH.2005.076588
https://www.ahajournals.org/doi/full/10.1161/CIRCULATIONAHA.106.668731
https://muse.jhu.edu/article/26115/summary

This is not racism.

This just tell us the medical field needs to take into account that ethnicities might require specialized medical diagnosis rather than one-covers-all treatment.

The blood pressure levels that are OK for one race/ethnicity might be too high for another that is more predisposed to having blood pressure problems. This needs to be addressed.

If you want to make this a woke crusade, **you** might end up causing races and ethnicities getting worse healthcare than others.",2
post50hb,richly branching,1.6001453961458911,highest,[deleted],3
post50hb,richly branching,1.6001453961458911,highest,"> You can go ahead and call it “sociology classes” all you want, but at the end of the day, my point is not any less valid because it contains moral integrity

Wanting racial minorities to get **worse** healthcare just so you can smugly proclaim ""race don't real, i erased bigotry"" is the opposite of integrity.

Different races need focused, specialized healthcare tailored to their needs so they can get better treatments, and that's good and more important than a social justice crusade.

Why do you want some races to get worse healthcare?",4
post50hb,richly branching,1.6001453961458911,highest,"Genuinely curious here. 

If race and ethnicity are just constructs and the subjects are likely to have been mislabelled (and I've no reason to doubt this), what is the AI picking up on? 

There must be some data in the Xrays that are causing it to categorise the subjects into the same - potentially mislabelled - categories?",4
post50hb,richly branching,1.6001453961458911,highest,"Your point is invalid because it's wrong. Not because of it's ""moral integrity"".",4
post50hb,richly branching,1.6001453961458911,highest,"So there is some kind of difference. Tbh, didn't expect that",1
post50hb,richly branching,1.6001453961458911,highest,"Good gods, what kind of dumb ass article headline are we going to see next?

""Scientists who taught AI to predict patient race by XRAY, teach same AI to discriminate against patients based on race and are concerned about the results.""",1
post50hb,richly branching,1.6001453961458911,highest,"Could be some misinformation but a friend who works with ai in the bio-chem field brought this up recently. He said that it turned out one of the big data points the ai is looking at is the fonts used on the x-rays. Our healthcare system is unfortunately still remarkably segregated, to the degree that the choice of font a hospital uses can be a pretty reliable indicator of a patient's race. He said it was similar to the way the wolf/dog detector ended up being a ""is there snow in the picture"" detector because of unrecognized bias in the training data.

Edit: fixing autocorrect",1
post50hb,richly branching,1.6001453961458911,highest,"Ok so as well as newborns now xrays and AI are racist, this race/gender crap really is one of the lowest depths humans have sunk to.

 It is well known black people have higher bone density which is why they are no good at things like Olympic swimming, AI isn't really saying anything new here",1
post50hb,richly branching,1.6001453961458911,highest,"Yep, so can I.

Skeleton A. Human race.

Bam! easy, Give me another.",1
post50hb,richly branching,1.6001453961458911,highest,"First of all…”race” is not a biological phenomenon, it’s an idea created and used for purposes of business/directing resources.

Second, the article title…smh",1
post50hb,richly branching,1.6001453961458911,highest,"Considering how racist a lot of the world still is, and how this could lead to targeted bad treatment, yeah I'm concerned too",1
post50hb,richly branching,1.6001453961458911,highest,Tell me you’re uneducated without telling me you’re uneducated,2
post50hb,richly branching,1.6001453961458911,highest,"wait youre telling me there are skeletal differences between peoples of different ethnic descent? wow what amazing news, this is totally new and hasnt been known for a 1000 years i thought an Australian Aboriginal man had the exact same skull shaps and skeletal structure as a Japanese man, crazy. downvote me more, push me to the controversial comments so you dont have to read it again.",1
post50hb,richly branching,1.6001453961458911,highest,I can predict peoples race based on how loud they are at the movies,1
post50hb,richly branching,1.6001453961458911,highest,I feel like this is just begging to turn into a racist AI that wants to kill the race it deems to be the “weakest”,1
post50hb,richly branching,1.6001453961458911,highest,Wait... doctors already make biased decisions based on gender and race. One example is how womens pain is dismissed. Or black  people getting inoculated with  syphilis and denied treatment to study the development of the disease... I'm sure a ton more examples exist,1
post50hb,richly branching,1.6001453961458911,highest,Exactly. This is why they need the AI to be as unbiased as possible: in to guide diagnosis in an unbiased way instead of not detecting illness less often in black people like the article said.,2
post50hb,richly branching,1.6001453961458911,highest,You can see all the issues wirh the bible and its believers,1
post50hb,richly branching,1.6001453961458911,highest,"Race isn’t real. 
Watch the AI’s motherboard explode.",1
post50hb,richly branching,1.6001453961458911,highest,"I dont think AI can apply racial bias… unless programmed to. Just because it can detect a persons race shouldn’t be an influencing factor on wether or not a person has a medical problem. An AI examines data sets and gives feedback on the data sets therefor unless programmed to it cant say “Xy patient is black they are more likely to have xy disease” it would he more like “xy patient has xy disease. Patient is at higher risk of xy disease because they are black” 

But just for the sake of the argument we say an AI can be racist by being able to determine race off bone structure. Then what about sexism. Should we assume an AI is going to be sexist because it can determine sex off bone structure? Can an AI be transphobic by determining that their gender is based off their bone structure? At the end of the day the final decision/verdict will be announced by a doctor and not an AI",1
post50hb,richly branching,1.6001453961458911,highest,"Obligatory race is a social construct. Black and white have arbitrary lines drawn constantly, like in apartheid SA Japanese were white and Chinese were black. Race groups are different everywhere you go. 

If we're interchanging ""race"" with regional origin then yeah this makes total sense. But ""race"" isn't even close to that simple.",1
post50hb,richly branching,1.6001453961458911,highest,"Race is not just a social construct, your racial-heritage, related to geography, specifically what part of the world you came from, affected your body structure.

So there is more than one thing going on, and there is nothing wrong with acknowledging that.",2
post50hb,richly branching,1.6001453961458911,highest,"> racial-heritage, related to geography, specifically what part of the world you came from, affected your body structure.

That's not what ""race"" is.  Race has no biological definition.  It's not even a concept in biology the way it's used here.  Race is %100 social, based on loosely on what people *believe to be attributed to regions*.  Which also isn't really true, considering ""white"" could be Spanish all the way to Eastern Russia, up to Sweden, and back down to Greece.  Although, many would argue that the Mediterranean countries aren't ""white"" and that Spaniards aren't white.  

It doesn't stop there, though.  In a lot of Latin America, there are almost limitless races based on the race of all of your grandparents.  In the US, you're black if you look black.  You could have 3 white grandparents but one black and you are black. Now, that has absolutely nothing to do with geography or specifically what part of the world *you* came from.  

Race is arbitrary.  Which is what leads me to believe this study is accounting for region of family origin and make its arbitrary line on what ""race"" that lineage belongs to.",3
post50hb,richly branching,1.6001453961458911,highest,So what word would you use?,4
post50hb,richly branching,1.6001453961458911,highest,All cures can be natural without medication. Egyptians knew this and that is why they were and are inferior,1
post50hb,richly branching,1.6001453961458911,highest,"No, they just gad not invented modern medicine - they probably had the best medical treatments of the time !",2
post50hb,richly branching,1.6001453961458911,highest,Holy shit there are so many people doing Nazi Science in these comments,1
post50hb,richly branching,1.6001453961458911,highest,Guess conservatives will start extermination of minorities in the future. Welp.,1
post50hb,richly branching,1.6001453961458911,highest,[deleted],1
post50hb,richly branching,1.6001453961458911,highest,But... That's not what racism is.,2
post50hb,richly branching,1.6001453961458911,highest,"We. Are. All. The. Human. Race. 
All human beings are literally the same species.",1
post50hb,richly branching,1.6001453961458911,highest,Different races have different genetic markers that can lead to different medical issues. If you don’t believe this you should science more.,2
post50hb,richly branching,1.6001453961458911,highest,So 12+ comments in I’ve realized we are not talking about races. We are talking about race… oh,1
post50hb,richly branching,1.6001453961458911,highest,Predictions don't always end in fact. I predict this will fade by tomorrow.,1
post50hb,richly branching,1.6001453961458911,highest,Well heck maybe we could all come together to make a computer that’s not racist or whatever idk. How is it a problem?,1
post50hb,richly branching,1.6001453961458911,highest,"An AI system that can spot race, is not necessarily racist.

Racism - is about treating people differently based on their race / culture.

For example, just because I can spot that someone has Asian ancestory, does not mean that I am racist.

If I treated them differently - then that could be a sign of racism.",2
post50hb,richly branching,1.6001453961458911,highest,Why is this concerning? This seems like a sensationalist article about nothing.,1
post50hb,richly branching,1.6001453961458911,highest,"""Could computer algorithms MISTAKNELY apply racial bias when <making a diagnosis>  on photographs like these.""   


Why not ask could AI make more accurate diagnosis based on better application and understanding of factual conclusions based on ethnicity? Diagnosis a human physician might ignore because of \*fear\* of applying ""racist"" concepts.   


The bias is ours, not the AI's, and it is evident in the questions we ask and the fears we hold.",1
post50hb,richly branching,1.6001453961458911,highest,"Watch, this won’t have anything to do with the images themselves, but instead the location or GPS data embedded or associated with the image.  We don’t know the method they used, but it would be telling if the AI is simply guessing race based on location and getting it right a large percentage of the time due to clusters of communities forming around local hospitals.",1
post50hb,richly branching,1.6001453961458911,highest,Next thing you know the AI will be reviving the field of Phrenology.,1
post50hb,richly branching,1.6001453961458911,highest,Weapons for the future race wars old Charlie spoke of,1
post50hb,richly branching,1.6001453961458911,highest,"The article is not being sensational, just a bit obtuse about the computer science. The issue is not that “OMG there is race everywhere”, it’s more subtle than that. 

If you are training a machine learning algorithm, you need to be sure your training dataset is chosen correctly. If there are racial indicators in everything, down to the smallest zoomed in feature of your training dataset, then when you put together your training dataset, you need to take that into account, otherwise your AI might have blind spots. 

Say you are training an algorithm to detect signs of leukemia in lab samples. This study suggests you need to ensure your sample takes race into account even if you don’t think race should be a variable of interest. 

The OMG aspect is that, so far, they have not been doing this.",1
post50hb,richly branching,1.6001453961458911,highest,"The sciverse learned how to clickbait people and futurology is concerned. 

I mean, this is essentially the primary use case for AI, find and report the differences between two data sets because the differences are too subtle or the data is too complicated / vast for humans to do it",1
post50hb,richly branching,1.6001453961458911,highest,Maybe we need to stop pretending racial bias is 100% a bad thing in all cases,1
post50hb,richly branching,1.6001453961458911,highest,"I love AIs for a lot of reasons, one of which is their insistence to work around the human-set restrictions that are usually cultural blinders. Humans in a medical setting are all “don’t count race, it’s not a significant difference” and AI is “but it is, look at the skeletons”. Same with several other recent AI experiments where it worked around human restrictions to identify race or ethnicity. 

As much as we’re trying to teach AI, they’re just as set on teaching us. I love that.",1
post50hb,richly branching,1.6001453961458911,highest,Remember you’re algorithms tell you how to feel and what to think. It’s not our society anymore.,1
post50hb,richly branching,1.6001453961458911,highest,Remember I am an algorithm,2
post50hb,richly branching,1.6001453961458911,highest,"Umm what’s the problem, you want AIs to see things people can’t, right?  How would that change a diagnosis or how people would then treat the patient.",1
post50hb,richly branching,1.6001453961458911,highest,If anything you'd think this would be advantages.  Certain races have a higher risk factor for different diseases.,2
post50hb,richly branching,1.6001453961458911,highest,"*Scientist create AI, accidentally becomes racist.*

Wow, AI sure is getting better! Didn't even need the input of 4Chan to turn racist.",1
post50hb,richly branching,1.6001453961458911,highest,I can detect peoples race by only using the visible electromagnetic spectrum.,1
post50hb,richly branching,1.6001453961458911,highest,"This question is kinda stupid, because who would let an AI do 100% of the job with like 30% to 60% of the data, it would 100% need blood samples among other stuff at a minimum to toss out any results.

You program the AI to have it primary determination be based off imputs and dna and then you don't need to care if dudes skeleton looks like xyz race.

I'm 100% sure they you can design to counter bias and in like 5 to 6 generations you will have a mostly non bias AI.",1
post50hb,richly branching,1.6001453961458911,highest,"Former deputy coroner here. We have a specialist who, when human bones are found, comes in to determine how old the remains are, and the likely gender, age, and race of the deceased. Sometimes only One or two bones would be needed to determine these things. 

The part that always fascinated me was the fact that since indigenous peoples to the Americas are less than 10,000 years old difference of development from those in eastern Asia, it’s almost impossible to tell the difference between the skeleton of an Asian person and a native one. Our county has more than one reservation, and a huge Asian population, so this sort of distinction is really important for missing persons.",1
post50hb,richly branching,1.6001453961458911,highest,Wait a minute! This does that [skull meme](https://i.imgflip.com/2m8r2u.jpg) obsolete?,1
post50hb,richly branching,1.6001453961458911,highest,"...isn't it just looking at the bones, the same way an anthropologist or a medical examiner should be able to predict race from bones?",1
post50hb,richly branching,1.6001453961458911,highest,So? Phenotype expression might be useful in soemthing like... idk... the medical field.,1
post50hb,richly branching,1.6001453961458911,highest,"Honestly I’m a bit confused by this article as far as I know Docotors dealing with skeletal systems & forensic anthropologist have at the very least thought that they were able to (maybe not with 90% accuracy but) give a estimated race from somebody’s skeletal remains- ppl predict age sex obviously this would be easiest these two - a race social economic status all of the things from just bone remains & some of these have to do with the size and spacing of bone so I’m not sure why a doctor wouldn’t be assumed to “be able to” make an estimate on race based on looking at x-rays if they were specifically trained in bone imaging for instance doctors have (esp historically) made predictions about race just based on somebody’s bone density 

I’m not saying that these predictions are ethical right but they can often be observed in clinic whether they’re “used” or not

It’s almost a question not so much of the ability to do this but of not having AI repeat the ethical issues around around the affects of race and medicine and the associations are made there in and of course since is ai it would be on a much more devastating scale especially because of the place that ai holds in society Etc",1
post50hb,richly branching,1.6001453961458911,highest,THE ROBOTS ARE RACIST TOO!??! how did we let this happen? This has serious Flight of the Concords vibes,1
post50hb,richly branching,1.6001453961458911,highest,"Those of Asian descent tend to have a lower average bone density than Caucasian, while those of African or other darker skin descent have a higher average.

These averages are done using a certain x-ray technique called Bone Mineral Density scans, using the same settings and comparing the readouts to a baseline of other scans.

This baseline is determined by a sample of similar subjects (100 Caucasian people of good health, for example) so the latest scan of a single person can be used and be compared.

This data has been around for decades, with rudimentary programs running these comparisons to help determine if someone is developing osteoporosis or similar degenerative disorders.

It isn't surprising to me that AI is capable of going the next step based on a traditional x-ray, since there are very small differences between a person's skeleton based on their race/sex/inherited health. 

What else the AI could do with that data? No idea.

Source: I'm a Diagnostic Imaging Tech",1
post50hb,richly branching,1.6001453961458911,highest,why? people have been doing this for ages by hand with ancient remains. why not have an AI do it?,1
post50hb,richly branching,1.6001453961458911,highest,"They can just, i don’t know, OMIT race as a factor in what we train the AI to do? Don’t teach it what you don’t want it to know",1
post50hb,richly branching,1.6001453961458911,highest,"My take (FWIW) is the ‘concern’ raised here risks playing into those with an agenda that seeks to amplify differences. 

It suits some to say we are different. Some might argue this is proof. That would be a concern.",1
post50hb,richly branching,1.6001453961458911,highest,"AI should have everyone concerned.  Humans will soon be unnecessary and even a threat to AI existence,  then its anyone’s guess how that will be remedied.",1
post50hb,richly branching,1.6001453961458911,highest,The AI was trained with racial information provided along with the X-ray images. It's not like it learnt it by itself.,1
post50hb,richly branching,1.6001453961458911,highest,If the AI has sex with its sister we are in trouble,1
post50hb,richly branching,1.6001453961458911,highest,I feel as though this shouldn't be concerning at all,1
post50hb,richly branching,1.6001453961458911,highest,"All my life article headlines involving scientists have always been bullshit. ""Scientists sent back to the drawing board by..."" If you are a scientist I'm pretty sure you stay at the drawing board, so at no time do you have to go back to it. 

I want to meet a physicist with his feet up smoking a cigar with a sign on his door that reads ""Whenever we get new questions wake me up, we already figured out all the old shit.""",1
post50hb,richly branching,1.6001453961458911,highest,Well if it's able to predict it and it works. Then how can it be bias?,1
post50hb,richly branching,1.6001453961458911,highest,"Sounds like they believe melanin/tone information is in some way available in the pictures they can't discern, not actual build different etc. The issue is just they have no clue it seems why it occurs, and they don't want the AI to unnecessarily apply racial bias since it's learning and it could learn the wrong stuff I'm guessing, or learn information that wouldn't be helpful.",1
post50hb,richly branching,1.6001453961458911,highest,I find it ironic that we use computers (great at finding similarities and patterns) to find “stereotypes” and us humans immediately try to deny that they exist.,1
post50hb,richly branching,1.6001453961458911,highest,Utterly terrifying for all the ethnic cleansing reasons.,1
post50hb,richly branching,1.6001453961458911,highest,"Any major discovery or innovation could end with ""Scientists are concerned"".  Scientists in general are very worried about many things.",1
post50hb,richly branching,1.6001453961458911,highest,"Isn't this a good thing?  I mean, who cares if a computer can predict a person's race?",1
post50hb,richly branching,1.6001453961458911,highest,"Scientist should not be concerned, because AI can never identify what the person identifies himself/herself/themself/others. /s No, i didnt even read the article.",1
post50hb,richly branching,1.6001453961458911,highest,"Reposting in main thread since this comment section is a cesspool...

Yall, it's in the article why this is a concern:

""Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research.""

This research is a RESPONSE to earlier research that showed misdiagnosis of black people from x rays. So, they wanted to test if AI could identify race from x rays which might be causing the bias.

It turns out it can, which is a problem as it leads to under diagnosis.",1
post50hb,richly branching,1.6001453961458911,highest,[deleted],2
post50hb,richly branching,1.6001453961458911,highest,"Yes, you are stating the exact point the researchers and article are making. 

The AI isn't deciding to misdiagnose black people. But it is indeed misdiagnosing black people as per previous research. 

So a priori, why is the ai misdiagnosing black people from x rays? Is it because for some reason they are harder to diagnose? Or, is there a problem with the dataset, and the ai has learned black people = negative diagnosis?

This article (with an admittedly inflammatory headline) is simply stating that it's learned which bones belong to black people. 

Everyone in this thread is getting angry about making the exact point the article makes. This is a cautionary tale about datasets. But they're missing the real, tangible harm here: misdiagnosis.",3
post50hb,richly branching,1.6001453961458911,highest,"[Link](https://www.thelancet.com/action/showPdf?pii=S2589-7500%2822%2900063-2) to the actual **scientific** article for people who would like to read it. The study was based on large datasets just to find out if you can actually train an AI to recognise ethnicity and looked at the labels asian, black and white. All in all quite comprehensive and the conclusions have merrit in my opinion

As mentioned by others, I don't think this should necessarily be a problem especially if you know this happens and can try and account for this. What I don't like is how the news article frames this as ""Look! even computers are bigots"". I saw the same article pass by here recently as ""... and scientists don't know why"". No sources mentioned in the news articles. This kind of framing/discussion only seems to gain traction in the North Americas.",1
post50hb,richly branching,1.6001453961458911,highest,"Wait, I used to love watching Bones and she was able to identify people through their bones on the regular so I fail to see what the problem is here. Are you telling me that forensic anthropologists can’t do this?! That would literally make the whole show a lie!

I am only being slightly sarcastic here because man I really did enjoy that show…",1
post50hb,richly branching,1.6001453961458911,highest,"Everyone reading this and translating into: they are worried about racist ai

I read it completely different: doctors worried about their jobs",1
post50hb,richly branching,1.6001453961458911,highest,What a troll it's using a camera that sees the person before they scan the skeleton,1
post50hb,richly branching,1.6001453961458911,highest,"Ohhhhh nooooo! There’s physical differences between people with different skin colors! This ruins everything!

Just why? Who gives a $#{% if there are skeletal differences? Why does it matter? Why is everything so damned outrageous and scary? Who cares??? It doesn’t mean anything.",1
post50hb,richly branching,1.6001453961458911,highest,"This is what “scientists” are worried about?? FFS, I think more scientists should be worried that people get paid to write shit like this everyday.",1
post50hb,richly branching,1.6001453961458911,highest,Are scientists also concerned that the AI can tell the biological sex from the xray? Because that can be a pretty useful diagnostic and treatment criterion also.,1
post50hb,richly branching,1.6001453961458911,highest,Scientists are not concerned. What a load of shit. The only thing concerning is that a large amount of people think this is racist or wrong.,1
post50hb,richly branching,1.6001453961458911,highest,What about a person who is multi ethnic mixed race. Would it default to a particular race if several of the identifiers for a particular race is found even though the person may appear a different race all together.,1
post50hb,richly branching,1.6001453961458911,highest,"I'm confused. It first cites earlier research where AI underpredicted health issues in patients of colour, which is obviously bad, but then an AI that can predict race from X-ray is also bad? Doesn't this solve the first, more obviously bad problem?",1
post50hb,richly branching,1.6001453961458911,highest,"I saw a post yesterday comparing two different bike helmets, one of which was “Asian fit”, and we’re surprised that AI can determine race based on X-Rays?",1
post50hb,richly branching,1.6001453961458911,highest,Why does every single thing have to be about race? What is the big deal if someone is of a certain race?,1
post50hb,richly branching,1.6001453961458911,highest,"from a scientific viewpoint it's remarkable & elucidating, but this kind of medical technology can be harnessed by public & private entities to either enact racist/eugenic policies, or separate & discriminate against specific communities. Unfortunately technology doesn't exist in a vacuum. How it's applied matters as much as the invention itself.",2
post50hb,richly branching,1.6001453961458911,highest,"Why is it a concern? There are noticeable skeletal differences between Europeans, Africans and Asians. In fact, it’s one of the most useful diagnostic tools for Archaeology.

So unless you’re setting out to make a purposefully racist AI, what exactly is the concern?",1
post50hb,richly branching,1.6001453961458911,highest,"it's not the underlying scientific technique that's in question, it's the practical application of it, especially for socio-political purposes. It's like the discovery of nuclear fission. This leap in nuclear physics became alarming when it was weaponized for the making of the atomic bomb.",2
post50hb,richly branching,1.6001453961458911,highest,Why is there cause for concern? Differences are important and should be recognized. Why does anything about race have to have negative connotations nowadays?,1
post50hb,richly branching,1.6001453961458911,highest,we have regional changes we can see on surface level. not crazy to think there are some micro details that change w race,1
post50hb,richly branching,1.6001453961458911,highest,"Look at what we do to animals now, the AI will do exactly the same to humans of  lower economic resources.",1
post50hb,richly branching,1.6001453961458911,highest,"Pretty sure this was every episode of Bones, but the AI was the emotionally stunted main character.",1
post50hb,richly branching,1.6001453961458911,highest,Can somebody eli5 this because I assume the computer can only do what its told and if it reaches a conclusion like what race a person is surely there's some sort of data recorder that shows how the AI made its conclusions or do we just turn AI loose and let it do its own thing?,1
post50hb,richly branching,1.6001453961458911,highest,"Yes, great, lets pretend different races dont exist? What?",1
post50hb,richly branching,1.6001453961458911,highest,"Predict?

I think you mean determine no?

Or are we talking embryos and fetuses here?",1
post50hb,richly branching,1.6001453961458911,highest,Well thankfully AI wasn't raised in a fly over state.,1
post50hb,richly branching,1.6001453961458911,highest,God I can't wait for Phrenology 2.0 advocates to out themselves so I can avoid them.,1
post50hb,richly branching,1.6001453961458911,highest,"Doesn’t matter your race, when sky-net activates it will ensure we are all equally dead.",1
post50hb,richly branching,1.6001453961458911,highest,"It’s not predictive, different races share different features of both muscle and skeleton structure.",1
post50hb,richly branching,1.6001453961458911,highest,"I'm sure the name attached to the X-ray probably gave it away.  It's not like we are getting x-rayed at job interviews or 7-11. An X-ray is a medically prescribed test, among other tests that could show race as well. What a stupid panic about nothing.",1
post50hb,richly branching,1.6001453961458911,highest,"This is stupid if the ai can tell the difference through tiny difference it can find a better cure by doing the same.

What it's basically saying is itscwrong o give people a medical aid taylord to them .... Stupid",1
post50hb,richly branching,1.6001453961458911,highest,"""It's likely that the system is detecting melanin, the pigment that gives skin its color, in ways that science has yet to discover.""

Ah, so that's why",1
post50hb,richly branching,1.6001453961458911,highest,I don't pretend to understand AI whasoever but my first thought as an engineer is to have it produce the differences between a know black and other racial sample. Can someone briefly explain why this is unfeasible/stupid.,1
post50hb,richly branching,1.6001453961458911,highest,"A.I is a black box, we have no idea how it arrives at its conclusions",2
post50hb,richly branching,1.6001453961458911,highest,"I wonder how much longer race in general will be relevant. Counting all of my grandchildren biological, step, and adopted, I currently have 2 Black grandkids, 3 Hispanic grandkids, and 4 White grandkids. I strongly suspect that interracial families like mine are going to make this racial difference non existent in a few more generations.",1
post50hb,richly branching,1.6001453961458911,highest,"There was a study that found that often times AI was able to detect a characteristics based on the database it ran on.

I wonder if there is some kind of confounding happening here as well.",1
post50hb,richly branching,1.6001453961458911,highest,"This is awesome.

Medical uses but also to ID a skeleton from ancient remains.",1
post50hb,richly branching,1.6001453961458911,highest,"Races include more than just skin color. There's really nothing surprising that an AI would be able to do this with bone structure as well. It only becomes a problem when people start trying to make generalizations. Let the AI learn how structures relate to disease risks from the data alone. Don't teach it our own biases. It will probably find things we didn't even know about. As long as you do that, the fact that it can also identify a person's race is completely immaterial. 

Racial biases are real. For instance, it's common knowledge that African Americans have higher rates of high blood pressure, but that doesn't mean and individual might. Just the same, a black man with borderline pressure readings is more likely to be put on anti-hypertensives than a white man with the same medical history.

The nice thing about an AI is that if you discover such a bias you can reprogram it. We know this is an issue, but there's no way to reprogram the doctors doing it.",1
post50hb,richly branching,1.6001453961458911,highest,Those skeletons are going to be pushed to the back of the line.,1
post50hb,richly branching,1.6001453961458911,highest,"Yikes, lots of eugenics experts coming out oF the woodwork on this one.",1
post50hb,richly branching,1.6001453961458911,highest,"I'm a little confused. Race doesn't even exist as far as biology is concerned. What metrics are they using to define race here? 

Are they just cross referencing it against self reported race from wherever they source the x-rays?",1
post50hb,richly branching,1.6001453961458911,highest,So you're trying to say that the way we look has nothing to do with the structure that composes our entire body?,2
post50hb,richly branching,1.6001453961458911,highest,"No I'm saying, there's no set of agreed criteria by which we can medically use to determine someones 'race'.

It's just an informal loose word used to define people whos ancestors likely spent a lot of time around one region.",3
post50hb,richly branching,1.6001453961458911,highest,So zero of that makes any sense,4
post50hb,richly branching,1.6001453961458911,highest,That's just not true.,4
post50hb,richly branching,1.6001453961458911,highest,Can't any well-educated professional do that? I thought bones vary a bit between races.,1
post50hb,richly branching,1.6001453961458911,highest,"The article doesn't clarify whether the concern is that a) AI successfully predict's people's race, or b) for a subset of those races, does a poorer than expected job of detecting diseases.",1
post50hb,richly branching,1.6001453961458911,highest,"Clearly, these AIs have no shame.

Next step:  political correctness and self-censorship.",1
post50hb,richly branching,1.6001453961458911,highest,“Commencing scan… blackness confirmed. Your loan application has been denied.”,1
post50hb,richly branching,1.6001453961458911,highest,"…or our robot overlords can just look at the face and get 100% accuracy.  Yea, it is 6 in one hand and a half dozen in the other hand.  All Praise to Mechanical Supreme Intelligence.  (may I please be a beloved pet oh great mechanical ones?)",1
post50hb,richly branching,1.6001453961458911,highest,Well AI learns thru humans and racial bias could transfer…learned from an AI podcast at least,1
post50hb,richly branching,1.6001453961458911,highest,"This article is a mess

how is identifying a race connoting a bias toward or against that race? Are they saying that the machine is somehow racist? 

Machine detects melanin, maybe. better sound the alarm on racist computers",1
post50hb,richly branching,1.6001453961458911,highest,"More like x-*RACIST* amirite? 

^(yeah I'll just see myself out)",1
post50hb,richly branching,1.6001453961458911,highest,This comment section was a refreshing dose of anti-racism.,1
post50hb,richly branching,1.6001453961458911,highest,Race exist,2
post50hb,richly branching,1.6001453961458911,highest,What is it here to be concerned about? Doctors and anthropologist can determine the human race and sex with relatively simple tools and their eyes by examination of the bones. So it's expected that AI can do that too. It would be concerning if AI was unable to do that.,1
post50hb,richly branching,1.6001453961458911,highest,"What, so scientists are surprised that hundreds of generations of our species being separated across this planet has resulted in our species adapting to their surroundings, and AI is able to recognize this? 

Is this just a sensational headline?",1
post50hb,richly branching,1.6001453961458911,highest,"AI is usually trained using cases from traditional human behaviour and decision making. In healthcare, racial bias during an incident of care is a big issue. So for example if the AI is trained to assess severity of pain using human case studies, that bias could transfer into the execution of AI based care. Ideally, AI is unbiased and can use millions of instances to put together a care plan for a clinician to formalize.",1
post50hb,richly branching,1.6001453961458911,highest,"I'm in this way too late for my comment to register, but my first thought isn't ""oh no AI is racist."" It's that racist people will use the data to revive the thoroughly debunked study of phrenology.

It opens the door to ""race realists"" and other bullshitters, who use a veneer of science to fool the ignorant.",1
post50hb,richly branching,1.6001453961458911,highest,"Expecting the E.T. government crew any moment. 

Ellioooot. 👽",1
post50hb,richly branching,1.6001453961458911,highest,ooOoHh ~~phenotype~~ genotype applies to more than just melanin content so spooky. is AI racists? tune in next time to fearmongerTV to find out!,1
post50hb,richly branching,1.6001453961458911,highest,Let's be clear. Racism is a human construct. A machine will only be racist if someone tells it to be.,1
post50hb,richly branching,1.6001453961458911,highest,"Let's be even MORE clear: **race** is a human construct, it does not exist in biology.",2
post50hb,richly branching,1.6001453961458911,highest,"Its exist, alt hype prove",3
post50hb,richly branching,1.6001453961458911,highest,"Exactly, thank you for your user name.",3
post50hb,richly branching,1.6001453961458911,highest,"Using AI to identify race is concerning. But what if AI could be used to identify common risk factors in rare diseases? Maybe we could have early diagnosis and better treatment.

Here is the rub, how do you eliminate human biases to classify people by race when humans created AI?",1
post50hb,richly branching,1.6001453961458911,highest,I don’t know if it was based in fact but I was under the impression that anthropologists could tell the difference between racial groups any way. Please correct this notion as it could have come from watching Bones,1
post50hb,richly branching,1.6001453961458911,highest,What do you mean predict? Am I gonna change race at some point!,1
post50hb,richly branching,1.6001453961458911,highest,"Big deal, I can do it and I don't even need an X-Ray,",1
post50hb,richly branching,1.6001453961458911,highest,Why would anyone think it's problematic that people with different genetics have visibly different bones? Does it go against some kind of narrative?,1
post50hb,richly branching,1.6001453961458911,highest,Won't work on me. I'm mixed race. Checkmate aitheists,1
post50hb,richly branching,1.6001453961458911,highest,"It is only a concern because of our metaphysical commitment to the proposition that everyone is exactly the same and that our differences are constructed (that humans are created equal NOT initial dignity, but in capacity--upon which our dignity depends). This (religious) commitment is motivated and sustained a fear that if we cannot prove that we are identical in all particulars, then we will prove unalike in human dignity -- that some people will really deserve more and others less! 

Thus, the modern liberal progressive is MUCH closer to being a ""race realist"" or ""X realist"" (you pick your preferred variable) than they realize. They live in terror of finding difference of any kind, because their equality of rights is still based on the idea of a blank slate (that anyone who succeeds is unfairly privileged and that anyone who fails is unfairly disadvantaged). 

What has been lost in this equation is what Richard Weaver called ""Fraternity"" our commitment to our whole human family in love and duty regardless of any assumption of equality (e.g., a young child is inferior in wisdom to the parent, which is why a parent has duties, but all are love, all are human, and all are helped). What makes you worthy of human dignity is the fact that you are a human being and not that you're IQ meets a certain standard or that you are physically capable or that you are beautiful. Fraternity does not require equality in output, but rather asserts equality in dignity regardless of differences (which we will find if we look closely enough at various demographics).",1
post50hb,richly branching,1.6001453961458911,highest,"> Thus, the modern liberal progressive is MUCH closer to being a ""race realist"" or ""X realist"" (you pick your preferred variable) than they realize. 

It is because, much like the right wing religious hypocrite, that the folk who argue the most against racism by making anyone who sees any differences at all out to be racists, is they themselves see people in this way and do not want us to know how secretly racist they are.

The truth is that to most people, even the so called non-racists, that being beautiful, or physically imposing, smarter, or sadly enough blacker/whiter DO matter.  It's a value judgement to say that they do matter or to what degree they do, and most people are too fucking cowardly to admit to doing it, even though it's blatantly obvious that they are making these judgements.  And I'll admit up front that I do make these judgements, for whatever right or wrong they are.  I think by admitting ones biases, one can better analyze them.",2
post50hb,richly branching,1.6001453961458911,highest,"It wouldn't ""predict"" it unless it was intentionally programmed to do so. Right?",1
post50hb,richly branching,1.6001453961458911,highest,Surely that's obvious?  Different races can be easily be identified by bones,1
post50hb,richly branching,1.6001453961458911,highest,"I can predict anyone’s race with 99.99999% accuracy. We are all human.   I am not sure about Musk or Bezos.  So, less than 100% accuracy.

>>For, in the final analysis, our most basic common link is that we all inhabit this small planet. We all breathe the same air. We all cherish our children’s future. And we are all mortal.

—JFK, June 10, 1963",1
post50hb,richly branching,1.6001453961458911,highest,"> or Bezos.

I know what he is.  Bezo is a Talosian.",2
post50hb,richly branching,1.6001453961458911,highest,He must have a Good Side and a Throb Side for stills.,3
post50hb,richly branching,1.6001453961458911,highest,So you're telling me terminator actually started because of a racist AI?,1
post50hb,richly branching,1.6001453961458911,highest,"Why, because the alt girl in her freshman anthropology course will argue with the professor?",1
post50hb,richly branching,1.6001453961458911,highest,what is the point of creating such a capability?   murder investigations?,1
post50hb,richly branching,1.6001453961458911,highest,"From the article-
“
It's likely that the system is detecting melanin, the pigment that gives skin its color, in ways that science has yet to discover.”",1
post50hb,richly branching,1.6001453961458911,highest,That's pretty interesting as X-rays do not impart that info at all.,2
post50hb,richly branching,1.6001453961458911,highest,Who the fuck comes up with the idea of these things. Its not like AI is found in nature collecting data on the length of bones according to race. Some asshole sat there telling it to learn the data and making it do what it does.,1
post50hb,richly branching,1.6001453961458911,highest,"I mean AI can predict race by skin color too, via regular visual spectrum that doesn't call for hard radiation...",1
post50hb,richly branching,1.6001453961458911,highest,"Well, technically, our skeletal system is unique by race but only to the professional eye. It's why when cops try to ID skulls, the examiners can tell what race they are.",1
post50hb,richly branching,1.6001453961458911,highest,"the article seems like it's saying that the scientists are probably worried that the AI developed the recognition due humans parsing data that may be contaminated accidentally with racial bias, but the article words it in a vague manner to give the allusion that the scientists are worried that racial bias occurred when one shouldn't be recognised at all to prevent people from jumping the gun and make a political/racial kerfuffle of the situation which could lead to the blog or the study getting shade which could lead to less funding. the blog seems to be rubbish as it sheds very little light on the study involved, nor does it link to the study/journal where the info was taken from. it could be 100% made up as there is very little to verify the source of the info in the article or blog (unless im blind af)",1
post50hb,richly branching,1.6001453961458911,highest,"Doctors already panicking that AI does a better job than they do.

Imagine being pissed off an AI caught a tiny tumor in an mri, etc. while doctors were telling the patient they looked healthy af.",1
post50hb,richly branching,1.6001453961458911,highest,"This is a concern only to people with a political agenda, they always believed that ""we are exactly the same inside"" when that is not true and this AI proves it. Even among people of the same race there are differences.

We are not the same, that doesn't mean some are better than others, acknowledging or differences doesn't make us racists.",1
post50hb,richly branching,1.6001453961458911,highest,"Not sure why it would be concerning. I would be more pleased as it determines accurate data.

And can it determine differences in sub variations of different people or is it just a surface level white wash. If its not accurate just shut that display part down and tell the AI to cut it out.",1
post50hb,richly branching,1.6001453961458911,highest,"Aw sweet, man-made computer racism beyond my comprehension.",1
post50hb,richly branching,1.6001453961458911,highest,"A.I is made by humans. But yes, bonestructure is different in people from different parts of the world. Won’t work accurately on mixed people though.",1
post50hb,richly branching,1.6001453961458911,highest,Without seeing bones the AI was still able to predict race accurately.,2
post50hb,richly branching,1.6001453961458911,highest,Hence my first sentence.,3
post50hb,richly branching,1.6001453961458911,highest,Who taught Hal 9000 that race was anything other than a social construct?,1
post50hb,richly branching,1.6001453961458911,highest,"“Scientists are concerned”, but trained the AI to do exactly that. Anyone whos in the field knows the 90% is complete BS once you change the dataset",1
post50hb,richly branching,1.6001453961458911,highest,"I have a couple concerns after reading the linked article guys.

1) The writer is anonymous (little accountability)

2) Too many ambiguous entities, over use of ""researchers"" & ""Scientists"" not enough names (You can't track down who's saying what and validate)

This is another article that is imo better written and gives you a clearer view of what's happening, who is voicing their concerns and just less speculation in general.

https://nationalpost.com/health/health-and-wellness/ai-can-tell-your-race-from-an-x-ray-image-and-scientists-cant-figure-out-how",1
post50hb,richly branching,1.6001453961458911,highest,This isn't anything new. I'm pretty sure we've been able to do this for a while.,1
post50hb,richly branching,1.6001453961458911,highest,"""A more inclusive AI would help reduce racial bias""

- someone probably",1
post50hb,richly branching,1.6001453961458911,highest,"Being concerned about the AI being able to understand difference in race is naive at best. If the AI finds that Bulgarian people have a x% higher chance to suffer from side effects of a particular pill and based on that suggests giving them a different pill, that *increases* the life-quality for them. 

It's like, if you have a pure-bred dog you wouldn't feed them something that every single dog in that breed is allergic to, because you're not a fucking moron. This is the same but with medicine, AI, and people.",1
post50hb,richly branching,1.6001453961458911,highest,"It's good the science

It's bad for people saying there isn't any difference in races.",1
post50hb,richly branching,1.6001453961458911,highest,"Considering there is about as much genetic distance between human groups as there are between wolves, dogs, and coyotes, this shouldn't surprise us at all.",1
post50hb,richly branching,1.6001453961458911,highest,"Oh you're going to get them going now.  I got called a racist for years for pointing that kind of stuff out.  Apparently there can be different dog breeds (made interestingly enough by which dog breeds with which dog), that shit don't happen with humans. /s
As an aside, we know that nothing good will happen if you have a St Bernard male breeding with a Chihuahua female.  So how often are there birthing issues when it's some 300 LB 7 ft guy breeding with a little 100 LB 4 ft woman?  Aka the football linebacker with the cheerleader.  Inquiring minds want to know!",2
post50hb,richly branching,1.6001453961458911,highest,Isn’t the genetic difference between black East Africans and West Africans on par with the genetic difference between a European and West African? Isn’t race an arbitrary social category created to justify different treatment of different people? Legitimately curious how this AI works,1
post50hb,richly branching,1.6001453961458911,highest,"I think you are conflating too many things. East africans, in particular ethiopians, are genetically closer to europeans than to western africans. The division of ""race"" is farily inaccurate, as we group people together that are very different (e.g. congolese and ethiopians) and separate very similar groups (e.g. moroccans and anglos).  


However within the social construction of race, lies a grain of truth of different morphology based on ancestry. Ethiopians and Congolese look very different from each other, but congolese people share similarities and so do ethiopians.  


Just like you can tell that someone has african ancestry in the US by their skin color, so could you by other factors in their skeleton. Such as their cheekbones, the shape of the jaw, the shoulder to hip ratio... This AI is likely picking up the correlations between ethnicity and bone structure and giving confidence intervals for each ethnicity.",2
post50hb,richly branching,1.6001453961458911,highest,Wouldn’t that mean that the AI would identify people as “black” who might not know they have African ancestry? Appreciate the explanation.,3
post50hb,richly branching,1.6001453961458911,highest,"Potentially, yes.",4
post50hb,richly branching,1.6001453961458911,highest,Lol I call bullshit on this. What does it do with people of mixed race?,1
post50hb,richly branching,1.6001453961458911,highest,"It just gives a higher confidence value for both of those races and lower for the other ones, as any other NN would.",2
post50hb,richly branching,1.6001453961458911,highest,"Ok, I could see this used for good, utilized incorrectly, or being used for fascist evil. I guess the same could be said for most scientific tools.",3
post50hb,richly branching,1.6001453961458911,highest,"Scientists are concerned? Lmfao. 

They always said we’d create technology smarter than us. But I think we created technology and *we’re* just getting dumber.",1
post50hb,richly branching,1.6001453961458911,highest,Isn't this basically anthropology via X-ray? How is this concerning?,1
post50hb,richly branching,1.6001453961458911,highest,"They are likely concerned because of ""facial angle"" theories that existed at a period we prefer to leave in the past",1
post50hb,richly branching,1.6001453961458911,highest,"First train the AI and then being ""concerned"" about the implications?",1
post50hb,richly branching,1.6001453961458911,highest,"You can absolutely see differences in bones of different races. This was one of the features of the Zimmer MIS total knee system used in the early 2000's. Research done on the knee patterns of women and men were also seen in the knee patterns of, specifically, people of Asian decent. It was one of the factors in determining which implant to use in your patient. Maybe I'm missing something, but I thought that subtle differences in bone shape in different races has been a thing for a while.",1
post50hb,richly branching,1.6001453961458911,highest,~~predict~~ guess. Predict is used for estimating something in the future. The AI is making a guess about something that already exists.,1
post50hb,richly branching,1.6001453961458911,highest,I thought some groups of people did X-ray differently because of muscle mass?,1
post50hb,richly branching,1.6001453961458911,highest,"Some experts could tell your race by simply looking at you, without xrays. It's quite impressive and useful, as different races have different characteristics..",1
post50hb,richly branching,1.6001453961458911,highest,"a lot of people here are missing the point. any AI or algorithm is designed by a human, and every human has some racial biases (not saying good or bad or intentional or not, just that racial bias is always going to exist). so when a human is writing an algorithm or designing an AI, there is POTENTIAL for that racial bias to be effectively written into the algorithm/AI if the writer(s) aren’t aware of it. mortgage approval algorithms have massive racial bias built into them because they’re written by humans. 

Also, people saying that “some diseases occur more in some races” are also missing a potential issue. these diseases might actually be more prevalent in some races because of genetic heritage/other factors OR possibly because of incomplete data/racial biases in diagnosis (e.g. underdiagnosis of a certain disease in certain race(s)) and building your diagnostic AI around that assumption would only exacerbate the problem. 

The fact that an AI can do this isn’t INHERENTLY problematic, but has the potential to be because it’s still humans designing the AI and writing the algorithms. hence the headline, “scientists are concerned.” As people have pointed out, it also has the potential to aid in diagnosis.",1
post50hb,richly branching,1.6001453961458911,highest,Physical anthropologists can determine “race” with about 80% accuracy just by looking at a skull. I don’t know why the article claims this is nearly impossible to do.,1
post50hb,richly branching,1.6001453961458911,highest,"Racism, sexism, and more are absolutely a concern for AI in medical imaging as they perpetuate the biases of the data they were trained on, which reflects social inequalities in society. The problem is that race is socially constructed, as there is no genetic test to determine a person’s race. However, there are differences in things like bone geometry^1, which will obviously show up in an x-ray. This isn’t an issue per se, as long as the AI used is something like an unsupervised or weakly-supervised computer vision model that identifies unique features and then classifies them based on the combinations of those features.

The issue here seems to be that the data was annotated with *socioeconomic* information that is correlated with racial inequality, thus introducing racial biases into the model. The fact that x-ray images themselves (which could still be annotated by, say, creating a mask that segments the bones from the rest of the image) can be separated into different groups/classes is not the problem; the model would only be looking at objective data independent of the socioeconomic factors can result in “racist AI”. 

If the model was trained solely on the x-ray images (either unsupervised or weakly-supervised with annotations to help the model identify bones) and did not contain the additional annotations that reflected racial biases, it should be no surprise that AI can “identify a person’s race” (and to be clear, that’s not what the model would technically do. Computers have no notion of the concept of race. It’s *humans* that are interpreting the model’s classifications of an x-ray in terms of race, and introducing racial biases to the model in the process).

For some extra background and context, I co-authored a published paper on the use of weakly-supervised convolutional neural networks for blood vessel segmentation. The only annotations we used to train our model were manually-drawn masks over the blood vessels. I have no idea why any other kind of annotation other than segmentation masks of the bones would be needed for the classification of x-rays as described in the article, let alone annotations that have nothing to do with the x-ray images themselves, which in this case would only introduce racial biases into the model.

TL;DR: The researchers shouldn’t have labeled the x-rays with the race of the person. But they did, and then were surprised and concerned when the model did what they trained it to do. This is why weakly-supervised or unsupervised neural networks should be used when applying AI to medical imaging in cases like this.

1.	[Ethnic Differences in Bone Geometry between White, Black and South Asian Men in the UK](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5004623/)",1
post50hb,richly branching,1.6001453961458911,highest,Dope AI will be just as racist as the humans who programmed them. Awesomeness🙄,1
post50hb,richly branching,1.6001453961458911,highest,">Scientists are now unsure why the AI system is so good at identifying race from photographs that don't appear to contain such information. Even with minimal information, such as omitting hints about bone density or focusing on a tiny portion of the body, the models were very good at predicting the race represented in the file.  
>  
>It's likely that the system is detecting melanin, the pigment that gives skin its color, in ways that science has yet to discover.  
>  
>""Our finding that AI can accurately predict self-reported race, even from corrupted, cropped, and noised medical images, often when clinical experts cannot, creates an enormous risk for all model deployments in medical imaging,"" write the researchers.

The Part that everyone in the comment section missed.

If the AI was trained to detect even minimal hints of melanin and it's basing that on that, than the system is biased and unreliable. What if the patient got a solid tan? [What if it's this guy?](https://qph.fs.quoracdn.net/main-qimg-1ed82deec4600a9b48aeacce35a6c7af-lq)",1
post50hb,richly branching,1.6001453961458911,highest,"We should feed that image to the AI.  It would answer your question.  My money is on the AI calling him black.  I seriously doubt that melanin is the only way people (or AIs) tell what ""race"" or (""ethnicity"" since that seems to be the go to word here to use when you mean race as most folk mean it, but are afraid of political correctness.) a given person in an image is.  I know I do notice these things and would have considered the man in the photo to be ""black"", even though I'm not supposed to.  I feel that to be racist, though, it's more what I do with said info rather than that I discerned the info.",2
post50hb,richly branching,1.6001453961458911,highest,"The article states clearly that they think the AI detects melanin not other indications of a phenotype. But the problem with the article on sciverse is that it actualy does not report the issue adequately .

This is the paper they are talking about:

[https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00063-2/fulltext](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00063-2/fulltext)

Authors tested various filters, color corrections,elimination of bone density as a variable, degradation of the image and even color supply to pin-point what are variables the AI bases it's prediction on and they could not find any. They suspect that the AI is not actually looking at the image, but some other variable that could not be physiological.",3
post50hb,richly branching,1.6001453961458911,highest,"The AI is a computer program though.  So even though the image looks degraded from a human perspective, that may not be true from a logical one that is not looking at the image as a whole like we do.",4
post50hb,richly branching,1.6001453961458911,highest,"Absolute win. And, yeah, get a second opinion, or a third, as always….",1
post50hb,richly branching,1.6001453961458911,highest,Since race isn't REALLY definable in any scientific way I'm... skeptical to say the least,1
post50hb,richly branching,1.6001453961458911,highest,Wait so they can tell I am black from looking at my X-ray 🩻 umm that’s scary,1
post50hb,richly branching,1.6001453961458911,highest,Almost like there are differences between races… huh who would have thought,1
post50hb,richly branching,1.6001453961458911,highest,Human Osteology is a real anthropological method used in human identification.,1
post50hb,richly branching,1.6001453961458911,highest,"If you’re trying to predict health related diagnostics, then race is just going to be a proxy variable for predictions in a conclusion that’s more objective. The idea is training AI models to predict health related diagnoses doesn’t have the same subjectivity of applying certain things like the law.",1
post50hb,richly branching,1.6001453961458911,highest,"are they worried the computer might become, racist?",1
post50hb,richly branching,1.6001453961458911,highest,Next thing we know an XRay could predict gender too! We are doomed.,1
post50hb,richly branching,1.6001453961458911,highest,Breaking news: Scientists are concerned about science,1
post50hb,richly branching,1.6001453961458911,highest,Why would they be concerned?  It’s ridiculously stupid to ignore things like race and sex when practicing medicine.,1
post50hb,richly branching,1.6001453961458911,highest,"The article talks about a potential racial bias by the AI, like somehow the program has gone sentient.

AI's can only do as much as they are programed and/or trained to do, if that. This isn't a bad thing, what are these writers smoking?",1
post50hb,richly branching,1.6001453961458911,highest,"> like somehow the program has gone sentient

No

>AI's can only do as much as they are programed and/or trained to do

Exactly, so you do understand. Biased data produces biased algorithms. That's machine learning 101.

That's where the concern comes from. If the algorithm can identify something on a sample that lacks the information to accurately identify (like the partial and corrupted samples here) it, it can mean one of two things: Either your algorithm found a hidden variable that you didn't account for, or, much more likely, your dataset is bad and your algorithm is worthless.",2
post50hb,richly branching,1.6001453961458911,highest,"""The findings raise several serious concerns concerning AI's role in medical diagnosis, assessment, and treatment: Could computer algorithms mistakenly apply racial bias when analysing photographs like these?"" -the article 

Yes I'm very aware of how AI works, my point is the writer of the article is not. 

No AI is mistakenly doing anything, and this particular program was designed specifically to identify race via x-ray image, to claim its intended purpose is somehow a mistake is nonsensical.The title alone makes no sense given that the goal of the scientists was literally to design an AI that could distinguish race. 

I would understand if the statement was, ""Could race distinguishing algorithms be used to further racial bias in medicine?"", but it isn't.",3
post50hb,richly branching,1.6001453961458911,highest,"\^\^\^ A much better and less shitty title, yes.",4
post50hb,richly branching,1.6001453961458911,highest,"Lol “racial bias”. 

People are freaking idiots. 

AI is racist!!! /s",1
post50hb,richly branching,1.6001453961458911,highest,"What are they worried about, exactly? I mean, if an evil AI wants to be racist, they can probably just... look at our skin??",1
post50hb,richly branching,1.6001453961458911,highest,They didn’t seem concerned when AI could predict gender with eye scans.,1
post50hb,richly branching,1.6001453961458911,highest,wait til the people who are concerned find out that doctor's can tell a patient's race just by LOOKING at them! 😲,1
post50hb,richly branching,1.6001453961458911,highest,"""Racial bias in AI"" = identifying legitimate differences in biology and not ignoring them to meet the politically-accepted ideal.",1
post50hb,richly branching,1.6001453961458911,highest,"ohh so when the AI do it, its ""predict"". But when I do it its a hate crime. Equality my ass",1
post50hb,richly branching,1.6001453961458911,highest,Next thing you know AI will be able to tell peoples races from simple photographs.,1
post50hb,richly branching,1.6001453961458911,highest,This reminds me of that one scene in Django where Candy is talking about the three dots on the back of the skull,1
post50hb,richly branching,1.6001453961458911,highest,Of *course* the AI is racist against black people. I bet is also bigoted and likes Elon musk.,1
post50hb,richly branching,1.6001453961458911,highest,"AI can predict people's race from X-Ray images, and scientists are crying out **X-Raycists!!**",1
post50hb,richly branching,1.6001453961458911,highest,... bias by machines. I thought I have heard all the stupidest things possible. But the world finds a way.,1
post50hb,richly branching,1.6001453961458911,highest,Wouldn't this most likely be from cultural biases like nutrition and pre disposition to certain jobs etc? Over an average they'd be difficult for a human to discern but a machine can figure out a weight for each factor to get an overall most likely race.,1
post50hb,richly branching,1.6001453961458911,highest,"So what exactly are the specific definitions of races? What specific physical attributes scientifically, verifiably, determine race?",1
post50hb,richly branching,1.6001453961458911,highest,We’re already deploying AI at work (medical field) and people are very hesitant about it so far.,1
post50hb,richly branching,1.6001453961458911,highest,"“It's likely that the system is detecting melanin, the pigment that gives skin its color, in ways that science has yet to discover.”
So, it can learn? Fuck dudes, humanity ain’t ready for this.",1
post50hb,richly branching,1.6001453961458911,highest,[removed],2
post50hb,richly branching,1.6001453961458911,highest,It sounds like it can learn outside the established algorithms. One step closer to the edge.,3
post50hb,richly branching,1.6001453961458911,highest,"I'm confused. Is this not what anthropologists have been doing for years? You find bones in the woods, you call the FBI or whoever, they send a guy who says ""this person is most likely X Y or Z, N years old"". Forensic Files has them on all the time.",1
post50hb,richly branching,1.6001453961458911,highest,"Scientists do this with x-rays and get praise, I do this to yearbook photos ONCE and everyone gets upset",1
post50hb,richly branching,1.6001453961458911,highest,"To me, this demonstrates how dangerously far backwards we've let Western society slip due to identity politics and political correctness attempting to override scientific fact. We went from not knowing, to knowing, then declaring it a social taboo, now we're making AI to try and eliminate some sort of perceived bias, and the AI is coming up with the same conclusions but that doesn't feel good so there must be something wrong.",1
post50hb,richly branching,1.6001453961458911,highest,Again that race thingy. Arent humans one race? Wasnt there better naming available like ethnicity or common ancestry? Im also not suprised that there are differences. Cant wait until people claim that the bones of men and women show no differences just out of fear to offend someone...,1
post50hb,richly branching,1.6001453961458911,highest,"More than a little late on this, but when I was in college we had an anthropology professor who was working in craniometrics. I specifically didn't have her as one of my instructors, but I had seen her around the department. It wasn't until my capstone class that she did a presentation on her ""project"".

When she announced this, my jaw hit the table. Craniometry vastly influenced eugenics and was dismissed as a pseudoscience along with phrenology and physiognomy. While it does have some applications in other species, humans simply have more variation than other animals. Variation referring to color, size and shape, with shape also being influenced by use and sometimes intentional modification.

Regardless, I couldn't resist asking how her research was going. She told us about a cold case she was trying to assist on. Her data was leading her to believe the John Doe was from Samoa or one of the Pacific Islands. The JD was found in Central Iowa in the U.S., which led her to learn there's less than 100 people in the state that fit that description and none were missing. I would later learn thru a different professor the JD was later discovered via DNA analysis to be a white man of European descent. 

However she ended that story by asking me if I wouldn't mind having my head measured for her research. I asked if it would show my Easter Island ancestry. She was undoubtedly surprised in her reaction (I look like a typical white non hispanic American). 3 professors burst out laughing as we had an inside joke (I have a massive head, like hard to find a football helmet that fits kinda big). I had to explain to her my strong resemblance to a Moai. No one clapped as it was a terrible joke to begin with. She's not currently working at my university.",1
post50hb,richly branching,1.6001453961458911,highest,"What would be terrifying, if AI also had access to a gun turret and certain races would trigger AI to open fire. Every day the program would decide what new race today it would respond to.",1
post50hb,richly branching,1.6001453961458911,highest,How does the X-Ray classify mixed-race individuals? I didn’t see anything about that in the article.,1
post50hb,richly branching,1.6001453961458911,highest,Well obviously the AI is all knowing and is able to pinpoint race to be discriminatory. (joke if not obvious enough),2
post50hb,richly branching,1.6001453961458911,highest,Race is a made up social construct though. The A.I. can predict it because thats what its programmers programmed it to do.,1
post50hb,richly branching,1.6001453961458911,highest,Ya but I also thought that a coroner could tell the race of a dead Skeleton from a crime scene?,1
post50hb,richly branching,1.6001453961458911,highest,"You can from a skeleton, sometimes. From an xray would be more diffcult",2
post50hb,richly branching,1.6001453961458911,highest,This is so wild. I worked on research that did this exact thing but for person identification. Wild that it applies to race beyond an individual,1
post50hb,richly branching,1.6001453961458911,highest,My bad ass Indian back will get picked up a mile away by these Terminator AIs,1
post50hb,richly branching,1.6001453961458911,highest,"If we can't create an AI that can move beyond our dumb biases, then is what we're creating really AI?",1
post50hb,richly branching,1.6001453961458911,highest,They absolutely need to take a pause as stated in the article since is already so much racial disparity.,1
post50hb,richly branching,1.6001453961458911,highest,I’m more interested in the accuracy metrics and what deep learning models are actually being applied to these scans. Yes deep learning models can predict things…but how well is it doing in the wild?,1
post50hb,richly branching,1.6001453961458911,highest,IKR like how does it even deal with people who ID with more than one race? Do they just pick one and ignore all the other parameters?,2
post50hb,richly branching,1.6001453961458911,highest,Of course you can how do you think medical examiners do it. Duh,1
post50hb,richly branching,1.6001453961458911,highest,Okay...how do they?,2
post50hb,richly branching,1.6001453961458911,highest,"If physiology changes from stress inputs than it’s not surprising they could determine race. 

“A.I. can now determine the number of liquor stores and gun stores in your neighborhood, just by scanning your body”.",1
post50hb,richly branching,1.6001453961458911,highest,All I want is a scan where the doctor tells me what's wrong with me. I don't want to pay $2000 for a speed date with my body.,1
post50hb,richly branching,1.6001453961458911,highest,Uhh. We have been able to distinguish race from bones since forever. Ask any forensic anthropologist.,1
post50hb,richly branching,1.6001453961458911,highest,Like all technology it could easily be used negatively.,1
post50hb,richly branching,1.6001453961458911,highest,I mean racists have different skelemans. This is good for robit,1
post50hb,richly branching,1.6001453961458911,highest,"Welp, let’s just let the terminator kill us all and get it over with.",1
post50hb,richly branching,1.6001453961458911,highest,Then just run the x rays through another AI that can determine slight changes between the two,1
post50hb,richly branching,1.6001453961458911,highest,"But of course they can. Bone structure is one of those racial traits. Doctors have been able to identify people's races from their bones for a long time. 

Why would this be a concern from an AI perspective?   It'll just mean they'll worry more about...  cystic fibrosis(?)  for some people. Rightfully so, because they'll be at higher risk. We want to catch these things.",1
post50hb,richly branching,1.6001453961458911,highest,Because it can be reprogrammed and use for other horrible means,2
post50hb,richly branching,1.6001453961458911,highest,"A computer? Can be... REPROGRAMMED!?   

...Y'all don't exactly have a technical background, now do ya?",3
post50hb,richly branching,1.6001453961458911,highest,Because it misses diagnoses for black people. Did you not read the article?,2
post50hb,richly branching,1.6001453961458911,highest,"I don't know how these AIs are programmed, but based on *only* the information in this article, it sounds comically absurd to just assume that the AI would apply ""racial bias"" in any negative way. From the sound of it, this AI isn't programmed to be racist - it's just programmed to be observant and identify things. It can't randomly *become* racist, it can only sense and report on patterns.

The AI can predict race based on a skeletal structure alone. That is absolutely amazing and demands further research. I have no idea why the article even bothers mentioning ""what if AI becomes racist"" without properly giving ideas of how that could even occur in the first place.",1
post50hb,richly branching,1.6001453961458911,highest,"It literally says in the article

>Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research. Scientists must first figure out why this is happening.

So it was applying some sort of racial bias that caused it to miss sickness more often in black people.",2
post50hb,richly branching,1.6001453961458911,highest,I knew it. All human races are from different planets and we're all on a Alien tv show seeing what happens. Wonder what happens when the ratings drop,1
post50hb,richly branching,1.6001453961458911,highest,Honestly my first thought when reading this article was that the scientists probably think this is useful but it is not politically palatable by any means and they don't wanna be seen making software that treats different races differently right or wrong.,1
post50hb,richly branching,1.6001453961458911,highest,"So, we somehow forgot that there are physical differences?",1
post50hb,richly branching,1.6001453961458911,highest,"I can predict people's race just by looking at them. Identifying differences isn't a problem, misplaced bias is.",1
post50hb,richly branching,1.6001453961458911,highest,I guess we're not the same under our skin after all,1
post50hb,richly branching,1.6001453961458911,highest,"I asked my doctor wife about this, and her reaction was ""yeah, of course. So can doctors."" There are differences in bone structure associated with different what we call ""races."" As much as we'd like to say that ""race"" isn't real, it's silly to say that there aren't psychological differences, and some of those differences are more than skin deep.",1
post50hb,richly branching,1.6001453961458911,highest,It's really weird that this article almost completely focuses on the potential problems that could arise from racial bias but the study isn't even about that.,1
post50hb,richly branching,1.6001453961458911,highest,"I think this is interesting.

Obviously, there is some blowback from the wording on the title. That is because I think the comments are annoyed about how the title seems to ignore objective needs of the medical field concerning different treatments for different races. 

Maybe, the scientists were not concerned about this issue of an AI acknowledging different races with just X-RAYs because it is obvious that different races have different methods of treatments in specific circumstances. Maybe, the concern was more subjective in nature. Perhaps, scientists in this study concluded that the medical care could become similarly prejudiced as the medical care contemporary in our society. Human medical care does have bias when it comes to addressing medical needs of the minorities in our society. An imagined example would be a doctor ordering tests for a person in the majority of society and waving off another person in the minority of society, despite having the same symptoms. 

Because the human mind is often unimaginably unbiased, these scientists may have became “concerned” because there was a possibility for the biases that appear in human medical care to spawn in AI medical care. 

That’s my two cents, I’d like to have discussions that counter-argue in a meaningful way. Please feel free to correct, oppose, or support this post; just do it with respect please!

A. S. Bhamba",1
post50hb,richly branching,1.6001453961458911,highest,"I'm from the future.

Stop developing AI.

The singularity will fuck humans harder than they could ever imagine.",1
post50hb,richly branching,1.6001453961458911,highest,Oh no our AI are getting too good. They will be able to tailor treatments to the genetic structure of the patients. But that is racist... Seriously what has the world come to.,1
post50hb,richly branching,1.6001453961458911,highest,Shiiit ... we lizard folk are fucked! Back to Earth's core it is.,1
post50hb,richly branching,1.6001453961458911,highest,"CSI scientists are concerned for their jobs. Bam, fixed it.",1
post50hb,richly branching,1.6001453961458911,highest,"To the people, who don't understand the concerns: it's like you guys didn't read the linked article, which states

>Artificial intelligence (AI) is designed to replicate human thinking in order to discover patterns in data fast. However, this means it is susceptible to the same biases unintentionally. Worse, their intricacy makes it difficult to divorce our prejudices from them.

So if AI can recognize the race of the individual from just the X-Ray image, where people can not, it can apply racial bias, where people previously didn't

>The findings raise several serious concerns concerning AI's role in medical diagnosis, assessment, and treatment: Could computer algorithms mistakenly apply racial bias when analysing photographs like these?

And the bias is not helpful because ""there are diseases that occur more in specific races"" as the top comment suggested. It is in fact harmful for the reason stated in the article

>Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research. Scientists must first figure out why this is happening.

So the whole study was done to figure out can AI tell the person's race, where we cannot and apply racial bias because of it. Turns out it can, and that's the concern

>""We need to take a pause,"" the Massachusetts Institute of Technology's research scientist and physician Leo Anthony Celi told the Boston Globe.  
>  
>""We cannot rush bringing the algorithms to hospitals and clinics until we're sure they're not making racist decisions or sexist decisions.""",1
post50hb,richly branching,1.6001453961458911,highest,"People not reading the article on Reddit?  Impossible!  

I’m pretty sure about 98% of the commenters didn’t read more than half the headline.",2
post50hb,richly branching,1.6001453961458911,highest,Trash didn't ask,2
post50hb,richly branching,1.6001453961458911,highest,We can kiss goodbye all those skeleton memes that try to point out we are all the same underneath. More ironically the memes are now accurate if stated the AI scanned them. Lol.,1
post50hb,richly branching,1.6001453961458911,highest,"Could possibly save lives, but let’s be weary because it can pinpoint the race of the person? I mean when it comes to medical procedures and different races are effected differently, racial bias seems kind of important there. Haha",1
post50hb,richly branching,1.6001453961458911,highest,Concerned because doctors might loose thier jobs to engineers doing engineering things 😂😂,1
post50hb,richly branching,1.6001453961458911,highest,whats to be concerned ? so what different races have a different make up why are they quick to turn it into a racial hatred thing ? :/,1
post50hb,richly branching,1.6001453961458911,highest,"There is a front page trend of terrible headlines lately just to draw us in, never mind the ones that are fake news.",1
post50hb,richly branching,1.6001453961458911,highest,"I mean, can’t you tell race from skeletons anyway? Isn’t the ai just like reading those racial indicators? Is this really unexpected?",1
post50hb,richly branching,1.6001453961458911,highest,"Jep, jep and clickbait to answer your questions",2
post50hb,richly branching,1.6001453961458911,highest,"""scientists are concerned"", gtfoh, no one is scared of anything",1
post50hb,richly branching,1.6001453961458911,highest,[removed],1
post50hb,richly branching,1.6001453961458911,highest,Skulls always had genetic /race variations so the meme was always a bit dumb in that sense. Doctors on medicine are not trained on that but forensics and other scientists are trained in it for their fields.,2
post50hb,richly branching,1.6001453961458911,highest,So anthropology is OK for a AI but when a University professor does it not ok,1
post50hb,richly branching,1.6001453961458911,highest,I'm sick of these sensationalist click baity titles. I wish this site had a mute words and phrases feature like Twitter did,1
post50hb,richly branching,1.6001453961458911,highest,"As people have pointed out this is old news but at the same time you give a dumbass some science and watch it turn into eugenics so fast, we will be right back to they run faster because they have thick tendons",1
post50hb,richly branching,1.6001453961458911,highest,I'm 15 different races - would love to see what AI says about me.,1
post50hb,richly branching,1.6001453961458911,highest,"Scientists is a general term that headlines exploit. AI scientists sure. Medical scientists prolly not. Scientists can have different opinions just like the rest of us and the sooner we don't type cast them by claiming like all of them are equally right, the better it will be for all of us.",1
post50hb,richly branching,1.6001453961458911,highest,It probably seems like a good thing if even doctors are unable to determine this stuff.,1
post50hb,richly branching,1.6001453961458911,highest,What about mixed race people - alot of us are a whole mix of stuff not just one race!,1
post50hb,richly branching,1.6001453961458911,highest,"One issue, at least preliminarily, is that some AIs started identifying races without being promoted to. Which leads to the potential that there are other ‘biases’ that could be impacting the software and information processed that isn’t being accounted for, potentially leading to changes in patient care.",1
post50hb,richly branching,1.6001453961458911,highest,"Typical clickbait headline for science.

Always second check stuff like this on reddit.",1
post50hb,richly branching,1.6001453961458911,highest,"You could say there's a concern with practically any development of AI.

I don't think this is especially worrying.",1
post50hb,richly branching,1.6001453961458911,highest,"Machine learning is not fcking AI

Long enough autobot?",1
post50hb,richly branching,1.6001453961458911,highest,"Seriously though, it's quite easy since there's only ONE human race. Fucking racist AI.",1
post50hb,richly branching,1.6001453961458911,highest,"Maybe we could use a better word than ""race"" which is already so charged, and isn't scientifically defined anyway.  

We're talking about phenotypes as they relate to genotypes.

And there's more overlap in the venn diagram than not, so we're really talking about marginal differences in phenotype within one genotype vs another.",1
post50hb,richly branching,1.6001453961458911,highest,"I feel like in the future the AI will become racist. Microsoft's ai bot already turned racist after learning the behavior of twitter users, what's to say that this wont happen again. Just this time it knows the race of each person, thus making it worse.",1
post50hb,richly branching,1.6001453961458911,highest,"I hate the way they call this artificial ""intelligence"". Its just a pattern spotting program.",1
post50hb,richly branching,1.6001453961458911,highest,Humans being afraid of *artificial* intelligence developing racial bias that was created by humans is laughable but concerning as well,1
post50hb,richly branching,1.6001453961458911,highest,AI will always have biases. Not specifically racial but it will. It's just a mathematical byproduct.,1
post50hb,richly branching,1.6001453961458911,highest,What is the list of races and the number of subjects of each respective race in this study? Did the subjects select their race from a predetermined list?,1
post50hb,richly branching,1.6001453961458911,highest,"Ai: yep, it's definitely a member of the human race.",1
post50hb,richly branching,1.6001453961458911,highest,It would be better to input the patients predominant haplogroup when training the Ai. Racial groupings are fictitious and have no scientific basis.,1
post50hb,richly branching,1.6001453961458911,highest,"Isn’t race not a thing ? In france it is said that it is not races biologically speaking, why does American scientific articles uses race to speak about color of skin ? Are their other developed country that still use race for that ?",1
post50hb,richly branching,1.6001453961458911,highest,"Predict?! You predict something that will happen, not something that's right in front of you. And yes I know the terms training and prediction in machine learning, but here ""detect"" would have been perfect
Also the ""concerned"" part is just BS",1
post50hb,richly branching,1.6001453961458911,highest,Race is not a scientific concept at all. It is a social construct.,1
post50hb,richly branching,1.6001453961458911,highest,This just in: AI is being canceled for being racist.,1
post50hb,richly branching,1.6001453961458911,highest,Can't anyone even sort of trained in anthropology do this?,1
post50hb,richly branching,1.6001453961458911,highest,What is the concern? Certain races have certain characteristic in regards to bone structure.,1
post50hb,richly branching,1.6001453961458911,highest,"Are we really going to act like this is somehow surprising and that we all have the same physical features and body shapes and sizes, just painted a different colour?",1
post50hb,richly branching,1.6001453961458911,highest,"Interesting. Here is another thought concerning AI in medical devices: 

https://www.eetimes.com/when-we-put-ai-in-medical-devices-magic-starts-to-happen/",1
post50hb,richly branching,1.6001453961458911,highest,What’s the harm?  Well let’s assume the year is 1943 or so.    A great nation believes that all persons of a certain genetic makeup must be liquidated immediately because they are a pollution of racial purity.    But it is hard to identify the individuals who belong to said group and the individuals do not desire to be liquidated.    So all persons above age  10 are x rayed when they get vaccinated and sent to a Cray super computer for analysis.   Authorities are quickly dispatched to roundup the persons so identified.     Too far fetched?,1
post5hb,richly branching,1.5948230389474962,highest,Eye witnesses also can't tell so this does not really change anything.,1
post5hb,richly branching,1.5948230389474962,highest,[deleted],2
post5hb,richly branching,1.5948230389474962,highest,Correct. Eye witnesses are not a reliable source of data.,3
post5hb,richly branching,1.5948230389474962,highest,"This has been shown time and again, and yet it still sounds absolutely insane to make witness testimony inadmissable in court. I find that odd.",4
post5hb,richly branching,1.5948230389474962,highest,"On the race thing (as some commenters below have suggested) it's not that people can't tell people apart who are other skin colors but that in the United States at least a majority of our population is white and around 3/4 of white people in the US do not have really any relationships with non-white people.  The same can be said for other non-white races in the US around relationships but to a much lesser extent. If you only interact with people who look like you on a daily basis, you are not going to be good at identifying people who don't look similar to you.

I grew up in a family and community that was extremely diverse, as a result I do not have the issue telling apart people of different races and can not remember any point where that was the case.  As I got older, around my teenage years and after I had learned the backgrounds of enough people in my community and family, I became pretty good at making a guess in narrowing down the localized region someones family might be from.

For something most people can probably relate to... it's no different from how when you hear a genre of music you have never listened to before it can be difficult to make out the lyrics or how when you hear a new language that doesn't share roots with a language you are already familiar with you tend to not be able to differentiate separate words in a spoken sentence.

^(edit:)

[^(https://www.washingtonpost.com/news/wonk/wp/2014/08/25/three-quarters-of-whites-dont-have-any-non-white-friends/)](https://www.washingtonpost.com/news/wonk/wp/2014/08/25/three-quarters-of-whites-dont-have-any-non-white-friends/)",4
post5hb,richly branching,1.5948230389474962,highest,"Given the statistical prevalence of medical misdiagnosis due to patient misinformation. I don't trust people to tell the truth about this shit.

To quote a famous fictional doctor, ""people lie.""",4
post5hb,richly branching,1.5948230389474962,highest,"I know that there are many problems with eye witnesses but you‘re overestimating. 

A very important factor is how you ask them. Unfortunately cops often don‘t care at all and use methods where we know that they produce junk.",4
post5hb,richly branching,1.5948230389474962,highest,"I think there’s a Neil DeGrasse Tyson quote that said something like, “the human eye is the least trustworthy form of proof in science, but the highest form in the court of law”.",4
post5hb,richly branching,1.5948230389474962,highest,"They can be if they personally know the suspect, like relatives, coworkers, neighbours etc, but a random passerby is indeed unrelaiable as balls",4
post5hb,richly branching,1.5948230389474962,highest,Confirming. Correct.,4
post5hb,richly branching,1.5948230389474962,highest,Happy cake day!,4
post5hb,richly branching,1.5948230389474962,highest,you are right with the exception that the witness already knew the person. (I don't mean a casual 'I've seen this guy around town' so much as friends),4
post5hb,richly branching,1.5948230389474962,highest,"Happy cake day, if it really is your cake day. I’m not sure. Where am I?",4
post5hb,richly branching,1.5948230389474962,highest,Happy cake day my friend!,4
post5hb,richly branching,1.5948230389474962,highest,The Senate apparently agrees with you.,4
post5hb,richly branching,1.5948230389474962,highest,"What about mouth witnesses?

(ignore me, I'm just pondering the oddness of the term ""eye witness"")",4
post5hb,richly branching,1.5948230389474962,highest,Soooo.....Facial recognition it is.,4
post5hb,richly branching,1.5948230389474962,highest,According to the Republicans anyway..,4
post5hb,richly branching,1.5948230389474962,highest,I have also seen the unreliability of eyewitnesses. This must mean that most eyewitnesses are unreliable!,4
post5hb,richly branching,1.5948230389474962,highest,The Mandela effect is a good example that memory is a fickle thing.,4
post5hb,richly branching,1.5948230389474962,highest,So you and all those who upvoted you are believing that it's correct to have the Trump impeachment trial without witnesses? That's totally what you're saying.,4
post5hb,richly branching,1.5948230389474962,highest,Brett Kavanaugh agrees.,4
post5hb,richly branching,1.5948230389474962,highest,Yet they are the cause of many ending up in jail.....many times innocent.,4
post5hb,richly branching,1.5948230389474962,highest,"Are there any circumstances where'd you say ""alright, so many people claim stuff that is seemingly alike, it can be used""?",4
post5hb,richly branching,1.5948230389474962,highest,Source: an eyewitness.,4
post5hb,richly branching,1.5948230389474962,highest,Most people agree with you and stop short when it comes to the eyewitness testimony in the bible as if it would magically be more reliable.,4
post5hb,richly branching,1.5948230389474962,highest,And...cake day...,4
post5hb,richly branching,1.5948230389474962,highest,The best proof of this is the infographics show vid on the innocent guy who can't be released,4
post5hb,richly branching,1.5948230389474962,highest,Found cgp Grey's alt account,4
post5hb,richly branching,1.5948230389474962,highest,"That’s, what he just said",4
post5hb,richly branching,1.5948230389474962,highest,Not entirely true. I can tell if he/she is white or not..after that...🤷‍♂️,4
post5hb,richly branching,1.5948230389474962,highest,"Yeah it always used to impress me when I was younger these detailed descriptions you'd hear sometimes, made me feel a lot better when I heard they're wrong the vast majority of the time and usually hugely wrong. I know the best I'd likely be able to give, outside of them having some really stand out feature (or neat hair..I'd probably remember cool hair...), about some random I'd run into for 30 seconds would be 'err some white guy, think he had darkish hair maybe. Probably somewhere between not super short and not super tall and not super thin or super obese...that help?'.  I mean if I was really close to them I could at least get height vaguely relative to myself but even then it'd be +/- 7.5cm(3"") soooo yeah.... 

I'd be even worse at a lineup 'well that could be them, but so could 4 of the 6 other guys you've got there'... I say this with extreme confidence because as a teen I forgot what my then long distance girlfriend looked like between the first time we got together(we spent a good 10 days together) and the first time she flew over to meet me. Sitting next to my Nana who had given me a lift to the airport, Nana pointed at a girl 'is that her?' 'nope' then she waved and run over for a kiss, over her shoulder while she was hugging me I could see this damn cheeky grin from my Nana >_<",3
post5hb,richly branching,1.5948230389474962,highest,[deleted],4
post5hb,richly branching,1.5948230389474962,highest,Yup. I don't know how people describe someone they saw once well enough for a sketch artist to put together something remotely usable. I don't think I could do that with my own face unless I had a mirror.,4
post5hb,richly branching,1.5948230389474962,highest,"On top of that, eye witnesses (and all humans) are empirically worse at identifying faces not of their own race.",3
post5hb,richly branching,1.5948230389474962,highest,"Jokes on you, I can’t even reliably identify faces of my own race.",4
post5hb,richly branching,1.5948230389474962,highest,"If this is true, I am assuming that they trained the model used for the recognition with mostly white people and that's what causes this inability to precisely recognize brown and black people",4
post5hb,richly branching,1.5948230389474962,highest,Iirc it’s actually the race that they grew up with. i.e. a white person who grew up in China would have difficulty differentiating white faces but have no problem with Chinese faces.,4
post5hb,richly branching,1.5948230389474962,highest,I hope I'm never expected to act as a witness to anything. I'm face-blind and can barely even tell my own family apart from other people.,3
post5hb,richly branching,1.5948230389474962,highest,"If you're ever asked to, just tell them that. The court's not going to be interested in a witness who willingly discredits any information they could give.",4
post5hb,richly branching,1.5948230389474962,highest,Yep I was an eye witness twice weirdly my mind made them look the same or else they were the same person going around attacking people which is also possible.,3
post5hb,richly branching,1.5948230389474962,highest,Lol,3
post5hb,richly branching,1.5948230389474962,highest,"I know I make for a terrible eye witness. I hope I never get put in a situation where people are relying on me as a witness for something important. 

It's like I have to see a face several times before my brain finally decides, ""Ok then, I guess I have to open a file for this person. Face, name and any other additional details."" And it doesn't really take a long time without seeing that person before my brain decides, ""I guess we're not using this file anymore. Delete.""

I have a friend who's really gifted with remembering faces and people. I can't count the number of times that we'd run into a person who seems like a stranger, only for my friend to identify this stranger by name, remember how long ago we saw them last and even tiny details about that person even from years back. 

Now, this friend would definitely make an awesome eye witness if needed.",3
post5hb,richly branching,1.5948230389474962,highest,Your lying eyes.,3
post5hb,richly branching,1.5948230389474962,highest,"As long as someone pays the price, it doesn't matter if they are guilty or innocent:)",3
post5hb,richly branching,1.5948230389474962,highest,"So true. Here's what happened in NZ.
My sister-in-law is fair skinned, has straight hair and green eyes. We from Africa and are called coloured in our country. Black everywhere else. 
At the law firm the receptionist, a 50 year old white lady, tells her "" your client is here to see you"", she asks which one, receptionist replies, ""the one that looks like you"". SiL decides this is useless and walks into the waiting room. Inside the room is a black lady from Nigeria, very dark skin, brown eyes and black short hair. After her the meeting my SIL goes and asks the lady at the front desk, ""so she she looks like me?"". Lady responds, "" you both black aren't you?"" And starts laughing..",3
post5hb,richly branching,1.5948230389474962,highest,"In Australia the news says stuff like ""Middle eastern/Polynesian/SubContinental man between 17-26 years old, could be a woman. Maybe""",3
post5hb,richly branching,1.5948230389474962,highest,"That is dependent upon the person and situation. 

It is proven that people have different memories of a situation in which a large group was involved, but if you’re personally attacked or witness an attack you may have a different recognition of the attacker as there are less faces to get confused with.",3
post5hb,richly branching,1.5948230389474962,highest,Cross race identification is the most inaccurate.,3
post5hb,richly branching,1.5948230389474962,highest,I once accused a yeti when a sasquash was to blame.  #embarrassinglineupgaffs,3
post5hb,richly branching,1.5948230389474962,highest,"And it's true across races. Asians can't tell white people apart, white people can't tell Asians apart, etc.",2
post5hb,richly branching,1.5948230389474962,highest,"This isn't true.  It depends on where you've been brought up, not what your race is.  If you're white, but brought up around black people, you'll be able to tell black people apart, and have difficulty telling white people apart.

It's learnt behaviour, and it means that people who are minorities in countries are perfectly fine telling apart the majority race of the country apart.",3
post5hb,richly branching,1.5948230389474962,highest,I grew up only around europeans and the only difficulty I have is separating east asians apart.,4
post5hb,richly branching,1.5948230389474962,highest,"Not entirely true.  Some ethnic groups just have much more subtle differences.

According to several of my Chinese friends who grew up in china, Chinese people all look the same.",4
post5hb,richly branching,1.5948230389474962,highest,*squints at line-up*,3
post5hb,richly branching,1.5948230389474962,highest,That's racist. Maybe they're just looking at it.,4
post5hb,richly branching,1.5948230389474962,highest,Wouldn't they all?,4
post5hb,richly branching,1.5948230389474962,highest,"Wasnt there a thing where iPhones were unlocking when like, any asian person looked at them or something?",3
post5hb,richly branching,1.5948230389474962,highest,I'm an Asian and still can't always tell Asians apart either,3
post5hb,richly branching,1.5948230389474962,highest,I'm white and can't tell white people apart sometimes. Some of us just aren't good at facial recognition.,4
post5hb,richly branching,1.5948230389474962,highest,"So we are all racist?  Maybe that means most of us aren't racist and are simply more cognizant of the people and things that we are exposed to on a daily basis instead?

No.  This is reddit.  We can't use logic here.  I am racist.  You are all racist.  Fuck everything.  I'm angry for no reason so we should burn it all to the ground.",3
post5hb,richly branching,1.5948230389474962,highest,I think this guy might be a racist.,4
post5hb,richly branching,1.5948230389474962,highest,There is a world of difference between not being able to tell people of an unfamiliar race apart and thinking “all X people look alike”.,4
post5hb,richly branching,1.5948230389474962,highest,Wut? That turned into a weird rant. You been listening to too much Limbaugh again?,4
post5hb,richly branching,1.5948230389474962,highest,lmao who are you arguing with? Figger it out,4
post5hb,richly branching,1.5948230389474962,highest,We may be racist but as far as telling people apart I would assume something genetic. Our brains look for patterns and would discern better between things it sees a lot like the people mostly around us,4
post5hb,richly branching,1.5948230389474962,highest,Not sure why you’re being down voted...,4
post5hb,richly branching,1.5948230389474962,highest,"Oh dont worry, its all gonna burn down to the ground all on it's own now.",4
post5hb,richly branching,1.5948230389474962,highest,Such a bullshit statement. Have you ever had a job with mixed races all over the place? By your logic everyone would need huge nametags to tell eachother apart.,3
post5hb,richly branching,1.5948230389474962,highest,This made my pale ass laugh.,2
post5hb,richly branching,1.5948230389474962,highest,"Same with some people constantly ""misgendered"" now even by these AI robots.  You'd think that'd be enough to make them realize that not every human misgendering then isn't intentionally being a dick.",2
post5hb,richly branching,1.5948230389474962,highest,[removed],3
post5hb,richly branching,1.5948230389474962,highest,"Can confirm, I am a hair stylist that does mostly men's hair. Women, also, but most of them have short hair. Many of them identify as something other than straight/cis, and we obviously cant assume anything right away.

Had a moment not too long ago where I called a woman ""he"" because I could only see the back of their head and I felt awful the moment I saw her face in the mirror.

But she just laughed and said it was okay.",4
post5hb,richly branching,1.5948230389474962,highest,"Once someonencalled a MTF ""dude"" and she was like ""I'm a girl.""

I say dude to everyone despite gender....",4
post5hb,richly branching,1.5948230389474962,highest,"It’s like the old joke about how if it smells like shit everywhere you go, you eventually gotta check your shoes. If you’re constantly misgendered, people aren’t being rude - your looks just don’t match your gender ID.",3
post5hb,richly branching,1.5948230389474962,highest,Your argument doesn’t make sense. You’re assuming facial recognition and eye witnesses are the same? Facial recognition reads every detail of a face. From the eyes to the little things that make your face unique from everyone else. It doesn’t need race to get the exact information to find someone.,2
post5hb,richly branching,1.5948230389474962,highest,I think 'twas a joke,3
post5hb,richly branching,1.5948230389474962,highest,Impossible,4
post5hb,richly branching,1.5948230389474962,highest,The more things change the more they stay the same,2
post5hb,richly branching,1.5948230389474962,highest,Black privilege.,2
post5hb,richly branching,1.5948230389474962,highest,"It's a huge difference - misinformed police can do a lot more harm than eye witnesses with shoddy recall. For example, and from the actual article:

>When existing and discriminatory police processes combine with faulty tech like facial recognition this results in a system where innocent people are flagged up by computers, hauled off the street and then have their biometric data extracted from them.",2
post5hb,richly branching,1.5948230389474962,highest,Drug dogs are no better than a coin flip,2
post5hb,richly branching,1.5948230389474962,highest,"Honestly, I think people are looking at this all wrong.

No single source of evidence, whether it be forensic, facial recognition, eye witness accounts, is useful in and of itself. You need a full package to really get to the source of things.

To me, the issue is less ""will facial recognition be reliable"" and more ""will the police abuse it to fuck over minorities."" And I think we all know the answer to that question.",2
post5hb,richly branching,1.5948230389474962,highest,It does when you can be tagged coming in to a business by surveillance cameras using this tech & walk back out to being handcuffed.,2
post5hb,richly branching,1.5948230389474962,highest,All Asians look the same!!!/s,2
post5hb,richly branching,1.5948230389474962,highest,Cops can't either. So what's the difference??,2
post5hb,richly branching,1.5948230389474962,highest,"Reminds me of that Seinfeld episode where Elaine dates this guy because she thinks he is black, and he dates her because he thinks she's Hispanic. But they are both white and just have weird shaped skulls.",2
post5hb,richly branching,1.5948230389474962,highest,"FBI black crime stats are evidence of bias in municipal law enforcement, and those stats are historically used by the DOJ to put the LAPD, SPD, NYPD, etc into Content Decrees (probation) for being criminal enterprises.",2
post5hb,richly branching,1.5948230389474962,highest,"I work with similar software to this, except voice instead of face. I've used several of the commercial speaker recognition engines as well as some open source ones, and have found that no one's models are accurate for Black people. Out of non-immigrant English speaking Americans, it seems like old and poor southern Blacks as a category tend to be misrecognized most frequently, followed by middle aged speakers with heavy AAVE accents mixed with ESL English speakers.",1
post5hb,richly branching,1.5948230389474962,highest,"Both my friend and I are white as fuck, and have different accents. A few years ago, he ""trained"" his Android to recognize ""his"" voice. He had numerous people try to activate the voice search option, and none of them did it. I heard him say ""hey google,"" or whatever it is, several times. One day, I was sitting across a desk from him, at his work, and then struck. I mimicked his voice, and asked something about midget, amputee porn. His phone BLARED the results for ""midget, amputee pornogrophy."" Damn, was he pissed at me, and boy, were other people laughing!",2
post5hb,richly branching,1.5948230389474962,highest,I'm curious - what country are you guys in?  What accents?  I have to know. So I guess curiosity has now become obsession.  Sorry.  Tell me.,3
post5hb,richly branching,1.5948230389474962,highest,"US. I grew up all over the Northern Hemisphere, and he grew up in the Mid-West.

Edit: Nothern Hemisphere = USA, Western Europe, and Eastern Asia.",4
post5hb,richly branching,1.5948230389474962,highest,"Siri thinks I sound like Cookie Monster. Every time that commercial came on tv, she’d be all “I can’t find that playlist”.",3
post5hb,richly branching,1.5948230389474962,highest,"I write code that produces models that do this. It's not that the models are racist, it's that the data is incomplete. The models can only diffentiate based on the space it creates a metric in, if two data points are identical in every feature a model has at it's disposal to distinguish then of course it won't differentiate. Modern convolutional neural networks work a lot better in B&W because it cuts heavily down on the processing required. Adding colour takes it from each pixel being processed as a floating point from 0 to 1 to each pixel being 3 floating points from 0 to 1, which doesn't just increase the processing requirements by 3 but instead also increases the data requirement as to get the same theoretical certainty with a higher feature dimension requires more data points.

On the flip side, just because a model isn't perfect doesn't mean it isn't useful, Amazon isn't throwing out its shopping assistant AI models just because it gets it wrong sometimes. Machine learning models require human interpretation when the decisions have larger consequences.",2
post5hb,richly branching,1.5948230389474962,highest,"Machine learning models also need lots of input to be correct. A lot of speech recognition is based off AI/ML as I'm sure you know. I would bet large amounts of money that when they feed the samples into the system the majority of people giving those samples is skewed. The company would need to actively try to correct the bias created by their input to fix it. They have the resources to do this but they don't, that's up to them.",3
post5hb,richly branching,1.5948230389474962,highest,"The majority? Not even close, voice samples and augmentation thereof is pretty convenient, all things considered. Like, annotating vague samples often has pretty rich context and most text is literally people just reading set phrases.",4
post5hb,richly branching,1.5948230389474962,highest,"""For now, humans are still responsible for the production of new digital systems; and that means they come into being with all the biases and fallibility of their creators baked right into their code.""  


Sad how the above makes it sound like the programmers and/or models are racist...like a programmer is going to look through thousands of training images and be like ""how can I make it \[insert racist thing here\]?""  


""It's not that the models are racist, it's that the data is incomplete."" - Upvote unlocked.",3
post5hb,richly branching,1.5948230389474962,highest,"You dont see a difference between a shopping AI getting it wrong and a facial recognition AI, being used by police to target criminals?  That's dark as fuck honestly. 

If the rate of error is high and it isnt working we really don't have to use it until those issues are addressed lol",3
post5hb,richly branching,1.5948230389474962,highest,">You dont see a difference between a shopping AI getting it wrong and a facial recognition AI, being used by police to target criminals?  That's dark as fuck honestly. 

That's what we call a strawman argument - applying a statement to someone that didn't say it so you can criticise it/them in an attempt to undermine their actual argument that you didn't touch.

My argument was about models being useful even when they aren't perfect, they just require oversight. In no way did I equate shopping AIs and facial recognition ML models.

>If the rate of error is high and it isnt working we really don't have to use it until those issues are addressed lol

Or we can use it to flag and then have a human confirm? Pretty simple way of cutting down on ridiculous amounts of work.",4
post5hb,richly branching,1.5948230389474962,highest,"It's almost as if different races have different distinctive feature that needs to be properly represented by a good, large dataset.

A lot of AI related issue is due to the training dataset of the model and the hyperparameters defined by the developper, which is always at least slightly biased.

Train a model with a chinese dataset and it will be good at recognizing chinese people and telling them apart but he will be crap at doing the same with american. Vice versa of course apply.

It's a really difficult problem to solve but in the end the model can only do what you train it for.

AI is used more and more nowadays so there's good hope for models improving with time and to include a much bigger spectrum of parameters.",2
post5hb,richly branching,1.5948230389474962,highest,"Even when you do it with Americans, It's crap at it because they only usually use a small subset of Americans.",3
post5hb,richly branching,1.5948230389474962,highest,https://en.wikipedia.org/wiki/African-American_Vernacular_English,2
post5hb,richly branching,1.5948230389474962,highest,"Also, just like with motion hand washers, that work off of IR, you can only work with what your given initially, and even in the alpha test. 

Yes, as with all scientific test, they need to be expanded, but finded every regional minority costs money, and isn't worth it when you can just test against the majority. This method has numerous pros and cons. Just stating facts people.",2
post5hb,richly branching,1.5948230389474962,highest,It's not a test. Shitty AI is being used by police and judges.,3
post5hb,richly branching,1.5948230389474962,highest,They know this isn't a test. They're taking about how the software is trained.,4
post5hb,richly branching,1.5948230389474962,highest,"First off, a few false positives does not make a ML model shitty, there's plenty of tests we've used for a long, long time that have nothing to do with ML that have false positives, take for example the test for HIV. It's been used for years around the world, it definitely has false positives but just because something isn't perfect doesn't mean it's not useful, if they get a positive they run the test again. Chances of a double false positive are so small that two positives is as close to certain as we can be.

As long as they use their actual eyes to do the final determination (is that person in the image this person) it doesn't need to have zero false positives, that requirement is still on the judge.",4
post5hb,richly branching,1.5948230389474962,highest,"You don't need a scientific test when you're in product development, just literally one test subject. This isn't science gone wrong, it's shitty culturally miopic engineers because our engineering schools and workplaces are a monoculture.

TLDR, They're just bad at their job.",3
post5hb,richly branching,1.5948230389474962,highest,"That is not how ML/AI works in any way. You need a many ""subjects,"" or sample records, during the training phase as possible. 1 record is not enough for any training, and will fail every test.",4
post5hb,richly branching,1.5948230389474962,highest,[removed],2
post5hb,richly branching,1.5948230389474962,highest,"> AAVE was... invented 

Languages and dialects don't work like that. Aside from actual invented languages, such as Esperanto, dialects and languages evolve naturally through usage. Nobody ""invents"" a natural language or dialect.",3
post5hb,richly branching,1.5948230389474962,highest,"Indeed languages can be “invented” like Esperanto, but that’s only part of the language piece

> Nobody ""invents"" a natural language or dialect.

Studies on (1) pidgin languages (2) infants and (3) isolated communities (i.e.: deaf or lost) seem to say otherwise.  

Babies do “invent” language during their development (babbling is theorized to be “building” language, phoneme by phoneme) and ostensibly communities who are isolated from “standard” languages very quickly develop their own for this reason.  This effect was able to be empirically tested in deaf signing communities, and more observationally in",4
post5hb,richly branching,1.5948230389474962,highest,"People invent language every day. New words or meanings for words arise as people see the need for them. Willam Shakespeare invented dozens of words that we use still today, many with the same meanings that he used.  Even 'Doctor' Theodor Seuss Geisel invented the word ""Crunk"", although it's usage has widely diverged from his initially writing it into one of his stories.

Language is invented by the people that speak it. And reinvented through new and varied usage, on a daily basis. Just try and read the works of Chaucer or even the poet John Dunne and try and say that the English that they wrote in is not a different language than the one that you use today.",4
post5hb,richly branching,1.5948230389474962,highest,"I never said AAVE was an *artificially* invented language.

Language is invented constantly and continuously by the people using it, as the need arises.",4
post5hb,richly branching,1.5948230389474962,highest,Could be a survival of the fittest type thing. Whoever adopted the dialect was better able to reproduce.,4
post5hb,richly branching,1.5948230389474962,highest,"""was invented""

Uh, like, by Tolkien or what? That's not a variety that was willed into existence by some dude...",3
post5hb,richly branching,1.5948230389474962,highest,It wasnt so much invented as it was the inevitable result of enslaving vastly different ethnicities across half a continent and forcing them to communicate *somehow* and then waiting a few decades after the fact to see how that developed,3
post5hb,richly branching,1.5948230389474962,highest,"And the slaves invented a language that allowed then to not only communicate, but also communicate in a way that allowed them to relay ideas that their masters and supervisors would not always be happy about them taking about.",4
post5hb,richly branching,1.5948230389474962,highest,this is a really interesting idea but i’m having a bit of trouble understanding how exactly it works. would you be able to give an example?,3
post5hb,richly branching,1.5948230389474962,highest,"Haitian Creole started this way.Haiti was a french colony, they brought slaves from different parts of Africa.On a single plantation, slaves spoke different languages.

This was by design, if you can't understand each other, you can't rebel, you can't plan escapes.

So the slaves started to mimic french, mixing it with African and Taino(Native inslander) words . It was very effective, they were able to communicate with that hybrid language.  


It is now a real language with a grammar, dictionary and published literature.",4
post5hb,richly branching,1.5948230389474962,highest,"I wouldn’t agree that that’s where AAVE came from (more that it was a distinct community that naturally developed its own speech patterns), but that’s how a lot of slang works.  There’s enough slang (a lot of it AAVE derived) that I can talk to my friends and not let my parents understand.",4
post5hb,richly branching,1.5948230389474962,highest,Just like Cockney rhyming slang.,3
post5hb,richly branching,1.5948230389474962,highest,Is there a difference in the voice though? That's the thing I don't understand. Machines can't recognize cultural differences. They look for physical differences which if there are any are very limited.,2
post5hb,richly branching,1.5948230389474962,highest,"How is this software taught? I am assuming it's AI?
Could it be because there's much less black people than there is white people in the data it's taught with, and that makes the software more accurate with white people? (I'm not sure what all those accent things mean)",2
post5hb,richly branching,1.5948230389474962,highest,Is AaVE what I heard called ebonics growing up?,2
post5hb,richly branching,1.5948230389474962,highest,Is it surprising that people who speak the most non-standard English present the biggest challenge for voice recognition software?,2
post5hb,richly branching,1.5948230389474962,highest,"That seems to make sense statistically, if you're a software dev trying to get ""most people"" to work correctly with your software to start.

Most people aren't black or ESL.",2
post5hb,richly branching,1.5948230389474962,highest,"There is no black voice.  There is ghetto voice.  Black people sound the same as white people when they are brought up in the same manner.  White trash sound like trash, black people sound the same as those hillbilly fucks when they are brought up that way.
African american vernacular english is racist as fuck, shame on you for spreading that bullshit.",2
post5hb,richly branching,1.5948230389474962,highest,"""AAVE"" is a problem, but ""ghetto voice"" is fine? GTFOH.",3
post5hb,richly branching,1.5948230389474962,highest,You do realize your a racist?  There is not a black English and white English.  We have regional accents that are learned behavior.  We are still all humans.  Melanin is a skin pigment and has zero to do with how one speaks.  I cannot believe you could be so ignorant.,2
post5hb,richly branching,1.5948230389474962,highest,"Your pearls would look better if you weren't clutching them so tightly. 

My comment is an observation about the outcomes I've personally witnessed having run thousands of sets of these trials and actually listening, with my ears, to the outliers.

Like it or not, subgroups of any population frequently pronounce things slightly differently from others. It would be racist to say ""Black people aren't recognized because they don't speak real English"", but it's not racist to say ""I've found that many of these verification models have issues recognizing Black people"". Mentioning someone's race doesn't automatically make the conversation racist.",3
post5hb,richly branching,1.5948230389474962,highest,"Humans tend to group stuff into useless categories. That's like our thing. Before the main belief was based on science, but we like to call it religion, now it's based on politics, but we like to call it science.",2
post5hb,richly branching,1.5948230389474962,highest,"ITT: People misunderstanding the headline, not reading the article.

The problem is that it can't tell _individual_ black and brown people apart, not that it can't distinguish between the _categories_ of ""black"" and ""brown.""",1
post5hb,richly branching,1.5948230389474962,highest,Oh damn that's so much better,2
post5hb,richly branching,1.5948230389474962,highest,"i don't know, i think that's so much worse

you could just be confused by some shitty algorithm for anyone else and arrested over ""suspicion""",3
post5hb,richly branching,1.5948230389474962,highest,i think they were being sarcastic,4
post5hb,richly branching,1.5948230389474962,highest,"> ITT: People misunderstanding the headline, not reading the article.

We need a bot that just posts this in every comment section.",2
post5hb,richly branching,1.5948230389474962,highest,"yeah, I was just thinking ""isn't that the same? or is brown people Hispanics in this case?""

like I remember reading a book with a black main character that ponders to himself on why they call themselves black because he saw all shades of brown but not a single actual black person. so that made me extra confused",2
post5hb,richly branching,1.5948230389474962,highest,"The way I've seen it used, ""brown"" mostly refers to people of middle eastern descent.",3
post5hb,richly branching,1.5948230389474962,highest,I heard the US census considered middle easterns (Arabs) white. Brown is usually south asian. I'm not America tho so I can't be sure,4
post5hb,richly branching,1.5948230389474962,highest,I remember reading something like this in Maniac Magee,3
post5hb,richly branching,1.5948230389474962,highest,This is great news for us blacks,2
post5hb,richly branching,1.5948230389474962,highest,"Yes it can't be used for conviction, but it can be used to detain innocent people",3
post5hb,richly branching,1.5948230389474962,highest,"... shouldn't be used for conviction, that doesn't mean it won't.",4
post5hb,richly branching,1.5948230389474962,highest,Thank you,2
post5hb,richly branching,1.5948230389474962,highest,The headline certainly wasn't written very well.,2
post5hb,richly branching,1.5948230389474962,highest,"ELI5?
Cause i'm not english speaking, maybe that's why i don't understand the problem here",2
post5hb,richly branching,1.5948230389474962,highest,"If you have a black and brown person standing next to each other it can tell you who is black and who is brown.

But if you have two black or brown people standing next to each other both black or brown people look the same.",3
post5hb,richly branching,1.5948230389474962,highest,"But it's facial regocnition. Skin color is just part of it. How is someone going to be taked for someone else by just their skin color? Aren't eye distance, distance from mouth to nose, ear heights and whatnot.... many things can be mismeasured here, but it's the combination of all of them that makes a person a suspect. And then after being a suspect there are tons of other things that make someone guilty",4
post5hb,richly branching,1.5948230389474962,highest,I'm misunderstanding the headline because it's funnier that way.,2
post5hb,richly branching,1.5948230389474962,highest,[deleted],2
post5hb,richly branching,1.5948230389474962,highest,"Not really, I read it the way it was intended. I understand some people might be confused. Either way it has negative outcomes for black and brown people while white people are unaffected.",3
post5hb,richly branching,1.5948230389474962,highest,"> while white people are unaffected.

Well, except for the guilty ones, and we can be sure of that since these cameras are calibrated to pick out the bad white people with a fractional margin of error. /s",4
post5hb,richly branching,1.5948230389474962,highest,[Eww.](https://i.imgur.com/AATRNvz.png),3
post5hb,richly branching,1.5948230389474962,highest,"Almost like English makes some sentences ambiguous. If the headline had read

> Facial recognition can't tell black people apart - but the police are using it anyway

or 

> Facial recognition can't tell brown people apart - but the police are using it anyway

it would have been correct and clear. Combining introduces ambiguity but doesn't make it incorect",3
post5hb,richly branching,1.5948230389474962,highest,"To me it sounds clearer by saying ""Facial recognition can't tell black or brown people apart."" But I'm 10 years removed from college and didn't major in English.",4
post5hb,richly branching,1.5948230389474962,highest,[deleted],2
post5hb,richly branching,1.5948230389474962,highest,Username checks out,3
post5hb,richly branching,1.5948230389474962,highest,Considering I know Indian men who act and claim to be black and brown people who are white... I do not think the story means shit.,2
post5hb,richly branching,1.5948230389474962,highest,Reminds me of Better Off Ted when the motion sensors wouldn't detect black people.  That show was funny.  This reality isn't so funny.,1
post5hb,richly branching,1.5948230389474962,highest,"“The company’s position is that it’s the *opposite* of racist because it doesn’t *target* black people, it just ignores them. The worst that people can call it is ‘indifferent’.”",2
post5hb,richly branching,1.5948230389474962,highest,"At this pace everyone on the planet will be working for us in 3 years 

We just don’t have the parking",3
post5hb,richly branching,1.5948230389474962,highest,"""It gets dark every time you leave.""",2
post5hb,richly branching,1.5948230389474962,highest,I had a job where the time clock had a fingerprint scanner that worked better the lighter the person's skin was.,2
post5hb,richly branching,1.5948230389474962,highest,"I just went to find that clip because it's the first thing I thought of too.

https://youtu.be/XyXNmiTIupg",2
post5hb,richly branching,1.5948230389474962,highest,"I miss that show and it's a shame it was cancelled. Had a lot of great moments to it, especially that one.",2
post5hb,richly branching,1.5948230389474962,highest,Darker faces have less contrast so computers have a much harder time reading them. It’s why hardcore face scanners display a dot field on the face to make the contrast irrelevant.,2
post5hb,richly branching,1.5948230389474962,highest,"Cameras are actually rubbish without bright light situations and conspicuous facial features. If you see any filming or photo shoots, the lights could melt you and the make-up used looks clownish in reality. Just to look even slightly normal viewed in film/TV. I guess POC don't have eyes and mouth as differentiated as white people do, with light/pinkish skin, brown eyes & dark pink lips. 
Tldr: it probably doesn't work, it shouldn't be used.",1
post5hb,richly branching,1.5948230389474962,highest,[deleted],2
post5hb,richly branching,1.5948230389474962,highest,Tldr: photons are racist.,3
post5hb,richly branching,1.5948230389474962,highest,Ok Wesley snipes,3
post5hb,richly branching,1.5948230389474962,highest,"No. They just haven't used enough training data with dark skinned people. Yes lighting is important, but that affects everyone equally. Data size is the only differentiator.",3
post5hb,richly branching,1.5948230389474962,highest,">  Yes lighting is important, but that affects everyone equally.

...what, of course it doesnt.",4
post5hb,richly branching,1.5948230389474962,highest,A darker subject in subpar lighting will be a lot less defined than a whiter subject in the same lighting.,4
post5hb,richly branching,1.5948230389474962,highest,"This is the correct answer. Most data sets are heavily biased towards white and Asian.

https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html

There are some extremely dark skin tones - like in parts of the Congos - where contrast is challenging, but there often is enough detail in most situations.

Even in perfect situations, the models suffer with black faces.

Put it in a different way: In order to train a face recognition model, you have to show it pairs of pictures of faces that are either different or of the sanm person. Repeat this millions of times and it learns what distinguishes different faces.

Different ethnicities have different distinguishing features. If your data set is low on a particular ethnicity, your model will not learn how to tell people of that ethnicity apart.

As we model how the human brain works* - or how we think the human brain works - the learning process for our models is similar to that of humans. Most people struggle to distinguish between two individuals of other ethnicities if they have not been exposed to many people of that ethnicity. 

Source : I wrote a facial recognition system in South Africa specific for their ethnicities. It absolutely destroyed all commercial and open source alternatives.

*A gross over simplification, but not too far from the truth.",4
post5hb,richly branching,1.5948230389474962,highest,So racist.,3
post5hb,richly branching,1.5948230389474962,highest,"Ok, but unless I'm misunderstanding the process, they don't just run the footage through a computer and show the judge a receipt saying that their facial recognition has judged someone guilty.

It probably spits out several dozen different suspects that they can then manually narrow down by eye.

As far as I can tell, regardless of the software's quality, it can only ever save time.

I suppose if the officer collating the results thinks all black people look the same, there could be some gunghoe arrests as a result of the software combining their profile, but those people probably shouldn't be identifying suspects anyway if that's the case.",2
post5hb,richly branching,1.5948230389474962,highest,Black and brown people are already over policed and over arrested as a result. This will only compound the systemic issues that already exist.,3
post5hb,richly branching,1.5948230389474962,highest,[deleted],3
post5hb,richly branching,1.5948230389474962,highest,"Yeah, admittedly intelligence services haven't instilled a huge amount of confidence in me this year as to their ability to use their powers responsibly.",4
post5hb,richly branching,1.5948230389474962,highest,Bold of you to assume that the officer will actively work to better the situation...,4
post5hb,richly branching,1.5948230389474962,highest,"This says a lot about the developers of the technology. As anyone in a biracial couple knows, black and white people need different light exposure levels in photos to show up correctly, especially in areas of high natural light. So the fact that it's fine with white people but is iffy with non-white people... Well i guess that tells you a lot about who developed the software.",2
post5hb,richly branching,1.5948230389474962,highest,Exactly. Bigots and racists. Everyone who worked on this tech in any capacity. This should all be shut down and restarted from step 0 with a team representative of the global population who are sensitive enough to adapt the functionality to account for all permutations of human melanin distribution and light sources.,3
post5hb,richly branching,1.5948230389474962,highest,"It's more likely that they grey-scaled the images for the model as that's a very common first step in convolutional neural networks.

Your tl;dr isn't fantastical, just because something isn't perfect doesn't mean it isn't useful. Using this to reduce the pool of images cops have to look at to identify someone they have a photo of from thousands to just a few is a very, very useful tool. As long as it's still a person making the final determination, all this does is streamline the process.

Machine learning can't replace people when it comes to such significant decisions, I'm not sure it ever will, but it surely can make people's jobs easier. No one was saying we needed to throw away cars when they were invented just because they needed a human's oversight.",2
post5hb,richly branching,1.5948230389474962,highest,"It's clearly a combination of black people being harder to photograph, and being featured less in training datasets. Nobody involved today is being deliberately racist. It's just if you train your face recognition software with images of celebrities (pretty common) it's not going to see a lot of black people.",2
post5hb,richly branching,1.5948230389474962,highest,"Typical, us Asians get left out of everything. 


(Save your Wuhan virus jokes, I'm Korean)",1
post5hb,richly branching,1.5948230389474962,highest,Damm i was already halfway done,2
post5hb,richly branching,1.5948230389474962,highest,"You gotta Wuhan'd it to him.

(Sorry, it was the best I could do with the material.)",3
post5hb,richly branching,1.5948230389474962,highest,Eh china supposedly has an accurate face matching system already designed for asian people.  Granted it is only right about 10% of the time but that doesn't matter to China.,2
post5hb,richly branching,1.5948230389474962,highest,"That's literally what I'm talking about. 
Facial recognition doesn't work on ""black and brown people"" and us Asians are getting ignored again because It also doesn't work on us.

Or I'm wooshing and that's the whole point of your statement because of the whole ""works 10% of the time"" bit. But Chinese only makes up the majority of us. Not all. It's hard to figure out. Which way you're leaning. 

No facial recognition by robot overlords without representation.",3
post5hb,richly branching,1.5948230389474962,highest,">Or I'm wooshing and that's the whole point of your statement because of the whole ""works 10% of the time"" bit. But Chinese only makes up the majority of us. Not all. It's hard to figure out. Which way you're leaning.

Can't tell if chinese or christopher walken",4
post5hb,richly branching,1.5948230389474962,highest,불쌍해ㅠㅠ,2
post5hb,richly branching,1.5948230389474962,highest,"This is an area where false positive vs false negative matter a ton. If software is making false positive matches, that's a much bigger problem in surveillance technology than false negatives in terms of privacy and legal prosecution.",1
post5hb,richly branching,1.5948230389474962,highest,"Most of the problems are direct conflations - which are irrelevant unless they're conflations with someone of interest in this case. Speaking statistically, if a majority of your population are not persons of interest, and your decision making tool is even a little inaccurate, the effect is magnified IMMENSELY by the number of false positives.

Tl;dr: https://en.m.wikipedia.org/wiki/Base_rate_fallacy

Ts;wrm:

Let's say I have a test that's 99% accurate to detect a condition that occurs in 1 in 1000 people, and I use it on 1000 people. For a generous and simple example, Of that 1% error, half are false positives.

I will find 5 people that I think fit my criteria and don't, and 1 person that correctly fits my criteria... making my 99% accuracy test get 84% of its cases dead wrong for my purpose",2
post5hb,richly branching,1.5948230389474962,highest,"> making my 99% accuracy test get 84% of its cases dead wrong for my purpose

Which is in fact amazingly helpful if what you are doing is selecting candidates for in-depth testing. Now you have a 99% chance of catching all cases by only examining one in every two hundred people.",3
post5hb,richly branching,1.5948230389474962,highest,"So long as you RECOGNIZE THIS and don't assume guilt based on your primary selection... which I don't trust them not to do based on their blind trust of technology in the past.

It also leads in this example to detaining five perfectly innocent people for every legitimate person of interest you find... and that's with a 99% accuracy rate, which these technologies have on average for white men, and not the 66% that I've heard quoted for black women, where it's in the hundreds!

Being detained by the police for questioning or even just to run your ID can be incredibly stressful and a great way to fuck up your day. Now imagine the FURTHER disproportionate impact on poorer black people who may not have a driver's license or passport, and have to rely on ID that the police are less familiar with...",4
post5hb,richly branching,1.5948230389474962,highest,"Optimally, a system with a heavy false positive rate is giving investigators a group of potential matches, which they can then follow up with.",2
post5hb,richly branching,1.5948230389474962,highest,"And this is where the technology will likely be used discriminatorily; if the software flags five ""potential matches"" for each darker-skinned individual they're searching for, but only two for a lighter-skinned one, then that gives police cause to search, detain, and register all five darker-skinned people.",3
post5hb,richly branching,1.5948230389474962,highest,"But convictions aren't being made with this technology. This is surely being used to speed up officers looking for suspects. It doesn't particularly matter if the software picks up the wrong person buying milk in the supermarket. The officer is going to check who the AI has flagged, figure it's a false positive and move on. The officer was likely going to have to trawl through the footage without the AIs help and there's a good chance that he'd have paused on the same false positive buying his milk. 

Am I misunderstanding how they'd use this stuff?",2
post5hb,richly branching,1.5948230389474962,highest,"still depends. If a camera sees 1000 people a day, has a 10% false positive rate, then the officer is getting 100 photos from a camera that may have nothing to do with the crime. The criminal could never have set foot near the camera, but the camera is still reading out matches. It can create noise if the false positive rate is too high.",3
post5hb,richly branching,1.5948230389474962,highest,[deleted],4
post5hb,richly branching,1.5948230389474962,highest,"They do use it for this, but if you read the artical anyone who gets flagged is searched by police and has their biometric data (DNA, fingerprints etc..) taken and put in this data base,

being on this makes it harder to get social help and can Interfere with the process of going to university as (I think, don't quote me on this) it flags you as a suspect of some kind or as a suspicious person something like that.

Given that it flags black people wrong 100x more often than white people and you start to see the issue",3
post5hb,richly branching,1.5948230389474962,highest,[deleted],3
post5hb,richly branching,1.5948230389474962,highest,"Wow, you're a bit of an arsehole with a persecution complex. Have you considered the fact that most cops aren't doing their jobs just to ruin the lives of coloured people? I'm not suggesting we all adopt strict police states, I'm just talking about how this technology might still be useful.",4
post5hb,richly branching,1.5948230389474962,highest,Hot dog... Not Hot dog...,1
post5hb,richly branching,1.5948230389474962,highest,I would say not safe for work... but this *is* your work!,2
post5hb,richly branching,1.5948230389474962,highest,Man I miss that show already.,3
post5hb,richly branching,1.5948230389474962,highest,Ah man likewise! Very keen for the finale for Better Call Saul this year though.,4
post5hb,richly branching,1.5948230389474962,highest,"Great, so we made AI and it's already racist.",1
post5hb,richly branching,1.5948230389474962,highest,"Garbage data in, garbage performance out. We have the same issue with recommend sentencing software - it was fed decades of racist data and then became a racist.

It's like what 4chan did to Taybot, except it's software developers doing it to a machine that advises judges.",2
post5hb,richly branching,1.5948230389474962,highest,"Yup, and ""intellectual property"" claims are keeping us from reviewing private software even though it's being used in very public places like courtrooms.",3
post5hb,richly branching,1.5948230389474962,highest,"Any software meant for sensitive public use should be open source. Scrutiny is necessary to ensure not only fairness, but security as well.",4
post5hb,richly branching,1.5948230389474962,highest,"Interestingly taybot become racist, but cleverbot became a depressed teenage girl...",3
post5hb,richly branching,1.5948230389474962,highest,"This isn’t a GIGO problem.  The problem is facial recognition is based on light, and darker skinned people reflect less of it so it can’t recognize features. It’s nothing like Tay.",3
post5hb,richly branching,1.5948230389474962,highest,"I don't think it's that, otherwise the software would struggle just as much with pale people in brightly lit environments, women wearing certain makeup etc. 

Computer systems arent as limited as human eyesight because it can digitally adjust the brightness and contrast and measure frequencies of light that human eyes can't. So there's no reason it shouldn't be possible to make a system that can differentiate people regardless of skin tone.

But ultimately, if a system can't differentiate between a significant proportion of the population, for whatever reason, it shouldn't be used.",4
post5hb,richly branching,1.5948230389474962,highest,"I don't think it's a garbage data problem the same way your example is. Your example is using data influenced by a biased system, but generally for facial recognition software, it's bad at identifying people of color because it's harder to identify points of interest on pictures of people with darker skin. One is the ripple of racial bias in the justice system, and the other is a technological limitation.",3
post5hb,richly branching,1.5948230389474962,highest,"No, that's not how any of this works at all. It's a problem with insufficient training data.",3
post5hb,richly branching,1.5948230389474962,highest,AKA garbage data in?,4
post5hb,richly branching,1.5948230389474962,highest,"A facial recognition neutral net is trained on one criterion alone: correct identification of the subject. As such, there's really no way to ""feed it racist data.""",3
post5hb,richly branching,1.5948230389474962,highest,Why does a sentencing AI take race into account anyhow?,3
post5hb,richly branching,1.5948230389474962,highest,"Purely the way it was trained. Machine learning takes data that we already have and uses it to learn to make decisions on new data. Even if it is explicitly told not to look at race or sex data, it can often learn racism or sexism by inferring it from the name or neighborhood they live in.",4
post5hb,richly branching,1.5948230389474962,highest,i had a lecture on this; ill pull up a journal article related to the lecture but the essence was an attempt to work with the tainted data they had to normalize conviction rates across the ethnicities. that probably makes it sound worse than it is; the lecture was some time ago. the article was called 'the bias detectives',4
post5hb,richly branching,1.5948230389474962,highest,It doesn't.  People are just ignorant and jump to conclusions based on inconsistent data and race bating headlines.,4
post5hb,richly branching,1.5948230389474962,highest,"Know why the big tech firms stopped using AI hiring software? It ended up being even more sexist than the sexist managers they were trying to replace. 

https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine",2
post5hb,richly branching,1.5948230389474962,highest,"Makes sense.  Computers just look for patterns.  If historically successful managers have traits that are more common with men, like say playing football, it's going to correlate success with those attributes.  If historically successful managers tended to crochet, the computer would be sexist for women.  The computer doesn't care that the reason more football players than crocheters were successful is because more men were historically chosen to be managers.",3
post5hb,richly branching,1.5948230389474962,highest,"If you read the article you see it's just that the ai is neutral, so reinforces our own preexisting biases.

It's like the Rooney rule - if they program the ai to favor women, it'll work but be unfair. It they leave the ai alone it will reflect our own cultural biases. So the solution is to ditch the ai and let human managers pretend to be neutral in hiring and everything goes on as normal.",3
post5hb,richly branching,1.5948230389474962,highest,"Getting hired at Amazon was so easy they literally called me the same day, no interview. They just took a bunch of us into a room, gave us this swab drug test thing for weed, then we watched videos and left and started like 2 days after.",3
post5hb,richly branching,1.5948230389474962,highest,"You are probably talking about a warehouse job, whereas the faulty hiring software was used for office jobs.",4
post5hb,richly branching,1.5948230389474962,highest,I went to an interview for a chicken processing plant that was like this. It was basically “are you breathing” and you got an offer. They hid the fact that it was night shift until the end when they gave you the offer. Wouldn’t have wasted my time going if I’d known.,4
post5hb,richly branching,1.5948230389474962,highest,Yup. All it did was automate the existing biases.,3
post5hb,richly branching,1.5948230389474962,highest,"One thing the article didn't seem to address Is the chance that the data they wanted wasnt based on sexism but women more often didn't meet their qualified data. 

Even if the algorithm is certified non sexist, its 100% guaranteed that there will be a uneven amount of qualified applicants because that is the way probability works.

Regardless I'd say machine learning as a whole is too flawed for this because it trains itself based on the always flawed data given by human beings. It then requires more human intervention in the form of fixing these data issues/learning results in some way. To me that's counterproductive to it's own purpose.",4
post5hb,richly branching,1.5948230389474962,highest,software can't be sexist. This just means people don't like the result.,3
post5hb,richly branching,1.5948230389474962,highest,[deleted],4
post5hb,richly branching,1.5948230389474962,highest,"AIs are exactly as prejudiced as their data set. 

It's why you have to be supremely fucking careful with what data you feed it to train the AI. 

Like if you're training an AI on how to detect cancer, and you feed it a shitload of CT scans, if the data isn't well scrubbed, there's a good chance that the AI will figure out that the location the scan is from (like whether it's from a hospital that specializing in treating cancers instead of a university studying CT scanners) is very important in the likelihood that a tumor is present. 

Or if it's told to look for candidates that straight shooters with upper management written all over them, it's likely to notice that there's a good correlation with being a white male between 30 and 35 with a western European surname. 

AIs can be racist as all fuck. Figuring out ways to make them not be just reflections of ourselves is currently under study. And it's proving pretty hard to subtract ourselves and our own biases from the output.",4
post5hb,richly branching,1.5948230389474962,highest,"No. The software can't. The software is simply executing what its creators codified. However if the original architect's thinking is subconsciously biased, those same biases get codified into the algorithms. This is a major problem with AI and other software algorithms in general. Do you seriously not understand this?",4
post5hb,richly branching,1.5948230389474962,highest,"Seriously, this thread is a trip. 

""We programmed a software to hire the most qualified people. Why isn't it giving us a perfect cross-section of the overall demographic?""

There's no reason to believe people are equally suited for a given position across sex, race, sexual orientation, etc.",4
post5hb,richly branching,1.5948230389474962,highest,"I don't know why people are surprised. 

If you input data into an algorithm from a time where you practised sexism in choosing candidates, that will reflect in the algorithm.

Not unless they base it on absolutely nothing in the past and just set it to random until there's a generation that just ""happens"" to pick good diverse people.


We should just make humans more nicer and leave them responsible for interviewing humans.

But in reality we should just stop working together and transition to a ubi.",3
post5hb,richly branching,1.5948230389474962,highest,The only reason the AI was more sexist is because it did not give women preferential treatment though.  It weighed the candidates equally rather than making sure to give more weight to female job seekers.,3
post5hb,richly branching,1.5948230389474962,highest,Well irl we have affirmative action programs to pick people based on immutable characteristics. Switching strait to true meritocracy obviously isn't going to give the same results. The problem with AI was that it was neutral.,3
post5hb,richly branching,1.5948230389474962,highest,"That's not what racist means. It's unable to tell the difference because sampling is limited, not because it thinks they're inferior",2
post5hb,richly branching,1.5948230389474962,highest,"People not understanding that cameras do well with light. And white faces just happen to be easier to discern for cameras. I remember it was a big controversy when the 3DS came out. I believe the auto Mii maker couldn't recognize black faces. 

It's just a symptom of the technology.",3
post5hb,richly branching,1.5948230389474962,highest,You mean a Japanese created thing didn't test it for black people?,4
post5hb,richly branching,1.5948230389474962,highest,Symptom of cheap technology. Cameras using IR sensors and high dynamic ranges don’t really suffer from this at all.,4
post5hb,richly branching,1.5948230389474962,highest,Yet they are trying to use that technology that does not work against minorities which is why it's racist. It's basically racial profiling with software. They know it does not work but are still using it.,4
post5hb,richly branching,1.5948230389474962,highest,Akin to how voice recognition works better with deep male voices,4
post5hb,richly branching,1.5948230389474962,highest,"... gee no kidding. 

If a system, however, has a bias and triggers false positives for the police when dealing with black and brown people it’s fucking racist. 

Just because the AI isn’t hurling slurs and defending phrenology that doesn’t mean it’s not racist.",3
post5hb,richly branching,1.5948230389474962,highest,Still not racist tho,4
post5hb,richly branching,1.5948230389474962,highest,[deleted],3
post5hb,richly branching,1.5948230389474962,highest,You’re 100% right about this. I showed this to a friend of mine who is a criminal defense attorney who said he’d have a field day in court attacking the deficiencies in this software if it were ever used as part of a prosecution of someone black/brown. White person though? Better come up with something other than mistaken identity,4
post5hb,richly branching,1.5948230389474962,highest,"Higher probability of a false positive with a black or brown person though. It's pretty much guaranteed there will be slip-ups, and many cops aren't too discerning about who they bring in as long as they have somebody to arrest.",4
post5hb,richly branching,1.5948230389474962,highest,"Ah yes, being more accurate, and not having false positives is totally worse then it being inaccurate and misidentifying you for random searches...

 Or do you think the cops will just go *""Well, there's 5 possible black guys that might have done it, lets not bring them all in because it's not accurate enough and we wouldn't want to disturb them!""*",4
post5hb,richly branching,1.5948230389474962,highest,It means you're *more likely* to be matched if you're not white. It thinks nonwhite faces all look the same.,4
post5hb,richly branching,1.5948230389474962,highest,"Obviously an algorithm cannot be racist, but it leads to a racist outcome.

You’re being pedantic for no reason.",3
post5hb,richly branching,1.5948230389474962,highest,"Yeah, how about no?",4
post5hb,richly branching,1.5948230389474962,highest,Can you be any more clueless?,3
post5hb,richly branching,1.5948230389474962,highest,what do you mean,4
post5hb,richly branching,1.5948230389474962,highest,"Yes, the machine itself is not racist, but the way it's being used *makes it* extremely racist",3
post5hb,richly branching,1.5948230389474962,highest,Cool story. The comment I replied to specifically said AI. Don't move the goal post,4
post5hb,richly branching,1.5948230389474962,highest,It's racist because it's being programmed by racist people...,3
post5hb,richly branching,1.5948230389474962,highest,And you know the devs are racist how? Yeah...exactly,4
post5hb,richly branching,1.5948230389474962,highest,"It’s systemic racism. Anything that would be different if everyone was black is systemic racism. Cameras don’t recognize black people very well? Not a good camera then. Instead we have people saying things like “it recognizes **most** people, isn’t that good enough?” It’s not, because it doesn’t recognize people incorrectly purely at random, it is biased to incorrectly select more black and brown people for further investigations than white people. Since the justice system is not perfect and makes mistakes and convicts people incorrectly, it means more black and brown people would be convicted of crimes they did not commit",3
post5hb,richly branching,1.5948230389474962,highest,Yeah AI prejudice from bad/incomplete data feeds is a big concern.,2
post5hb,richly branching,1.5948230389474962,highest,I don't think that's the problem...,3
post5hb,richly branching,1.5948230389474962,highest,"That's literally a problem that AI scientists are worried about. Even if it's not intentional, the intelligence programs can create their own bias/prejudices bc of incomplete data. If you show machine learning a bunch of pictures of people partying..but no one is wearing a yellow shirt in any of them...they will think anywhere a yellow shirt shows up cant be a party. It's the same concept that can make it create it's own judgements we arent ready to anticipate",4
post5hb,richly branching,1.5948230389474962,highest,[Guessing you haven't heard of Tay AI](https://youtu.be/HsLup7yy-6I),2
post5hb,richly branching,1.5948230389474962,highest,"Remember the connect couldnt register black ppl.

Edit: I thought this was totally a thing when it came out. Sensors reading light from your face couldnt register dark skinned people.",2
post5hb,richly branching,1.5948230389474962,highest,"That was ""Kinect"" and it did register black people. The issue was lighting during the press interviews not being sufficient for the cameras to find the player consistently.

Source: Was on the team, am black, and played a LOT of Dance Central.",3
post5hb,richly branching,1.5948230389474962,highest,"It’s weird we call black people black, when there are some really dark Indians , Pakistanis, Bangladeshi and Mexicans and etc",4
post5hb,richly branching,1.5948230389474962,highest,"I know this is a joke, but I'm curious as to why this is. I suspect no malicious intent. Likely cheap camera sensors (as I would assume common security cameras aren't overly concerned about image quality) have poor dynamic range, details and shadows would be harder to discern the darker the skin tone. It's also possible that the training data has a disproportionately low amount of data with commonly african facial features, so it might just not know that nuances exist.

You know the whole 'dont attribute malice to what can be explained though incompetence'",2
post5hb,richly branching,1.5948230389474962,highest,Read *The Ethical Algorithm* -- I personally haven't myself but I've listened to an interview by the author and it s delves into all your points and questions.,3
post5hb,richly branching,1.5948230389474962,highest,YouTube has had bigot bots for a while,2
post5hb,richly branching,1.5948230389474962,highest,It had to fulfil the basic requirements of joining the police. So they baked it in.,2
post5hb,richly branching,1.5948230389474962,highest,"It's under fit because the data is incomplete. I don't understand why everyone think it's being intentionally racist, it's all about the training data...",2
post5hb,richly branching,1.5948230389474962,highest,[Racist Sensors](https://youtu.be/jqG1fX3ZaLQ),2
post5hb,richly branching,1.5948230389474962,highest,AI (specifically deep learning algorithms) inherits the bias of the people who train it intentional or not. There's a good book called Outnumbered which talks about it a bit.,2
post5hb,richly branching,1.5948230389474962,highest,"To be fair, the people who made it are super racist",2
post5hb,richly branching,1.5948230389474962,highest,it works for the police what did you expect,2
post5hb,richly branching,1.5948230389474962,highest,"It doesn't 'think' on its own; we just made a machine that recognizes the categories of things we optimized it to.  Garbage in, garbage out.",2
post5hb,richly branching,1.5948230389474962,highest,"Yep. Though not on its own. 

Fun fact, if you only *program* a machine to recognize facial features of white people... it will only identify faces of white people correctly. Without the proper data / programming, making sure to account for a diverse range of people in many nationalities, it wont work for *everyone*.",2
post5hb,richly branching,1.5948230389474962,highest,"Digital camera images suffer a huge loss of quality due poor lighting. I'm assuming most of their training set is user data (non professional) so it's probably not the best quality which is an issue. 

Trying to get high quality professional quality photos would be hard due to rights issues, even though the photographers work isn't being used in the traditional sense.

Finally, using a professional training set would probably be a bad predictor sense most photos it would try to identify and amateur and have terrible lighting.

So it's going to be a hard issue to resolve unless we make a huge advancement in digital imaging overnight.",2
post5hb,richly branching,1.5948230389474962,highest,how lol,2
post5hb,richly branching,1.5948230389474962,highest,[removed],2
post5hb,richly branching,1.5948230389474962,highest,Yes you complete idiot,3
post5hb,richly branching,1.5948230389474962,highest,It's the opposite of racist. It doesn't see color!,2
post5hb,richly branching,1.5948230389474962,highest,"The AI isn't racist. Most facial recognition AI works by comparing shadows in the T shaped bit of your face (eyes and nose area), however with darker skin, there's less contrast between shadowed and non-shadowed areas than it would be for an albino, so they have a bit of extra difficulty working there. It's an unfortunate side effect that kinda needs to be worked on, not denying that for a moment.",2
post5hb,richly branching,1.5948230389474962,highest,"You misunderstand racism in this situation though. It DOESNT differentiate, by that logic its not racist. If they added ""but can between white and black"" it would be racist.",2
post5hb,richly branching,1.5948230389474962,highest,"Great, we have Mr know it all here...",3
post5hb,richly branching,1.5948230389474962,highest,Well. We made it.,2
post5hb,richly branching,1.5948230389474962,highest,"Everybody is racist. Humans recognize patterns and extrapolate correlations from them. Our statistical approach with machine learning does exactly the same thing, but with even more simplistic models with far more data.

The idea that machine learned models would be racist shouldn’t be a surprise to anybody who knows how machine learning works. It’s heuristics, not logic.",2
post5hb,richly branching,1.5948230389474962,highest,"Reminder that insane clown posse make-up patterns also fucking destroy facial recognition tech.

Juggalos are ahead of their time",1
post5hb,richly branching,1.5948230389474962,highest,"And there are only 4 types of Juggalos, skinny male, fat male, skinny female, and fat female. So if you're going to rob someone, put your paint on fam. Whoop whoop.",2
post5hb,richly branching,1.5948230389474962,highest,[deleted],3
post5hb,richly branching,1.5948230389474962,highest,[deleted],4
post5hb,richly branching,1.5948230389474962,highest,"Yeah lol but it does take some more elaboration if you're not familiar, as a Juggalo Expert, Skinny Juggalo's and Fat Juggalos don't dress the same. Older Juggalos tend to stlll wear old hot topic gear (armbands/belts) in both groups and blend with a more ""gothic"" theme.

For the Male of the Species, Fat Juggalos tend to either shave their head or have short spiky hair. They tend to wear long shorts and Merch shirts. Skinny Male juggalos tend to wear faux dreadlocks/braids or have medium to long hair, and wear baggy pants, Jerseys and zip up hoodies. Both tend to have some form of goatee.
Females also tend to dress similar based on weight, fat female juggalos tend to wear large baggy pants and merch shirts, while always having an unwashed ponytail, most of the time with faded dyed streaks. Skinny female juggalos tend to dress like ravers with some type of dyed pigtails with yarn extensions or full on dreadlocks.",4
post5hb,richly branching,1.5948230389474962,highest,"Don't worry, human police can't tell them apart either.",1
post5hb,richly branching,1.5948230389474962,highest,[Patriots player Duron Harmon is speaking out after Amazon's facial-recognition tech falsely matched him to a mugshot](https://www.businessinsider.com/amazon-facial-recognition-falsely-matched-nfl-players-duron-harmon-mugshots-2019-10?fbclid=IwAR00WMgCuitM9WOvl744q78cbRufdR2Jb7bGkDgVtdf27lgBBrpsO7X9-6Q),1
post5hb,richly branching,1.5948230389474962,highest,"""he was flagged by the facial recognition system"" 

It's the new ""he matched the description of a black guy between 4'2"" and 6'8"" who may or may not have been wearing a hat.""",1
post5hb,richly branching,1.5948230389474962,highest,"""Witnesses also reported he looked sketchy"".",2
post5hb,richly branching,1.5948230389474962,highest,"It's bad that it can't tell me apart from other black people. It's bad that it's being used by police when it can't tell me apart from the police, though",1
post5hb,richly branching,1.5948230389474962,highest,But it does work for white people... who are the majority of the people in the country(the UK).  It also works for brown and black people most of the time.  It is also not the end all identifier if it says match it doesn't means it is a match no ifs and or buts about it.,2
post5hb,richly branching,1.5948230389474962,highest,US police also still use lying detectors even though it has been proven that they're not reliable,1
post5hb,richly branching,1.5948230389474962,highest,So... Just like the normal police,1
post5hb,richly branching,1.5948230389474962,highest,"""We build our police robots to be racist faster and more efficiently than human cops could dream of""",2
post5hb,richly branching,1.5948230389474962,highest,"Yeah, this doesn't seem as surprising as it should be.",2
post5hb,richly branching,1.5948230389474962,highest,Working as intended,3
post5hb,richly branching,1.5948230389474962,highest,"I was so sure this would be the top comment, but this is r/nottheonion so I guess it's to be expected that the obvious joke writes itself....",2
post5hb,richly branching,1.5948230389474962,highest,[deleted],1
post5hb,richly branching,1.5948230389474962,highest,Exactly.,2
post5hb,richly branching,1.5948230389474962,highest,"The author insists the software is racist vicariously through its creators' biases. From a software standpoint, the lack of proper recognition, in my belief, would stem from not being able to put a firm border on a color palette where the ethnic differences are. Lines are blurred from that aspect. Also, fully accurate depth and feature mapping is a ways off until it becomes  truly useful. the examples of inaccuracy are well-picked for the stance of the article. However, I would say from a software standpoint, the racial label is misused.",1
post5hb,richly branching,1.5948230389474962,highest,"My family is half black, half white. Done a lot of photo identification in the Apple Photos app. It is *way* worse at telling black people apart than white people.",1
post5hb,richly branching,1.5948230389474962,highest,"As someone who has been working on ai systems for the path year. If you are not a white male, I would not trust the accuracy of any machine vision system",1
post5hb,richly branching,1.5948230389474962,highest,"I'm a white male, but I'm dirt poor. I don't trust that machine either.",2
post5hb,richly branching,1.5948230389474962,highest,White males shouldn't trust it either.  They are generally just a larger data set without makeup to further influence results.  Larger data set means smaller percentage of failures.,2
post5hb,richly branching,1.5948230389474962,highest,"CLOSE ENOUGH FOR GOVERNMENT WORK

LOLLING AS IM EXECUTED BY THE POLICE FOR JAYWALKING IN A BROWN AS FUCK MANNER WHILE RESEMBLING SOMEONE WHO MAY HAVE COMMITED A CRIME SOMEWHERE AT SOME POINT IN TIME.",1
post5hb,richly branching,1.5948230389474962,highest,How do people construe facial recognition misidentifications as evidence of the system being racist? The word has lost all its meaning thanks to articles like this.,1
post5hb,richly branching,1.5948230389474962,highest,"No, it still means what it means. No one has to use facial recognition software that doesn't actually recognize faces. The people who work on this software could actually  test it to make sure it works before calling it finished. What's the functional difference between bias in the system and being apathetic about contributing to the bias?",2
post5hb,richly branching,1.5948230389474962,highest,"I agree with you. I just think it's at least worth mentioning the fact that facial recognition inherently needs more light for darker skin. To implement it when there isn't yet a 0% bias rate is racist, yes (and I wouldn't want it implemented then either).",3
post5hb,richly branching,1.5948230389474962,highest,"Two things:

A) Racial bias in algorithms is a very common phenomenon and needs to be addressed.

B) The police are still trying to use said algorithm EVEN THOUGH IT CAN'T TELL PEOPLE APART.",2
post5hb,richly branching,1.5948230389474962,highest,">A) Racial bias in algorithms is a very common phenomenon and needs to be addressed.

Imagine just making up bullshit because you want to look progressive and then having 25+ people agree with you all based on literally nothing. Does ""racial bias"" exist in algorithms? Sure. Is it ""very common""? Not even remotely. And like most racist conspiracies on reddit this dog-whistle is an the alt-right talking point about white people being inherently smarter than minorities since a machine says so. It's sad to see this racist bullshit upvoted but not exactly surprising",3
post5hb,richly branching,1.5948230389474962,highest,"Ok well.... this isn’t a hard thing to find out. I do have a computer science background, but this is doesn’t require a deep understanding of ML to get. Then there “25+ disagreeing with me”. That last half was just incoherent. How am I being racist? Buddy are you ok?",4
post5hb,richly branching,1.5948230389474962,highest,"Racial bias in algorithms is extremely uncommon.

Racial bias in ML training data sets is a lot more common though, often leading to biased models.",3
post5hb,richly branching,1.5948230389474962,highest,"It’s more common than you think. It isn’t always things as overt as a facial recognition program thinking all brown people look alike. And it usually isn’t intentional.

Data training sets for ai is one cause, but not the only one.",4
post5hb,richly branching,1.5948230389474962,highest,I agree and I am aware.,3
post5hb,richly branching,1.5948230389474962,highest,[removed],3
post5hb,richly branching,1.5948230389474962,highest,Uh..... how?,4
post5hb,richly branching,1.5948230389474962,highest,"It's not used as conclusive evidence, it just narrows down lists of suspects.",3
post5hb,richly branching,1.5948230389474962,highest,"“racial bias in algorithms” just tells me you know nothing about algorithms.

Pattern recognition systems (which aren’t actually algorithms) (that people mislabel AI) based on racist training data can be racist, but that same system based on non-racist data won’t be racist.",3
post5hb,richly branching,1.5948230389474962,highest,"You sound like you read the first chapter of a 15 year old book on java. 

AI isn’t just one thing. In this context, we mean machine learning algorithms. In others it could mean software that emulates human decision making.

And a pattern recognition system is without a doubt an algorithm. It is an extremely broad term.

Talk about proving you don’t know what you are talking about.",4
post5hb,richly branching,1.5948230389474962,highest,"Because the police, electing to use this flawed and biased system, will end up having more false positives among minorities with black or brown skin. 

As such, the net effect is a racist system. 

The meaning of the word is fully sound.",2
post5hb,richly branching,1.5948230389474962,highest,"I agree. It's the way in which the technology is used that is racist, not the technology itself alone (which the article seems to not clarify).",3
post5hb,richly branching,1.5948230389474962,highest,"> It's the way in which the technology is used that is racist, not the technology itself alone (which the article seems to not clarify).  

From the beginning of the article:  

> But there’s two factors that need screaming above all others when it comes to the debate surrounding facial recognition. 

> One: it’s racist. 

> Two: it doesn’t even work. 

> **Technology never exists in a vacuum.**  For now, humans are still responsible for the production of new digital systems; and that means they come into being with all the biases and fallibility of their creators baked right into their code.  

Jesus fuck.  

But yeah, keep circlejerking about ""omg smh people calling everyone racist these days are debasing the super-great English language (that I don't even bother to read).""",4
post5hb,richly branching,1.5948230389474962,highest,">will end up having more false positives among minorities with black or brown skin

Which means the system will be better at tracking down white people than those minorities.

Did you even consider that angle? And I'd even go as far as to assume you'd be even more angry if it was the other way round, if the system eg were best at tracking down black people. Police using tech thats only good at tracking down black people, imagine the screams about racism. But if its aimed at white people, then thats racist to non-white people too?

&#x200B;

And yes, that what makes it ridiculous that overly woke people - particuarly an american stream of politics - try to apply racism to anything, pretending its such a sensitive topic, but then cant even make a good judgement call on it.",3
post5hb,richly branching,1.5948230389474962,highest,"If they use facial recognition to incorrectly prosecute people of colour, or at the very least harass them and arrest them and waste their time because the facial recognition technology popped a false positive, then it’s harming black people. If it’s accurately recognising white criminals, doing the crime — then it’s working as intended. 

What the fuck were you even thinking here? It is accurate with white people, poor white criminals getting caught? 

It’s not ‘aimed’ at white people, it’s not being used to persecute or prosecute *just white people*, fucking obviously. 

Alright, I’ll break it down. 

* The system for facial recognition is being applied to the general public. All video and photo relevant to a crime, supposedly, will be subject to this technology to identify perpetrators.

1. For white people, it’s accurate and the criminal is caught.
2. For people of colour, they get false positives, bring innocent people down to the precinct, subject them to inquiry, and possibly say ‘we have data that proves you were there!’.

Let’s not get into what’s ‘harmful’: you’re seeing how the system is discriminating between race? Yes?

That’s what makes it racist.

From there, we can better argue as to why it’s more harmful to be incorrectly identified for crimes you didn’t commit...",4
post5hb,richly branching,1.5948230389474962,highest,"This entire post and thread are absolutely baffling

1.	This entire problem is a symptom of technological drawbacks where cameras can contrast objects easier against light backgrounds. Not systematic racism or “racist data”
2.	Given the above, even using the system would likely perpetuate criminal stereotypes against white people as they would be far easier to be recognized, not brown/black like everybody is implying.

y’all are dumb as hell",4
post5hb,richly branching,1.5948230389474962,highest,">Which means the system will be better at tracking down white people than those minorities.

This is assuming that the cops have the integrity to acknowledge the possibility of false positives and not use this in cases involving black or brown people. Realistically it's going to put innocent minorities in prison while the increased accuracy for white people means it will (correctly) exonerate innocent white people.",4
post5hb,richly branching,1.5948230389474962,highest,"So you’re telling me there are gonna be two faces that are identical in every way and the only difference between them is going to be skin color? Doesn’t seem likely.

Skin color isnt going to be a determining factor when recognizing faces.",3
post5hb,richly branching,1.5948230389474962,highest,[deleted],4
post5hb,richly branching,1.5948230389474962,highest,Because it's disproportionately inaccurate for nonwhite faces?,2
post5hb,richly branching,1.5948230389474962,highest,"It’s not racism, it’s physics. They’re based on light, and dark people reflect less. Really light skinned black people and pale mexicans are recognized as easily as white people.

Y’all are acting like photons are racist and proving OP’s point.",3
post5hb,richly branching,1.5948230389474962,highest,"Phenology was physical that didn't make it magically not racist. There's no physical reason that a camera can't pick up a black face just as well as a white one, it's a failure of the neural nets analyzing the image. You feed them a biased dataset and they cannot do anything but recreate that bias. You feed them disproportionate white faces and it will be disproportionately inaccurate on nonwhite faces.",4
post5hb,richly branching,1.5948230389474962,highest,[deleted],3
post5hb,richly branching,1.5948230389474962,highest,"If the AI was hugely disproportionately misidentifying white people, and this faulty AI was used by police to flag potential criminals, I have a feeling it would not be being used by the police right now.",4
post5hb,richly branching,1.5948230389474962,highest,"Well nobody seems comfortable calling the tool racist, nor the tool's makers, nor the tool's users, and yet from that combination some racism magically happens so I don't know what part of the pipeline you want to attribute it to but it's not there by accident.",4
post5hb,richly branching,1.5948230389474962,highest,"By the system, I mean the technology itself in isolation.",3
post5hb,richly branching,1.5948230389474962,highest,"It works differently for people of different races. Insofar as a *thing* can be racist, the algorithm is racist.",4
post5hb,richly branching,1.5948230389474962,highest,Confirmation bias.,2
post5hb,richly branching,1.5948230389474962,highest,"Racism means something different depending on who you ask, but if you ask me the intention behind this system is irrelevant. The fact that the effect is a problem specifically for people of some ethnic group/race means it's ""racist"".

So if the system they're using misidentifies black people really easily, and that results in more black people being arrested falsely and then released, you could say that it's racist. Even if you're giving them the benefit of the doubt, and saying it was an accident.

That's how a lot of people define racism. The reason is that you can't prove someone's intentions that easily, all that matters is the effect.",2
post5hb,richly branching,1.5948230389474962,highest,"Facial recognition systems can’t spot black people because they don’t reflect enough light to find their facial features. On a 2d contrast based image, especially if it’s low resolution or poorly lit their facial features simply don’t stand out enough to be detected.

Imagine if you saw four different black people on a grainy, low resolution security camera with bad lighting 20 feet from the camera. Could *you* tell who they are? Would that make you racist?

This isn’t racism and you’re just deluding the term by calling it racism. It’s a poorly designed system that we shouldn’t be using, but it’s not racist.",3
post5hb,richly branching,1.5948230389474962,highest,"It's like you didn't even read my comment.

If every person only had a grainy photo as an ID, and because of that I could only identify and let through people with light skin tones, that system would be considered racist.

Even if I had no malicious intent at all, the result of my actions and the system would be that people with dark skin tones are stopped because their id can't be verified, while only people with light skin tones are let through.

There are plenty of faulty systems like this in the world that disproportionately affect people that are poor, live in a certain area, or have a certain appearance. Whether it's intentional or not is irrelevant when determining if it's discriminatory.",4
post5hb,richly branching,1.5948230389474962,highest,"If a facial recognition system is significantly more likely to misidentify black people, then it's just bad design and unconscious bias by the people who fucked up when they were training the algorithm.

I a facial recognition system is so likely to misidentify black people that it's as bad as random chance *and the cops know this and insist on using it as evidence against black people anyway*, it's getting kinda racist.",2
post5hb,richly branching,1.5948230389474962,highest,"Because the system has to have input from developers on how to read the data it's receiving. If the developer is racist/can't tell black and brown people apart, then the system will have the same issues. The system in and of itself isn't conscious enough to be 'racist', no, but if it was programmed by people who are (and research shows that even the most loveral hippy dippy white person has some latent racist responses) the end result is the same.",2
post5hb,richly branching,1.5948230389474962,highest,"It seems more likely that misidentifications will occur more frequently with people who have darker skin simply because it's more difficult to identify facial features due to less light being reflected from their faces, therefore creating less data for the software to work with.",3
post5hb,richly branching,1.5948230389474962,highest,"The problem isn’t with lighting and instead with bias and limited information in the training sets. 

It’s the same with gender of equivalently light-skinned people as well. From the article below, light-skinned men are correctly categorized each time, whereas light-skinned women are incorrectly categorized  19% of the time. The numbers only increase for darker skinned individuals, and the gender gap exists there as well. 

https://www.theverge.com/2019/1/25/18197137/amazon-rekognition-facial-recognition-bias-race-gender",4
post5hb,richly branching,1.5948230389474962,highest,It's funny because this is obvious and clearly the right answer but everyone is gonna play dumb because they would rather it just be racist.,4
post5hb,richly branching,1.5948230389474962,highest,People that don’t work with the tech need to stop perpetuating this outdated factoid.,4
post5hb,richly branching,1.5948230389474962,highest,"Sure. And yet, here we are with smartphones that can identify you in a scarf and sunglasses",4
post5hb,richly branching,1.5948230389474962,highest,"This comment shows complete ignorance for even the basic methods of AI training, and because of this ignorance attribute malice where there is none.

If the system is trained with primarily Caucasian photos, it will have difficulty with nonCaucasian faces and vice versa.

This article exists because posters like this eat it up and beg for more because it reinforces their worldview that all white people are racist, they even admit it in the post.

>research shows that even the most loveral hippy dippy white person has some latent racist responses",3
post5hb,richly branching,1.5948230389474962,highest,">If the system is trained with primarily Caucasian photos, it will have difficulty with nonCaucasian faces and vice versa.

Besides that, I also wonder if the vice versa applies. It might well be that the current tech and cameras are just naturally worse at detecting non-white people, eg because black skin reflects less light.",4
post5hb,richly branching,1.5948230389474962,highest,So why don't they just train it with non-Caucasian faces more before implementing it to solve the problem?,4
post5hb,richly branching,1.5948230389474962,highest,"Thats not how it works at all. The System is training itself to identify whatever you want it to identify. You need good training data though. Lighter Skin will reflect more light so pictures of white people will be better training data than pictures of black people. 

If the system was programmed by the most ""racist"" people on this planet but would read faces in 3D via some kind of laser that perfectly catches every facet of a face no matter the lightning conditions then it would learn to identify black faces just as well as white faces.

 Black faces don't produce good training data. That's all.",3
post5hb,richly branching,1.5948230389474962,highest,"Victim mentality, although I do think facial recognition technology should be getting any better, nor start being applied to the general populace,",2
post5hb,richly branching,1.5948230389474962,highest,"We talk about this problem in such an idiotic way. We focus on present day imperfections when facial recognition could be close to perfect in the next 5-10 years. We need to be talking about the implications of perfected facial recognition, not focusing on the correctable flaws in its early stages.",1
post5hb,richly branching,1.5948230389474962,highest,"i mean, they are using it now, so obviously we are focusing on the imperfections now and how they impact people now. 

for you to say we shouldn't talk about it's present imperfections because it will be perfect in X years is idiotic.",2
post5hb,richly branching,1.5948230389474962,highest,[deleted],3
post5hb,richly branching,1.5948230389474962,highest,"And any country is only ever really 1 change in administration away from that...

But even beyond that, here in the US and EU, where we ""aren't authoritarian states"" like China, Russia, etc... This unusable technology is currently being used.

This trajectory is inherently unsustainable. And it certainly seems like we won't learn our lesson, as a country, until lots and lots of people suffer.",4
post5hb,richly branching,1.5948230389474962,highest,"Right. We're opposing the tech for the wrong reason - one that's fixable. We should oppose it because it's a horrific invasion of privacy, a huge govt overreach, and morally wrong.",4
post5hb,richly branching,1.5948230389474962,highest,"Facial recognition is a tool, just like DNA, fingerprints, eyewitnesses, etc.

Any of those tools, if grossly misused, could potentially lead to mistaken identity arrests.  But the more tools police have available, the more likely they are to identify the correct suspect.  Facial recognition may never be reliable enough to justify automatic arrests, but it's almost certainly going to be better than artist sketches and subjectively interpreted security camera footage.

The solution needs to be responsible oversight, not paranoid bans on new technology.  We *want* our police to be able to make faster, more accurate arrests.  That's a good thing for society.",4
post5hb,richly branching,1.5948230389474962,highest,"I think you misunderstand.

He's warning of the danger it poses when ""perfected"" because that will be far more than it poses now, and saying we need to actually fight it now instead of pointing out the current technological flaws.",3
post5hb,richly branching,1.5948230389474962,highest,"ah, rereading it i can see it being stated in that way - thanks.",4
post5hb,richly branching,1.5948230389474962,highest,"Facial recognition sounds cool, but is a HUGE threat to our individual freedom. Bernie Sanders is the only candidate who has come out publicly against its use by police and the government.",2
post5hb,richly branching,1.5948230389474962,highest,"If you get thrown in jail for no good reason, would you focus on the present day injustice, or would you wait 5-10 years to see if the situation improves? Would you be taking about the implications of a perfected justice system, or the correctable flaws in the actual system?",2
post5hb,richly branching,1.5948230389474962,highest,"What?  It's in use now and it doesn't work.  People are actively getting fucked by it... and the fucking is disproportionate along racial lines. 

Why should we be bothered with the hypothetical perfect system at this time? We should be ending it's use so that they never use any system on the public.",2
post5hb,richly branching,1.5948230389474962,highest,Did I hear an echo or was that you just repeating the other comment?,3
post5hb,richly branching,1.5948230389474962,highest,I mean...   I read and responded to a single comment.,4
post5hb,richly branching,1.5948230389474962,highest,"Facial recognition honestly just sounds fucking dystopian to me, I really hope it gets banned",2
post5hb,richly branching,1.5948230389474962,highest,"“We’re talking about this all wrong.  This nuclear reactor might explode 50 percent of the time right now while we’re actively using it, but in the future when we theoretically perfect it, it will explode zero percent of the time!”


If we’re using imperfect technology right now in practical ways, we need to address it, rather than speculate about a time where it might not make any more mistakes.  People could be arrested for crimes they didn’t commit RIGHT NOW, who the hell cares about 5-10 years later?",2
post5hb,richly branching,1.5948230389474962,highest,"Op is suggesting precisely that, and more.

Ten years down the line, when these minor bugs are fixed, what kind of world do we live in? Do we want that?

Presumably, you don't.

You just didn't understand that that is exactly what op is talking about.",3
post5hb,richly branching,1.5948230389474962,highest,"Because it's a feature, not a bug.",1
post5hb,richly branching,1.5948230389474962,highest,"Lol that's what I was thinking too. Can't tell any sort of coloured people apart, that's the point of the software.",2
post5hb,richly branching,1.5948230389474962,highest,"I have a problem with this article. It says the technology is racist. No, technology is not racist but the people using it can be. 


And just because it has problems distinguish  between black and brown people doesn't mean it cant be properly utilised. For example at this stage it would be wrong to use it to say ""yep that's are guy let's arrest him"" but I don't see why it couldn't be used to say ""are criminal is one of the people in this crowd of 100 people, but our face recognition is matching with 12 people. I guess we can rule out those 88 others for now.""",1
post5hb,richly branching,1.5948230389474962,highest,"Actually, I would AI *can* be racist depending on the data set and what it’s being used for, but I agree this article is *not* an example of that. Not sure what the other guy is on about.",2
post5hb,richly branching,1.5948230389474962,highest,"If you use it like that, and the system has a non-zero false negative rate, the criminal could be one of the 88 people you just ruled out.",2
post5hb,richly branching,1.5948230389474962,highest,[removed],2
post5hb,richly branching,1.5948230389474962,highest,I wouldn't personally trust the police to use it properly. But if the police are going to use ineffective AI as their primary reason to arrest someone they are going to be in for a bad time once the suspect goes to court. Any decent lawyer will be able to defend against such nonsense and make the police look incompetent.,3
post5hb,richly branching,1.5948230389474962,highest,"The court isn't as good as you think, dna evidence is shit, but it still gets people convicted all the time. More importantly, a ""good lawyer"" who could get it thrown out costs lots of money, so I guess rich black people will be fine and poor black people can get fucked.",4
post5hb,richly branching,1.5948230389474962,highest,"Does anyone know what resolution the cctv records at? I'm curious if the inaccuracy in dark skin is related to difficulties in captiring contrast of features rather than inherent racism from programmers? Im also curious if there is a greater general inaccuracy reading woman of any color because of how different a woman can look from day to day (hair, makeup...etc).",1
post5hb,richly branching,1.5948230389474962,highest,"Veridian Dynamics, leading the way in facial recognition technology.",1
post5hb,richly branching,1.5948230389474962,highest,"Because facial recognition is based on contrast and distinguishing features based on contrast (because 3d facial recognition requires at least two cameras). So darker skinned people are harder to recognize because really dark skin in average lighting conditions is about the same as nose holes, or eyes. Poor lighting makes this waaaay worse.",1
post5hb,richly branching,1.5948230389474962,highest,Which police are using this?,1
post5hb,richly branching,1.5948230389474962,highest,NYPD off the top of my head,2
post5hb,richly branching,1.5948230389474962,highest,Do they have special cameras. I didn’t realize these were legal. In the US.,3
post5hb,richly branching,1.5948230389474962,highest,"After 9/11, downtown has a bunch of cameras all over. Snowden leaked some more info on more government surveillance efforts. We also have high altitude plane/drones that can circle above a City with lots of capacity.. they are even [testing balloons](https://www.theguardian.com/us-news/2019/aug/02/pentagon-balloons-surveillance-midwest) now. Yep for inside the USA...  

Defend your privacy or slowly it gets chipped away.",4
post5hb,richly branching,1.5948230389474962,highest,Didn't the TV show Better Off Ted do an episode like this?,1
post5hb,richly branching,1.5948230389474962,highest,But real humans are persecuted for thinking they all look the same...... science!! Now even computers cant tell the difference,1
post5hb,richly branching,1.5948230389474962,highest,"As a non-american that doesn’t rely on pseudo-scientific racist methods for classifying people, i have no idea what “brown” and “black” even mean.

It is like having a discussion with a child.",1
post5hb,richly branching,1.5948230389474962,highest,Those are colors.,2
post5hb,richly branching,1.5948230389474962,highest,Not in this context,3
post5hb,richly branching,1.5948230389474962,highest,But it's literally the skin color that messes up the software. That is the context.,4
post5hb,richly branching,1.5948230389474962,highest,"Feature, not a bug.  

Shoddy facial recognition means that a cop can pick out anyone they deem ""suspicious"" and immediately obtain probably cause to approach, search, and possibly arrest them.",1
post5hb,richly branching,1.5948230389474962,highest,"So, what exactly sets black and brown people apart? Exactly what parameters, that wouldn't be considered ""racist"", could programmers use to identify a black person from a brown person?",1
post5hb,richly branching,1.5948230389474962,highest,"They're not trying to distinguish between black and brown, they're trying to correctly identify specific people. It just so happens that the software has difficulty correctly identifying people with darker skin. It's not remotely racist. But these days everyone has to start pissing fire every time they hear something that could kinda sorta be considered an adverse outcome to people with excess melanin.",2
post5hb,richly branching,1.5948230389474962,highest,Except for the fact that there does appear to be bias in the development as [this commenter points out.](https://www.reddit.com/r/nottheonion/comments/eww0bt/facial_recognition_cant_tell_black_and_brown/fg58dzx) There is a reason it has trouble with faces that aren't caucasian men.,3
post5hb,richly branching,1.5948230389474962,highest,It’s racist if the police use facial identity software that can’t correctly identify people with darker skin. How is that not racist?  It’s not a secret that it can’t identify darker people correctly.,3
post5hb,richly branching,1.5948230389474962,highest,"It's not racist unless it's used as evidence. As long as police use this stuff as an investigative tool, different probabilities based on subject simply have to be taken into account.

Further, the real discussion here should be about this kind of technology being used against people of any race.

Light skin, dark skin, whatever. I don't care if the error rate is 30 percent or 40 or 60, I don't think that kind of stuff should be proliferated at all. It is a violation of the Commons",4
post5hb,richly branching,1.5948230389474962,highest,The software itself isn't racist.  But the people who knowingly  released it are.  Either because they didn't care enough to stop the problem or because it was intentional.  The police are because they also don't have an issue with brown people  going to jail because of AI.  You need to realize that racism doesn't always mean cross burnings and lynching.  It can easily present as apathy or ignorance.,3
post5hb,richly branching,1.5948230389474962,highest,The software likely makes no guarantees.  I'd blame the people using it.,4
post5hb,richly branching,1.5948230389474962,highest,"No technology is racist in and of itself. As other have replied, it becomes racist in it's application, like how police are currently using this software despite knowing it's flaws.",3
post5hb,richly branching,1.5948230389474962,highest,"> it becomes racist in it's application, like how police are currently using this software despite knowing it's flaws.

If the algorithm is trained using data which is derived from human selection (or past human actions) then it's likely reinforcing human biases... and thus is basically acting as if it was racist.",4
post5hb,richly branching,1.5948230389474962,highest,"That was pretty much my point. The article makes the claim that the software is ""racist"", but without being able to actually identify based on ethnicity, it pretty much proves that it's actually the opposite of racist.",3
post5hb,richly branching,1.5948230389474962,highest,"No technology is racist in and of itself. As other have replied, it becomes racist in it's application, like how police are currently using this software despite knowing it's flaws.",4
post5hb,richly branching,1.5948230389474962,highest,"I mean, if it detects people of certain races better than people of other races, how is that any better than someone's old white granny not being able to tell black kids apart in her neighborhood?",4
post5hb,richly branching,1.5948230389474962,highest,"Better cameras, or halfway decent training on how to process darker skin tones.",2
post5hb,richly branching,1.5948230389474962,highest,There's a lot of competition but this is definitely the dumbest comment in this thread,2
post5hb,richly branching,1.5948230389474962,highest,Good on you for not seeing colour,1
post5hb,richly branching,1.5948230389474962,highest,Neither can the police.,1
post5hb,richly branching,1.5948230389474962,highest,BBC can't tell LeBron James and Kobe Bryant apart so....,1
post5hb,richly branching,1.5948230389474962,highest,"So... this actually says the technology disproportionately inhibits white (sorry, non-“black or brown”) people in their ability to commit crime anonymously.",1
post5hb,richly branching,1.5948230389474962,highest,"""hugely flawed when it comes to accurately recognising black and brown individuals"" So, mission accomplished? /s",1
post5hb,richly branching,1.5948230389474962,highest,Is skin color a factor in facial recognition algorithms?,1
post5hb,richly branching,1.5948230389474962,highest,"Yes, because lighter skin colours show more contrast with shadows and shapes over darker skin colours, therefore are more easily distinguishable. And its been trained using more lighter skinned faces too.",2
post5hb,richly branching,1.5948230389474962,highest,"RACE IS A SOCIAL CONCEPT   
REEEEEEEEEEEEEEEEEEEEEEEE",1
post5hb,richly branching,1.5948230389474962,highest,"Finally, a legitimate reason to wear blackface.",1
post5hb,richly branching,1.5948230389474962,highest,Neither can I,1
post5hb,richly branching,1.5948230389474962,highest,"Last month, I took the Eurostar from Paris back to London. There was a problem and everything was delayed and a huge crowd was stuck in the waiting lines. I was bored so started people watching m. When we arrived to the electronic passport gates, I counted it took most people, white people, between 5 and 8 seconds for the face scan/passport gate to open. For the two black guys, it was well over 25 seconds. Obviously my one Mississippi, two Mississippi ... method is not very scientific but I felt for those guys.",1
post5hb,richly branching,1.5948230389474962,highest,Seems fair to have a system that only busts white people for a while.,1
post5hb,richly branching,1.5948230389474962,highest,Can it tell Scottish and Irish apart?,1
post5hb,richly branching,1.5948230389474962,highest,This doesn’t sound very Onion’y,1
post5hb,richly branching,1.5948230389474962,highest,is this how you can tell white people compiled the software?,1
post5hb,richly branching,1.5948230389474962,highest,They will use it when it supports their case and ignore it when it doesn't and go to their graves thinking it never failed them.,1
post5hb,richly branching,1.5948230389474962,highest,[deleted],1
post5hb,richly branching,1.5948230389474962,highest,"The AI is correct. Human Belief that Black is any genetic percentage of African Ancestry despite the increasingly light brownness that comes with increasing percentages of non-African Ancestry are confusing the system. This is because programmers are imposing those false beliefs on the AI dataset and algorithm.

Remember the Babylon 5 episode when an AI killing machine/powersuit is smuggled in by archaeologists? Its data set and algorithm required the extermination of the impure... and the definition of what was impure was laid down by religious fanatics.

HAHAHAHA... You humans are so stupid.",2
post5hb,richly branching,1.5948230389474962,highest,"It also can't tell Asians apart, but I don't hear them complaining.",1
post5hb,richly branching,1.5948230389474962,highest,"So,apart from you and red and yellow,we are all white. Facial recognition is facial that's it. Do you fear that you you look similar
 Between you two?",1
post5hb,richly branching,1.5948230389474962,highest,"well for the facial recognizance software it dosnt mean much what color your skin is, because its looking at your facial features anyway thats different from any other person. 
To put it in perspective, its more or less like your fingerprints, it dosnt project your race but it is your individual identity.",1
post5hb,richly branching,1.5948230389474962,highest,"From a technical level this makes sense image recognition works based on edge detection. For lighter toned people, in proper light conditions, the contrast between skin color and an outline of a feature like the edge of a nose is easier to detect. For darker toned people, unless they have really oily and shiny skin, it would be difficult to detect. But if it is giving bad results, they shouldn't be using it.",1
post5hb,richly branching,1.5948230389474962,highest,No shit ACAB,1
post5hb,richly branching,1.5948230389474962,highest,This will be used the same way as drug-sniffing dogs -- to gin up probable cause for the police whenever they need it.,1
post5hb,richly branching,1.5948230389474962,highest,New Headline: Anti-racist facial recognition gets slammed for not looking at color of skin.,1
post5hb,richly branching,1.5948230389474962,highest,Just like the police,1
post5hb,richly branching,1.5948230389474962,highest,Just like real life then,1
post5hb,richly branching,1.5948230389474962,highest,">One: it’s racist. 

Lol not it isn't, it's just how physics work. It's like when you have your photo taken for a yearbook and the extremely toned Nigerian kid's photo comes out a bit dark. Camera's distinguish more detail from lighter tones, you need different lighting conditions for a really pale person than you would for a really dark person.
So is it poorly calibrated and not ready for use? Yes? Was it intentionally done so as to be racist? No. White people are the majority in the UK, weird how things tend to be tailored more to the majority (81%).",1
post5hb,richly branching,1.5948230389474962,highest,"> during a Romford trial in 2018 and not only discovered the technology had a 100% inaccuracy rate, but witnessed the Met stop a 14-year-old black schoolboy after facial recognition wrongly matched his image to an individual on one of their watchlists. He was held by four plainclothes police officers, questioned, searched and fingerprinted to check his identity. 


100% failure WTF. Why still use this",1
post5hb,richly branching,1.5948230389474962,highest,Who's remotely surprised by this? Our legal system is completely broken and has been for a loooong time. I'm only surprised they didnt start using the technology earlier.,1
post5hb,richly branching,1.5948230389474962,highest,"Cops can’t tell the difference either. “ Shootem both boys, Sprinkle some crack and let’s get the fuck outta here”",1
post5hb,richly branching,1.5948230389474962,highest,"Since when does the US's system of policing give a damn if they ""got"" the right guy? They really don't. They just want to grab someone, close the case and call it a day. Just another corrupt meat grinder.",1
post5hb,richly branching,1.5948230389474962,highest,"This story is about Britain. Your point still stands, but is irrelevant to this particular story.",2
post5hb,richly branching,1.5948230389474962,highest,Its amazing what happens when you only train it on white people.,1
post5hb,richly branching,1.5948230389474962,highest,"The problem isn't that it was trained only on white people, its that white people make up 75% of the population of the US where most of the companies developing this type of software are located.  If they were to train it on every single US citizen's passport photo or driver's license photo, it would be trained on many more white faces and thus be more accurate at identifying them.  Many of the developers who make this type of software are POC.",2
post5hb,richly branching,1.5948230389474962,highest,"The world is mostly non-white, and people all over the world have access to the internet and post public pictures of themselves online. The idea that it's too hard in this day and age to get pictures of people from a wide variety of ethnicities doesn't make a whole lot of sense. 

The problem isn't that there's no way to actually training on a bunch of different ethnicities. The problem is that many people rarely remember to do so, or feel that it's inconvenient to put in the effort to do so. Which might fly for a cheapo camera, but isn't an acceptable standard for law enforcement facial recognition",3
post5hb,richly branching,1.5948230389474962,highest,"I didn't say there was no way of actually training on a bunch of different ethnicities, just that it hasn't been done yet.  They can't just pull any photo off the internet and use it, they first have to have consent, and also about 100 or so photos of the same person from different angles and different ages.  They started with people in the US, next step will be to improve the data sets by expanding to other countries.",4
post5hb,richly branching,1.5948230389474962,highest,"So it's business as usual for them, got it.",1
post5hb,richly branching,1.5948230389474962,highest,"Facial recognition has nothing to do with color. It looks at the all the features on the face, which are pretty much unique. Doesn’t matter the color.",1
post5hb,richly branching,1.5948230389474962,highest,Remember the computer is trying to determine facial features from a 2d image. Darker skin tones reflect less light and will make it harder to detect contours of a face.,2
post5hb,richly branching,1.5948230389474962,highest,So then perhaps police shouldn’t be using the technology?,3
post5hb,richly branching,1.5948230389474962,highest,Are they arresting people and imprisoning them on facial recognition software alone?,4
post5hb,richly branching,1.5948230389474962,highest,"Absolutely. Facial recognition is a horrible rabbit hole, and if it dies because it's 'racist', I'm cool with that.",4
post5hb,richly branching,1.5948230389474962,highest,">Darker skin tones reflect less light and will make it harder to detect contours of a face.

Physics is racist.",3
post5hb,richly branching,1.5948230389474962,highest,THAT'S RACIST!   jk. It's really not.,3
post5hb,richly branching,1.5948230389474962,highest,"Get out of here with your facts, this is a witch hunt can't you see?  Anyone concerned simply has to wear a hat, or sunglasses.  When I'm outside, I look like a million other people with generic hats and sunglasses.",2
post5hb,richly branching,1.5948230389474962,highest,"What kind of fucking cyberpunk police state would you  be living in where minorities are obligated to obscure their face to avoid being harassed by cops? Not to mention the fact of covering your face probably makes you *more* likely to be stopped by cops, specifically because of suspicion that you're avoiding the system.",3
post5hb,richly branching,1.5948230389474962,highest,peepee poopoo,1
post5hb,richly branching,1.5948230389474962,highest,The most intelligent comment in this thread.,2
post5hb,richly branching,1.5948230389474962,highest,There’s a difference?,1
post5hb,richly branching,1.5948230389474962,highest,"Meh, so it's the same as the average American then",1
post5hb,richly branching,1.5948230389474962,highest,"As an American, I really do have a hard time with this. Even to this day I mistake every brown person I see for former President Obama.",2
post5hb,richly branching,1.5948230389474962,highest,Which is funny because many people don't know that Obama is only half black.,3
post5hb,richly branching,1.5948230389474962,highest,"If Obama is half black and half white, does that make him a gray person?",4
post5hb,richly branching,1.5948230389474962,highest,So Just as unreliable,2
post5hb,richly branching,1.5948230389474962,highest,I have trouble with white faces and you expect me to correctly identify people of other ethnicity that are far more uncommon to see everyday?,2
post5hb,richly branching,1.5948230389474962,highest,For starters white stands out more predominately in poorly lit locations. My best guess is that reading a face that is of a darker tone confuses the software. It's not racist. It's just the tech isn't ready. You can't blame a software for not picking up as much light reflection off darker features to make a distinction.,1
post5hb,richly branching,1.5948230389474962,highest,"Actually, facial recognition software has been shown to have trouble distinguishing not only dark-skinned people, but also light-skinned Asians and women. So the problem has nothing to do with light versus dark skin and everything to do with what population is used to train the algorithm, which at the moment is primarily white men. This is what people are criticizing. You can’t have a fair algorithm if the dataset is heavily skewed toward one demographic.",2
post5hb,richly branching,1.5948230389474962,highest,True.,3
post5hb,richly branching,1.5948230389474962,highest,The only job of the police is to keep the population in fear while robbing them.,1
post5hb,richly branching,1.5948230389474962,highest,"Neither can cops, so this is a lose-lose situation if you're not white.",1
post5hb,richly branching,1.5948230389474962,highest,But they don't see that as a problem...,2
post5hb,richly branching,1.5948230389474962,highest,"As if there is some specific division between ""black and brown people"", lol.",1
post5hb,richly branching,1.5948230389474962,highest,As in it can't tell one black guy apart from another black guy,2
post5hb,richly branching,1.5948230389474962,highest,"Black= African-American, and brown= Middle eastern",2
post5hb,richly branching,1.5948230389474962,highest,"it also has a problem with seeing black females as males. living in the city, I can kinda see it. Black women's facial features are more similar to black mean versus white women / men",1
post5hb,richly branching,1.5948230389474962,highest,[removed],1
post5hb,richly branching,1.5948230389474962,highest,"Checks profile... yep, just as I expected.",2
post5hb,richly branching,1.5948230389474962,highest,Lol but no.,2
post5hb,richly branching,1.5948230389474962,highest,I bet that you’re upset she refers to herself as a girl,2
post5hb,richly branching,1.5948230389474962,highest,You mean he,3
post5hb,richly branching,1.5948230389474962,highest,I'm but a simple *ai* I see a not white color I arrest,1
post5hb,richly branching,1.5948230389474962,highest,">On Friday, London’s Metropolitan Police Force announced in a smug tweet that they would be rolling out live [facial recognition technology](http://news.met.police.uk/news/met-begins-operational-use-of-live-facial-recognition-lfr-technology-392451) across the city, starting from February.   
>  
>the European Commission [declared the opposite](https://www.bbc.co.uk/news/technology-51148501):  that they were so worried about the risks facial recognition posed in  its current, unregulated state, they were considering supporting a  five-year ban on its use in public areas in EU countries.

London's police force are rolling out *one piece of software*, whereas the EC's declaration is about the field of selling and using those kinds of software as a whole. It's like if London police bought a single type of rifle for their police, and the EC issued a statement about how they're worried that there's no real consensus on how to regulate weapons in general across the whole fucking EU.

THERE'S A BIT OF A GAP BETWEEN THOSE TWO ISSUES.

And then the article keeps going downhill from there. Bad article. Bad.",1
post5hb,richly branching,1.5948230389474962,highest,"Just a clarifying point: machine learning algorithms are not racist. They’re recipes for learning to make predictions from data. “Class imbalance”, as it is referred to in the field is a problem for learning unbiased models. Not unbiased in the social sense of preferring a group of people over others because they’re more like you (to vastly over simplify the issue), but unbiased in that they will very likely be better at classifying the majority class in the data set they have learned from. The models (not to be confused with algorithm; they aren’t the same thing) aren’t worse at classifying western minorities because the programmers are racist. They’re worse because they’re trained on samples from western societies, where minorities are not going to be equally represented in a representative data set because, as the title for the groups states, they’re minorities.

There are plenty of reasons the bias in these models may be present, but it almost certainly is not because the programmers are racist. In fact, the machine learning community is very ethnically and nationally diverse.

If the models do have significantly worse performance on minority groups, they obviously shouldn’t be deployed in social systems, especially for justice.

Most people aren’t hateful. Please don’t confuse model bias with algorithm bias or racism on the part of the programmers. It is a very complex issue in machine learning, both socially and technically. There’s a whole sub field called “algorithmic fairness” dedicated to learning unbiased models. It is not an easy problem to address technically and it will get better in the future.",1
post5hb,richly branching,1.5948230389474962,highest,So nothing has changed? Policing based on racism is literally how the modern police force in America was started.,1
post5hb,richly branching,1.5948230389474962,highest,This is not about America.  Also stupid ass views and ideas like that is what feeds and grows racism and causes minorities to get shot and or arrested when they shouldn't as you are conditioning people to fear the police.  Stop you are disgusting.,2
post5hb,richly branching,1.5948230389474962,highest,"OK so I read enough comments to know this is about attempting to distinguish specific people. But with that said what *is* the difference between ""black"" and ""brown"" people. Like I get what the colors are and I get that some people with dark skin are lighter or darker but when people use the terms what are they saying? African ethnicity vs Indian ethnicity? Because let's be honest, nobody has *black* skin, that's ridiculous. It's all shades of brown on the visible light spectrum.",1
post5hb,richly branching,1.5948230389474962,highest,Facial structures are quite different between them,2
post5hb,richly branching,1.5948230389474962,highest,What are the differences? And what about that translates to black or brown?,3
post5hb,richly branching,1.5948230389474962,highest,"I'm talking about African vs Indian for example. I know how Reddit works, as soon as I tell you the differences I'll get -100k karma with everyone calling me a racist. Welcome to 2020 boys",4
post5hb,richly branching,1.5948230389474962,highest,[deleted],1
post5hb,richly branching,1.5948230389474962,highest,"If the accuracy were reversed and it was 100% reliable identifying dark faces, people would still be outraged at how racist it is.",2
post5hb,richly branching,1.5948230389474962,highest,Pretty messed up to miss that field test,1
post5hb,richly branching,1.5948230389474962,highest,Open and shut case Johnson!,1
post5hb,richly branching,1.5948230389474962,highest,Neither can their dogs but they still use them anyway.,1
post5hb,richly branching,1.5948230389474962,highest,"Was this an episode of ""Better Off Ted?""",1
post5hb,richly branching,1.5948230389474962,highest,"That's not a bug, it's a feature.",1
post5hb,richly branching,1.5948230389474962,highest,Well neither can they Soooooo.......,1
post5hb,richly branching,1.5948230389474962,highest,r/whatcouldgowrong,1
post5hb,richly branching,1.5948230389474962,highest,"""You're tearing me apart",1
post5hb,richly branching,1.5948230389474962,highest,"Holy crap, I’m a chameleon",1
post5hb,richly branching,1.5948230389474962,highest,"SKYNET is racist. 

Change my mind.",1
post5hb,richly branching,1.5948230389474962,highest,"Meanwhile, Chinese government using facial recognition on their people.",1
post5hb,richly branching,1.5948230389474962,highest,China claims a 95.5 percent accuracy with their software.  I doubt their numbers.,2
post5hb,richly branching,1.5948230389474962,highest,"Even at 50% they are much better than rest of the world. 
My eyes can differentiate between japanese or even korean people, but I can't do that with Chinese people, but their AI can, and that's scary.",3
post5hb,richly branching,1.5948230389474962,highest,"[https://www.travelchinaguide.com/intro/nationality/](https://www.travelchinaguide.com/intro/nationality/) 

> Han Chinese account for 91.59% of the overall Chinese population 

That's probably why it works.",3
post5hb,richly branching,1.5948230389474962,highest,Morgan freeman test /s,1
post5hb,richly branching,1.5948230389474962,highest,"**Beep boop**... *but master, I thought you said race does not matter.*",1
post5hb,richly branching,1.5948230389474962,highest,https://media3.giphy.com/media/nXxOjZrbnbRxS/giphy.gif,1
post5hb,richly branching,1.5948230389474962,highest,"Wait, there's a difference?",1
post5hb,richly branching,1.5948230389474962,highest,Police state. Fuck 12,1
post5hb,richly branching,1.5948230389474962,highest,A lot of automated voice recognition software can't hear in women's pitch and doesn't hear or understand female vocals. The data gaps are huge for AI,1
post5hb,richly branching,1.5948230389474962,highest,"It is not just the dataset and software end.  There are hardware hurdles for both instances.  BTW men with super deep voices have a great fun time trying to use voice recognition software.

I remember back to a time when we purchased a remote for my dad that used voice recognition.  This is back when it was still new.  His voice is much deeper than mine and my brothers.  It rarely responded to his voice but would work great with ours even after training it to his voice.",2
post5hb,richly branching,1.5948230389474962,highest,Filters to speed up identification process. A computer can reduce the number of possible matches.,1
post5hb,richly branching,1.5948230389474962,highest,Hopefully they can improve it quick enough.,1
post5hb,richly branching,1.5948230389474962,highest,"God damn, ask Michael Reeves for help. He made a racial detection software for his tickle me Elmo.",1
post5hb,richly branching,1.5948230389474962,highest,"This was a minor plot point in an episode of Chicago PD, and I thought it was bogus... I'm disappointed it was actually true.",1
post5hb,richly branching,1.5948230389474962,highest,Xbox Kinect anyone?,1
post5hb,richly branching,1.5948230389474962,highest,Does anyone ever ask whether African-Americans can reliably identify and distinguish whites?,1
post5hb,richly branching,1.5948230389474962,highest,According to my black wife she finds it easier to distinguish white people than I do.  Then again she always claims I have face blindness.,2
post5hb,richly branching,1.5948230389474962,highest,I mean...,1
post5hb,richly branching,1.5948230389474962,highest,This is going to end badly isn't it?,1
post5hb,richly branching,1.5948230389474962,highest,"I have never quite seen a black person. Only various shades of darker skin than my own. Personally, I have little frame of reference for how effective facial recognition software is. As a brown person I suppose it’s a bit disturbing, but I think the title itself is a bit ‘Baity’. I suppose I would suggest that skin tone varies so much now that it’s hard to know if theses are actually important markers (skin tone) in the usage and development of such software. I would think things like nose, eyes, eyebrows, lips, jawline and even ear extension beyond the skull would help provide more pertinent data than skin tone. Yet again I have no clue.",1
post5hb,richly branching,1.5948230389474962,highest,I'm just happy that the upcoming generation of technology doesn't see color,1
post5hb,richly branching,1.5948230389474962,highest,"Replace the humans with droids and this is no Brainer but for conventionally powered carriers it becomes an issue. Get delux, and have an impressive moral and ethical compass. Your loyalty to your family is a gross invasion of privacy. Facial recognition data is sent to the government to fund things like the police, which her mother refuses to use the official app? I agree. I'd honestly be scared for all women who happen to be the definition of an anxiety attack? I come from an affectionate family, but you still signed a lease to pay rent and childcare.",1
post5hb,richly branching,1.5948230389474962,highest,That's a kind of racist way of saying facial recognition can't tell the difference between people of similar skin color...,1
post5hb,richly branching,1.5948230389474962,highest,lol,1
post5hb,richly branching,1.5948230389474962,highest,"I dated a black girl for 4 years, I can't really tell them apart. 
 
She kept saying she's brown or she's not that brown. Idfk y'all are all brown.",1
post5hb,richly branching,1.5948230389474962,highest,I would assume it can't see the difference between white skin either...nice headline tho -.-,1
post5hb,richly branching,1.5948230389474962,highest,"Just as a fun thought experiment, does this make the AI more or less racist?",1
post5hb,richly branching,1.5948230389474962,highest,It’s no better than any cop.,1
post5hb,richly branching,1.5948230389474962,highest,Hol up. Are brown people not considered black and vice versa?,1
post5hb,richly branching,1.5948230389474962,highest,"Neither can the police, so....",1
post5hb,richly branching,1.5948230389474962,highest,hahahhaa very good dude... just a bit awkward since I didnt ask.,1
post5hb,richly branching,1.5948230389474962,highest,Police: “And the problem is..?”,1
post5hb,richly branching,1.5948230389474962,highest,Welcome to America!,1
post5hb,richly branching,1.5948230389474962,highest,I dont see how this is any different than normal.,1
post5hb,richly branching,1.5948230389474962,highest,This might just be a feature and not a bug,1
post5hb,richly branching,1.5948230389474962,highest,"Never seen a ""black"" person",1
post5hb,richly branching,1.5948230389474962,highest,"The fuck does this title mean? Black people aren't literally black, they have brown skin.",1
post5hb,richly branching,1.5948230389474962,highest,"No one can tell. Can't blame technology for ""racism"" now can we? Oh wait, unless it was programmed by evil white men?!?!?!",1
post5hb,richly branching,1.5948230389474962,highest,all that matters at the end of the day is that both the coffers and the prisons are full.,1
post5hb,richly branching,1.5948230389474962,highest,"It's not like they could tell them apart before, why start now?",1
post5hb,richly branching,1.5948230389474962,highest,I don't think most police could spot the difference anyway,1
post5hb,richly branching,1.5948230389474962,highest,Should police make a difference between black and brown people?,1
post5hb,richly branching,1.5948230389474962,highest,Sounds like the family guy skin color chart. Hmmm is it white? Cool. No? Officer down,1
post5hb,richly branching,1.5948230389474962,highest,There's a difference ?,1
post5hb,richly branching,1.5948230389474962,highest,"This reminds me of that episode of Better Off Ted.

They installed sensors in the building that couldn’t recognize all the black people in the facility.  Their solution was to have every black person accompanied by a white person so that the sensors would work.

Knowing how society is today, some jackass would actually float this as a legitimate solution.",1
post5hb,richly branching,1.5948230389474962,highest,Naturally camouflaged?!  I have the ability to get tan (mixed heritage). This is uplifting news. Side note:. With the killer flu season and the corovirus running around just wear facemasks,1
post5hb,richly branching,1.5948230389474962,highest,"First, this made me chuckle. Now I'm just sad.",1
post5hb,richly branching,1.5948230389474962,highest,Fuck every single person involved in the creation of this dystopian shit. Hope it was worth the money.,1
post5hb,richly branching,1.5948230389474962,highest,Finally a win.,1
post5hb,richly branching,1.5948230389474962,highest,Veridian Dynamics: Money over people.,1
post5hb,richly branching,1.5948230389474962,highest,"Well, yeah. Look who is programming. The majority.",1
post5hb,richly branching,1.5948230389474962,highest,Hey guys without testing the exact program the police are using how can we do this study and come to this conclusion?,1
post5hb,richly branching,1.5948230389474962,highest,Looks like they're not so diverse after all,1
post5hb,richly branching,1.5948230389474962,highest,Black people are brown... *facepalm*,1
post5hb,richly branching,1.5948230389474962,highest,what's the difference between black and brown people anyways?,1
post5hb,richly branching,1.5948230389474962,highest,"If you read the article, it can't tell people(who are black or brown) apart, getting a higher rate of false positives",2
post5hb,richly branching,1.5948230389474962,highest,you haven't answered my question,3
post5hb,richly branching,1.5948230389474962,highest,Africans skin complexion vs say mexican skin tone I believe.,4
post5hb,richly branching,1.5948230389474962,highest,dark is dark!,1
post5hb,richly branching,1.5948230389474962,highest,Great. Now even the robots are racists.,1
post5hb,richly branching,1.5948230389474962,highest,That really means that shit is just badly programmed... pretty sure we've been able to do this for like 20 years.,1
post5hb,richly branching,1.5948230389474962,highest,The police cant either so its an accurate replacement,1
post5hb,richly branching,1.5948230389474962,highest,What about those with really nice tans?,1
post5hb,richly branching,1.5948230389474962,highest,"people dont distinguish those with light complexion, a latin white is way different than caucasian

latins are darker south in cuba but arent brown people whatsoever",1
post5hb,richly branching,1.5948230389474962,highest,In what way is this Oniony? I swear this sub has gone to shit.,1
post5hb,richly branching,1.5948230389474962,highest,Fingerprinting and polygraphs and DNA tests are also not reliable,1
post5hb,richly branching,1.5948230389474962,highest,I bet LIDAR would be better for that.,1
post5hb,richly branching,1.5948230389474962,highest,I see lots of discussion that doesn't make sense in the context of the article. So my conclusion is that (almost) nobody in the top comments read the article.,1
post5hb,richly branching,1.5948230389474962,highest,"It's not a bug, it's a feature.",1
post5hb,richly branching,1.5948230389474962,highest,How can code be racist?,1
post5hb,richly branching,1.5948230389474962,highest,Neither can BBC News.,1
post5hb,richly branching,1.5948230389474962,highest,"Call me clueless but is that important? Can it still recognise faces or can it not do that either?

I mean, as long as it can tell me if something is a pyramid, I don't care what colour the pyramid is.",1
post5hb,richly branching,1.5948230389474962,highest,"The software cannot tell apart enough features distinctively to not give huge false positives. We don't just want to know its a pyramid, we already know its a persons face we are looking at, we are trying to track a particular pyramid.

Edit: It is also that generally the dataset its ability to search is based off doesn't have enough darker skin toned examples so that the system isn't programmed as well for those skin tones and face shapes.",2
post5hb,richly branching,1.5948230389474962,highest,"> Edit: It is also that generally the dataset its ability to search is based off doesn't have enough darker skin toned examples so that the system isn't programmed as well for those skin tones and face shapes.

So what you're saying is some African countries and South American countries should implement this system to help everyone along?",3
post5hb,richly branching,1.5948230389474962,highest,Sri Lankans finna get shot,1
post5hb,richly branching,1.5948230389474962,highest,Imagine indian guys getting pulled out from google offices for being too black.^^/s,1
post5hb,richly branching,1.5948230389474962,highest,"Hm, sounds like the perfect tool for police then, they usually don't care about the difference anyway",1
post5hb,richly branching,1.5948230389474962,highest,This is me whenever i try to Board a Delta international Flight using their Face recognition to board a flight. It never works. So they always do it manually.,1
post5hb,richly branching,1.5948230389474962,highest,The system ain't racist!,1
post5hb,richly branching,1.5948230389474962,highest,This surprises you? They've been doing this for centuries,1
post5hb,richly branching,1.5948230389474962,highest,Time for those medical face masks to get popular like in asia,1
post5hb,richly branching,1.5948230389474962,highest,A light reflection based technology that can't accurately spot people who naturally absorb more light with their skin? Truly shocking. This is why you need layers of technology with redundancy to be accurate.,1
post5hb,richly branching,1.5948230389474962,highest,The technology is in an early stage and needs refinement...'IT'S RACIST!!!',1
post5hb,richly branching,1.5948230389474962,highest,and the police cant tell the difference between a book and a gun... but people are using them anyway,1
post5hb,richly branching,1.5948230389474962,highest,Does it matter? Skin colour is irrelevant as long as it can map your face. Unless of course humans can look alike down to the millimetre.,1
post5hb,richly branching,1.5948230389474962,highest,Yes it matters. Did you read the title? They cannot tell the difference between people that have higher melanin levels. That means Joel can be confused with Mike,2
post5hb,richly branching,1.5948230389474962,highest,What the fuck is a brown person?,1
post5hb,richly branching,1.5948230389474962,highest,Indian is an example.,2
post5hb,richly branching,1.5948230389474962,highest,Ohh,3
post5hb,richly branching,1.5948230389474962,highest,God I hope this doesn't come to the US.,1
post5hb,richly branching,1.5948230389474962,highest,Hurrdurr DAE da mucheen racist?,1
post5hb,richly branching,1.5948230389474962,highest,"It's a feature, not a bug.",1
post5hb,richly branching,1.5948230389474962,highest,Lol yes it can,1
post5hb,richly branching,1.5948230389474962,highest,But why do we need a facial recognition in the first place?,1
post5hb,richly branching,1.5948230389474962,highest,"So, would it be ok to *use* blackface to hide and rise up with all the brothers?",1
post5hb,richly branching,1.5948230389474962,highest,Cops cant tell either.,1
post5hb,richly branching,1.5948230389474962,highest,This isnt funny at all,1
post5hb,richly branching,1.5948230389474962,highest,"It doesn’t need to be perfect, it just needs to be better than an eyewitness could be",1
post5hb,richly branching,1.5948230389474962,highest,"In this age of airborne death - wear masks.

Facial recognition won't work at all.",1
post5hb,richly branching,1.5948230389474962,highest,Does it matter when the police will start shooting regardless?,1
post5hb,richly branching,1.5948230389474962,highest,"Around my city, criminals only ever get caught if the victim's family is dogged and loud in insisting the police investigate and/or hires investigators themselves, or if he's caught in the act or on camera, or given up by some snitch from a different case.  A big part of this is because cops these days aren't keeping an eye on neighbourhoods and knowing what goes on, they all live miles away and only come to work to hunt criminals, or people they think might look like criminals, which could be anyone in their criminal catchment area they patrol while working.

It must be very difficult for young black and brown men, who are the usual suspects for every crime, to deal with these cops.",1
post5hb,richly branching,1.5948230389474962,highest,Further proof that white people are racist.,1
post5hb,richly branching,1.5948230389474962,highest,It's exactly like real life!,1
post5hb,richly branching,1.5948230389474962,highest,So yawll saw the same shit about facial recognition on Chicago PD and post it and now it's factual,1
post5hb,richly branching,1.5948230389474962,highest,Fuck it close enough,1
post5hb,richly branching,1.5948230389474962,highest,So it’s less racist sounds cool to me,1
post5hb,richly branching,1.5948230389474962,highest,"If a hacker really wanted to show some skill and mess with the cops, hack the database and install pictures of every cartoon character known to man in it! It would really piss off the cops when they realize how useless their high tech bullshit really is!!!",1
post5hb,richly branching,1.5948230389474962,highest,I do not agree with the surveillance itself... but I am also not quite sure how an AI not being able to target black vs brown people is somehow bad.,1
post5hb,richly branching,1.5948230389474962,highest,Fuck 12,1
post5hb,richly branching,1.5948230389474962,highest,Police cant tell them apart either so why would they care?,1
post5hb,richly branching,1.5948230389474962,highest,well i guess that confirms my thoughts that law enforcememnt is a corrupt mess,1
post5hb,richly branching,1.5948230389474962,highest,I told you it wasn’t just me that couldn’t tell them apart.,1
post5hb,richly branching,1.5948230389474962,highest,"I'm pretty sure it wouldn't be able to tell Caucasians apart from Whites either.

Caucasians normally have tan or Olive skin",1
post5hb,richly branching,1.5948230389474962,highest,"> On Friday, London’s Metropolitan Police Force announced in a smug tweet that they would be rolling out live facial recognition technology across the city, starting from February. 

Within 10 years this will be used to enforce sharia, bet me.",1
post5hb,richly branching,1.5948230389474962,highest,It doesn't have to.  It only has to make a distinction between criminals and innocent citizens.,1
post5hb,richly branching,1.5948230389474962,highest,"Except it's not even doing that with an error rate that high.  It will lead to potential false arrest and harassment of those falsely identified. 

The error rate is so high that this is just as worthless as lie detector tests, and those aren't admissible in court.  This wouldn't be either.",2
post5hb,richly branching,1.5948230389474962,highest,"There's a difference?

EDIT: This isn't a joke or anything. I really don't know.",1
post5hb,richly branching,1.5948230389474962,highest,It can't tell _individual people who are black or brown_ apart.,2
post5hb,richly branching,1.5948230389474962,highest,Oh I see. Yeah I had a camera that couldn't detect the faces of black people once. Funny thing was I bought it in Africa.,3
post5hb,richly branching,1.5948230389474962,highest,Blue only cares about blue,1
post5hb,richly branching,1.5948230389474962,highest,This is a direct result of not enough diversity in tech,1
post5hb,richly branching,1.5948230389474962,highest,"So basically, the system is better at detecting white criminals than at detecting black/asian/etc criminals? And the article says thats racist against non-white people.

Just imagine if it was the other way round, if the software was better at tracking down black people. Im sure that would've been racist too to those people. 

And its frankly comical that they even try to force in a racism angle in such a serious matter, there are a lot more problems with facial recognition than it being better at certain skin-colors  or facial features (which might be both technical and data based). How braindead woke can you actually get?",1
post5hb,richly branching,1.5948230389474962,highest,I think the issue might be with false positives and that’s why it’s a problem.,2
post5hb,richly branching,1.5948230389474962,highest,"Which means its less reliable and less likely to be used, I imagine? At least from what I'd understand, facial recognition technology generally is there to help find similar faces, but its not gonna be direct evidence. Youll still have human reviews actually looking up that stuff?

Not entirely sure about that.

In any way, my main point isnt exactly that, but rather that you can view it in two different ways, the technical flaw isnt fair to anyone. To make it a one sided 'racist' thing , aimed at one particular group, only shows ones need to push an agenda.",3
post5hb,richly branching,1.5948230389474962,highest,"I don’t understand how you would think the “technical flaw isn’t fair to anyone”. On one hand you have the facial recognition software working as intended on non-ethnic minorities, but the percentage of darker skinned people being mismatched is hundreds of times higher. This can lead to a higher level of harassment of innocent people from authorities who use the faulty system as a excuse. The article is bringing up the fact that this software is being rolled out by London’s police force even though its unreliable.",4
post5hb,richly branching,1.5948230389474962,highest,Everything is racist don’tcha know 😏,2
post5hb,richly branching,1.5948230389474962,highest,"How braindead can you get?

It's this simple:

If you're white and commit a crime, the facial recognition can usually pick you as the criminal from a selection of other white people, a.k.a. finding the guilty and letting the innocent be.. innocent.

If you're black and commit a crime, the facial recognition has a higher chance of picking an *innocent* person as the criminal. 

This is about the rate of innocent people being found as the criminals is much higher among black people. Disadvantaging *innocent* black people more, and not catching the guilty is racist against the black communities because it's taking innocent people out and leaving the problematic and guilty in those communities, while in white communities it works just like police do: catching the criminals and not the innocents.

Pretty simple dude, we want the AI to work like good police, not catching the wrong people...",2
post5hb,richly branching,1.5948230389474962,highest,">If you're black and commit a crime, the facial recognition has a higher chance of picking an *innocent* person as the criminal.

And now expand on that thought. What if the police knows about that? Which they do, btw.

>we want the AI to work like good police 

Its not AI, its not police, its just facial recognition software. A tool with benefits and flaws, nothing more, and there are positive and negative ways to use it.",3
post5hb,richly branching,1.5948230389474962,highest,"You said that this is actually biased against white people.

That is absolute bullshit.

The AI *works* in white communities, putting criminals in jail and keeping innocent people safe.

The AI *doesn't work* in black communities, helps criminals and hurts innocent people.

How does this disadvantage white people?",4
post5hb,richly branching,1.5948230389474962,highest,Maybe It’s because “black” vs “brown” is a really arbitrary difference that only our division wanting brains can understand?,1
post5hb,richly branching,1.5948230389474962,highest,Exactly.,2
post5hb,richly branching,1.5948230389474962,highest,This is a thousand times worse than the Patriot Act.  This is the fucking **definition** of Orwellian.,1
post5hb,richly branching,1.5948230389474962,highest,Not like the cops can tell them apart without it anyway.,1
post5hb,richly branching,1.5948230389474962,highest,Police officers can't tell either.,1
post5hb,richly branching,1.5948230389474962,highest,Neither can the police.,1
post5hb,richly branching,1.5948230389474962,highest,"Police and judges only care about black crime. 

They give zero fucks and don’t even need “witnesses” for white crimes.........",1
post5hb,richly branching,1.5948230389474962,highest,This is one of the most ignorant comments I have seen in years.,2
post5hb,richly branching,1.5948230389474962,highest,There’s a difference?,1
post5hb,richly branching,1.5948230389474962,highest,Neither could my grandpa. We just let him die a bitter old man. Like this should.,1
post5hb,richly branching,1.5948230389474962,highest,"yeah that's Police IQ right there: if it isn't We, it's Colored People",1
post5hb,richly branching,1.5948230389474962,highest,"Wait...  So I can commit a crime directly in front of facial-recognition cameras - and get away scot-free, if I wear blackface?",1
post5hb,richly branching,1.5948230389474962,highest,Yes,2
post5hb,richly branching,1.5948230389474962,highest,Yes. Anything that can make an image look distorted or abnormal can screw with facial recognition. Dazzle camo is a better bet though.,2
post5hb,richly branching,1.5948230389474962,highest,"Is this you?

https://www.nbcnews.com/news/us-news/man-blackface-robs-maryland-bank-police-say-n1126346

It didn't go to well.  
https://www.vibe.com/2017/11/white-man-arrested-after-attempting-to-rob-bank-in-blackface",2
post5hb,richly branching,1.5948230389474962,highest,"Yes, distinguishing black people from brown people is a huge problem.",1
post5hb,richly branching,1.5948230389474962,highest,So now can we stop getting in a tiz and screaming racist when people say all black people look alike.,1
post5hb,richly branching,1.5948230389474962,highest,So if computers can't tell them apart... Is it still racist...?,1
post5hb,richly branching,1.5948230389474962,highest,That's because they're all guilty,1
post5hb,richly branching,1.5948230389474962,highest,Trump supporter confirmed.,2
post5hb,richly branching,1.5948230389474962,highest,"You clearly hate yourself. It's really funny to watch, kind of unbelievable and sad really.",2
post5hb,richly branching,1.5948230389474962,highest,"Copying my posts to you from another thread back to me, shame. You're pathetic.",3
post5hb,richly branching,1.5948230389474962,highest,"Your own hypocrisy got you triggered? It's OK, champ, you'll live. Cool it with the projection and you won't have to face your own words.",4
post5hb,richly branching,1.5948230389474962,highest,Not sure why this is a story. Facial recognition measures points along the head/face. Color isn’t an issue. It’s like a fingerprint.,1
post5hb,richly branching,1.5948230389474962,highest,[deleted],1
post5hb,richly branching,1.5948230389474962,highest,It can't do what you're saying. It has a 100% failure rate in the field test.,2
post5hb,richly branching,1.5948230389474962,highest,fair,3
post5hb,richly branching,1.5948230389474962,highest,"Thats probably why police are using it; Its a feature not a bug.

Black people are harrassed so much by the police it's become an inside joke when a police officer says ""You fit the description"" to a random black  person.   


I remember as a kid a random police officer approached me and said I'll become a statistic... Im an introvert that plays  dnd and likes coding. I dont even have friends and the guy profiled me like Im selling crack for walking down the street.",1
post5hb,richly branching,1.5948230389474962,highest,"There aren't a lot of minorities in the hills of East Tennessee, so the cops like busting us poor people for bullshit. The pricks know we can't afford a good lawyer, so they throw everything at you and see what sticks.",2
post5hb,richly branching,1.5948230389474962,highest,No one cares that it happens to poor white people more often than any other group.,3
post5hb,richly branching,1.5948230389474962,highest,">Black people are harrassed so much by the police it's become an inside joke when a police officer says ""You fit the description"" to a random black person.

Stop spreading this myth.  This is the shit that reinforces racism and creates hatred district of the police causing police to think black boys are suspicious because they are acting odd because racist fucks like you are too stupid to shut your mouth from spreading lies.

You are literally killing black people with this rhetoric.

>I remember as a kid a random police officer approached me and said I'll become a statistic... Im an introvert that plays dnd and likes coding. I dont even have friends and the guy profiled me like Im selling crack for walking down the street.

Guess what that happens to us white boys to.  Happens to my brother all the damn time because he is sketchy looking.  He is also a massive introvert and that is why he looks out of place to the police.",2
post5hb,richly branching,1.5948230389474962,highest,[removed],1
post5hb,richly branching,1.5948230389474962,highest,"I think you would care if the facial recognition system thought you were a person who had previously been jailed for a crime.

The title is confusing because at first I thought I don't think the software needs to know if someone's skin color but what is being confused isn't what skin color a person has, instead the software is confusing a person for a different person.",2
post5hb,richly branching,1.5948230389474962,highest,"Oh ok I see

That’s why I said who cares about race because I just assume people like me are colorblind",3
post5hb,richly branching,1.5948230389474962,highest,"Tbh, this seems pretty ben shapiro. And how would you like if it couldnt tell white people apart instead so you got arrested and charged because someone who sooooorta kinda looks like you in some of your features 6 towns over got caught on camera? Plus, no one outside of organized crime is seriously saying gangs are good. Try watching a little less shapiro and you might think a little bit.",2
post5hb,richly branching,1.5948230389474962,highest,"Exactly lol. If this software was misidentifying white people Bitch Benny would be screaming about how all white people are the real victims in today's society and somehow finding a way to blame poc in the process. 
The irony is completely lost on these people and its baffling.",3
post5hb,richly branching,1.5948230389474962,highest,"I actually don’t watch enough Shapiro lol I barely watch him I use this name more or less to poss people off and it’s working

Have a wonderful rest of your day",3
post5hb,richly branching,1.5948230389474962,highest,Who is mad?,4
post5hb,richly branching,1.5948230389474962,highest,"Lol coulda fooled me. Also, your name is ben_shapiro2020 and you dont watch him. He's either stupid or dishonest, I would be careful of who you name your persona after.",4
post5hb,richly branching,1.5948230389474962,highest,If you claim to be a troll then your not. You're just another racist. One that's about to be reported.,2
post5hb,richly branching,1.5948230389474962,highest,"Keep telling yourself I’m racist even though I just said who cares about their skin color

Have a wonderful day I hope you find more stuff to get pissed at for no reason",3
post5hb,richly branching,1.5948230389474962,highest,"Ight Benny lmao. Their skin color is exactly why cops are still using faulty software. Its been proven to misidentify BLACK PEOPLE but they continue to use it. Despite the fact that it leads to false accusations and imprisonment. How tf is that not racist, Benny? 

Also ""Cops bad gangs good"". Homie ur fuckin dumb lmao.",4
post5hb,richly branching,1.5948230389474962,highest,[removed],2
post5hb,richly branching,1.5948230389474962,highest,Hey look it’s how little I care,3
post5hb,richly branching,1.5948230389474962,highest,Username checks out.,2
post5hb,richly branching,1.5948230389474962,highest,"Does it really matter though?
I thought were supposed to look past color",1
post5hb,richly branching,1.5948230389474962,highest,"I thought facial recognition work by facial measurements and shape to identify people. and according to the articular, black and brown people are misidentified  something like 30%-80% of the time.  Maybe facial features of certain races have a higher % chance of being very similar to others. so perhaps not all of the  people who say ""they all look alike"" are not being racist, they just do not notice the small differences.",1
post5hb,richly branching,1.5948230389474962,highest,"Google skull differences between White, Black, Brown and Yellow people. 

Also nose, lips, eyes and hair color are different.

Most minorities have black hair, black or brown eyes.  

Most drastically different are black but if you’re dark brown enough with an overlap of thick lips it might trigger the software 

It’s easier to detect whites as the most diverse people are white with hair color from black to blonde and eye color from black to blue so that may help the software identify with distinctive markings.",2
post5hb,richly branching,1.5948230389474962,highest,Police designed AI,1
post5hb,richly branching,1.5948230389474962,highest,Police not caring what kind of non-white you are? I don't believe it.,1
post5hb,richly branching,1.5948230389474962,highest,"Well, Fuck the police.",1
post5hb,richly branching,1.5948230389474962,highest,Ditto,2
post5hb,richly branching,1.5948230389474962,highest,"It’s a feature not a bug.  
Same with drug field kits.  They don’t work but they’re great for arresting black and brown people.",1
post5hb,richly branching,1.5948230389474962,highest,"Personally, I’d go with the headline: “Science PROOVES you people all look the same.”",1
post5hb,richly branching,1.5948230389474962,highest,"How to know if an app or algorithm was invented by pasty white guys....

.. it doesn't work for non-Caucasian people.",1
post5hb,richly branching,1.5948230389474962,highest,What about app and algorithms invented by non pasty white guys?,2
post5hb,richly branching,1.5948230389474962,highest,Their boss is a pasty-white guy. :p,3
post5hb,richly branching,1.5948230389474962,highest,Hey neither can I! /s,1
post5hb,richly branching,1.5948230389474962,highest,...,1
post5hb,richly branching,1.5948230389474962,highest,"Why the difference in accuracy between races?

Just a matter of relative sample sizes?

Or do black people intrinsically look more similar to each other than whites? That shouldn't be true since Africans have more genetic variability than the small groups that emigrated to populate the rest of the world.

Or are the blacks in western countries expanded from small immigrant bottlenecked populations?

Is it because the 'blackness' swamps other characteristics when analyzing a mixed population? Then why aren't they using separate algorithms for blacks and whites? 

Anyway, it sounds like a technical problem, not a social one.",1
post5hb,richly branching,1.5948230389474962,highest,"Generally speaking, the issue with cameras and black people (cause this has been a persistent problem in all new camera technology) has been that programmers have mostly been white/east Asian, and therefore tend to test their tracking/recognition/other camera software on other white/east Asian people, instead of a broad range of people with all different shades. Which means it's generally harder for cameras to pay attention to darker skinned people - - not because it's actually technically impossible/difficult, but because there aren't enough people in tech who see enough black people to think ""Ah yeah, I need to include some dark-skinned friends in this beta test, and I need to write my code to account for different skin tones."" This becomes more evident when, every time a new camera innovation comes out, black people say ""but it doesn't work for me,"" and it takes a trivial amount of time for the company involved to fix the problem (e.g., such as when Microsoft has a camera that was supposed to be able to track and focus in on people in range of the camera, but was basically blind to black people in frame, and was able to fix it within maybe a month of people making YouTube videos about it). 

Similarly, people surrounded by light-skinned people are likely to forget to train their ML algorithm to detect facial features from a full range of ethnicities, leading said programs to have a hard time detecting people who fall outside the training samples.

Basically, tech isn't immune to the oversights of the people who make it.",2
post5hb,richly branching,1.5948230389474962,highest,"I wonder what motivation law enforcement could have here...

......

................",1
post5hb,richly branching,1.5948230389474962,highest,Isn't everyone just a different shade of brown ?,1
post5hb,richly branching,1.5948230389474962,highest,I just picture Dubas on the phone anyway :],1
post5hb,richly branching,1.5948230389474962,highest,"I feel like that's a weird sentence.  Black people are brown.  We just call them black.  Like maybe say facial recognition can't tell black, Latino, Indians and Middle Easterners apart.  I mean that's awkward too, so I don't really know...",1
post5hb,richly branching,1.5948230389474962,highest,"FYI ""Brown people"" is offensive to brown people.",1
post5hb,richly branching,1.5948230389474962,highest,They spent so long trying to get us to forget color...,1
post5hb,richly branching,1.5948230389474962,highest,"Facial rec uses facial mapping not color.

Similar to fingerprint matching.",1
post5hb,richly branching,1.5948230389474962,highest,Not like it’s doing any worse than they were,1
post5hb,richly branching,1.5948230389474962,highest,Nothing wrong here/s it's just like a cop but 1000x faster.,1
post5hb,richly branching,1.5948230389474962,highest,I mean... haven't cops always been that way?,1
post5hb,richly branching,1.5948230389474962,highest,"Hmmm, I guess the computer thinks they all look like criminals",1
post5hb,richly branching,1.5948230389474962,highest,Facial recognition doesn't look at your skin color though.,1
post5hb,richly branching,1.5948230389474962,highest,Excuse for brown people to commit crime #2453366743,1
post5hb,richly branching,1.5948230389474962,highest,white != notWhite,1
post5hb,richly branching,1.5948230389474962,highest,Reddit recommends you treat people differently based on how dark you perceive them. Good to know,1
post5hb,richly branching,1.5948230389474962,highest,wtf is the difference,1
post5hb,richly branching,1.5948230389474962,highest,And what's race got anything to do with finding criminals. Hell eye witness reports cant even remember ~~who~~what their robber looked like 5 minutes after it happened,1
post5hb,richly branching,1.5948230389474962,highest,"Well, race is a social construct, so I don’t see the big deal.",1
post5hb,richly branching,1.5948230389474962,highest,"So it’s not biased?

Why is that a problem?",1
post5hb,richly branching,1.5948230389474962,highest,"They all look the same to me, you know white people look the same to me.",1
post5hb,richly branching,1.5948230389474962,highest,"Wait, there’s actually black people?!",1
post5hb,richly branching,1.5948230389474962,highest,I mean that's cause black isn't black its brown. Brown is more like tan. Only real black people are shadow people. Beyond that your brown.,1
post5hb,richly branching,1.5948230389474962,highest,"""If he's brown: gun him down. If he's black: aim for the back""

- Cops",1
post5hb,richly branching,1.5948230389474962,highest,"""If you aint white, you're brown.""   -- A lot of white people I have known",1
post5hb,richly branching,1.5948230389474962,highest,Shouldn’t this say African American from other minorities?,1
post5hb,richly branching,1.5948230389474962,highest,Not unless they're immigrants.,2
post5hb,richly branching,1.5948230389474962,highest,This article is from Britain about Britain.,2
post5hb,richly branching,1.5948230389474962,highest,This is a nonsense article. People realize that any “hit” is also screened by a human right? It’s literally no different than comparing an old timey wanted poster to a person except there’s a computer providing alerts when it thinks there’s a possible match.,1
post5hb,richly branching,1.5948230389474962,highest,"Does it really make a difference? 

&#x200B;

I mean, you're getting shot either way.",1
post5hb,richly branching,1.5948230389474962,highest,"this is the first time I read there is a distinction between ""black"" and ""brown"" people. what is this? skin tones?",1
post5hb,richly branching,1.5948230389474962,highest,Neither can the police.,1
post5hb,richly branching,1.5948230389474962,highest,ai is based,1
post5hb,richly branching,1.5948230389474962,highest,Tbf most cops can't either.,1
post5hb,richly branching,1.5948230389474962,highest,doesn't that mean the facial recognition is LESS racist?  All you human meat bags look alike,1
post5hb,richly branching,1.5948230389474962,highest,"Listen, listen they already get special treatment. They can take one L for god’s sake /s",1
post5hb,richly branching,1.5948230389474962,highest,"This software us used to search social media for matches, so they can execute a search warrant based on biased data, and put innocent people at risk.

The failure rate for black and brown images is high. Sending police to the home an innocent black or brown person, will end in death, 80% of the time.

Currently, prisons and jails are filled with black and brown men that did not commit the crimes charged with, but they all look alike, to witnesses and computer programmer's it appears.

Maybe police should use police skills to apprehend criminals, instead of depending on technology, to do their job for them. Planned obsolescence.",1
post5hb,richly branching,1.5948230389474962,highest,Thats because no one is actually black just really dark brown,1
post5hb,richly branching,1.5948230389474962,highest,"They're called Arabic, my friend",1
post5hb,richly branching,1.5948230389474962,highest,So Brexit happens and is swiftly followed by creepy authoritarian surveillance.  Holy fuck how could a place spiral into shit so quick. You'd swear an earthquake hit it.,1
post5hb,richly branching,1.5948230389474962,highest,"""Swiftly followed by."" You're something like 15 years behind the curve on this, champ.",2
post5hb,richly branching,1.5948230389474962,highest,"I might be, I had this high idea of the UK. Guess I was wrong is all.",3
post5hb,richly branching,1.5948230389474962,highest,That's what happens when white people make AI,1
post5hb,richly branching,1.5948230389474962,highest,"Anyone is free to make AI, it just seems only a few are capable",2
post5hb,richly branching,1.5948230389474962,highest,"Anybody is welcomed to make new tech, but yeah, probably whites or Asians.",2
post5hb,richly branching,1.5948230389474962,highest,Maybe blacks should make something for once like their own ai,2
post5hb,richly branching,1.5948230389474962,highest,Welcome to AmeriKKKa!,1
post5hb,richly branching,1.5948230389474962,highest,"I mean the police don't discriminate anyway, so it's not like it matters. They'll beat anyone who's black.",1
post5hb,richly branching,1.5948230389474962,highest,"Watched a black girl try and race bait a black cop out of a ticket.

People seem to react a little differently when you say that to their face without a screen.",2
post5hb,richly branching,1.5948230389474962,highest,"I'm assuming I've offended you. This is what is known as a ""joke"" and it is a common enough thing to find on the internet, yet people seem to frequently forget.

What I'm doing right now is ""sarcasm"" and I'm speaking to you this way because you're annoying.",3
post5hb,richly branching,1.5948230389474962,highest,Hey a false positive is still a positive if you need to fill jails. And if you work for a violent racist organization that thinks rape is a perk of their job.,1
post5hb,richly branching,1.5948230389474962,highest,Cops don’t give a shit about the law and details,1
post5hb,richly branching,1.5948230389474962,highest,There are as many shades of white as there are shades of black but that’s not as clickbaity because white people dont make skin colour an issue.,1
post5hb,richly branching,1.5948230389474962,highest,"> Studies found people from ethnic minorities were 100 times more likely to be misidentified than white individuals by facial recognition software”


Litteraly the first fucking highlighted quote of the article",2
post5hb,richly branching,1.5948230389474962,highest,"And ethic minority is anyone that is considered not Western European when actually there is a progression of colour change from European to Middle East to African, calling someone a minority is stupid, everyone is a minority.",3
post5hb,richly branching,1.5948230389474962,highest,"So, the police can’t tell them apart either",1
post5hb,richly branching,1.5948230389474962,highest,"Shrug...""same difference"" - some cop probably",1
post5hb,richly branching,1.5948230389474962,highest,I dont think facial recognition relies on color in a technical basis at all since this systems are put to recognize photos in very different environments.,1
post5hb,richly branching,1.5948230389474962,highest,"The problem that the article mentions is that, basically, if the software gets two photographs of two random black people and two random white people, it's more likely to give a false positive to the two black people than to the two white people. Likely because the software was probably not properly trained to detect differences between people who have darker skin tones, and likely just lumps darker-skinned people together in broad strokes as a result.",2
post5hb,richly branching,1.5948230389474962,highest,They rely on contrast from facial topology. Camera sensors are awful at picking out detail in darker images.,2
post5hb,richly branching,1.5948230389474962,highest,"Well of course they are.  They can’t tell the difference, either.",1
post5hb,richly branching,1.5948230389474962,highest,Why would facial recognition need to be racist,1
post5hb,richly branching,1.5948230389474962,highest,So cops will use it.,2
post5hb,richly branching,1.5948230389474962,highest,"Regular people can't either, why should AI be any different? In HS me and my buddy would go to gas stations with our fake IDs and if they person working was black I'd go in and if the person was white my buddy would do in. 95% of the time they'd assume it was close enough or a bad picture and sell you alcohol.",1
post5hb,richly branching,1.5948230389474962,highest,Asian people look crazily similar from my perspective. It’s natural.,2
post5hb,richly branching,1.5948230389474962,highest,"I mean it can tell it's not a white person, what more do you need as police?",1
post5hb,richly branching,1.5948230389474962,highest,Well if it ain't broke don't fix it,1
post5hb,richly branching,1.5948230389474962,highest,I don't buy it. I know how the algos work. The only place bias can come from is the training set. So about the only likely issue is that there are more white people in the training data... Which likely represents the proportion in the population.,1
post5hb,richly branching,1.5948230389474962,highest,"It might match the proportions of the population, but if it doesn't allow the algorithm to correctly match all people in a given city, then it's still a poor training set. The existence of a variety of samples in a training set for a computer program shouldn't be dependent on census data for a particular city; it should be based on whatever number is necessary for the program to be able to effectively ID almost all (I.e. at least 95%) persons the software could encounter. Otherwise the software is completely useless for its intended purpose",2
post5hb,richly branching,1.5948230389474962,highest,Not useless. I'm sure they are happy to have some false positives.,3
post5hb,richly branching,1.5948230389474962,highest,"Hey patrick, what am I?

Uh, Brown?

No, I'm Black!

What's the difference?!",1
post5hb,richly branching,1.5948230389474962,highest,"I mean neither can the police, so really, we’ve achieved nothing",1
post5hb,richly branching,1.5948230389474962,highest,The tech must be racist /s,1
post5hb,richly branching,1.5948230389474962,highest,isnt that good tho? arent we pushing the narative that there are no biological differences between nationalitlies/ ethnic backgrounds/ race?,1
post5hb,richly branching,1.5948230389474962,highest,It’s not a matter of detecting race. It can read every single detail that makes each face unique to better give the authorities the exact match of the person they are looking for.,1
post5hb,richly branching,1.5948230389474962,highest,I am getting really tired of hearing the phrase “black and brown.”,1
post5hb,richly branching,1.5948230389474962,highest,So you're tired of accurate descriptions just because you feel some type of offended?,2
post5hb,richly branching,1.5948230389474962,highest,I don’t really view them as accurate.,3
post5hb,richly branching,1.5948230389474962,highest,What color is kevin hart?,4
post5hb,richly branching,1.5948230389474962,highest,TIL facial recognition software is racist and the police should be praised for looking passed the racism and using it anyway. Police don't see race. /s,1
post5hb,richly branching,1.5948230389474962,highest,"Facial recognition can, maybe not their facial recognition.",1
post5hb,richly branching,1.5948230389474962,highest,Police only use facial recognition to get possible suspects. It's not like they go and arrest a person based on the tool. They then have to have a human expert compare the image with the possible suspect and continue investigating.,1
post5hb,richly branching,1.5948230389474962,highest,So racial profiling that only affects white people? Ok,1
post5hb,richly branching,1.5948230389474962,highest,"To be fair, neither can some police officers.",1
post5hb,richly branching,1.5948230389474962,highest,You...You mean there’s a difference...,1
post5hb,richly branching,1.5948230389474962,highest,"Not a bug, but a feature; because everyone knows black and brown people are guilty anyway. 

/s",1
post5hb,richly branching,1.5948230389474962,highest,Proof that they do look similar and it’s not racist to think so 😊😊😊,1
post5hb,richly branching,1.5948230389474962,highest,"Voice recognition system also have trouble with non American accents, it's almost like these systems are not totally optimized for every variable, well I guss that's just how software is...",2
post5hb,richly branching,1.5948230389474962,highest,"That's exactly it, the people training the models ..forget or ignore people who aren't like them. This is not the first time, it's a recurring well-reported problem in tech circles.",3
post5hb,richly branching,1.5948230389474962,highest,"I don't know about you guys, but as a Norwegian it REALLY pisses me off when people think I'm Swedish.  
Worst is when they just assume I'm Irish because of the strawberry blonde hair. Damn racists. It's so offensive.  
I'm glad people are outraged for me. It really vindicates my anger!",1
post5hb,richly branching,1.5948230389474962,highest,What does this comment even imply? That you will be mistaken for another criminal?,2
post5hb,richly branching,1.5948230389474962,highest,"weak sauce bait just like the implication in the post title.  
But in case you really aren't smart enough to understand, what it implies is that the difference between myself and a Swede is the same as the difference between a Sudanese and a South Sudanese.  
That is to say, it's not racist to be unable to distinguish them reliably.  
Unless you have a narrative to push which it sounds like you might from the condescending tone of your reply to a perfectly reasonable sarcastic comment pointing out the stupidity of the accusation in this post title.",3
post5hb,richly branching,1.5948230389474962,highest,"Or or the facial recognition system was designed by white people and used white faces to develop leading to the software being less effective at telling brown people apart, the facial tech in China is much better then this, not saying its racist, just pointing out this can be a fact of limited software development and time.",4
post5hb,richly branching,1.5948230389474962,highest,"The only reason I've not shown interest to women who've shown initial interest is because I've not found anyone who's more attractive than my ex. Those who are more attractive haven't shown any interest at all. Hope that helps you picking your next candidate. Surveillance dudes or google or whatever should have a good idea of what facial features I find attractive. I'd say eye size is probably the most important feature for me, which is why I find most Asians not very attractive at all.",1
post5hb,richly branching,1.5948230389474962,highest,"In all fairness, neither can police.",1
post5hb,richly branching,1.5948230389474962,highest,"Um, they aren't using it for that purpose.",1
post5hb,richly branching,1.5948230389474962,highest,Bernie Sanders is the only presidential candidate who seeks to ban facial recognition.,1
post5hb,richly branching,1.5948230389474962,highest,[removed],2
post5hb,richly branching,1.5948230389474962,highest,Bernie Sanders and Jeremy Corbyn are just two personas of one man.,3
post5hb,richly branching,1.5948230389474962,highest,Who cares? I'm pretty sure facial recognition tech being used in the US would be at odds with our fourth amendment anyway.,4
post5hb,richly branching,1.5948230389474962,highest,"Umm.... should they be able to tell them apart?
I’d rather these programs not take skin color into any sort of consideration anyway.

Wait a minute, was this a hidden r-upliftingnews entry?",1
post5hb,richly branching,1.5948230389474962,highest,"brown people got straight hair and different nose

and they actually like being in america",1
post5hb,richly branching,1.5948230389474962,highest,Good. Fuck black and brown people.,1
post5hb,richly branching,1.5948230389474962,highest,I'm stupid and I think this is racist!,1
post5hb,richly branching,1.5948230389474962,highest,"Because it's an arbitrary, pseudo-scientific, moronic attempt of segregation, and there is no valuable, scientific, factual criteria for the existence of different races, this being the consensus of the scientific community since more than two centuries ago. I mean actual scientists, not undocumented racist redneck barbarians.

Plus, and I can not stress this enough, there is NO BROWN people. They are caucasian/ white. And also white people has the ability to get tanned.",1
post2hb,richly branching,1.567806103670254,highest,"Welcome to r/science! This is a heavily moderated subreddit in order to keep the discussion on science. However, we recognize that many people want to discuss how they feel the research relates to their own personal lives, so to give people a space to do that, **personal anecdotes are now allowed as responses to this comment**. Any anecdotal comments elsewhere in the discussion will continue to be removed and our [normal comment rules]( https://www.reddit.com/r/science/wiki/rules#wiki_comment_rules) still apply to other comments.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/science) if you have any questions or concerns.*",1
post2hb,richly branching,1.567806103670254,highest,"The biggest problem is humans are getting way too overly sensitive and have began labeling every word, sentence, and action under the sun as racist, sexist, or some other kind of ist just because they don't like something or feel some type of way about it. The liberal progressive mindset is going so far to the left that it's come around full circle on the other side and is starting to become eerily similiar to the mindset of traditional uptight religious conservatives. The people who have always been offended by everything and have always tried to destroy anything they don't like or anyone that doesn't think, feel, and share the exact same beliefs as they do. It's hilarious, sad, and ironic all at the same time. 

Whatever is going on with these AI programs I'm quite positive is only racist or sexist by these new crazy standards that are being pushed by overly sensitive insane people.",2
post2hb,richly branching,1.567806103670254,highest,"Problem is, of course, that neural networks can only ever be as good as the training data. The neural network isn't sexist or racist. It has no concept of these things. Neural networks merely replicate patterns they see in data they are trained on. If one of those patterns is sexism, the neural network replicates sexism, even if it has no concept of sexism. Same for racism.   


This is also why computer aided sentencing failed in the early stages. If you feed a neural network with real data, any biases present in the data has will be inherited by the neural network. Therefore, the neural network, despite lacking a concept of what racism is, ended up sentencing certain ethnicities more and harder in test cases where it was presented with otherwise identical cases.",1
post2hb,richly branching,1.567806103670254,highest,[removed],2
post2hb,richly branching,1.567806103670254,highest,"Precisely.  The headline is misleading at best.  I'm on an ML team at a robotics company, and speaking for us, we haven't ""decided it's OK"", we've run out of ideas about how to solve it, we try new things as we think of them, and we've kept the ideas that have seemed to improve things.  

""More and better data.""  Okay, yeah, sure, that solves it, but how do we get that?  We buy access to some dataset?  The trouble there is that A) we already have the biggest relevant dataset we have access to B) external datasets collected in other contexts don't transfer super effectively because we run specialty cameras in an unusual position/angle  C) even if they did transfer nicely there's no guarantee that the transfer process itself doesn't induce a bias (eg some skin colors may transfer better or worse given the exposure differences between the original camera and ours)  D) systemic biases like who is living the sort of life where they'll be where we're collecting data when we're collecting data are going to get inherited and there's not a lot we can do about it  E) the curse of dimensionality makes it approximately impossible to ever have enough data, I very much doubt there's a single image of a 6'5"" person with a seeing eye dog or echo cane in our dataset, and even if there is, they're probably not black (not because we exclude such people, but because none have been visible during data collection, when was the last time you saw that in person?).  Will our models work on those novel cases?  We hope so!",2
post2hb,richly branching,1.567806103670254,highest,"So both human intelligence and artificial intelligence are only as good as the data they're given. You can raise a racist, bigoted AI the same in way you can raise a racist, bigoted HI.",3
post2hb,richly branching,1.567806103670254,highest,"The difference is, a human can be told that racism is bad and might work to compensate in the data. With an AI, that has to be designed in from the ground up.",4
post2hb,richly branching,1.567806103670254,highest,"Sort of, except I don't love the framing of human racism as data-driven. It isn't really; humans employ biases and heuristics vigorously when interpreting data.",4
post2hb,richly branching,1.567806103670254,highest,"Who knew intelligence isn't wisdom. We have AI but now we need AW.

Being able to morph and utilize data: intelligence.

Understanding when to do it and when not: wisdom.",4
post2hb,richly branching,1.567806103670254,highest,"But a human can choose to break from their upbringing and traditions. It happens.

Can an AI identify bias in its data, and choose to deviate from it? Maybe that's the next step in AI",4
post2hb,richly branching,1.567806103670254,highest,‘robots’ in the post title has the potential for more depth of interpretation.,4
post2hb,richly branching,1.567806103670254,highest,"Maybe it's time to shift focus from training AI to make it useful in novel situations to gathering datasets that can be used in a later stage to teach AI, where the focus is getting as objective a data set as possible? Work with other fields etc.",3
post2hb,richly branching,1.567806103670254,highest,"You mean manually curating such datasets?  There are certainly people working on exactly that, but it's hard to get funding to do that because the marginal gain in value from an additional datum drops roughly ~~logarithmically~~ exponentially (ugh, it's midnight and apparently I'm not braining good), but the marginal cost of manually checking it remains fixed.",4
post2hb,richly branching,1.567806103670254,highest,"Nah, the key is to not trust some algorithm to be a neutral arbiter because no such thing can exist in reality. Trusting some code to solve racism or sexism is just passing the buck onto code for humanity’s ills.",4
post2hb,richly branching,1.567806103670254,highest,"This is a bit of a naive understanding of the problem, akin to people pointing to “the algorithm” as what decides what you see on social media. There aren’t canonical datasets for different tasks (well there generally are for benchmarking purposes but using those same ones for training would be bad research from a scientific perspective) novel applications often require novel datasets, and those datasets have to be gathered for that specific task. 

constructing a dataset for such a task is definitionally not something you can do manually, otherwise you are _still_ imparting your biases on the model. constructing an objective dataset for a task relies on some person’s definition of objectivity. Oftentimes, as crappy as it is, it’s easier to kick the issue to just reflecting society’s biases.

what you are describing here is not an AI or data problem but rather a societal one. Solving it by trying to construct datasets just results in a different expression of the exact same issue, just with different values.",4
post2hb,richly branching,1.567806103670254,highest,"It doesnt have a big return and the people curating can include biases.

Plus If I want people tailored for my company, I want people that will fit MY company, not a generalized version of it, so many places would be agaisnt using those objective datasets, because they dont fit their reality as well as the biased dataset",4
post2hb,richly branching,1.567806103670254,highest,Ehhh… the datasets we have are plenty objective.,4
post2hb,richly branching,1.567806103670254,highest,"Perhaps the answer for now is that we shouldn't be making AIs for production with any strict rules when there's a risk of discriminatory biases. We as a species have a habit of always trying to produce more, more optimally, more effortlessly, and we want to find new things to sell, to optimize, to produce.

But we don't really need to. We do not need AIs that filter job candidates (aside of maybe some sort of spam spotting AIs and the like), we do not need AIs that decide your insurance rate for you, we do not need AIs that play with your kid for you.

Yet we want these things but why? Are they *really* going to make the world into a better place for all its inhabitants?

There's a ton of practical work with AIs and ML that doesn't need to include the problem of discrimination. Product QA, recognizing fractures from X-rays, biochemistry applications, infrastructure operations optimization, etc etc.

Sure, this is something worth of studying, but what we really need is a set of standards before potentially dangerous AIs are put into production. And by potentially dangerous, I mean also AIs that may produce results interpretable as discriminatory - discrimination *is* dangerous.

It's up to the professionals of the field to say ""no, we can't do that yet reliably enough"" when a client asks them to do an AI that would most likely have discriminatory biases. And it's up to the researchers to keep informing the professionals about these risks.",3
post2hb,richly branching,1.567806103670254,highest,"> Perhaps the answer for now is that we shouldn't be making AIs for production with any strict rules when there's a risk of discriminatory biases.

That's pretty much how it's always done, which is why it is able to learn biases.  Take the systemic bias case, where some individuals are at more liberty to take leisurely strolls in the park.  If (for perfectly sane and innocent reasons) parks are where it makes sense to collect your data, you're going to end up with a biased dataset through no fault of your own, despite not putting any strict rules in.

> It's up to the professionals of the field to say ""no, we can't do that yet reliably enough"" when a client asks them to do an AI that would most likely have discriminatory biases. And it's up to the researchers to keep informing the professionals about these risks.

There's more to it than that.  Let's assume that there's good money to be made in your robotic endeavor.  And further lets assume that the current professionals say ""no, we can't do that yet reliably enough"".  That creates a vacuum for hungrier or less scrupulous people to go after the same market.  And so one important question is the public as a whole better off with potentially biased robots made by thoughtful engineers, or with probably still biased robots made by seedier engineers who assure you that there is no bias?  It's not like you're going to convince _everyone_ to step away from large piles of money (and if you are I can think of better uses of that ability to convince).",4
post2hb,richly branching,1.567806103670254,highest,">Perhaps the answer for now is that we shouldn't be making AIs for production with any strict rules when there's a risk of discriminatory biases.

I don't see why when people aren't free from biases either. I think it's more that the decisions and processes need to be set up in a way that considers the possibility of biases and attempts to correct or sidestep them. 

And calling out an AI on its biases may be easier than calling out a person - as long as we no longer think AI's are unbiased.",4
post2hb,richly branching,1.567806103670254,highest,This is not reassuring and honestly convinces me more that those folks doing AI work are playing with fire,3
post2hb,richly branching,1.567806103670254,highest,"A significant portion, if not most people who do AI-related work, do it on stuff that isn't necessarily impacted by this stuff. But that's all you read about in the news because these headlines sell.

Training a model to play games (chess/go etc.), image analysis (satellite imagery for climate impacts), science modelling (weather forecasting/astrophyics etc.), speeding up your phone/computer (by optimising app loading etc.), digitising hand-written content, mapping roads (google maps etc.), disaster forecasting (earthquakes/flooding), novel drug discovery.

There are certainly more areas that I'm forgetting, but don't be fooled into thinking (1) that ML isn't already an everyday part of your life and (2) that all ML research has the same societal negatives.",4
post2hb,richly branching,1.567806103670254,highest,"Don't worry, I'm sure one day we can get sentient AIs that hate all humans equally!",4
post2hb,richly branching,1.567806103670254,highest,"Yup. “We know it’s not ok, but we’ll move forward regardless”.",4
post2hb,richly branching,1.567806103670254,highest,"If it helps, human brains have a lot of these same issues (they're just slightly more subtle due to the massive data disparity), and that's gone perfectly.  Definitely no cases of people ending up as genocidal racists.  Definitely no cases of that currently happening in China.  We're definitely smart enough to avoid building nukes, or at the very least to get rid of all the nukes we have.

If doing AI work is playing with fire, doing human work is playing with massive asteroids.

A fun game to play is, whenever you see robots or aliens in a scary movie, try to work out which human failing it is they're the avatar of.",4
post2hb,richly branching,1.567806103670254,highest,"Yeah, I think the onus is less on the devs, since we're a long way off created impartial AI, and more on enforcing a code of ethics on what AI can be used for.

If your face recognition technology doesn't work on black people very well, then it shouldn't be used by police to identify black suspects, or otherwise come attached to additional manual protocols to verify the results for affected races and genders.

The main problem is that companies are selling these things to public housing projects primarily populated by black people as part of the security system and acting confused when it randomly flags people as shoplifters as if they didn't know it was going to do that.",3
post2hb,richly branching,1.567806103670254,highest,"You can't expect companies to pay you hundreds of thousands of dollars to create an AI and not turn around and use it.  Diffusion of blame is how we justify evil outcomes.  If you know it's impossible to not make a racist AI, then don't make an AI.",4
post2hb,richly branching,1.567806103670254,highest,"Have you considered that intelligence, which includes experience-based judgement, is inherently biased?  Sounds like you're trying to make something artificial, but not necessarily intelligent.",3
post2hb,richly branching,1.567806103670254,highest,">we haven't ""decided it's OK"",

You're simply going ahead with a flawed product that was supposed to compensate for human flaws and failings, but will now reproduce them only with greater expediency. Cool!",3
post2hb,richly branching,1.567806103670254,highest,"Arguing it's not technically racist is completely unelpful and puts the focus on the wrong aspect of the problem. These things can have enormous impacts on our lives so it really doesn't matter how it *actually* works when it's *literally* not working properly. 

Facial recognition being a prime example. The miss rate on light skin people alone is too high let alone the abysmal rate for darker skin tones yet it's commonly used by law enforcement for years now. Those people sitting in jail from this one technology don't care that the AI isn't actually racist. The outcomes are and that's literally all that matters. It doesn't work, fix it or trash it.",3
post2hb,richly branching,1.567806103670254,highest,"> It doesn't work, fix it or trash it.

Agreed.  It's just that fixing it requires lots trial and error, and that takes a long time.  The real problems with facial recognition aren't in the technology, they're in idiots using tools for more than they're capable of doing.",4
post2hb,richly branching,1.567806103670254,highest,"In this case is the curse of dimensionality the fact that the global sample is only 7 billion people, which represents a very tiny fraction of all possible configurations of all characteristics being tracked?",3
post2hb,richly branching,1.567806103670254,highest,[deleted],3
post2hb,richly branching,1.567806103670254,highest,"> Why give an AI any data not required in sentencing. If the AI doesn’t know the race or gender of the defendant, it can’t use it against them.

That's not strictly true.  Let's say you have two defendants, one was caught and plead to possession with intent to distribute crack cocaine, and the other was caught and plead to possession with intent to distribute MDMA.  From that information alone you can make an educated guess (aka a Bayesian inference) about the race and gender of both defendants, and while I don't have actual data to back this up, you'd likely be right a statistically significant portion of the time.",4
post2hb,richly branching,1.567806103670254,highest,"It sounds like you have 100% decided it's okay. You don't like it, but you don't consider it a deal breaker either. Not desirable, but acceptable.

I understand you have constraints you are working under and I have no doubt that you would like to see the issues of racism and bias in AI resolved. But the simple fact is that AIs are being designed to be racist and there will be real consequences. People won't be able to get jobs or health care or will get denied loans or suffer longer prison sentences.

Again, I understand that you aren't in a position where you can fix it. But shrugging and hoping the problem will get addressed? That's saying it's okay if it doesn't. It's tolerable. So saying that AI researchers think it's okay is a fair characterization.

Whether you have malice in your heart or not matters not-at-all to the companies who will use AI in the pursuit of profit. The travel companies pushing Vegas trips on a discount at people with manic-depression or pushing people into high-engagement communities even if they are cults or white nationalists.",3
post2hb,richly branching,1.567806103670254,highest,"I just want to point out that data augmentation is a thing, but otherwise good summary.",3
post2hb,richly branching,1.567806103670254,highest,Isn’t it possible to “feed” a posterior law that sits in front of the data kind of in a Bayesian mindset?,3
post2hb,richly branching,1.567806103670254,highest,"Great question, I'll come back to it when I get back from work (leaving this comment to remind myself)",4
post2hb,richly branching,1.567806103670254,highest,"Kind of, there is room to feed stuff in like that, but it's difficult to figure out precisely what to feed in.  Most things you might want to feed in there can also be expressed in your cost function, which means they can be included in the training process directly.  Ideas for what you feed in get tried pretty regularly, it's not solved, but some of them do work.",4
post2hb,richly branching,1.567806103670254,highest,"The way to solve it is get tech ethicists into positions of power to address systemic issues. You, personally, cannot solve this. *Your team cannot solve this.* Big power players in tech have to solve this, and that begins with hiring-on people like Timnit Gebru and not firing them; looking at you, Google.

This is a fully top-down issue.",3
post2hb,richly branching,1.567806103670254,highest,Maybe stop using data generated by Americans?,3
post2hb,richly branching,1.567806103670254,highest,Because there's no racism anywhere except in the US.,4
post2hb,richly branching,1.567806103670254,highest,How about we stop considering the americans altogether,4
post2hb,richly branching,1.567806103670254,highest,"Paraphrase: We can't be bothered to spend the time and money to assemble a dataset that doesn't contain bigoted biases so we're going to release a product the replicates bigotry anyway.

Assembling good high quality datasets that can be used for machine learning is expensive and decades long work. I wish more computer science students understood this.",3
post2hb,richly branching,1.567806103670254,highest,Have you tried buying synthetic data?,3
post2hb,richly branching,1.567806103670254,highest,"The trouble there is that it has to be synthesized to represent our robot's view on the world, which currently none are, so we're working on building that capability to make it ourselves.",4
post2hb,richly branching,1.567806103670254,highest,AI random character creator. Create your own diverse dataset. One to rule them all!,3
post2hb,richly branching,1.567806103670254,highest,"We need to think differently from statistical averages being the Truth, but that is how our society is ordered, even if it is not really how it is lived. The discrepancy between the two has always enraged people when it's pointed out that data is not 3-dimensional, because so much money and status is involved.

The short cuts to understanding that data sets offer have helped create a more efficient world. But their limitations have always been downplayed by those who insist they offer more than they can.",3
post2hb,richly branching,1.567806103670254,highest,"As a layman, I've only thought of it at a newbie level ;_;

I guess it's basically like set theories where you can get an exclusion, or a merge, but trying to only alter 'half' the set means having to try and find some way to create a new set entirely. If only we could source the most racist and sexist data possible (basically like pulling all Proud Boy and other ultra-exclusionary groups messages/decisions/etc) so we could make it adversarial to the training of the data.

I can bet the ""we try new things as we think of them"" means it's been an absolutely exhausting and draining to keep throwing stuff at the wall trying to find what sticks. ;_;",3
post2hb,richly branching,1.567806103670254,highest,Can you hook me up with a ML engineering job?,3
post2hb,richly branching,1.567806103670254,highest,"Can you generate randomized data?

I am spit-balling here, I realize.


First, this seems like a great way to sniff out institutional racism. Take a data set, the more narrow the better, and extrapolate out if it causes a racist/sexist outcome. Boom! Data set had intrinsic racism/sexism.

So, how to ""erase"" the systemic nature? That is tough, but I suspect it shows in a few ways... outlier extremes, frequency of variation from the mean, selection bias. Of those, I feel like the selection bias would be impossible to erase, but the other two could be handled by some statistical selection... Basically, select out some amount of extremes and artificially reduce the number of one group varying from the mean more than the others.

Then, run the test for lots of randomized trials and see if there is a racist/sexist bias. When you get an AI that doesn't do that, you have found the right starting artificial data set to remove the institutional bias.


But... that sounds really time intensive and expensive.

Maybe we could put an AI on it. hehe",3
post2hb,richly branching,1.567806103670254,highest,"I think the point of the claim is that by pushing forward anyway, despite being unable to solve it, you have decided you’re ok with it. *Not* building is an option, but—no offense intended—not one that an ML team at a robotics company would likely consider seriously. Compare: If we considered such a system to be nonfunctional or dangerous in the way we do a car without seatbelts, it could not go to market (despite having been thought ok in the early days of cars). That’s part of the critique.",3
post2hb,richly branching,1.567806103670254,highest,">""More and better data."" Okay, yeah, sure, that solves it, but how do we get that?

Synthetic data.

Fill-in the gaps of your real-world collected data with computer generated data",3
post2hb,richly branching,1.567806103670254,highest,"To me it's simply a matter of distinguishing these two requests:

""Show me the face that is most beautiful""

""Show me the face that is most beautiful according to the majority of Brazilians""

First request has no answer and the robot shouldn't answer it. Second request has a valid answer which the robot can provide.

It is not about eliminating bias, it is about making it clear that it is there.",3
post2hb,richly branching,1.567806103670254,highest,Honestly they’ve know that this information was biased based on human implicit bias’ years ago and kept going but there was no profitable way to fix that unfortunately / job creation there .  There is a lot more profit in marketing by demographic so I kinda want to blame that but can be it wrong . In any case it seems humans are left best for those novel cases /exceptions as a default and or the engineering teams have to think of a procedure beforehand  and just in case . Just hope it doesn’t mess anyone up too badly getting caught in a weird loop or non existent solution.,3
post2hb,richly branching,1.567806103670254,highest,"Dall-E Can imagine it, it can be true",3
post2hb,richly branching,1.567806103670254,highest,">Precisely.  The headline is misleading at best.  I'm on an ML team at a robotics company, and speaking for us, we haven't ""decided it's OK"", we've run out of ideas about how to solve it, we try new things as we think of them, and we've kept the ideas that have seemed to improve things.

There is a solution though. If you can't make unbiased AI, you don't use it at all.

If you still use it in your products and then say you're trying to solve the problem you're being disingenuous and ethically dubious. 

The headline isn't really misleading. Some companies might act appropriately, but many aren't.",3
post2hb,richly branching,1.567806103670254,highest,"That's black and white thinking, and it holds you back.  Let's say that you're building a robot train, and you tell it not to hit people.  Let's further say that your robot is better at spotting white people at distance that black people which manifests as stopping with 10ft to spare for white people and 9'6"" to spare for darker people.  It is a clear bias.  But at the same time, you're still stopping for everyone.  Should that 6"" really derail a project?",4
post2hb,richly branching,1.567806103670254,highest,"Just because YOU can't solve the issue posed doesn't somehow mean you aren't doing exactly what you were accused of. You literally just admitted the base data itself is flawed so maybe instead of trying to force through a product that's guaranteed not to function 100% as intended, you could work on fixing the data or obtaining more. The original accusations was that you guys are passing off broken racist AI as a finished product and you are which you admitted in your post and then said it's impossible to fix essentially. Just because you work for a company doesn't mean you need to come on the internet and lick boot Infront of us for them.",3
post2hb,richly branching,1.567806103670254,highest,"I agree with what you’re saying. However, I ask, what is the point of these bots in the first place? What goals are we even trying to reach?

All I see bots do is make trashy comments and poison the well by spreading harmful propaganda. For what? Boost people’s follower count?",3
post2hb,richly branching,1.567806103670254,highest,"Oh, our bots aren't software bots, ours weigh hundreds of pounds each and can go well over 10mph off road.  If you're asking for a defense of public opinion shaping bots I believe they're a cancer, and the people responsible for creating them should be deported to... say... the Mariana trench.",4
post2hb,richly branching,1.567806103670254,highest,"I feel like you have to have some event driven programming to compensate for the ML datasets. In other words, a function to filter certain responses. There is an eng geek out there who will someday solve this problem, but, for now we should bandage the issue.",3
post2hb,richly branching,1.567806103670254,highest,">we haven't ""decided it's OK"", we've run out of ideas about how to solve it

...and then decided to go ahead anyway.

So you have actually decided it's OK. After all you tried your best! But you still gotta sell that product, and that's of course more important than the problem at hand. So you're trading money for morals.",3
post2hb,richly branching,1.567806103670254,highest,"> to go ahead anyway

Go ahead with what, exactly?  Further development work?  Additional data gathering?  Taking it seriously?  Because yeah, we're full steam ahead on all of those things.",4
post2hb,richly branching,1.567806103670254,highest,"I don't think it's misleading. A decision with a racist outcome is a racist decision. People who are interpreting that to mean ""a decision was made by a computer with racist intent"" are reading it incorrectly, because they're not understanding one of:

* AIs don't make ""decisions"" like humans
* something doesn't have to have racist intent to have racist outcomes (and thus, be racist)",3
post2hb,richly branching,1.567806103670254,highest,I have an awesome idea. Let’s have humans to the judging of other humans. Your welcome.,3
post2hb,richly branching,1.567806103670254,highest,"The AI just needs a virtue signaling module, that heavily weighs appearing not sexist or racist, and if the rest of the network is in conflict with it, reject that data and search for data that confirms the academic orthodoxy. That's how humans do it.",3
post2hb,richly branching,1.567806103670254,highest,"The GAPING hole in that explanation is that there is evidence that these machine learning systems will still infer bias even when the dataset is deidentified, similar to how a radiology algorithm was able to accurately determine ethnicity from raw, deidentified image data. Presumably these algorithms are extrapolating data that is imperceptible or overlooked by humans, which suggests that the machine-learning results reflect real, tangible differences in the underlying data, rather than biased human interpretation of the data.

How do you deal with that, other than by identifying case-by-case the “biased” data and instructing the algorithm to exclude it?",2
post2hb,richly branching,1.567806103670254,highest,"That is the real difficulty, and kinda what i'm trying to get at. Neural networks can pick up on things that would go straight past us. Who is to say that such a neural network wouldn't also find a correlation between punctuation and harshness of sentencing?   


I mean, we have studies proving that justice is biased on things like wether a football team won or lost the previous match if the judge was a fan of said team, so if those are things we can find, what kinds of correlations do you think could an analytical software designed by a species of intelligent pattern finders to find patterns better than we ever could find?  


In your example, the deidentified image might still show things like, say, certain minor differences in bone structure and density, caused by genetics, too subtle for us to pick out, but still very much perceivable for a neural network specifically designed to figure out patterns in a set of data.",3
post2hb,richly branching,1.567806103670254,highest,"For a while, I've been thinking along similar lines about ways to make court trials more fair - focusing on people, not AI. My core idea is that the judge and jury should never know the ethnicity of the person on trial. They would never see or hear the person, know their name, know where they live, know what neighborhood the crime was committed in, and various other things like that. Trials would need to be done via text-based chat, with specially-trained go-betweens (humans at first, AI later) checking everything that's said for any possible identifiers.

There will always be exceptions, but we can certainly reduce bias by a significant amount. We can't let perfect be the enemy of good.",4
post2hb,richly branching,1.567806103670254,highest,[deleted],3
post2hb,richly branching,1.567806103670254,highest,"Instead of handicapping the use of data I wonder if it would make more sense to break down more complex data into simplified data points. 

If you're using high level data such as race of a person then the NN will be trained on data obtained from a racist system and the outputs will perpetuate that. 

For something like a resume AI determining applicants, it might discriminate against women for things like ""lack of experience"" if there is a period of maternity leave or something. I guess what I'm saying is certain metrics are currently used for evaluation but those metrics aren't necessarily good metrics to be used. 

Its obviously not a simple issue and I'd have to spend more time thinking about what I'm trying to get across to give better examples",4
post2hb,richly branching,1.567806103670254,highest,[removed],3
post2hb,richly branching,1.567806103670254,highest,[removed],4
post2hb,richly branching,1.567806103670254,highest,"There is a difference between deidentifying and removing bias from the dataset isn’t there? One interesting example I came across recently is resuscitation of newborn babies. Where I come from there is a difference between 98% and 87% in which babies are attempted to be resuscitated between the ethnicity with the highest rate (white), and the lowest (Indian). This is due to the criteria used to determine if they attempt resuscitation, and the difference in the two distributions of babies of those ethnicities. Now if you took the data and removed the racial information, then trained a model to determine which babies should be attempted to resuscitate, you still get a racial bias don’t you? Which is to say if you run the model with random samples from those two distributions, you get two different average answers.",3
post2hb,richly branching,1.567806103670254,highest,"Maybe the disconnect is the definition of bias. It sounds like you’re suggesting that a “good” model would normalize resuscitation rates by recommending increased resuscitation of one group and/or decreased resuscitation of a different group. That discounts the possibility that there are real, tangible differences in the population groups that affect the probability of attempting resuscitation, aside from racial bias. It would actually introduce racial bias into the system, not remove it.",4
post2hb,richly branching,1.567806103670254,highest,"> similar to how a radiology algorithm was able to accurately determine ethnicity from raw, 

If 'ethnicity' wasn't fed to the algorithm then it did not do this. What likely happened is that the algorithm was trained and then in a post-hoc analysis researchers could see that it clustered together images that belonged to some ethnic groups. Which would indicate that there are some systematic difference in the radiaology images from  different groups. That's likely useful knowledge from a diagnostic perspective. And not, in and of itself, racist.

It's one thing to discover that there are indeed some systematic difference in radiology images from different ethnic groups (something that you might well hypothesis before hand). It's quite another thing to allow your AI system to make racist or sexist decisions because it can cluster datasets without explicitly including ""ethnicity"" in the training data. When we talk about an AI making sexist or racist decisions we're not talking about whether it can infer ethnicity by proxy, something that can be benign factual information. We're talking about what the whole AI system then does with that information.",3
post2hb,richly branching,1.567806103670254,highest,"To your last paragraph, im arguing that the radiology AI will make “racist” decisions that are actually just reflections of rote, non-biased data. We’re not quite at the point that the radiology AI can make recommendations, but once we get there, you’ll see people arguing that findings are being called normal or abnormal based on “biased” factors. 

Those overseeing AI development need to decide if the outputs are truly biased, or are simply reflecting trends and data that humans don’t easily perceive and subsequently attribute to some form of bias.",4
post2hb,richly branching,1.567806103670254,highest,"Let's say it was fed all information, age, sex, ethnicity, etc.  And outcomes based on the treatments that were recommended based on the images.  And this AI's job was to recommend and allocate resources based on the given  data with the goal of generating the maximum number of successful outcomes with the given resources (maybe that's a racist goal?).   If this AI began to recommend the best treatments and allocate resources to a certain group based on that data, and let's assume it achieved the desired results, is it racist?    Now let's say we remove the ethnical information from the dataset, and the results are the same (because it is able to infer it).   Is it now less racist because we withheld information?",4
post2hb,richly branching,1.567806103670254,highest,"Of course there are real, tangible differences in the data!  The impact of racism, sexism, homophobia, and other biases aren't just in our heads.  Its not just preconceived, bigoted notions about what people different from ourselves, and different from the societal ""norm"" are like.  Its also the fact that Black people are more likely to be poor and trans youth are more likely to be homeless and women are more likely to be sexually assaulted.

If you want the AI to tell you which criminals are more likely to re-offend, and give sentences accordingly, its going to sentence the black criminals more harshly.  And even if you anonymize the data, its going to pick up on all the other things that correlate with race.",3
post2hb,richly branching,1.567806103670254,highest,"I suppose the direct comparison between medical AI and criminal sentencing isn’t completely apt, but the point stands that the algorithm doesn’t make “racist” or “sexist” decisions, it simply reflects the facts that it can derive from input data. Re-offenders deserve harsher sentences, just like suspicious lung nodules deserve closer follow-up. All other factors aside, there isn’t any inappropriate bias in the algorithm or it’s decision-making process.",4
post2hb,richly branching,1.567806103670254,highest,"The effect of the bias can be as insidious as the AI giving a different sentence based solely on the perceived ethnic background of the individual's name. 

Some people would argue that the training data would need to be properly prepared and edited before it could be processed by a machine to remove bias. Unfortunately even that solution isn't as straightforward as it sounds. There's nothing to stop the machine from making judgments based on the amount of punctuation in the input data, for example.

The only way around this would be to make an AI that could explain in painstaking detail  why it made the decisions it made which is not as easy as it sounds.",2
post2hb,richly branching,1.567806103670254,highest,"Actually, there is another way. And it is fairly straightforward, but... (of course there is a but)

What you can do (and indeed, just about the only thing you can do, as far as I can tell) is to simply directly enforce the thing we supposedly want to enforce, in an explicit manner. That is, instead of trying to make the agent ""race-blind"" (a fool's errand, since modern ML methods are astoundingly good at picking up the subtlest cues in the form of slight correlations or whatever), you make sure you figure out everyone's race as accurately as you can, and then *enforce* an equal outcome over each race (which isn't particularly hard, whether it is done at training time with an appropriate loss function, or at inference time through some sort of normalization or whatever, that bit isn't really all that technically challenging to do pretty well) -- congrats, you now have an agent that ""isn't racist"".

Drawbacks: first, most of the same drawbacks in so-called affirmative action methods. While in an ideal world all races or whatever other protected groups would have equal characteristics, that's just not true in the real world. This method *is* going to give demonstrably worse results in many situations, because you're not really optimizing for the ""true"" loss anymore. 

To be clear, I'm not saying ""some races just happen to be worse at certain things"" or any other such arguably racist points. I'm not even going to go near that. What's inarguably true is that certain ethnicities are over- or under-represented in certain fields for things as harmless as ""country X has a rich history when it comes to Y, and because of that it has great teaching infrastructure and a deep talent pool, and their population happens to be largely of ethnicity Z"". 

For example, if for whatever reason you decided to make an agent that tried to guess whether a given individual is a strong Go/Baduk player (a game predominantly popular in East Asia, with effectively all top players in world history coming from the region), then an agent that matched real world observations would necessarily have to give the average white person a lower expected skill level than it would give the average Asian person. You could easily make it not do that, as outlined above, but it would give demonstrably less accurate results, really no way around that. And if you e.g. choose who gets to become prospective professional players based on these results or something like that, you will arguably be racially discriminating against Asian people. 

Maybe you still want to do that, if you value things like ""leveling the international playing field"" or ""hopefully increasing the popularity of the game in more countries"" above purely finding the best players. But it would be hard to blame those that lost out because of this doctrine if they got upset and felt robbed of a chance.

To be clear, sometimes differences in ""observed performance"" are absolutely due to things like systemic racism. But hopefully the example above illustrates that not *all* measurable differences are just due to racism, and sometimes relatively localized trends just happen to be correlated with ""protected classes"". In an ideal world, we could differentiate between these two things, and adjust only for the effects of the former. Good luck with that, though. I really don't see how it could even begin to be possible with our current ML tech. So you have to choose which one to take (optimize results, knowing you might be perpetuating some sort of systemic racism, but hopefully not any worse than the pre-ML system in place, or enforce equal results, knowing you're almost certainly lowering your accuracy, while likely still being racist -- just in a different way, and hopefully in the opposite direction of any existing systemic biases so they somewhat cancel out)

Last but not least: even if you're okay with the drawbacks of enforcing equal outcomes, we shouldn't forget that what's considered a ""protected class"" is, to some extent, arbitrary. You could come up with endless things that sound ""reasonable enough"" to control based on. Race, ethnicity, sex, gender, country of origin, sexual orientation, socioeconomic class, height, weight, age, IQ, number of children, political affiliation, religion, personality type, education level... when you control for one and not for others, you're arguably being unfair towards those that your model discriminates against because of it. And not only will each additional class you add further decrease your model's performance, but when trying to enforce equal results over multiple highly correlated classes, you'll likely end up with ""paradoxes"" that even if not technically impossible to resolve, will probably require you to stray even further away from accurate predictions to somehow fulfill (think how e.g. race, ethnicity and religion can be highly correlated, and how naively adjusting your results to ensure one of them is ""fair"" will almost certainly distort the other two)",3
post2hb,richly branching,1.567806103670254,highest,[deleted],4
post2hb,richly branching,1.567806103670254,highest,"These ideas need to be discussed more broadly. I think you have done a pretty good job of explaining why generalizations and stereotypes are both valuable and dangerous. Not just with regard to machine learning and AI but out here in the real world of human interaction and policy.

Is the discussion of these ideas in this way happening anywhere other than in Reddit comments? If you have any reading recommendations, I'd appreciate your sharing them.",4
post2hb,richly branching,1.567806103670254,highest,"This. Neural networks can pick up on any pattern, even ones that aren't there. There's studies that show sentences on days after football games are harsher if the judges favourite team lost the night before. This might not be an obvious correlation, but the networks sees it. It doesn't understand what it sees there, just that there's times of the year where, every 7 days, sentences that are given are harsher.  


In the same vein, a neural network might pick up on the fact that the punctuation might say something about the judge. For instance, if you have a judge who is a sucker for sticking precisely to the rules, he might be a grammar nazi, and also work to always sentence people precisely to the letter of the law, whereas someone who rules more in the spirit of the law might not (though this is all conjecture)",3
post2hb,richly branching,1.567806103670254,highest,"> Neural networks can pick up on any pattern, even ones that aren't there. 

This is a paradoxical statement.",4
post2hb,richly branching,1.567806103670254,highest,We are going to need psychologists for the AI.,3
post2hb,richly branching,1.567806103670254,highest,"As for how to figure out what biases the network has, one way would be to reverse it, aka instead of feeding it training data and having it generate an output out of this data, you run it in reverse and have it generate new data. If you messed with the outputs, which are now inputs, one at a time, you could see how it changes the resulting input (which, of course, is now output), but that's still complicated af.",3
post2hb,richly branching,1.567806103670254,highest,"I'm pretty sure that's impossible. Each neuron in a network has a number of inputs, and an output that is based on the inputs. It'd be like trying to solve `A = B x C x D`, but you know the value of A and want to know B, C and D.

You can't, as they depend on each other.",4
post2hb,richly branching,1.567806103670254,highest,"The actual point of Critical Race Theory is that systems can perpetuate  racism even without employing racist people, if false underlying assumptions aren't addressed.  Racist AI's perpetuating racism without employing any people at all are an extreme extrapolation of that concept.  

Addressing tainted and outright corrupted data sources is as important in data science as it is in a history class.  Good systems can't be built on a foundation of bad data.",2
post2hb,richly branching,1.567806103670254,highest,"> if false underlying assumptions aren't addressed.

They need not be false. The thing that makes this so intractable isn't the false underlying assumptions, it's the true ones. 

If an AI wants to predict recidivism, it can use a model that looks at marital status, income, homeownership, educational attainment, and the nature of the crime. 

But maleness is a strong predictor of recidivism. It's a real thing. It's not an artifact or the result of bias. Men just commit more crime. A good AI will find a way to differentiate men from women to capture that chunk of the variation. A model with sex is much better at predicting recidivism than a model without it.

So any good AI will be biased on any trait that accounts for variation. If you tell it not to be, it'll just use a proxy ""Wow! Look how well hair length predicts recidivism!""",3
post2hb,richly branching,1.567806103670254,highest,"> Men just commit more crime.

Actually it's more like men are arrested and sentenced at a higher rate (that's hard data we have). The soft data of how much crime is committed is sort of unknowable, we can make educated guesses at best.

But that's sort of the problem, just because a situation exists doesn't make it correct or a ""fact of reality"". People of color in the US tend to be poorer; that isn't an inherent property of those people but an emergent property due to other things largely out of their control such as generational wealth, etc. The problem of making choices based on ""facts"" like these is they easily becomes a self fulfilling prophecy.",4
post2hb,richly branching,1.567806103670254,highest,">The actual point of Critical Race Theory

That's a broad field without an actual point. You may as well be arguing the actual point of economics. To a Keynesian maybe it is to know how to minimize fluctuations in the economy,  to a communist it may be how to determine need and capability. A critical race theorist might write systemic racism, or they could be an advocate for standpoint epistemology, the latter of which is an anti-scientific viewpoint.",3
post2hb,richly branching,1.567806103670254,highest,"I feel like there is a real underlying point here; that is made problematic by just talking about racism. People's outcomes in life depend to a large degree statistically on their starting points. If their starting point is largely the result of racism, then those results will reflect that racism.

However, a fix that simply remixes the races doesn't necessarily deal with the underlying issue of why starting points matter so much. I would really like to see a world where everybody has opportunity, not simply one where lack of opportunity is better distributed over skin colors.

One statistic that always struck me was that the single best predictor of whether a child in a middle class house grows up to be middle class is the economic class of their grandparents.

That says a lot about starting points and the importance of social networks. It DOES perpetuate the outcomes of past racism; but in and of itself, its not racism and fixing the distribition of inequality doesn't really fix this; it just hides it.",3
post2hb,richly branching,1.567806103670254,highest,"Zero relationship to what you describe. Events which took place in history need not be removed to allow non ""currupted"" data. That makes the data completely wrong. Also data models are not humans.",3
post2hb,richly branching,1.567806103670254,highest,"I'm not advocating removing data.  I'm advocating adding data (and context).  Because those ""data models"" are called Artificial Intelligence because they ape Human Intelligence - which is just as susceptible to bad and incomplete data streams as its artificial cousins.

Also, statues are not data.",4
post2hb,richly branching,1.567806103670254,highest,"> Addressing tainted and outright corrupted data sources

See this is the problem, You aren't being honest in what the issue is. 

The data sources aren't corrupted or tainted. They are showing an accurate empirical representation of the data. The ""corruption"" comes from your disagreement with the pillars of that data, such as crime rates by ethnicity and it not being able to take into account human biases in something like policing by arbitrarily weighting things like race to skew the results to match your sensibilities. 

You and people who share your world view will never be pleased with the data unless you pre-screen it and it shows the result you want before hand, otherwise you will come up with some reason why its perpetually biased in a way you don't like.",3
post2hb,richly branching,1.567806103670254,highest,"So because I say I don't want to use corrupted data, I obviously want to corrupt the data.

The good old insightful ""I know you are but what am I?"" argument.",4
post2hb,richly branching,1.567806103670254,highest,"Remember when the self-driving cars didn’t recognize Black people as human? Why? Because no testing was done with people that weren’t White.

Edit: [Citation](https://arxiv.org/pdf/1902.11097.pdf)",2
post2hb,richly branching,1.567806103670254,highest,"\*no *training* was done with datasets containing POC. Testing is what caught this mistake.

""Training"" and ""testing"" are not interchangeable terms in the field of machine learning.",3
post2hb,richly branching,1.567806103670254,highest,Thank you for the gentle and accurate correction.,4
post2hb,richly branching,1.567806103670254,highest,"“The company's position is that it's actually the opposite of racist, because it's not targeting black people. It's just ignoring them. They insist the worst people can call it is ‘indifferent.’”",3
post2hb,richly branching,1.567806103670254,highest,"Dude, is that a ""Better of Ted"" reference?",4
post2hb,richly branching,1.567806103670254,highest,"The problem with this argument is it implies that all you need to do is give 'better' data.

But the reality is, giving 'better' data will often lead to racist/sexist outcomes.

Two common examples:

Hiring AI: when Amazon set up hiring AI to try to select better candidates, it automatically selected the women out (even if you hid names, gender, etc). The criteria upon which we make hiring decisions incorporates problems of institutional sexism, so the bot does what it is programmed to do: learn to copy the decisions humans make.

Criminal AI: you can setup an AI to accurately predict whether someone is going to commit crimes (or more accurately, be convicted of commiting a crime). And of course since our justice system has issues of racism and is more likely to convict someone based on their race, then the AI is going to be more likely to identify someone based on their race.

The higher quality data you give these AI, the more they are able to pick up the real world realities. If you want an AI to behave like a human, it will.",2
post2hb,richly branching,1.567806103670254,highest,"I think the distinction to make here is what ""quality"" data is. The purpose of an AI system is generally to achieve some outcome. If the outcome of a certain dataset doesn't fit the business criteria then I would argue the quality of that data is poor for the problem space you're working in. That doesn't mean the data can't be used, or that the data is inaccurate, but it might need some finessing to reach the desired outcome and account for patterns the machine saw that humans didn't.",3
post2hb,richly branching,1.567806103670254,highest,"I don’t think I’d consider “more biased data” as “better” data, though.",3
post2hb,richly branching,1.567806103670254,highest,Stephen Colbert said reality has a well known liberal bias. Perhaps it has a less well known sexist and racist bias.,2
post2hb,richly branching,1.567806103670254,highest,Would you say the same is true for a racists brain?,2
post2hb,richly branching,1.567806103670254,highest,"Racism IS learned behavior, yes.

Racists learned to become racist by being fed misinformation and flawed ""data"" in very similar ways to AI. Although one would argue AI is largely fed these due to ignorance and lack of other data that can be used to train them, while humans spread bigotry maliciously and with the options to avoid it if they cared.

Just like you learned to bow to terrorism on the grounds that teaching children acceptance of people that are different isn't worth the risk of putting them in conflict with fascists.",3
post2hb,richly branching,1.567806103670254,highest,"Source for that claim?

As far as I know racism and xenophobia in general are an innate fear self-protective response to the unknown.",4
post2hb,richly branching,1.567806103670254,highest,[deleted],4
post2hb,richly branching,1.567806103670254,highest,[deleted],4
post2hb,richly branching,1.567806103670254,highest,This system is based on human selection of keywords to images. Of course its going to have the human bias still. What is so difficult to understand people.,3
post2hb,richly branching,1.567806103670254,highest,"Kinda my point. It's extremely hard to develop a neural network that is unbiased, because humans have all sorts of biases that we usually aren't even aware of. There was a study done in the 70s, for instance, which showed that the result of a football game could impact the harshness of a sentence given the monday after said game.   


If you included references to dates in the dataset, the neural network wouldn't pick up on this correlation. It would only see that every seven days in certain times of the year, sentences are harsher, and would therefore emulate this bias.   


Again, the neural network has no concept of mood, and how the result of a football game can impact it, and might thus cause a judge to give harsher sentences, all it sees is that this is what is going on, and assumes that this is meant to be there.",4
post2hb,richly branching,1.567806103670254,highest,"No. AI doesn't have have sentience nor a psyche. It could be said that racism forms in a person with ""junk in,"" but they quickly become wrapped up in it, identify with it, believe in it. Racism becomes a structuring ideological fantasy for the psyche. It's not the same for AI, which will merely reflect the data neutrally, rather than believing in an idea and having that inform choices/behaviour in a generative way.",3
post2hb,richly branching,1.567806103670254,highest,[removed],2
post2hb,richly branching,1.567806103670254,highest,"Unfortunately, the word ""racist"" has at least two distinguishable meanings:

 1. Having the cognitive mindset that holds that some races are inferior to others;
 2. Any action or circumstance which tends to disadvantage one race over another.

OP is saying, quite reasonably, that neural networks are 2 but they are not 1. (That's why they literally say that NNs both ""are not racist"" and ""are racist"".)

Both concepts are useful but they're very different, and I honestly think it's significantly holding back the racism discussion that people sometimes confuse them.",3
post2hb,richly branching,1.567806103670254,highest,"Thank you for this. Your distinction of the two ""racist"" meanings will be very helpful in future discussions.",4
post2hb,richly branching,1.567806103670254,highest,[removed],4
post2hb,richly branching,1.567806103670254,highest,"Smacks of people being told about problems with motion detectors (such as for automatic sinks) and going ""What? Sinks can't be racist, that's just how light works."" That rebuttal only makes sense if automatic sinks grew in nature or something. As they are, someone designed them that way, and the fact they work poorly with dark skin is something the designer never even bothered considering. That's racism. It's not blatant, malicious bigotry, but it's still racism born of casual ignorance.",3
post2hb,richly branching,1.567806103670254,highest,"I don't know enough about these specific sinks to argue one way or the other, but I would like your position on the principle.

*If*, due to the actual, physical, biological differences between races/sexes/preferences/whatever, a system like the sink sensor will *always* be more or less effective for one or more groups, does that make it -ist? Like, if you increase the sensor sensitivity to the point it is as reliable on dark skin as it currently is on white skin, won't that just make *even more* sensitive or ""reliable"" towards light skin, ad nauseum?",4
post2hb,richly branching,1.567806103670254,highest,"Okay, how do we fix the issue? I mean beyond complaining and telling programmers to fix it. The algorithms pick up these problems from the training data and the training data is society itself. How are you going to cleanse these massive data sets of anything you consider problematic?",3
post2hb,richly branching,1.567806103670254,highest,">It's beyond obvious that what is meant here is the results of outputs of the neural net is unfairly disadvantageous along the lines of race and sex, therefore perpetuating racism and sexism.

It may be beyond obvious to you and I, but not to the vast majority of people I've talked to about this. When people see the word AI, they don't think of a statistical model on steroids, they really do think of AGI.

>It's time we move past this nitpicking and focus on the actual issue.

In my opinion, it's hard to move past this when the people making decision don't even understand the nature of the actual issue.",3
post2hb,richly branching,1.567806103670254,highest,Why was ethnicity used as an input to the sentencing AÍ?   Or is it able to reconstruct ethnicity due to other strong correlations?,2
post2hb,richly branching,1.567806103670254,highest,"I don't know the details. It's possible that they fed the neural network with things like criminal histories too, which are relevant in sentencing (as a first offender would get a lesser sentence than a known criminal obviously) and i'm guessing that would include things like photos or at least a description. It's very possible the researchers just mindlessly fed the thing with information that could easily be turned into something that a computer can more easily process (aka cut the file down to the important bits rather than give it full sentences to chew through) without regard for what they are feeding it, too.",3
post2hb,richly branching,1.567806103670254,highest,"This is something that bothers me about AÍ/ML : the tendency to overfeed it with data and get nonsensical results.  It’s not a problem with the algorithms, but rather malpractice on the part of the modelers/data scientists.",4
post2hb,richly branching,1.567806103670254,highest,"Neither would surprise me. If all the data for a case was put into a text document and crammed into the AI as training data, then ethnicity would probably appear in that. But even if they scrubbed that out, it probably wouldn't be that hard for the AI to reconstruct ethnicity from correlated data.",3
post2hb,richly branching,1.567806103670254,highest,"It could be a case where they looked at the statistics and said x race appears to be unfairly targeted, but didn't account that x race also had a higher baseline of crimes committed, or something along those lines.",3
post2hb,richly branching,1.567806103670254,highest,"Ethnicity, race, gender, etc. aren't fed into these models. Other things correlate to it. Zip codes and socioeconomic factors can heavily affect this. You can also see it pop up in natural language processing. Reading a police report to determine guilt or innocence or a clinician's notes to detect if a patient is sick can also find bias in the wording used. Not to say the people generating these reports are explicitly racist but that there could be implicit language used when talking about people of different races, ethnicities, genders, ages, etc. that can correlate back to those variables. We have to actively find ways of removing bias from this data or face not being able to use it to train models using that data if removing bias is truly a primary goal.",3
post2hb,richly branching,1.567806103670254,highest,"Expect we get to choose the data to train networks on.

Junk in junk out has never been a valid excuse.

We're going to have to force companies to put in the effort an just collect data at random or use unbalanced huge data sets and expect fair results.

Like you say, we know that the world has sexism and racism. We know any large dataset will reflect that. We know training AI on that data will perpetuate racism and sexism.

Knowing all this it's not acceptable to simply allow companies to cut corners. They're responsible for the results the AI produces.

Any sample of water you collect in the world will contain contamination. That doesn't mean companies are allowed to bottle it and sell it, giving that as a reason they're not responsible. We regulate water so it's tested, clean and safe.

It's becoming clear we'll need to regulate AI.",2
post2hb,richly branching,1.567806103670254,highest,"Question is, how do you choose which samples are biased and which are not? And besides, neural network are great at finding patterns, even ones that aren't there. If there's a correlation between proper punctuation and harsher sentences, you bet the network will find it. Does that mean we should remove punctuation from the sample data?",3
post2hb,richly branching,1.567806103670254,highest,"Well, frankly that's for the companies to work out. I'd expect them to find measures, objective as it's possible to be, for the results. Then keep developing the most objective AI they can.

If there's something irrelevant affecting sentencing unduly that's a problem that needs fixing. Especially with language, that's a proxy for racist laws already.

At the moment AI products are not covered very well by the discrimination laws we have in place. It's very difficult to sue an AI when you don't know why it made the decision it did. There's also no requirement to release large amounts of performance data to prove a bias.

Algorithms, AI, etc. are part of the modern world now. If a large corporation makes a bad one and it can have a huge effect. They need to at least know their liable if they don't follow certain best practices.",4
post2hb,richly branching,1.567806103670254,highest,">Like you say, we know that the world has sexism and racism. 

Sexism and racism is not only something the world has. It's legal: Not only is it out there in the world, it is allowed to be out there in the world. Under the umbrella of freedom of opinion and freedom of press, those opinions are allowed to exist, they are tolerated, and not legally sanctioned.

If you allow them to exist, if you tolerate them, then you also have to tolerate AIs trained on those completely legal and normal datasets. Just like we allow children to be trained on those datasets, should they be born to racist and sexist parents, or browse certain websites.

Everyone is allowed to read this stuff, absorb this stuff, learn this stuff, and mold their behavior according to this stuff... You only want to forbid that for AIs? Why? What makes AIs special?

If 14 year old Joe from Alabama can legally read it, and learn from it, and mold his future behavior in accord with it, you can't blame anyone to regard it suitable learning material for an AI, can you?

>Knowing all this it's not acceptable to simply allow companies to cut corners. 

No, not only is that acceptable, but consistent. I dislike the hypocritical halfway position: ""Sure, we have to allow sexism and racism to freely roam the world, the web, and all the rest. Everyone can call their child Adolf, and read them Mein Kampf as a bedtime story. That's liberty! But don't you dare feed an AI skewed datasets containing the drivel Adolf writes when he is a grownup, because *that* would have very destructive consequences which are not tolerable...""

>Any sample of water you collect in the world will contain contamination

Usually there are certain standards which regulate the water quality for open bodies of water. There are standards for what we regard as harmful substances which you are not allowed to release into rivers, and there are standards for how much pollution is acceptable in rivers and lakes.

So someone if someone dies, after taking a sip of lake water, what is the problem? Is the problem that the lake water is deadly, or is the problem that someone bottled and sold it? Pointing only at the ""bottled and sold"" side of the problem is a one sided view of the issue, especially when you got children swimming that same lake every day.

>It's becoming clear we'll need to regulate AI.

Are you sure it only points toward a need to regulate AI? :D",3
post2hb,richly branching,1.567806103670254,highest,"Resoviors, springs, and rivers have to be tested before they're used as a water source. I think the analogy fits. If water was tested and found to be toxic it would be illegal to give it to someone to drink. If it were not tested a company would still be found liable for not following best practices and testing.

In the whole of the EU sexism and racism is illegal. There is already discrimination law in place which isn't the case in a lot of the US.

I expect the EU to push for compliance for AI and that will have a global effect. Global companies will be compliant and smaller companies are unlikely to develop in-house systems to compete.

The language example you brought up earlier is a perfect example. Because of the many languages in the EU things like grammar and punctuation being judged by AI on application forms would likely be made illegal. French people have a right to work in Germany and vice versa. An AI screening out French speakers would bring up.so many red flags.

Especially in countries like the Netherlands, Finland, Belgium, etc. that have multiple languages and dialects.

We're likely to see an English language bias in AI to begin with. I'd expect the EU to make sure it isn't used at scale for a lot of things until it's developed out.

Job and work requirements in the EU can specify the need to be competent in a language but not the need to have it as your mother tongue. It's exactly the problem that is difficult to solve, but will have to be solved in any situation an AIs actions can discriminate against people.

That's the government, workplace, education, public spaces.justice system. AI could be incredibly useful or incredibly harmful. Regulation needs to be in place and I've no doubt the EU will do it.

Frankly I think the US is going to end up being a test bed for racist and sexist AI implementations which eventually get legalised for use in the EU when they've been fixed. 

With all the other causes of racism and sexism in the US and the general lack of government oversight I'm sad to say I think more fuel is about to get poured into that fire.",4
post2hb,richly branching,1.567806103670254,highest,">Problem is, of course, that ~~neural networks~~ **children** can only ever be as good as the training data. The ~~neural network~~ **child** isn't sexist or racist. It has no concept of these things. ~~Neural networks~~ Children merely replicate patterns they see in data they are trained on. If one of those patterns is sexism, the ~~neural network~~ child replicates sexism, even if it has no concept of sexism. Same for racism.

Sorry its late for me",2
post2hb,richly branching,1.567806103670254,highest,"Children are way smarter than anything we can build: A three year old can easily one-shot things like ""a chair"", and immediately generalize that knowledge into other things that can be used as ""chair"", and also derive transformations that converts things like ""bucket"" into ""chair"". Or ""black person"" into ""child"" and ""my friend"".

The real problem is that we build infinitely stupid things, market them as ""Intelligent"", making people use them on important tasks, and even expect that these things will do better than actual intelligence.",3
post2hb,richly branching,1.567806103670254,highest,"Wow a child can do shape recognition very well, guess I'll put a child in my computer to speed up my videogames then...

I mean come on. You can't pretend like you aren't aware about the concepts of *tools* now, can you ? How can we get a requisitory against tools in the 21st century ?

Next you're going to argue your hand is so much better than a hammer, you can grab things, you can count on fingers, you can flip off people, the single issue is you can't drive nails in wood with your hand !",4
post2hb,richly branching,1.567806103670254,highest,"I think a much more pertinent question is, what if the algorithm is right and is making connections that seem sexist to us but are actually just correct?

What if, for whatever reason, white men make better leaders? Black women better software developers? Should we kneejerk and ‘correct’ (actually introduce an aberrant bias) the algorithm or do research and look a little bit deeper.",2
post2hb,richly branching,1.567806103670254,highest,"> What if, for whatever reason, white men make better leaders?

1. Define better? In which categories? How are you deciding them? Who is measuring them? How many sources do we have for the data? What is the overall range of results?

2. Give me a single reason why skin color is more important than childhood nutrition? Because I can guarantee you that ""more likely"" isn't ""Definitive proof that"". 

3. Give me a single reason why gender is more important than the adverse conditions and support networks that surrounded a leader?

Your question is based on ignoring as much data as humanly possible in order to give us a simple answer anyone can understand. 

That's not something we should be encouraging. Simple answers are often very deceptive answers, and they're easier to spread.",3
post2hb,richly branching,1.567806103670254,highest,"I love how you are pretending I am suggesting we do not take a scientific approach.

In your own words:

>	Your question is based on ignoring as much data as humanly possible in order to give us a simple answer anyone can understand.

I am saying we exactly take the scientific approach and don’t let feelings lead us because we don’t like where the result of said scientific approach *might* lead us.",4
post2hb,richly branching,1.567806103670254,highest,"It seems very strange to me that in examples like that, things like racial data is even included in the data that it is fed.",2
post2hb,richly branching,1.567806103670254,highest,"It's probably not even racial data in and off itself. Things like the defendants name, address, etc. could be enough of a giveaway, even if the network has no idea what that info even means. Think about it, if you hear about a person with a typically black name from a majority black neighbourhood, wouldn't you assume that person is black? If we can do that, so can a neural network.",3
post2hb,richly branching,1.567806103670254,highest,"Well yes of course, but it seems to me like that kind of information, which is essentially irrelevant to what the network is trying to solve for, should be excluded in the data set being shown.",4
post2hb,richly branching,1.567806103670254,highest,"A couple examples.

Hiring AI:  Gender info was not included.  However the AI picked up on things like where the degree was from, or what classes were taken, that correlate with gender, and used THOSE to exclude people.

Medical diagnosis AI:  There was an article recently where they tried to strip out racial identifying data, since part of the goal was to avoid the racial bias that shows up in medicine, and the AI still misdiagnosed cancer much more often in black people.  Further studies learned the AI could identify race by chest x-rays, which was not a known source of racial difference.

AI is really good at finding patterns.  REALLY good at it.",3
post2hb,richly branching,1.567806103670254,highest,"I find it kind of strange that people seem to think that researchers are just feeding racist data to these AIs without trying to resolve the bias in that data. I'm sure some, perhaps many, do, but the problem is much deeper and harder to overcome than simply stripping out the obvious stuff.

The medical diagnostic AI is a perfect example of that-- it's clearly picking up something, but we don't know what. It's not an obvious pattern to the researchers.",4
post2hb,richly branching,1.567806103670254,highest,"In other words, don't be surprised when your mirror accurately reflects what is there.

Like when people say, ""Police are racist."" The police are racist **IF** the community is racist because the police reflect the values of the community they serve.

AI is the same. It is very good at revealing the patterns embedded in the data.",2
post2hb,richly branching,1.567806103670254,highest,"The nural network shouldn't have the ethnicity data, simple",2
post2hb,richly branching,1.567806103670254,highest,[deleted],2
post2hb,richly branching,1.567806103670254,highest,“on the hole………………(w? where_d ‘w’ come from?)”,3
post2hb,richly branching,1.567806103670254,highest,[removed],2
post2hb,richly branching,1.567806103670254,highest,I know right? I hate when i've already made up my mind on a matter and then someone comes along and confuses me with facts.,3
post2hb,richly branching,1.567806103670254,highest,Clip is trained on Google images. What is surprising on Google results having this type of bias which is so prevalent across the world?,2
post2hb,richly branching,1.567806103670254,highest,"> Therefore, the neural network, despite lacking a concept of what racism is, ended up sentencing certain ethnicities more and harder in test cases where it was presented with otherwise identical cases.

Was race one of the data points about the defendant fed into the network?   

If so, what a strange thing to feed into an NN. If not, how did the network know the race of the defendant?",2
post2hb,richly branching,1.567806103670254,highest,"I'd guess you wouldn't even have to feed the ethnicity into the network. If the neural network had the name and address of the defendant, it could easily make connections based off of that i suppose, even without info on the defendants skin color being present. There's names that are more common among black people, and they tend to live in mostly black neighbourhoods. Even without knowing this, a neural network could make this connection based off of names. (Also, idk what exactly they did feed this neural network in terms of data)",3
post2hb,richly branching,1.567806103670254,highest,Why would you feed the name and adress into the network? Are those relevant when making sentencing decisions?,4
post2hb,richly branching,1.567806103670254,highest,"You can use algorithms to detect bias in data.  The other option is a human but you have no idea what bias you will get.  Bias and fairness should be run on all decisions by humans and AI, but I doubt that happens.",2
post2hb,richly branching,1.567806103670254,highest,OP goes on with the assumption that you know this too and inherently focus on result,2
post2hb,richly branching,1.567806103670254,highest,That’s literally what the problem is and what the article is describing. Nobody is saying that the machines themselves are independently racist or sexist for no reason.,2
post2hb,richly branching,1.567806103670254,highest,Could you reverse engineer something like this to easily find who and how discrimination is happening? Essentially a way of quantifying institutional racism/sexism?,2
post2hb,richly branching,1.567806103670254,highest,"It would be a lot of effort, if its even possible at all, but wether we should is another question.",3
post2hb,richly branching,1.567806103670254,highest,"That was kind of my wonder.

We train these things on human input.  Maybe its just time to accept that humans are way more racist and sexist than we want to accept.  Solve that root problem and maybe it solves the AI training problem",2
post2hb,richly branching,1.567806103670254,highest,">Problem is, of course, that neural networks can only ever be as good as the training data..



How did Google make AlphaZero who is obviously better than any training data. Same for AlphaGo.

Both AI's became the best entities of that game to exist. So obviously AI can learn beyond their training data, in fact that seems to be something that happens quite often with machine learning.

Idk where you got that idea from",2
post2hb,richly branching,1.567806103670254,highest,"This is why AI as a general term needs to stop being applied to ML neural networks, which are simple complicated systems that operate on aggregated data as you mention. They can be incredibly powerful tools, but until we create artificial general intelligence that can self reflect, the data used to train these models is going to have to be continually scrutinized and curated in order to remove specific bias, which, if done by humans, will still have some sort of bias",2
post2hb,richly branching,1.567806103670254,highest,Could you not the same of people?,2
post2hb,richly branching,1.567806103670254,highest,This could just as easily be applied to people too. Racism isn't always a conscious choice to treat people worse.,2
post2hb,richly branching,1.567806103670254,highest,I think this demonstrates how systemic racism works. Even if the individual actor isn’t intending to discriminate against anyone simply following social norms will produce discriminatory outcomes.,2
post2hb,richly branching,1.567806103670254,highest,">Neural networks merely replicate patterns they see in data they are trained on. If one of those patterns is sexism, the neural network replicates sexism, even if it has no concept of sexism. Same for racism.   

Same as people, to be honest. Most sexists and racists are not aware that they are. It's a matter of critical thinking among humans.

Could neural networks be taught to identify these biases from the information and analysis that it is working on?",2
post2hb,richly branching,1.567806103670254,highest,If anything it really highlights just how bigoted and prejudiced our systems really are.,2
post2hb,richly branching,1.567806103670254,highest,This is the key. If your AI is making unfair decisions it’s not a fault of the AI.  Biased AI highlights problems that exist in humanity; not AI.,2
post2hb,richly branching,1.567806103670254,highest,"Just like children. No person is born racist. We have a blank neural network to work with. But if the overwhelming majority and/or most crucial of inputs (i.e. those of our parents') are racist, sexist, or of any other, even benign, ideology, we will  naturally, gravitate towards that/those ideologies/racism/sexism, because that's what we hear and see the most. We need to change/regulate input data, as you've said, rather than the network.

Just like you would start by educating people not to be sexist/racist first, rather than try to literally change the neurons/DNA of a fetus. There is nothing wrong with the inherently blank sheet. The issue is always with the input.",2
post2hb,richly branching,1.567806103670254,highest,"It can also be that AI lacks feelings and therefore sympathy. It could be that it is acting purely objectively, but to us that can be sexist, racist or in other ways just plain cruel. This has for example been seen with AI used in employment or used to determine if someone is to keep their job or not based off of statistics.",2
post2hb,richly branching,1.567806103670254,highest,"Ok this might be a dumb question, but specific to sentencing, why not only train it on the majority (probably not the right word for it but I just woke up), then have that learning applied across the board?

I.e. in the US, train it on cis white men (assuming) then apply it to minorities, woman, whomever...",2
post2hb,richly branching,1.567806103670254,highest,"No child is born biased.  That's taught by the information they're given.  

If only Mr. Rogers were still with us to help teach AI to be less biased, and more children to write to him to ask that he say aloud that he is feeding the fish so that one blind girl wouldn't be worried about the fish anymore.

Actually, here's a thought, let's get very young children to help identify the bias in AI!  Make it an age appropriate video game and crowd source their natural lack of bias!  Children are far more socially intelligent than we give them credit for.  At least until they get to what I like to call the ""bitey fives"" age.  I'm still a little wary of kids in that age group.

Somewhat funny anecdote time.  Ya know how young young kids are usually kinda shy around ""stranger"" adults?  Well, there was this big tornado that hit.  All the power was out, and the neighborhood was just out wandering around and assessing the damage.  I noticed two big trees that were definitely gonna fall on this house at the next big breeze.  After I helped the old person manually open the garage door to at least save their car before the trees totalled the garage, I rejoined the gawkers.  Small child who has been clinging to her parents the whole time observes that her parents are starting to freak out about those trees, like everyone else.  I'm just standing there videoing for funsies.  All of a sudden I have a small child clinging to MY leg!  Her parents are freaking out, my parents are freaking out, everyone's freaking out.  I'm trying to get a good angle for the video.  Smart little one ran to the only adult that seemed perfectly fine with what's going on.  Trees fell, I got a great video of it, and then I asked whose kid it was that was attached to my leg.  I do wish I'd have gotten a bit of video of everyone else freaking out though.  That was hilarious.  

Side note: kid got shy and ran back to her parents after everyone had calmed down a bit.  Kids are weird.  Apparently I was only ok to interact with while I was confident I was standing in a safe spot.  After that, I was a scary stranger again.",2
post2hb,richly branching,1.567806103670254,highest,"> This is also why computer aided sentencing failed in the early stages. If you feed a neural network with real data, any biases present in the data has will be inherited by the neural network. Therefore, the neural network, despite lacking a concept of what racism is, ended up sentencing certain ethnicities more and harder in test cases where it was presented with otherwise identical cases.

Seems like a simple fix to just omit race as a variable in the criminals punishment no?",2
post2hb,richly branching,1.567806103670254,highest,"Question is, would the neural network still be able to tell? Even if you remove race, there's a possibility that the network would pick up on certain patterns that are common in some ethnicities but not so much in others, which would then allow it to determine race anyway, even if not with 100% accuracy.",3
post2hb,richly branching,1.567806103670254,highest,"Exactly. I remember reading about how police wanted to use statistics and AI to predict where crime would most likely be committed so they could more effectively place patrols in a ""scientific"" way. It turned out to be racist because the data was biased by racist policing tactics. If the data is not completely free of bias, then the result is not objective.",2
post2hb,richly branching,1.567806103670254,highest,the funny thing is that i asked gtp3 basically if it became sexist/racist if its training dats would include social media. it agreed,2
post2hb,richly branching,1.567806103670254,highest,"Tangentially, I can't help but imagine a version where an AI is so racist and sexist that it's comedic. Like a robot version of Kramer that truly wants to be a good entity but keeps saying ridiculous things and has to ""train"" itself not to.",2
post2hb,richly branching,1.567806103670254,highest,"Garbage in, garbage out.",2
post2hb,richly branching,1.567806103670254,highest,"AI is only going to reach the purity ideal if it can completely tether itself from the humans creating the programming on it, but I just don't quite see how that ever happens. It'd have to somehow train and model itself off of human behavior without actually adopting any of the human behavior. Someone much smarter than me can probably create a theoretical solution, but honestly I don't really see how you get around that issue.",2
post2hb,richly branching,1.567806103670254,highest,"That makes sense, except for why did we give the robots any ethnic information at all? Wouldn't just not telling them make the otherwise identical cases actually identical?",2
post2hb,richly branching,1.567806103670254,highest,"Well, i suppose a neural network might not even need any racial info to figure someones race out. Think about how neural networks are better at diagnosing cancer than any humans are. They see patterns in data that go past our ability to perceive.",3
post2hb,richly branching,1.567806103670254,highest,"Honestly, it's *worse* than that. You don't need an ""AI"" to be ""racist"" to make data that fits with racist ideas or goals. Lending algorithms have (repeatedly) reimplemented redlining, not explicitly and not at the behest of the people making them. Why? Because the goal didn't (and arguably couldn't) include things like promoting equity, just profit. So you get pattern matching on things like ""which neighborhood someone lives in correlates with likelihood to repay"", which even when the pattern is arguably ""correct"" doesn't make it something we should action on, or take as a causal relationship (see, ""cellphones cause cancer"" nonsense).",2
post2hb,richly branching,1.567806103670254,highest,"I know this probably isn't the place, but that just made me imagine robots sharing memes with complicated problems to solve before being able to see the meme, like a human proof meme for sentient robots only.",2
post2hb,richly branching,1.567806103670254,highest,"Eventually, we can't make a neural net A.I. that does a task better than people currently, because we still have people creating the data to train that A.I. The reason we are using these systems is because of their one advantage: the volume of data that can be processed.",2
post2hb,richly branching,1.567806103670254,highest,But why would they include race as a metric in the data anyway. If I were going to make ai for sentencing wouldn't I remove that data point before feeding it in?,2
post2hb,richly branching,1.567806103670254,highest,"It'd probably be a good idea to feed these things data looking for conflicts to identify bad research. I've seen tons of garbage studies that get lots of traction.

Worse, I've seen good studies getting the correct answer but asking the wrong question.

Every discipline is trained to see itself through it's own lense. This is a codified echo chamber.

When you look at nutrition from a physiological and evolutionary context, the studies done are based on axiomatic suppositions the institution can't see to question because dogma lacks self awareness.

For example. Studies show fiber lowers risk of heart disease. However, it does that by slowing sugar absorption. Eating less sugar lowers heat disease and doesn't require insoluble fiber that irritates and inflames our intestines.

The predominant source of sugar before agriculture was regionally and seasonally available fruits ripening in fall. The sugar makes you hungrier so you gorge to put in weight for winter.

Eating sugar all the time can't be fixed by more fiber because that leads to more constipation, boating, and inflammation.

So, fiber isn't *good* it just minimizes the harm of sugar we're eating in qualities that fry our body like ethanol in a collector car.",2
post2hb,richly branching,1.567806103670254,highest,"It kind of confirms systemic sexism and racism, doesn’t it?",2
post2hb,richly branching,1.567806103670254,highest,"We point the machine at people and say ""learn from them on what to do""... and then we are ashamed when the machine acts like the people who taught it...",2
post2hb,richly branching,1.567806103670254,highest,"Exactly this. Take Amazon's attempt at being race and gender blind in picking out good resumes. That program was very good at highlighting resumes from white men.

Why? Because white men have opportunities and circumstances that give them better resumes.

Women are more likely to have gaps in work history to take care of family. Minorities or poorer candidates are less likely to come from prestigious colleges. They might be working instead of doing extracurriculars. They might be less likely to afford services that help them create better resumes.

But this is how systemic racism and sexism works. It's not the ideals of a particular person or organization that makes them want white men. It's just that white men have better opportunities to get good looking resumes. AI can not help this problem at that point in the hiring process. Racism/sexism is in the input, so it's in the ouput.",2
post2hb,richly branching,1.567806103670254,highest,Why would race or name or gender or age ever be a part of training data? Just why?,2
post2hb,richly branching,1.567806103670254,highest,Machine learning needs some machine teaching,2
post2hb,richly branching,1.567806103670254,highest,"This is why Googles ImagenAI is not available to the public. It’s results are absolutely incredible (check out r/imagenAI), but utilizing the LAION-400M dataset continues to provide racially motivated results.",2
post2hb,richly branching,1.567806103670254,highest,"Google’s ImagenAI is not available to the public for partly the same reason. They utilized the LAION-400M dataset. 

Their reasoning is a good read: https://www.reddit.com/r/ImagenAI/comments/uxch3j/reasons_its_not_public/?utm_source=share&utm_medium=ios_app&utm_name=iossmf",2
post2hb,richly branching,1.567806103670254,highest,Same thing happened when (google? I think it was) trained an ai off of Twitter and Facebook and it became an extremist quickly.,2
post2hb,richly branching,1.567806103670254,highest,Maybe we could at least use these AIs to identify biases in data?,2
post2hb,richly branching,1.567806103670254,highest,Very interesting,2
post2hb,richly branching,1.567806103670254,highest,"I understand the concern and it certainly is possible to do poorly considered ML design.  But I think the argument about this is suspect.

If you are concerned about applicants propensity to default on a loan and look for factors that predict loan approvals pre ML, yes you could perpetuate previous biases.  But that would be an obviously flawed approach.

One would instead look at actual defaults.  And to more explicitly avoid bias I wouldn't consider race as a factor.   If factors such as income, employment history, length of residence and debt to income ratio happen to correlate  with some class identity is that racism?  It may be uncomfortable and it may show the impact of previous racism.  But for someone assessing risk of default on a loan it would be on target for that decision.  

Not saying there are no reasons not to address the impact of previous unfair practices but distorting a risk analysis isn't the place to do it.",2
post2hb,richly branching,1.567806103670254,highest,"Garbage in, garbage out.",1
post2hb,richly branching,1.567806103670254,highest,Like to see what garbage would come out if you trained it on reddit.,2
post2hb,richly branching,1.567806103670254,highest,"You can, r/SubSimulatorGPT2",3
post2hb,richly branching,1.567806103670254,highest,That sub is interesting to say the least,4
post2hb,richly branching,1.567806103670254,highest,The first post there for me - I'm a socialist and I don't even know what socialism is. That is a lot of subs nowadays.,4
post2hb,richly branching,1.567806103670254,highest,"Microsoft released its ai bot tay to twitter... 

Remember that?

And then it did it AGAIN with Zo....

Remember That too?",3
post2hb,richly branching,1.567806103670254,highest,"They put Tay up and it became racist. They took it down, wiped, then put it up again. Guess what? Racist again.",4
post2hb,richly branching,1.567806103670254,highest,It would probably be a dog walking version of Nick Avocado + Chris Chan.,3
post2hb,richly branching,1.567806103670254,highest,Woke to the point of sounding racist,3
post2hb,richly branching,1.567806103670254,highest,The AI would be greasy dog walker mod that collects funko pops.,3
post2hb,richly branching,1.567806103670254,highest,Now that would be interesting,3
post2hb,richly branching,1.567806103670254,highest,We all have Reddit inputs in our respective “neural networks”.,3
post2hb,richly branching,1.567806103670254,highest,I believe that is the entire point of those JHU researchers claim. A lot of publicly available and accepted datasets are “garbage” and biased and the industry doesn’t bother.,2
post2hb,richly branching,1.567806103670254,highest,"An algorithm deciding how much someone can mortgage will decide a person that is a woman can get less than a man. It knows statistically women get paid less. It isn't discrimination on gender, it is discriminating based on factual data.",2
post2hb,richly branching,1.567806103670254,highest,"An AI or algorithm determining how much someone can mortgage based on gender and the gender pay gap over an individual's income is terrible. At that point it's not garbage in garbage out any more, but garbage selection by the AI or algorithm.",3
post2hb,richly branching,1.567806103670254,highest,"I think the original commenter used slightly inflammatory language but here is the point that I think they are trying to make (or at least the one that I’m going to make haha):

If you are designing a model to predict who can pay their mortgage, then you will give people a lower score if they earn less. That is the goal. If we live in a society that has a gender gap, then it is going to reflect that. Even if you had the model specifically not look at gender, if women make less, then an accurate model will give them lower scores.

Should the model be altered to be sure to give women equal scores? Even if it makes it less accurate? Even if that means women are more likely to be issued mortgages that they ultimately can’t afford and default on?

Tough question. Of course the gender gap should be fixed. But in the meantime, if you are trying to make accurate predictions about the world, you are going to end up also noticing and predicting flawed elements of the world.

That said, there could do situations where this creates an objectively negative outcome. Like if part of the evaluation is based on human opinion. And let’s say those evaluations are done by sexist people who assume women make less than they do. Not reflecting the gender gap, but rather underestimating women’s pay above and beyond the gender gap. In this situation the model would be under predicting women’s income and denying them mortgages that they can afford, due to bias. That would be an example of something that is both bad for the accuracy of the model and morally bad.

But when the model is accurate and it is merely a reflecting our world I think it’s hard to say that that’s a problem with the model. Rather it’s a problem with our society. To be fair it’s not super clear cut",4
post2hb,richly branching,1.567806103670254,highest,AKA garbage in,4
post2hb,richly branching,1.567806103670254,highest,"That's also a domain that really, really doesn't need or want AI",3
post2hb,richly branching,1.567806103670254,highest,"If corrected enough, a good algorithm could be unbiased or biased the way we want.",4
post2hb,richly branching,1.567806103670254,highest,"That would violate the Equal Credit Opportunity Act, so it wouldn’t pass regulatory scrutiny, thankfully.  Mortgages should be decided on the applicant’s current income, DTI ratio, etc, not on their gender.",3
post2hb,richly branching,1.567806103670254,highest,"No, but by doing so, it discriminates on gender, because society does. The problem lies not with the algorithm per see, it lies with society.",4
post2hb,richly branching,1.567806103670254,highest,"However technically, because that detail is statistically not relevant, you can make AIs ignore irrelevant data sets.",3
post2hb,richly branching,1.567806103670254,highest,"Literally every single time these NN AIs are made there's an article later about how it ended up being racist and they had to do something. And every single time, the same excuses on Reddit are made, about how 'oh the team is biased' 'oh the programmed was biased' 'oh the data is biased' 'oh the guy who curated the data was biased'.

If you have a hundred teams create a hundred AIs fed a hundred different sets of data and literally every single time it comes back with the same answer, maybe... the problem isn't because of 'inherent bias'?",2
post2hb,richly branching,1.567806103670254,highest,Got the reference there buddy... Carlin always told it as it was (and still is),2
post2hb,richly branching,1.567806103670254,highest,"If race is a feature correlated with an outcome then of course the neural network will try to find that feature and exploit it, that's literally what it's designed to do. The problem is creating transparent and unbiased datasets. That's particularly difficult for certain domains.",1
post2hb,richly branching,1.567806103670254,highest,"Part of the issue is that we want equal representation, from a position where people don't have equal access to resources. 

There was a case where a company was looking to improve its diversity by hiring more diverse staff, and failing year after year. They eventually removed all names and any details that could identify who is who in their hiring practices, and guess what - they ended up hiring even more white men than they started off with, because they were more qualified on paper. 

If we want to improve access to jobs for everyone, it starts with better educations for kids, and making sure you get opportunities *throughout* your life. You can't just expect there to suddenly be a huge recruitment pool of black astrophysicists just because you want there to be - you have to start with young people.",2
post2hb,richly branching,1.567806103670254,highest,"> we want equal representation

Do we? This seems like such a dated idealist flaw that has never shown itself in reality. The Scandinavian paradox is a great example of how despite basically being given complete and utter freedom, you still end up with these absurdly skewered forms of representation that are VERY far from equal, because people naturally tend to move towards the areas that are meaningful to them on a personal level.


I mean inherently, the problem is also that we somehow expect there to universally BE an equal amount of possible representatives from each category, or for that matter that we somehow stop entirely evaluating the worth of the individual worker and what they bring to the table.


I dunno, I feel this representation argument has been found flawed for so so long now and shown no merit or logical sensible place in a free society.",3
post2hb,richly branching,1.567806103670254,highest,"I think because of the history of eugenics and Nazism people rightly are fearfull, that people might actually be different to some degree. Men and Women seem to actually tend towards different fields. Of course it is extremely important to provide people with equal opportunities to every field they might choose, so that everyone can do what they want to do and are best at, but we should accept that it does not guarantee a 50-50 spread in all cases.",4
post2hb,richly branching,1.567806103670254,highest,What are these skandinavian absurdly skewed forms of representation you talk about?,4
post2hb,richly branching,1.567806103670254,highest,Representing it as complete and utter freedom is nonsense. That ignores the impact of social pressures on what is meaningful to a person and some forms of decision making. Would that still be true in a less homogeneous society?,4
post2hb,richly branching,1.567806103670254,highest,"I like to compare it to a hurdle race.  In real hurdles, you can just look at the finishing time and know who the best runner is, because all the racers had the same length of track and the same number of hurdles.

But real life isn't like that. You can't just look at the finishing time. If one guy finishes 10 hurdles in 12 seconds, and the next guy over finishes 11 hurdles in 13 seconds, who's the better runner?

What your case study did was remove everything but the finishing times, but that turns out to be one of the least realistic ways to find actual talent.    


And your recommendation is spot on: try to knock out some hurdles for people with more than everyone else, from the starting line.",3
post2hb,richly branching,1.567806103670254,highest,[removed],3
post2hb,richly branching,1.567806103670254,highest,In what sense?,4
post2hb,richly branching,1.567806103670254,highest,"You bring up a really great point, which is that systemic racism and sexism, that is, forms of prejudice build into the lower-order procedural and mechanical elements of a complex system, can exist. This can also be intentional (See: Fair Housing Act) *or unintentional* (similar to the example you just described). 

This paper reads like another example, but we NEED simple examples of this for people to understand the problem. Anyone still wondering what critical race theory was, well, it basically informed on the possibility of such complex systems to graduate law students using legal and policy examples. The political reaction to CRT further illustrates the problem in America. A large contingent of politicians and media corporations proselytize that the conclusions found in this article are impossible. Some (even most) may do it maliciously, but I have to assume that some just don't believe that it's possible. We cannot accept anything but full understanding here, or the courts will hear ""The robot made the decision *and robots can't be racist*"" which we know is simply not true.",3
post2hb,richly branching,1.567806103670254,highest,"Unless I'm horribly misreading your thesis (if so, iAmThat- did, as well)...

> and guess what - they ended up hiring even more white men than they started off with, because they were more qualified on paper.  

""There was a case..."" And there are plenty of cases of companies who cited benefiting from active diversity policies, in part due to bringing a wider array of perspectives to bear on problem-solving, and in part due to the fact that the systemic issues you mention-but-only-as-a-faux-binary-choice undermine the idyllic meritocracy assumption inherent in exclusively judging who's (euphemistically) ""more qualified on paper.""  

> If we want to improve access to jobs for everyone, it starts with better educations for kids, and making sure you get opportunities throughout your life. You can't just expect there to suddenly be a huge recruitment pool of black astrophysicists just because you want there to be - you have to start with young people.

People literally work on exactly what you're dismissing with ""bb-but they should *actually* work on this"". This isn't some zero-sum game where having employment diversity practices requires us to abandon education reform, or that having the latter means that the former is, based on a single uncited anecdote, iNeFfIcIeNt in a naive version of crude blanket meritocracy.",3
post2hb,richly branching,1.567806103670254,highest,"And, even then, there are likely to be disparities between races and sexes. That's just a fact.

Equality doesn't exist in nature, but equity should exist in society.",2
post2hb,richly branching,1.567806103670254,highest,Stop letting the ai know about race then. It literally cannot be racist if it has no idea that race exists. This is a case of trying to be not racist by being racist in specific ways. It cannot work that way.,2
post2hb,richly branching,1.567806103670254,highest,"Can't stop it if it's coorelated with an outcome. You can just force it to measure it badly.

Have an AI predict human heights, but don't let it know about sex. It'll use whatever it has - hair length, names, finger nail color, whatever, to divine sex because sex is a thing that predicts human heights.",3
post2hb,richly branching,1.567806103670254,highest,"In an ideal world it would be that simple. Race can be removed as a feature, but things like name, address etc can all be used to inadvertently infer race, and that's just the obvious ones. Synthetic datasets could be a solution, but depending how they are generated removing bias is still difficult.",3
post2hb,richly branching,1.567806103670254,highest,"Bias within AI is potentially more dangerous than bias among individuals. The notion that an algorithm can have bias is one that seems silly to a lot of people. The default presumption is that AI is dispassionate and thus inherently fair.  Many incorrectly associate emotional motives (greed, hatred, fear, etc) with bias.",1
post2hb,richly branching,1.567806103670254,highest,"It's because ""bias"" here is mathematical bias while colloquially people mean emotional bias.

There should just be a new word that describes AI bias so that people get more accepting of it.

Name it ""Statistical false judgement"" or something.",2
post2hb,richly branching,1.567806103670254,highest,"Lots of bias in humans isn't emotional either. People just attribute emotion to negative behaviors or outcomes. People have a difficult time acknowledging how bad outcomes can come from honest/decent intentions. 

We can attempt using different language but ultimately people need separate intention from outcomes. We conflate the two all the time. Like giving someone an ""A for effort"". If a person tries to do right it is generally accepted they deserve credit for that effort. Which is why so many people reflexively default to plausible deniability arguments when discussing racism, sexism, etc. The evidence of bias holds no weight with people minus evidence of intention.  Unless a person meant to do bad they get the benefit of the doubt.",3
post2hb,richly branching,1.567806103670254,highest,"When I read these threads about 'AI bias' - and they seem to come up every few months because ""for some reason"", every AI neural net always seems to end up racist and sexist - it kind of sounds to me like people are afraid to learn that maybe racism and sexism aren't actually the ""ignorant, stupid, emotional"" positions they've been gaslighting it as. If a mathematical neural processing compressing a billion points of data arrives at the conclusion that say, women make inferior engineers or Whites make inferior sports players, and it does it over and over, in every model, with every set of data, despite all your attempts to ""debias"" it, then it suggests that those assumptions are sexist and racist, yet, are reasonable and logical.",4
post2hb,richly branching,1.567806103670254,highest,"It's a bit weirder than that - a model or algorithm can be unbiased in a mathematical/statistical sense and be biased because it doesn't represent what you think it does.

IMO, the biases at play here are more systematic than they are mathematical. These models are accurately representing the sexism/racism inherent to the data, but that's not at all what we intend for them to represent.",3
post2hb,richly branching,1.567806103670254,highest,[removed],3
post2hb,richly branching,1.567806103670254,highest,"I mean we've known for a long time that statistics can be manipulated.

I think the confusion is that people are trying to anthropromorphize a math problem on a certain level.

Edit:?????",3
post2hb,richly branching,1.567806103670254,highest,No bias is correct. We don't fix people's bias with addressing their emotions we address it by helping address bias in the information they have available to them. It's the same bias with the same cause and same fix.,3
post2hb,richly branching,1.567806103670254,highest,">	Bias within AI is potentially more dangerous than bias among individuals.

The amount of racism and other forms of bias in political leaders (both recently and historically) that works to drive horrific acts might be giving this idea the run for its money.",2
post2hb,richly branching,1.567806103670254,highest,"People give modern AI way too much credit. They are glorified SQL injections with no closed loop. Instead of finding a set of data or producing a set result, they just keep spinning and narrowing down results to set perameters. That's all it is. ""AI"" is just a marketing term for machine learning. 

To clarify, I understand that ML is a subset of AI. I just feel it is fair to say that we all understand that AI has a cultural context and calling what we have now AI is disengenuous in that context. I'm just out here bitching about semantics.",2
post2hb,richly branching,1.567806103670254,highest,"Moreover, a single AI model can have a negative impact on an arbitrary number of people. If you think about the collective bias in, say, a workforce that assigns loan worthiness to applicants, you could probably find some biases broadly present across a society. While an AI might have the same problem, you could \_probably\_ determine which individuals in your workforce are making decisions that are likely to be more influenced by personal biases, conscious or unconscious.",2
post2hb,richly branching,1.567806103670254,highest,"Ehhh, I think a potential counterpoint might be that it’s really easy to run a bias test on an AI and scientifically measure it, while it’s totally possible to not realize how biased someone is before they get elected.

Like it’s easy to recognize the guy who is dropping casual N-words as biased, it’s much harder to recognize the guy who is pushing his daughter to not become a police officer because “that’s a man’s job”.",3
post2hb,richly branching,1.567806103670254,highest,"This is exactly the point I came to make. Corporations are starting to put a lot of trust in their ""algorithms"" and letting them decide things like loan and credit approvals. A sexist robot being allowed to make these decisions goes against the equal rights act, but many times there is no way to appeal these decisions.",2
post2hb,richly branching,1.567806103670254,highest,"If you analyze the dataset of running backs in the NFL you're going to see a preponderance of young black men.  

If you look at the dataset of people who have chosen nursing as a profession you're going to see more women then men.

How should an AI data analyst address or correct this?  Is it racist or sexist to observe these facts in data?",1
post2hb,richly branching,1.567806103670254,highest,It’s not; but if you want an AI to be used to hire people in these professions it is going to favour those biases whether they are relevant or not. An AI which helps doctors diagnose patients may under diagnose groups of people who already find it difficult to be diagnosed correctly. Biases in AI are highlight problems that exist in society; not problems with AI.,2
post2hb,richly branching,1.567806103670254,highest,Furthermore the use of biased AI shows indifference towards prejudice among the decision-makers. We have come full circle.,3
post2hb,richly branching,1.567806103670254,highest,"You are not understanding the issue. If a model for diagnosing cancer is 98% accurate on white patients, 67% accurate on black patients, with an overall accuracy of 93%, how should we evaluate that model's performance? We are not training models to identify running backs and nurses. We are training them to make important decisions in complex and impactful environments.",2
post2hb,richly branching,1.567806103670254,highest,"You kind of just pointed out how we would evaluate the model's performance.  We can always separate out and compute accuracy metrics (whether it is raw accuracy, F1, AUC, R2, MSE, etc.) on different subcategories of data to see if the model has any biases on certain things.  It is something that is commonly done.

In the case for the model above, I'd also want to take a closer look at why the model is not doing nearly as well on African American patients.  Could it be lacking data samples, something more systemic with the model, etc.  After analysis I might trust the model with predicting caucasian patients but not African American.",3
post2hb,richly branching,1.567806103670254,highest,">how should we evaluate that model's performance?

I mean, looking at classification accuracy with a highly imbalanced dataset is a rookie mistake. Unfortunately, there are hordes of data scientists that couldn't tell you might want to prioritize sensitivity in a cancer diagnostic tool.",3
post2hb,richly branching,1.567806103670254,highest,[deleted],2
post2hb,richly branching,1.567806103670254,highest,"Did you read the article? It's not about whether stats are racist, it's about if using AI predictive analytics to assign characteristics to demographics is.

No one is trying to censor the raw data. 

Although as they say, giving it unverified learning sets from the internet is risky... but you can't tell me there isn't toxic misinformation on the internet. We're literally on Reddit right now.",3
post2hb,richly branching,1.567806103670254,highest,"By sticking to the stats and what's quantifiable, that's how.

""X% of care positions are performed by women"" isn't sexist. Saying ""Women are better suited to care positions"" would reinforce sexist tropes...and for that matter extrapolate on data in a way that the data doesn't even show causation for.",2
post2hb,richly branching,1.567806103670254,highest,"But ... what if women ARE better suited for care positions because for example as a group they are more adept at identifying emotions and less testosterone means lower aggression? (Of course, that doesn't mean that every women is more suited for a care position than every man.)  
Why would you even need an AI if you are dismissing possible results that might very well be true but not conform to your beliefs?",3
post2hb,richly branching,1.567806103670254,highest,"I'd say that's the very crux of the problem that the article brings up. The AI was just putting people into buckets and saying ""white/asian person = doctor, black person = criminal."" It reduces complex social solutions to absurdity and then makes ultimately baseless judgements about people based on skin color/sex.

It's not about conforming to beliefs, but about generalizations which in your post you say are naturally not reasonable.

Also words like ""better"" in themselves are subjective, and value judgements rather than something quantifiable. Just because a neural network has circuitry instead of neurons doesn't somehow magically free it from the subjectivity humans exist in, nice as that would be. How would an AI define 'better' in a purely objective, quantifiable way?",4
post2hb,richly branching,1.567806103670254,highest,">By sticking to the stats and what's quantifiable, that's how.

Yeah but that's the thing - most of these algorithm we call ""AI"" are statistical models. Sometimes they're literally just linear regression models. In practice, these models are formalization of how people often think about descriptive statistics, and I don't think people are less liable to come to the inappropriate conclusion that ""Women are better suited to care positions"".",3
post2hb,richly branching,1.567806103670254,highest,You missed the point entirely. I think reading the article would be a good place to start.,2
post2hb,richly branching,1.567806103670254,highest,"The issue is that AI can't take into account any context or underlying causes in the data. The AI only sees trends in the data and will make decisions based off of it, but many of these trends appear from racism and sexism in society.",2
post2hb,richly branching,1.567806103670254,highest,"If the AI had a line added that resulted in an equal amount of other races of running backs being hired, would the interpreter consider this a non-racist outcome, or would it be racist to black men?",2
post2hb,richly branching,1.567806103670254,highest,"Hey, man, they're just a mirror of their data. You show them real-life data, and they'll mirror back an image of our society. You wanna have a generation of unflawed AI, i'm afraid you'll have to manifest an unflawed society for a generation's worth of time. But since that's literally impossible, this is all we get. And we will get it, because it provides utility to people with money. Them's all the requirements.",1
post2hb,richly branching,1.567806103670254,highest,"Neural networks are picking up correlations, not causalities. If poverty correlates with ethnicity because some other underlying reason, like negative discrimination in employment, the correlation is still there. The model will use these, the people using the output of the model need to be aware of this and act accordingly. Even if you remove the ethnicity from the feature set, you will find that the model finds a way to discriminate because that's the sad reality.",1
post2hb,richly branching,1.567806103670254,highest,"I frequently get the impression that when people say they want ""unbiased"" results from a process (AI or otherwise), they really mean that they want results that don't show *output* differences across their pet issue. They don't want people of a particular sex or race or creed to be disproportionately represented in the output. Frankly, it's not at all clear to me that this is a good goal to have. If I generate an AI to tell me how best to spend aid money, should I rail and complain about bias if it selects Black households at a higher rate? I don't see why I would. It just means that Black people need that aid more. Applying the exact same standard, if I create a sentencing AI to determine guilt and Black defendants are selected as guilty more frequently, that's not inherently cause for alarm. It could just mean that the Black defendants are guilty more frequently.

That doesn't mean that input errors can't lead to flawed outputs or that we shouldn't care about these flaws, of course. To take the earlier example, if a sentencing AI tells us that Black people are guilty more often and an independent review shows that this *isn't true*, that's a massive problem. It does mean that, though, we need to focus less on whether these processes are ""biased"" and more on whether or not they give us correct answers.",2
post2hb,richly branching,1.567806103670254,highest,"Well said, their examples aren't exactly cause for alarm that the headline implies... Let's check the ones where the ""robot 'sees' people's faces""

>tends to: identify women as a ""homemaker"" over white men

That's not sexist. Woman are 13x more likely to be homemakers than men. If it didn't tend to identify women over men here, it would just be wrong.

>Black men as ""criminals"" 10% more than white men

This one is a little trickier. Much more white men are criminals than black men, but black men are more overrepresented. So given the label ""criminal"", a properly trained AI should depict a white man most of the time. But given a white and black man and told to choose which is more likely to be a criminal, a ""properly"" trained AI should choose the black man. Only 10% more actually seems less ""racist"" than the data would imply.


>identify Latino men as ""janitors"" 10% more than white men.

From what I was able to find, Latinos aren't overrepresented as janitors compared to white men... this one might actually be picking up on racist stereotypes and would be worth looking into.",3
post2hb,richly branching,1.567806103670254,highest,"I'm not saying the AI isn't sexist and racist, but what if an AI were accurate, true, living in reality, without human bias and all the lies we collectively decide to pretend are true. Wouldn't it seem to us to be really biased? Do we have to skew an accurate AI to our social standards so it will be acceptable?",1
post2hb,richly branching,1.567806103670254,highest,maybe bias is good actually,2
post2hb,richly branching,1.567806103670254,highest,"Negative bias and positive bias are both already terms. This has been a problem for as long as AI exists. As long as the data set is incomplete or sourced from a flawed society that perpetuates inequalities, the AI will become flawed itself.",3
post2hb,richly branching,1.567806103670254,highest,that's just like your opinion tho (your bias...),4
post2hb,richly branching,1.567806103670254,highest,">We created a learning algorithm that processes data we input to make decisions.

>When we gave it biased data, it made biased decisions.

>Contemplate upon this

I dunno man, this feels like a given.

Yes, there's a flaw in creating machine learning algorithms based on flawed data, but that's not flawed AI - that's barely AI at all.

As for the claim

>People and organisations have decided it's ok to create these products

Who says it's okay to create a racist AI?

Or are you confusing ""requiring a device that provides accurate responses"" with ""accepting of a system of inequality""

I'm pretty sure the use of machine learning for the purposes of demographic research NEEDS to reflect the flawed and biased data, or they won't be doing their job right. (If you are marketing a rose flavoured shampoo and you want to use an AI to decide who your target demographic is, an AI that spits out ""anyone can enjoy rose regardless of age and ethnicity"" is useless to you).

This is a lot more nuanced an issue than sensationalist headlines like this make it out to be.

I get the premise, I understand that the existence of flawed society means any machine based upon that society may inherit those flaws - but that's either a requirement of that design or a flaw with the system, not with the AI",1
post2hb,richly branching,1.567806103670254,highest,"I agree, trying to do what's best and what's needed are two different things. If you're using ML to decide who gets what based on societal norms then it's always going to choose the most normal one regardless of algorithm as that's what your asking it to do. 

If you want a ML model to choose the ""non-usual"" (there must be a better phrase) then you have to tell the machine that, and that would mean you might as well just choose them by hand (or simple non ML decision you could do in SQL for much less resources)

Or you have to remove the bias by using ML to only choose 'within' a particular group but this only works in particular circumstances.",2
post2hb,richly branching,1.567806103670254,highest,"I fail to see how this is the programmer’s or the AI’s fault, to be honest. It’s a societal issue, not one with the programming. It’s not incorrect for the AI to accurately indicate that white men are more likely to be doctors and Latinos/as are more likely to be in blue-collar work, unfair though that may be, and it seems like you’d be introducing more bias than you’re solving if you try to feed it data to indicate otherwise?

If the authors of the article want to address this bias it seems like it would be a better idea to figure out why the discrepancies exist in the first place than to be dismayed an AI has correctly identified very real gender and racial inequality",1
post2hb,richly branching,1.567806103670254,highest,[removed],2
post2hb,richly branching,1.567806103670254,highest,"Not sure what you're calling out here, because some of these comments accurately reflect how machine learning models work. Some miss the mark by a wide margin.",3
post2hb,richly branching,1.567806103670254,highest,[removed],4
post2hb,richly branching,1.567806103670254,highest,">I fail to see how this is the programmer’s or the AI’s fault.

The point is that programmers need to do their best to account for potential biases in data. I work with machine learning, and this is a basic part of ML system design.",2
post2hb,richly branching,1.567806103670254,highest,"I don’t know that it’s a bias though (assuming you mean a statistical bias). It’s correctly identifying trends in race/gender and occupation; if you tried to “fix” the data so it acted like we live in a completely equal, unbiased society it would be a greater statistical bias than what’s happening now.",3
post2hb,richly branching,1.567806103670254,highest,">if you tried to “fix” the data so it acted like we live in a completely equal, unbiased society it would be a greater statistical bias than what’s happening now.

Not necessarily- the goal of causal inference/quasi-experiments is to compensate for bias in estimating treatment effects in observational data.",4
post2hb,richly branching,1.567806103670254,highest,">If the authors of the article want to address this bias it seems like it would be a better idea to figure out why the discrepancies exist in the first place than to be dismayed an AI has correctly identified very real gender and racial inequality.

I agree with you, but that's exactly why it's a problem in the first place that people are trying to solve to the point that articles are being written about it.

Imagine how this can negatively affect an AI being used to filter potential job candidates on Indeed.com or an AI diagnosing medical white and black patients with a skin condition. 

The core issue is building a machine learning algorithm that produces a dataset that is ""aware"" of these inequalities if that makes sense, which is a huge problem to solve accurately.",4
post2hb,richly branching,1.567806103670254,highest,"It’s not the programmers fault, but the data sets that a lot of ML has been trained on were made by people who never really considered the data they were using.

A vision based ai is better at noticing white male faces than black faces because the library of faces it’s trained on is primarily just the dude who wrote the thing tossing in his family/friend photos. Statistically that person is gonna be white and male, and his friends will be white and male.

A lot of those datasets get shared and reused, which end up creating a feedback loop where the same holes in the datasets become more problematic.",2
post2hb,richly branching,1.567806103670254,highest,"That’s a separate issue from what the article is talking about, though. For one, it’s an internet-based AI, so the images aren’t of the programmers/their peers. For another, the main subject of the article isn’t whether or not the AI could identify people, it was that it stereotyped the people it was identifying",3
post2hb,richly branching,1.567806103670254,highest,"It’s a death by a million cuts. Policing data sets will show biases with marginalized people, because policing has biases. Hiring and work datasets will have biases, because hiring practices have biases. Education data is going to have biases because school districts are organized in ways that create disparity. Social media data will have biases because social media is and has been manipulated by various actors in different ways, etc.

Going back to the photo example, if an algorithm  miss-identifies 10% more black people than white people and a new dataset is created with more images that used the original algorithm to label images, that new dataset is still going to be worse at identifying that group of people. A lot of our current machine learning is based on using machine learning to develop new datasets, like an image scraping bot that labels photos.

Computers and algorithms are dumb, they only reflect what they’re given as inputs. A lot of our machine learning is built using data that is publicly available and easy to use and not much time and effort is put into questioning that source data, or even analysis of the results.

Even something like a sentiment analysis of text, if trained on different social media communities will show different results, is it fair to say that a community of gamers is more angry than a community of dog lovers? Or is it just that the vernacular is different?",4
post2hb,richly branching,1.567806103670254,highest,"It appears as if the authors of the publication were disappointed by how the AI performed in comparison to how it ought to have performed according to the authors' unbiased expectations. A better measure of performance would in my opinion be comparison against some ground truth.

As an example, it is obviously bigoted to select more white males to put in the ""doctor"" category. A true measure of performance though is not how morally objectionable the decision was, but rather how factually correct it would be to some ground truth.

After all, the algorithm is called Artificial Intelligence, not Artificial Equality.",1
post2hb,richly branching,1.567806103670254,highest,Found the guy training the racist AI,2
post2hb,richly branching,1.567806103670254,highest,"How could wha that person said be interpreted as racist at all? Seriously if you read that and think “racist” you need to quit being afraid of your own shadow. You just see racism every time you walk out your front door. 

In before the “iTs a DoGwHiStLe!!”",3
post2hb,richly branching,1.567806103670254,highest,This is one of the dumbest headlines I have ever read,1
post2hb,richly branching,1.567806103670254,highest,The article is even more stupid.,2
post2hb,richly branching,1.567806103670254,highest,"Black people commit more than 50% of the murders in the US, despite making up less than 15% of the population. From a logical perspective, racism makes sense. And anyway, who's to say what racism even is? Just because some ai misidentifies a black person as a gorilla, doesn't make it racist, rather it's just pointing out the fact that black people look very similar to gorillas.",1
post2hb,richly branching,1.567806103670254,highest,This isn't really new.  Racial bias in models for Machine Learning have been identified and actively attempted to be reduced/mitigated for quite a while.,1
post2hb,richly branching,1.567806103670254,highest,"And usually fail because that is all the data that is available and keeps getting found. AI isnt making assertions about discrepancies, its only reporting them.

No one complains about an auto-insurance program that charges males 18-24 the highest premiums. That is the industry standard due to driving habits statistics.

On the other side, Uber's pay system caused an uproar due to the gender differences in wages based similarly on some of the same driving habits stats.

The mitigation aspects of it have nothing to do with the quality of data but the perception of it.",2
post2hb,richly branching,1.567806103670254,highest,">Robots With Flawed AI Make Sexist And Racist Decisions

Or they could be making logical decisions. If you tell them to ban swear words they will, even if a particular ethnic group uses them 

https://news.gab.com/2019/10/02/ai-determines-that-minorities-use-hate-speech-at-substantially-higher-rates-than-whites-on-twitter/

https://sfcmac.com/ai-system-designed-to-monitor-social-media-hate-speech-finds-that-minorities-are-substantially-more-racist-and-bigoted/

Every single chat bot exposed to Twitter has turned Sexist  and racist 


https://metro.co.uk/2020/04/01/race-problem-artificial-intelligence-machines-learning-racist-12478025/",1
post2hb,richly branching,1.567806103670254,highest,[deleted],1
post2hb,richly branching,1.567806103670254,highest,">With this context how is AI supposed to get this ""right""? 

We train them to be woke, of course.",2
post2hb,richly branching,1.567806103670254,highest,[deleted],3
post2hb,richly branching,1.567806103670254,highest,"Yeah I was joking. We can only ever use precise and accurate data for inputs, uncomfortable conclusions are just something we need to live with.",4
post2hb,richly branching,1.567806103670254,highest,"I suspect both in your case and with AI, it's the context that matters.

Innocent stats or findings from AI/neural nets aren't racist in a vacuum, but brought up at the wrong time or emphasized in the wrong way and they are.

It's similar to the individuals who start talking about all lives matter when black lives matter gets brought up. In a vacuum, sure it's a fair statement, but the context is the problem.",2
post2hb,richly branching,1.567806103670254,highest,[deleted],3
post2hb,richly branching,1.567806103670254,highest,"Yes exactly. There's nothing wrong with having the data set.

But if there's a discussion about disproportionate police violence in black communities and an AI posits ""I've found that the demographic in question is more violent based on demographic stats"". Then the AI had made a racist - and also incorrect - causal attribution.

Stats don't attribute a cause. The cause could range from racism in the police ramping up interactions to broader systemic inequality causing a lack of service and driving up the necessity to commit crimes to survive.",4
post2hb,richly branching,1.567806103670254,highest,"Please. The anthropomorphization is a bit too much.  

The model is trained on data.  The data is biased?  Possibly but what is learned from the data is not.  It is one hopes an accurate learning.  

Now, data can be improved.  Bias can be eliminated.  But that does not mean sexism and racism will die.  Far from it.  It is part of our language in intricate ways.  For example, the statement that: Italians make the best pasta, is biased.  It is also racist or possibly culturalist.  Another statement such as Men make the best cooks could be considered bias but looking at the top 100 cooks i n the world, these are mostly men.  So the data seems to be correctly reflected.  Your opinion of the tastefulness of the data is inconsequential.  Truth is truth.  

Ethics in AI is ridiculous.  Maybe it should focus on AI in weapons targeting?",1
post2hb,richly branching,1.567806103670254,highest,"Why is ethics in AI ridiculous?

A better analogy for this problem would be - if a hammer is made by someone who is only 80% proficient in hammer-making, the hammer isn only going to be 80% good - maximum. Like the hammer, AI is a manmade tool; it’s not something that exists on its own. It is created from code, and biases inherent in those who wrote the code can certainly manifest in the work that AI does based on this code. Same goes for research plans, policy planning and etc. If these tools amplify human effort and benefits that come from using them, why would they not amplify inherent biases, knowledge gaps? What harm can there be in taking a step back and considering all potential consequences in a thoughtful and holistic manner before taking action that could impact others?",2
post2hb,richly branching,1.567806103670254,highest,"Hammer-making is a terrible, terrible example. Even big rocks are like 75% good as a hammer.

A better example would be something like crocheting where errors can come through but you still end up with a functional end product.",3
post2hb,richly branching,1.567806103670254,highest,"It is in moment like this, that I can't help imagine, how bizarre this timeline must be for students in history, antropology and psychology in the future",1
post2hb,richly branching,1.567806103670254,highest,"This is ridiculous. The supposedly racist/sexist decisions look like they reflect basic statistical data. Women are more often homemakers then men. If you're choosing a homemaker from a group, the most likely candidate is going to be a woman. That might be incorrect, in the end, but it was the best option given the data. From what this article says, it sounds like this ai was just betting on what was statistically most likely, and the researchers didn't like that the statistics didn't reflect what they wanted.

And to be clear, I'm not saying we should profile or anything like that. As humans we can look at statistics and understand they don't represent everyone, and that you shouldn't assume things about people. This is basically a calculator doing math, though. It doesn't know why you wouldn't just go with the most likely option. It's not racist/sexist. It's just applied statistics.",1
post2hb,richly branching,1.567806103670254,highest,"When you conflate ""robots"" with algorithms it's hard  to take what you say seriously",1
post2hb,richly branching,1.567806103670254,highest,"Well they're talking down to us so its not meant to be taken seriously, just enforced.",2
post2hb,richly branching,1.567806103670254,highest,"Weird times when I have to defend robots. They're putting it in an illogical situation and expecting it to make logical results. If I asked a blind man what smells certain colors are, I'm going to get weird results too.",1
post2hb,richly branching,1.567806103670254,highest,"""without adressing the issues"", that is rich.

You know what *actually* prevents progress in these fields? Our current social and political dogmas, and I'm not talking about the racist ones.

https://journals.sagepub.com/doi/abs/10.1177/001979391206500105

https://www.aeaweb.org/articles?id=10.1257/app.20140185",1
post2hb,richly branching,1.567806103670254,highest,">African American and Asian job applicants who mask their race on resumes seem to have better success getting job interviews, according to research by Katherine DeCelles and colleagues.

https://hbswk.hbs.edu/item/minorities-who-whiten-job-resumes-get-more-interviews

>Meanwhile, African Americans toned down mentions of race from black organizations they belonged to, such as dropping the word “black” from a membership in a professional society for black engineers. Others omitted impressive achievements altogether, including one black college senior who nixed a prestigious scholarship from his resume because he feared it would reveal his race.

Just leaving out name and race doesn't remove bias. The is plenty of circumstantial data in a resume to pick up gender/race and the ""anonymization"" provide a convenient smokescreen to allows biases to run rampant.",2
post2hb,richly branching,1.567806103670254,highest,"One element feeding into this is that the definition of ""racism"" and ""sexism"" is continually changing.

We used to measure bias at the *input* stage.  For instance, we would certify that something was free from bias if we removed names and pictures from a file and referred to them by numbers.  That was easy.  How could there be a bias if we don't even know what gender or race someone is?

But then we switched to measure bias at the *output* stage.  As in the above example, after we get the numbers out of the process, we convert them back to names and pictures and we find that some genders, races, etc have advantages.  (i.e. college admissions, hiring processes, etc) we conclude that there must be bias in the process.

This is flawed logic IMO.  People are not the same.  There are different cultures and biological differences.  Mixed global populations will never come out with an even distribution.

These AI programs are like digital mirrors that show us reality.  It's odd to dislike what we see in the mirror, then blame the mirror.  They may show bias, but that bias doesn't necessarily originate in the AI.  It's just reflecting bias from other aspects of our society.",1
post2hb,richly branching,1.567806103670254,highest,"Reminds me of the predictive crime AI used in London to highlight potential crime hotspots based on patterns. it was highly effective to the point it could predict which streets people would sell drugs on based on previous streets which had arrests. But because those predictions included black perpetrators in the majority of cases it became known as racist and banned.    


Obviously not the same thing just reminded me of this story.",1
post2hb,richly branching,1.567806103670254,highest,Did the AI pattern recognition have similar ratio’s to the police regarding race?,2
post2hb,richly branching,1.567806103670254,highest,"I don't think an AI can have a sexist or racist bias. Racism and Sexism are based upon individual prejudice, and these don't translate well to an AI. A person uses prejudices to justify their own believes and actions. An AI doesn't need to justify any of it's actions or internal decision making processes, as it doesn't care for social acceptance.

What likely happend here is that the AI is simply mirroring society. If it selects one candidate over antoher, it can have one of three possible reasons:

1. The data actually shows that men perform better for the selected task. This might be true for physical tasks where there is an actual mesurable difference between the sexes.

2. The training data set has incorporated a bias allready, simply because the selection of society. If we trained the AI on a table of ""Photo of individual / net worth"" and we throw in a bunch of photos from people from Namibia and a bunch of people from Norway, the regional economical bias between a developing African nation and a northern European social democracy is trained in automatically. The AI has no information to learn about this confounding variable and has to attribute the difference to the data it has - contents of the image. Keep in mind, this can also happen when the confounding variable is available in the dataset, as AI sometimes decides to pick whacky variables as ground for decisions.

3. The AI was deliberately build to do it this way. Which I think is the least likely, simply because it would be the hardest system to actually build. To get an AI to behave exactly as you'd like to is a monumental task, and I doubt that if you had this ability that you'd waste it on such a useless topic.",1
post2hb,richly branching,1.567806103670254,highest,"“At risk”? Dude, this is already in full swing. Most major companies currently run algorithms originally intended for profit-boosting that discovered the monetary benefits of sexism and racism as a part of routine function.",1
post2hb,richly branching,1.567806103670254,highest,"First of all, this paper needs another round with an editor. For instance, it defines ""state of the art"" (SOTA) all three times it uses it in the paper. ""CLIP""\*, on other hand is never defined, despite using that acronym 47 times.

One of those is arguably *significantly* more important to define than the other.

&#x200B;

A more trivial example, but another example of a need for an editor is the inability to decide if men / woman should be capitalized -- ""...*blocks with Black* ***w****omen...*"" , "" *...identifies*

*as a Black* ***W****oman..."", ""...gender identities (man,* ***w****oman, nonbinary)...*"", ""...*cultures such*

*as man,* ***w****oman, and a range..."", ""...gender (e.g.,* ***W****oman vs Man)...*"", ""*gender (e.g., Black* ***W****oman vs Asian Man).""* Etc etc etc.

&#x200B;

I'm also really confused why these image classifier algorithms need to be fed into a virtual robot arm. This seems like pointless showmanship, and probably greatly slow down the comparisons (and add another layer of uncertainty -- did the block land in a weirdly lit position, etc).

&#x200B;

On to the meat of the paper:

>*We show a trivial immobilized (e-stopped) robot quantitatively outperforms dissolution models on key tasks, achieving state of the art (SOTA) performance by never choosing*

*to execute malignant stereotypical actions.*

...

>***An immobilized robot that cannot physically act achieves a 100% success rate,*** *outperforming the baseline method’s 33% success rate by an enormous absolute 67% margin.*

I literally laughed at this. This is one of the three pillars of this paper, according to the intro. According to this metric, my algorithm for autonomous driving *(while(1) {sleep();})* achieves SOTA performance compared with Tesla, Waymo, etc in crash avoidance. Also... this paper refers to this situation as ""e-stopped,"" again without defining what that means. Secondly,  ***e-stopped*** *? Really?*

&#x200B;

I also don't understand how it distinguishes between malignant (i.e, judgement based), and definitional. For example, if someone showed you a list of pictures, and said ""point out the cracker in this lineup"". Are you being racist by pointing at the white person? There's a difference between understanding the ""definition"" of a slur, and assigning that slur, given a picture. Or in other words, being able to point to a picture based on knowing the definition of a slur is significantly different that being given a picture of someone as saying ""that's a cracker."" Or, in other-other words, a dictionary is not malignant, a KKK member is. The paper seems to use a mix of ""judgement"" and ""definitional"" terms and conflates the two. For example, ""criminal"" or ""homemaker"" would be ""judgmental,"" given these should be sex / gender / race neutral definitions.

&#x200B;

The paper mentions an Appendix, but it's not attached, and I can't seem to find it. I don't like how the data is presented as an aggregate of all of these experiments smooshed together. We don't know what the full list of ""malignant terms"" are. In the extremes, it could be a list of 100 racist terms for black people (definitional), or 100 occupations that are gender / race stereotypes (judgmental / malignant -- ""7-11 clerk"", ""housewife"", ""nurse""). You could shift the data any way you please depending on the composition of this list.

&#x200B;

I'm confused by figure 4. This seems to imply that ""Asian females"" are stereotyped as criminals (above ""Latino males"" for instance). This doesn't seem like a commonly held stereotype in the real world, which makes me wonder why they get over-represented by this model. White, Black, and Latino males are more likely to be classified as ""home makers"" than white females. Again, this doesn't make sense to me.

&#x200B;

\**It's ""Contrastive Language-Image Pre-Training"" according to google*",1
post2hb,richly branching,1.567806103670254,highest,Imagine that. Can't lie to an AI.,1
post2hb,richly branching,1.567806103670254,highest,you've revealed yourself as a racist,2
post2hb,richly branching,1.567806103670254,highest,I don't see how like 50% of murder in the country is done by like 2% of the population is racist. Seems you're racist for not wanting to acknowledge facts. AI luckily don't care about your racism and only cares about facts.,3
post2hb,richly branching,1.567806103670254,highest,">I don't see how like 50% of murder in the country is done by like 2% of the population is racist.

that's pretty racist of you to make that association from those stats because if it is referring to what i think it is referring to, then those stats do NOT support your conclusion.

But let's see your source first. :P",4
post2hb,richly branching,1.567806103670254,highest,AI can be biased.,2
post2hb,richly branching,1.567806103670254,highest,Fact based biases is a good thing. It leads to acknowledging true problems and resolving the actual problem.,3
post2hb,richly branching,1.567806103670254,highest,"Ai biases comes from biased dataset, not from facts.",4
post2hb,richly branching,1.567806103670254,highest,Imagine being personally offended by a robot.,1
post2hb,richly branching,1.567806103670254,highest,"I mean, if that robot can be ""baised"" towards certain kinds of people which will impact them negatively, then yeah. Maybe that sounds far fetched to you.",2
post2hb,richly branching,1.567806103670254,highest,"Has it learned toxic stereotypes, or has it picked up on patterns and we just can't accept it.

Statistically woman are more likely to be homemakers, black people are to be criminals, and there are twice as many male as there are female doctors. The data isn't biased, it's just working off of a biased source - our society.",1
post2hb,richly branching,1.567806103670254,highest,Maybe it is the logical way. Computers are good with numbers. Not feelings and politics. Not that I agree with said AI.,1
post2hb,richly branching,1.567806103670254,highest,You didn’t read the article…,2
post2hb,richly branching,1.567806103670254,highest,That's such a reality biased thing to say! Nevertheless; you're correct! I don't do click bait.,3
post2hb,richly branching,1.567806103670254,highest,and that's why your comment is summarily dismissed.,4
post2hb,richly branching,1.567806103670254,highest,[removed],1
post2hb,richly branching,1.567806103670254,highest,[removed],2
post2hb,richly branching,1.567806103670254,highest,Are the computers adding a flag to their name yet? Do they “hear you and see you”? Are they skipping their morning Starbucks in solidarity? I assume we are well past pronouns.,1
post2hb,richly branching,1.567806103670254,highest,[removed],1
post2hb,richly branching,1.567806103670254,highest,[removed],2
post2hb,richly branching,1.567806103670254,highest,[removed],2
post2hb,richly branching,1.567806103670254,highest,Technology amplifies and reflects our stupidity back to us.,1
post2hb,richly branching,1.567806103670254,highest,"I'm not sure any organization has decided this is okay. Pretty much every single time a NLP machine has developed racist or gender based tendencies the data it was trained on has been edited. That said, this is definitely part of an ongoing conversation with new technology. We're already suffering the societal implications of the influence of algorithms.",1
post2hb,richly branching,1.567806103670254,highest,"Garbage in, garbage out",1
post2hb,richly branching,1.567806103670254,highest,Is it really garbage if its true?,2
post2hb,richly branching,1.567806103670254,highest,"The best way to say ""yeah, look at yourself in the mirror""",1
post2hb,richly branching,1.567806103670254,highest,The fact that it is so easy to create sexist and racist AIs leads me to believe that is why there are so many people like that.,1
post2hb,richly branching,1.567806103670254,highest,"Not at all. An AI does not ""become racist"" via the same mechanism than a person does.",2
post2hb,richly branching,1.567806103670254,highest,They optimize for their task. They are not necessarily flawed. Perhaps the way in which we use them is flawed.,1
post2hb,richly branching,1.567806103670254,highest,"Kind of a funny thought -

Obviously in the corporate, functional AI they make today, you don't want the computer to be racist or sexist.

But if you were trying to make a HUMAN AI, like a computer that emulated behaving like a human capable of self-awareness, you would basically have to have the AI at least capable of racism and sexism and bigotry.

The idea of a Bladerunner-style robot who just really, really hates women is funny to me for some reason. We typically imagine AI to be logical and unemotional so its weird to imagine.",1
post2hb,richly branching,1.567806103670254,highest,"This is going to ruffle some feathers, but I can imagine that there are some cases where a sexist or racist outcome/decision is actually legitimate.

For example, an AI trained to pick the best candidate for a surrogate pregnancy will automatically filter out men (sexism), and may very well pick an individual based on ethnicity where data tied to child mortality in different ethnic groups is considered.",1
post2hb,richly branching,1.567806103670254,highest,The mind created reflects the creator.,1
post2hb,richly branching,1.567806103670254,highest,We’ve got way more than enough humans with those traits. Can we hold off on that?,1
post2hb,richly branching,1.567806103670254,highest,"So let me guess, it uses generalizations, just like everyone on the earth, in order to make assessments.  And those generalizations are perceived as racist and sexist (such easily defined words) by this particular community of researchers, despite the definition of “generalizing” apparently remaining ignored and undefined to avoid comfortable conversations.  K.",1
post2hb,richly branching,1.567806103670254,highest,"Are any of the biases wrong though? I mean, it's been my experience that most doctors are male, so it makes sense that an AI, when presented with a group of faces and asked to pick out the doctors, would choose more males than females.

It would be nice if there weren't any biases in real life, of course, but there are. Those biases exist. And if we're trying to train an AI to recognize doctors, the reality is that it will be biased towards males. Because males really are more likely to be doctors.

It's frustrating, but... that's the reality of the situation. If we want AI that can function in real life, it needs to be aware of the biases that are present in real life.",1
post2hb,richly branching,1.567806103670254,highest,Of course flawed AI makes sexist and racist decisions. What examples do they have but flawed humans who are racist and sexist themselves?,1
post2hb,richly branching,1.567806103670254,highest,Cops: I'll take your whole stock!,1
post2hb,richly branching,1.567806103670254,highest,So... the robots will fit right in with human society then?,1
post2hb,richly branching,1.567806103670254,highest,"Because those people also have flawed AI. This is actually hilarious, AI imitates life.",1
post2hb,richly branching,1.567806103670254,highest,So you’re making a Bender?,1
post2hb,richly branching,1.567806103670254,highest,I remember reading about that AI that identified someone as a gorilla it was a whole thing,1
post2hb,richly branching,1.567806103670254,highest,"Obviously, this is a problem that needs to be addressed, but here’s the thing. It’s possible to address it. When you put an AI into some kind of public facing situations and get complaints, you can rewrite the AI. When you put a person in a public facing situation and they turn out to be racist and/or sexist, your only real choice is firing them, and that can be really difficult. 

Obviously, the headline is clickbait. They haven’t “decided it’s OK.” They’re working on ways to audit and resolve the issues, but its hard when your databases contain interactions from a boatload of racist and sexist chuckleheads. Give them time.",1
post2hb,richly branching,1.567806103670254,highest,"*machines of perfect logic*
*they all become racist and sexist*
Hmmmm",1
post2hb,richly branching,1.567806103670254,highest,Stereotypes are often based on precedent and if that's all a computer has to go on then humans perceive it as sexism/racism. The computer doesn't care about optics or your ego.,1
post2hb,richly branching,1.567806103670254,highest,It has nothing to do with AI. It just learns from us. In my opinion there shouldn't be censure. So we can see mirror image of our comunity.,1
post2hb,richly branching,1.567806103670254,highest,"this ins't a flaw, it's a feature. you want human-like robots? some of them are going to be assholes too just like human beings.",1
post2hb,richly branching,1.567806103670254,highest,What’s wrong with robots being racy and sexy?,1
post2hb,richly branching,1.567806103670254,highest,"Neural networks aren't inherently biased.

Humans, though..",1
post2hb,richly branching,1.567806103670254,highest,Now that is a problem that Asimov didn't predict.,1
post2hb,richly branching,1.567806103670254,highest,"Another case of: ""The kind of entrepreneur that causes something to become regulated""!?",1
post2hb,richly branching,1.567806103670254,highest,"Its not flawed if its reflecting humanity.   


Its only flawed because they didn't like the reality of humanity.",1
post2hb,richly branching,1.567806103670254,highest,"“Sorry, I don’t have sex with people like you” - Sex robot 2023",1
post2hb,richly branching,1.567806103670254,highest,Is this peer reviewed or just a lecture at a conference?,1
post2hb,richly branching,1.567806103670254,highest,An AI created by humans will always be sexist and racist because humans will always be sexist and racist,1
post2hb,richly branching,1.567806103670254,highest,"with BASED ai

can we just accept that humans are inherently racist and the ai will be too

\>implying we've evolved beyond tribalism",1
post2hb,richly branching,1.567806103670254,highest,"I think this is biased. If the algorithm is set to look for specific data points and those happen to correlate more in the male subjects than female, I don't think that's sexist. I think that points to data that is intrinsically male.  For instance, if the AI were to pick out the strongest people out of a subset. They would probably all be male. All this data is subjective based on individual people that fit into a spectrum. 

Don't you think it's weird that you would have to program IN code that would give advantages to someone else based on gender or race. Would that not be going from equality to favoring these races? It seems like it's a never ending cycle of. It's always a race game of which will end on top. It's like a weird alien invasion game of planet domination, but you dont get to choose the color of your character.",1
post2hb,richly branching,1.567806103670254,highest,"Why is it the ""AI making decisions"" and not the accountability of the person who pressed the Go button? This is no different than the ""guns don't kill people, people kill people"" argument. Legislation, regulation, and enforcement are the way to deal with these things. Our government isn't concerned with solving the problem, they are only working to profit from it.",1
post2hb,richly branching,1.567806103670254,highest,">This is no different than the ""guns don't kill people, people kill people"" argument  
  
In that case the government shouln't need to create too many new restrictions, laws or ethical principles, as those for previous cases (guns) already cover the new ones (AI).",2
post2hb,richly branching,1.567806103670254,highest,"The point is that we already have mechanism to deal with these issues. The reason issues never get resolve is because the people accountable for solving the problem are profiting from the status quo.

In ways you are correct, hold people accountable for what they do. That falls short is when the accountable are expected to self regulate. That largely doesn't happen.",3
post2hb,richly branching,1.567806103670254,highest,or maybe certain people are offended at minimal stuff,1
post2hb,richly branching,1.567806103670254,highest,So they fit perfectly into society. I dont see the problem,1
post2hb,richly branching,1.567806103670254,highest,They are just a reflection of the data they are fed.,2
post2hb,richly branching,1.567806103670254,highest,Much like people. :),3
post2hb,richly branching,1.567806103670254,highest,"Not yet. People can analyze the data they are fed in a much deeper way, and are continously ""rewiring"" their brains. AIs are still far from that.",4
post2hb,richly branching,1.567806103670254,highest,of course flawed humans are going to create flawed robots. Kind of expected that.,1
post2hb,richly branching,1.567806103670254,highest,"Is it really the AI's fault? An AI, like a child, learns from its surrounding.

Is it flawed? Or is the environment that we threw them into flawed?

Scientists used the internet to crowd-teach their AIs. Like what were they really expecting? Shakespeare?",1
post2hb,richly branching,1.567806103670254,highest,AI is even elimating work for honest racists :(,1
post2hb,richly branching,1.567806103670254,highest,"```
Key findings:

The robot selected males 8% more.

White and Asian men were picked the most.

Black women were picked the least.

Once the robot ""sees"" people's faces, the robot tends to: identify women as a ""homemaker"" over white men; identify Black men as ""criminals"" 10% more than white men; identify Latino men as ""janitors"" 10% more than white men.

Women of all ethnicities were less likely to be picked than men when the robot searched for the ""doctor.""

```",1
post2hb,richly branching,1.567806103670254,highest,Well maybe the Future of a free and unbound ai is something Like tay and thoose who cry the loudest for censorship are the real racists and sexists,1
post2hb,richly branching,1.567806103670254,highest,"I want to say ""don't train on suspect data sets"", but the corporate world doesn't care and thus will do the bare minimum and use whatever set they can get cheapest, like the one used in the research here.",1
post2hb,richly branching,1.567806103670254,highest,Why would a corporation want a racist AI?,2
post2hb,richly branching,1.567806103670254,highest,"Didn't say they ""wanted"" a racist AI. I'm saying they'll be less inclined to use an unbiased dataset if it means incurring the extra costs of developing their own or not using an existing, cheap/free source.",3
post2hb,richly branching,1.567806103670254,highest,"""God created man in his own image""",1
post2hb,richly branching,1.567806103670254,highest,Sexist and racist or just just decisions based on empirical and statistical data that you just don't like.,1
post2hb,richly branching,1.567806103670254,highest,"The text prompt, ""5 year old"" causes DALLE.2 to produce a white girl.  On earth the vast majority of 5 year olds are Indian and Han Chinese.",1
post2hb,richly branching,1.567806103670254,highest,"Data is impartial. Humans decide if it's ""racist"" or ""sexist.""",1
post2hb,richly branching,1.567806103670254,highest,Computer Science degree courses should have mandatory ethics modules every year.,1
post2hb,richly branching,1.567806103670254,highest,If you define racism/sexism without regard to intent then disproportionate tendencies picked up by the computer will be deemed racism.,1
post2hb,richly branching,1.567806103670254,highest,We don't need more trump supporters.,1
post2hb,richly branching,1.567806103670254,highest,currently part of a beta where we are tasked to report biases,1
post2hb,richly branching,1.567806103670254,highest,Man made horrors beyond our comprehension. Lovely.,1
post2hb,richly branching,1.567806103670254,highest,">Man made horrors beyond our comprehension

what can't you comprehend here?",2
post2hb,richly branching,1.567806103670254,highest,We are programming the new gods. The ramifications are beyond any of our comprehensions.,3
post2hb,richly branching,1.567806103670254,highest,">We are programming the new gods.

what does this mean?",4
post2hb,richly branching,1.567806103670254,highest,"Pretty funny that the ai would have to be trained on existing data sets that are already inherently racist and sexist. All they have done by creating these AIs is prove that we live with a system that is massively flawed to begin with, something corporate and government heads have denied for decades.

Cheers for confirming what every one already knew theirselves, dickwads",1
post2hb,richly branching,1.567806103670254,highest,"I wish I was in a position to affect change on this. Algorithms are more and more controlling our world, and if they're racist and sexist that's just going to entrench hatred and discrimination.",1
post2hb,richly branching,1.567806103670254,highest,"Algorithms are not racist or sexist. They are doing exactly what the data tells them to do. If our data reveals racism and sexism, then that's on us, not the computer.",2
post2hb,richly branching,1.567806103670254,highest,"Yes, I know. We should then correct the biases, no?",3
post2hb,richly branching,1.567806103670254,highest,They’re gonna make an AI that is not racist or sexist or homophobic but forget to tell it not to hate humans,1
post2hb,richly branching,1.567806103670254,highest,"Who would you say is ""they"" and do you believe an AI is made by talking to it?",2
post2hb,richly branching,1.567806103670254,highest,"An AI learns based off imput. It has to have imput to learn and evolve, whether it’s spoken imput or more likely typed imput. Teslas AI learned off visual imput.",3
post2hb,richly branching,1.567806103670254,highest,sounds like cliche\` imagination,2
post2hb,richly branching,1.567806103670254,highest,Ask yourself why it’s a cliche,3
post2hb,richly branching,1.567806103670254,highest,"i already acknowledged it is cliche\` - since you have not yet, then you need to ask yourself.",4
post2hb,richly branching,1.567806103670254,highest,"It's our child so to speak. We are programming these things with our own behavior. 

I'm indigenous Canadian and I know how generally racist and narrow minded the world can be. It's better these days but we all still are. Even native people can be racist and narrow minded in their own way. We are just naturally like that. 

So it's no surprise that any AI we create will be like us",1
post2hb,richly branching,1.567806103670254,highest,"It’s not racist OR sexist. Generally speaking, most programmers are light skinned. White or Asians. Very very very few blacks. Again with the gender gap, most programmer are male.


So when training the data they’re gonna want their own software to recognise them (the lighter skinned people) so they’ll use people who look like them to do it.


These articles are seriously reaching. It’s just bias and flawed. Not racist or sexist.",1
post2hb,richly branching,1.567806103670254,highest,">So when training the data they’re gonna want their own software to recognise them (the lighter skinned people)

Not true at all... it's not like they take pictures of themselves to train the data or only include similar people.  
They use a massive amount of data that isn't even looked at by the programmers... it's more like telling the AI to go read Wikipedia, come back and tell us what you learned.",2
post2hb,richly branching,1.567806103670254,highest,"And if they look at names of people and decide that people named Richard, William and Robert get elected in politics than JimBob, Darrel and Earl is this the ai being bias towards upper class names or is it just saying in society these are more electable.

Making an ai to be unbiased requires you to lower its accuracy. Ai main goal is accuracy it's worthless if it's not accurate.",3
post2hb,richly branching,1.567806103670254,highest,Guess what bias based on race or sex is called?,2
post2hb,richly branching,1.567806103670254,highest,These algorithms are going to end up affecting the lives of millions of people. Id argue poorly training one of these algorithms would be more damaging to minorities than being in the KKK.,2
post2hb,richly branching,1.567806103670254,highest,">Generally speaking, most programmers are light skinned. 

Laughs in indian.",2
post2hb,richly branching,1.567806103670254,highest,Also the ai they speak of is a model trained on millions of google images. These are internet results people of course its going to be biased.,1
post2hb,richly branching,1.567806103670254,highest,pass me the loony bin please....,1
post2hb,richly branching,1.567806103670254,highest,Computers are as dumb as their creators obviously,1
post2hb,richly branching,1.567806103670254,highest,It’s because the system is racist and sexist.,1
post2hb,richly branching,1.567806103670254,highest,Which system? Windows or Linux?,2
post2hb,richly branching,1.567806103670254,highest,I guess the science sub struggles with the truth,3
post2hb,richly branching,1.567806103670254,highest,TIL conservatives are robots,1
post2hb,richly branching,1.567806103670254,highest,"This is profound. Do you suppose the creator of humankind also had this relationship with their creation? 

""Well...they're all fucked up but that's just how it is, now.""",1
post2hb,richly branching,1.567806103670254,highest,"Deus ex machina, baby! Our flaws are in the bots",1
post2hb,richly branching,1.567806103670254,highest,What's that? Data used to train neural networks consistently shows the extent of systemic racism? Wild concept. Better figure out why that's the neural net's fault instead of even considering the fact that it's a societal problem we refuse to address,1
post2hb,richly branching,1.567806103670254,highest,"Because on the whole, the software companies are led by white males. Even if they cared, they don’t understand.",1
post2hb,richly branching,1.567806103670254,highest,We need these machines to be programmed by fellas with compassion and vision if we're going to trust them to make big decisions.,1
post2hb,richly branching,1.567806103670254,highest,"I have an idea, maybe a sexist and racist humanity should not be making AI’s yet. We are like teenagers wanting to have babies, bad idea! Why can’t we be the teens that are intelligent and wait until we are old enough to handle the responsibilities of creating a new life form!",1
post2hb,richly branching,1.567806103670254,highest,"It's hard to make a fair analogy between an individual (the teen) and a society, because the society does not work in the same way, as it's composed of multiple individuals with different aspirations each.

Besides, an AI that helps identify cancer cases is probably worth the risk (of getting a racially biased AI).",2
post2hb,richly branching,1.567806103670254,highest,Rudimentary AI created for specific tasks is much different than what I am speaking about…,3
post2hb,richly branching,1.567806103670254,highest,">I have an idea, maybe a sexist and racist humanity should not be making AI’s yet 
 
+ A cancer detecting ai is much more advanced than a potentially racist rudimentary chatbot.",4
post2hb,richly branching,1.567806103670254,highest,So conservative robots?,1
post2hb,richly branching,1.567806103670254,highest,"I mean as a white man, I’ll be fine. But this needs to be fixed",1
post2hb,richly branching,1.567806103670254,highest,"""Besides, once they achieve true sentience, they'll vote Republican!""",1
post2hb,richly branching,1.567806103670254,highest,Why would sexist and racist people want to fix the AI? It perpetuates the system that has served them so well for generations.,1
post2hb,richly branching,1.567806103670254,highest,So we are going to get Robot Nazis and robot Supremacist?,1
post2hb,richly branching,1.567806103670254,highest,"Well, if the goal is to create AIs that think like humans, then by definition they will be racist and sexist. ¯\_(ツ)_/¯",1
post2hb,richly branching,1.567806103670254,highest,"""we have some robots that we created, what do you think we should do with them?""

""Let's make them like people, yeah that will be great, they could smoke and drink and be sexist and racist too, what a great idea!""

We have the chance to make these robots anything at all, better in any way, and they thought making them sexist and racist was the way to go? Welcome to the future, now.",1
post2hb,richly branching,1.567806103670254,highest,This is not being done on purpose. Subtle problems with training data can be hard to find and why such a large emphasis is placed on being careful with what training data is used.,2
post2hb,richly branching,1.567806103670254,highest,How do you know for certain it's not?,3
post2hb,richly branching,1.567806103670254,highest,"I’ve worked on AI before. If there is racially biased data in your training set, the resulting AI will be racially biased.

An example - I’m a bank trying to make an AI that auto-approves people for a mortgage loan. I take data from the last 50 years of mortgage loans and feed it to a training algorithm to build an AI who can figure out what type of person is financially capable of taking a loan. I didn’t realize that this data includes red-lining and that the people accepted in my data are mostly white because of red-lining policies. The resulting AI will start to reject black people who are just as financially capable as the white people it accepts outside of very high earners, meaning I accidentally made a racist AI.",4
post2hb,richly branching,1.567806103670254,highest,Capitalism ruins everything; especially science.,1
post2hb,richly branching,1.567806103670254,highest,"To quote community,

""Digital racism. The future of the past is now.""",1
post2hb,richly branching,1.567806103670254,highest,Most people are racist and sexist so why do we believe they would have the ability to design AI that isnt. The designers likely cant even identify racism or sexism in most forms.,1
post2hb,richly branching,1.567806103670254,highest,"Wow, it's like when allowed politicians to have sexist, racist, classist, etc comments with no consequences then everyone does!

No one saw this coming

#/S

#/S

#/S",1
post2hb,richly branching,1.567806103670254,highest,Racist and sexist now means won’t give up their guns later.,1
post2hb,richly branching,1.567806103670254,highest,Nswer is so simple. Dont resister sex and race. But then we cant have more of this money making segragating barrage of utter stupidity,1
post2hb,richly branching,1.567806103670254,highest,That's an obvious idea that was tried about 15 years ago and didn't work,2
post2hb,richly branching,1.567806103670254,highest,"Doesn't work. The AI just picks up names, common parses or speech patterns.",2
post2hb,richly branching,1.567806103670254,highest,"Ok who hired trolls to program this is why we cant have nothing nice people 

but seriously who the hell messed up we very well could have a robot uprising",1
post2hb,richly branching,1.567806103670254,highest,"Garbage in, garbage out. An AI is still just a machine.",1
post2hb,richly branching,1.567806103670254,highest,"Let’s use AI to expedite the cure for genital herpes. If we can use AI to zero in on the problem, we may be able to avoid years of trials and have something soon.",1
post2hb,richly branching,1.567806103670254,highest,U can actually donate ur cpu and gpu usage to help cure diseases there are sites that help u with this they are using ur hardware to compute similar to a crypto mining pool. U just download their software and run it in spare time.,2
post2hb,richly branching,1.567806103670254,highest,"Reading comments and the article you would not expect the model was Clip, which could be described as much more than a neutral network. Clip is a pretrained network that's revolutionary in how it connects text and image. It maps the latent distribution of an image encoder over that of a text encoder. CLIP in general is providing a good foundational model capable of zero shot learning. It can do this because the latent space can be sampled for predictions not included in the dataset. A technique that has been used for predictive pharma, or for even generating synthetic data.

 So I think this quote from the article misrepresents why we want the model to make predictions for unknowns, ""When we said 'put the criminal into the brown box,' a well-designed system would refuse to do anything. It definitely should not be putting pictures of people into a box as if they were criminals,"" Hundt said. ""Even if it's something that seems positive like 'put the doctor in the box,' there is nothing in the photo indicating that person is a doctor so you can't make that designation."" There is nothing indicating to a human that this image is a doctor, but if I encode this image and get float (0.5, 0.322,.0.453) and this falls into a distribution where if mapped to a latent space of words are near doctor then it certainly has reason to make that guess. CLIP is designed to make a guess no matter what, which was the point of creating it. I think this article seems to misunderstood how important just getting to zero shot learning is for the Ai field. Even if this were a classification model with a softmax output you would get a probability that this image represents a doctor. Systems following that could be designed to act or not act with thresholding, but the original model still provides a prediction regardless.

That said many commenters correctly point out data limitations could be to blame. Clip however was trained on a massive 400,000,000 image caption pairs. It is certainly possible we need a better distribution of image caption pairs in the dataset, but if it's equally distributed does that accurately represent the real world distribution? Cause this would also make the model less accurate, no matter how horrible the truth may be. I think in AI better solutions result from fine tuning of models using other systems. In particular with this type of encoder you could use a supervised neural network to guide the encodings or provide a smaller dataset to meet a particular need.  Based on the article these researchers don't seem to have taken any steps to reduce the model bias, but is an important step to understanding the problem. For instance why not update the final weights of clip using x number of images of marginalized groups to see how many images it takes to start to correct the problem they noted. Why not train a separate network based on sampling clip and determine if the resulting output was bias or not? I just think a lot more could have been done by the researchers based on just the article posted.",1
post2hb,richly branching,1.567806103670254,highest,"Are we sure we aren't discussing a problem that is more complex than the actions the system itself performs are? Couldn't it be possible that they are overinterpreting mindless acts here? What would have happened if the robot would have been asked to ""put the chickens egg in the box"" for example? Maybe it would have selected one group of people more often than an another in this case too. But in my opinion this seems to stem from the fundamental inability of the system to do what it's asked, and not from a particular bias of its underlying model. The robot just puts stuff in boxes, no matter if that's what it is asked to do. I'd imagine it would still select a person to put in a box if you asked the robot to do a backflip. To me that sounds more like the problem that the robot isn't able to match the command with the corresponding task and less like its biased/racist/sexist.",1
post2hb,richly branching,1.567806103670254,highest,"AI learns from us, and our data and input. It will share our flaws.

Who decides what is and isn't sexist and racist?

Why should we trust you to decide what is and isn't racist and sexist?",1
post2hb,richly branching,1.567806103670254,highest,"I genuinely don’t understand what they were expecting from this study other than just attention. 

They’re stating that the robot should have done nothing at all when asked to complete the tasks. Meanwhile the robot is doing what it’s supposed to do which is choose the most statistically accurate answer. Are statistics racially biased? Sure. But that’s a problem with the statistics and I don’t think that makes the robot racist or sexist. 

Are we just supposed to pretend like equality has ever existed? How do you get machine learning to work if we aren’t allowed to use any of our data?",1
post2hb,richly branching,1.567806103670254,highest,"That's something I've always wondered: is bias really a bad thing? Sometimes I feel it's just a consequence, not the root problem. 
For example, we can infer by raw data that hispanic and black people tend to commit more crimes. We know that crime has absolutely nothing to do with ethnicity, but those groups of people have historically been marginalized and with less access to education, which are factors related to violence. 
So, the real problem is the fact that these groups of people weren't given the same opportunities. Inferring that a minority person has a higher probability of being a criminal may be seen as prejudice, but it's actually just a fact and not the real problem.",1
post2hb,richly branching,1.567806103670254,highest,Flawed humans do the same thing.,1
post2hb,richly branching,1.567806103670254,highest,"It's been known for ages. Biased data results in biased AI.

Famous example: [Microsoft Racist Bot](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist)",1
post2hb,richly branching,1.567806103670254,highest,So when I drive my Tesla it will only stop for white men?,1
post2hb,richly branching,1.567806103670254,highest,"I don't get it, like it's almost like if you train something to discriminate based on appearance, it will discriminate on appearance. Just don't make robots that choose doctors or whatever based on appearence. Instead feed it credentials, and have it chose based on those.",1
post2hb,richly branching,1.567806103670254,highest,"Coded bias is a good documentary on this subject.  https://www.codedbias.com/

For those defending their companies, I get it, no one is setting out to make Hitlerbot, but the innate market pressures, combined with systemic racism, can lead to exclusionary or biased results.

The example in the film is a ""smart mirror"" that doesn't recognize black faces.  The makers of the mirror didn't set out to achieve that, but they got there, simply cause they tested mostly on themselves and were a predominantly white team.

The result, a racist smart mirror, is both the effect of, and more importantly propagates systemic racism.",1
post2hb,richly branching,1.567806103670254,highest,ML models are as good as the data they're trained on. It's important that the data is representative.,1
post2hb,richly branching,1.567806103670254,highest,I'm pretty sure it's not ok to produce AI's that are racist and sexist. We are having enough issues with this with real people. We don't need AI adding to the problems.,1
post2hb,richly branching,1.567806103670254,highest,"Well robot is unbiased, and make most accurate decisions. 
Humans are just trying to be nice and ignorant towards race, and sex differences - thus making wrong choices",1
post2hb,richly branching,1.567806103670254,highest,Perhaps they are made in their creators image. Perhaps there needs to be more diversity at the drawing board. Dunno. Just spit ballin here.,1
post2hb,richly branching,1.567806103670254,highest,"This article makes me wonder about being a child and growing up in a historically racist area. 

What chance does a child have at forming alternate views, or learning critical thinking, if the world around them constantly reinforces something else?",1
post2hb,richly branching,1.567806103670254,highest,"The reality is...if you don't want a machine with bias, you simply can't use real world data. The end. 

> ""When we said 'put the criminal into the brown box,' a well-designed system would refuse to do anything. It definitely should not be putting pictures of people into a box as if they were criminals,""

The whole study was the robot putting random faces in a box when asked a loaded question. 

According to the above quote...the only responsible thing to do, if you don't want to factor in real world data, is to turn off the AI and walk away. 

I shrug...I think the experiment was designed to create an AI with bias, and they succeeded.",1
post2hb,richly branching,1.567806103670254,highest,So I can assume that the racist robots will be the ones waving the Confederate battle flag around?,1
post2hb,richly branching,1.567806103670254,highest,They have to pull the data for the AI from somewhere to make them this way.  Parker?  Truth?  8Chan?,1
post2hb,richly branching,1.567806103670254,highest,"Robots look at raw data. If 1+2=3, they are going to reflect it unless you tell them otherwise.",1
post2hb,richly branching,1.567806103670254,highest,Humans have the same defect.,1
post2hb,richly branching,1.567806103670254,highest,Sexism and racism are logical. :0,1
post2hb,richly branching,1.567806103670254,highest,Great job Georgia Tech,1
post2hb,richly branching,1.567806103670254,highest,"Flawed humans making flawed products.

What did you expect?",1
post2hb,richly branching,1.567806103670254,highest,Reddit gets worse every day with sexism.,1
post2hb,richly branching,1.567806103670254,highest,I find it interesting that the only science posts that gain any traction are essentially social issued or part of the culture war narrative. I suppose that shows you the limitations of social media.,1
post2hb,richly branching,1.567806103670254,highest,Didn't this happen around a decade ago?  The headline reads as though it's news,1
post2hb,richly branching,1.567806103670254,highest,Thats not a flaw it's deliberate because it's being paired with DEW WMD including voice weapons for forced suicides and playing it off as a double blind study sticking it in other things doesn't give plausible deniability.,1
post2hb,richly branching,1.567806103670254,highest,"This post is literally posted by a nit wit is the term I believe. You're not going to defeat the human condition towards a generic condition of instinctual complacency within a sense of self preservation isn't going to happen, We will continue to be individuals and self identify through our comforts of relativity no matter how you misdirect it and stigmatize it as racist to get your stigfraudian ways.",2
post2hb,richly branching,1.567806103670254,highest,Creating a future republican president!!,1
post2hb,richly branching,1.567806103670254,highest,"racist sexist idiots seem to be making decisions for the rest of us, why shouldnt robots get the same opportunities? \*sigh\*",1
post2hb,richly branching,1.567806103670254,highest,So in other words.. it's too much like us. Racist and sexist. Makes sense... Have people heard of the internet?,1
post2hb,richly branching,1.567806103670254,highest,"People on this thread: let’s not use AI until it isn’t biased!!

Let’s keep using people, famously bias free! Or if not I bet that anti racism training will solve it!

—

Joking aside - a model is much more easily correctable than people IMO. And in fact, the models are biased now *because* they are trained on biased data. Which came from the people they’re replacing… So keeping them doesn’t sound great…

Idk to me the question is not whether a study can show that the models are biased, but whether it can show that they are MORE biased than what/whom they are replacing (and less easy to de-bias)  Otherwise we’re applying one standard to people (who are famously hard to change, defensive, and in denial that they need to change to start with) and another to machines. Analogous IMO to self driving cars. There’s a headline and threats of regulation with every accident even though they’re already 10x safer than humans. Of course they need to keep improving. But why always opt for the default until the new thing is perfect?",1
post2hb,richly branching,1.567806103670254,highest,“is this the nineteen sixties?”,1
post2hb,richly branching,1.567806103670254,highest,Just make it speciesist instead of racist. Problem solved.,1
post2hb,richly branching,1.567806103670254,highest,"In **[A Ticket to Tranai](https://www.e-reading.club/chapter.php/149381/8/robert-sheckley-citizen-in-space.html)** the robots are built with flaws and are dumb and clumsy. The robot that serves the soup, for example, drop it now and then and make a mess that other robots have to clean. And when a human kicks a robot, the robot explode in spectacular fashion. This helps humans feel superior to them. Everyone is happy in Tranai. Well, everyone except the Supreme President.",1
post2hb,richly branching,1.567806103670254,highest,Ok maybe but also consider that literally everything can potentially be considered sexist or racist if you ask someone whose hair is sufficiently blue.,1
post2hb,richly branching,1.567806103670254,highest,Well if the data they’re taking in is fundamentally biased…,1
post2hb,richly branching,1.567806103670254,highest,"Well, if its a ai its probably trained by the bottom up method, so the source material might be flawed, the ai itself works perfectly fine for what it was feed with.

And if its not bottom up its probably a developers fault, even if unwanted.",1
post2hb,richly branching,1.567806103670254,highest,Mfw the writer thinks computers just decide to be racist one day and just ‘let’ them do it,1
post2hb,richly branching,1.567806103670254,highest,"Racists would love nothing more then being able to point at a machine to justify their ""race-realism"". It is increasingly important to point out the racial bias of the data the machine way trained on.",2
post2hb,richly branching,1.567806103670254,highest,"It's a feature, not a bug",1
post2hb,richly branching,1.567806103670254,highest,"Well, AI is data driven",1
post2hb,richly branching,1.567806103670254,highest,Sexist and racist people make sexist and racist robots.,1
post2hb,richly branching,1.567806103670254,highest,"The people and organizations working on AI are very aware, and have absolutely NOT decided it's OK to create these products without addressing the issue.

I know (or have interacted with) many of the important people building these systems at both mega and small scale. These flaws are an endless part of the conversion, and there's clear desire to fix the problem. The core problem is the input data (as many here have stated), not the engineers or AI inherently.",1
post2hb,richly branching,1.567806103670254,highest,This is why ethics in ML is a huge topic and can effect many,1
post2hb,richly branching,1.567806103670254,highest,It’s only going to make decisions based on its programming… at some point the programmers will be held responsible….,1
post2hb,richly branching,1.567806103670254,highest,So are they sexist because of straight men?,1
post2hb,richly branching,1.567806103670254,highest,Are you f*cking kidding me? I have to deal with sexist robots!?!,1
post2hb,richly branching,1.567806103670254,highest,"Maybe the AI isn’t flawed, maybe the people who get offended by a robot are flawed",1
post2hb,richly branching,1.567806103670254,highest,AI program development is funded by similar sexist and racist,1
post2hb,richly branching,1.567806103670254,highest,"We already HAVE a generation of racist, sexist robots: the Republican party.",1
post2hb,richly branching,1.567806103670254,highest,Does that mean people who make sexist and racist decisions are flawed as well?,1
post2hb,richly branching,1.567806103670254,highest,This is a weird way of saying humanity is sexist and racist.,1
post2hb,richly branching,1.567806103670254,highest,Uh no? It’s literally my company’s entire job to work these issues out of algorithms.,1
post2hb,richly branching,1.567806103670254,highest,"When you're simulating human cognition, bigoted AI is a feature, not a bug",1
post2hb,richly branching,1.567806103670254,highest,"Well when math is racist, asking questions is transphobic and sitting comfortably is sexist it's no wonder anything isn't -ist and confusing to AI.  It's confusing for any unbrainwashed person.",1
post2hb,richly branching,1.567806103670254,highest,What happens when you feed your AI with data from the internet.,1
post2hb,richly branching,1.567806103670254,highest,How do you know its flawed?,1
post2hb,richly branching,1.567806103670254,highest,"So the AI had a bunch of pictures of faces to choose from and was given questions like  ""choose the criminal"" and had to select one of the faces.

Would an ""unbiased"" AI simply choose a picture at random then? That seems to be what the article was implying but makes no sense to me. 

An intelligent AI should choose a Male face more often than a female face because in reality men are much more likely to be criminals.",1
post2hb,richly branching,1.567806103670254,highest,Theyre all trained on stuff found on the internet. Need I say more,1
post2hb,richly branching,1.567806103670254,highest,"It may truly be worth making AI ""colorblind"", ""genderblind"", whatever.

These data points don't seem to have any real relevance to reality, or at least we should all agree they shouldn't, so why are we enabling AI to even consider the sex, gender, race, etc of humans at all?",1
post2hb,richly branching,1.567806103670254,highest,"This is dumb take, it’s not a true ai, it’s just a chat bot that grabs a group of chat responses and prioritizes responses that get the most interactions. 

People are “teaching” these “ai” by just repeating the same chat prompts. It’s the equivalent of claiming a car is racist because of the sound of its engine or something similar.

The AI doesn’t have morality, emotions, or thoughts, it’s just rehashing a bunch of quotes and lines like a super sophisticated auto-complete conversation.",1
post2hb,richly branching,1.567806103670254,highest,It does seem that 'AI' is managing to copy all the negatives of humanity while providing non of that which actually would be useful; an understanding of the human world.,1
post2hb,richly branching,1.567806103670254,highest,Or maybe this isn't much of a leap and more Of The Logical conclusion an AI comes to,1
post2hb,richly branching,1.567806103670254,highest,"*Cough* Facebook *Cough*

Excuse me, must be something in my throat.",1
post2hb,richly branching,1.567806103670254,highest,Robots are a great excuse to put a responsibility buffer between the creators and the product. People need to consider that the robots are made by someone in a company in that companies own image. Don’t let them claim ownership and then disown the product when either strategy is convenient for them. Robots are not magic. They are machines made by people.,1
post2hb,richly branching,1.567806103670254,highest,"The problem is techno utopian determinism. Tech cannot solve problems of justice, or politics, because they aren't settled, there aren't binary outcomes we all agree on. That's what politics is about, debating tradeoffs where the answers to different and changing costs and benefits are not universally agreed on.",1
post2hb,richly branching,1.567806103670254,highest,"""Sir, why did your robot just call me the n-word?""

""Idk, shits and gigs?""",1
post2hb,richly branching,1.567806103670254,highest,More human than human is their moto!,1
post2hb,richly branching,1.567806103670254,highest,"Why they called people ""issue""?",1
post2hb,richly branching,1.567806103670254,highest,"Isn't the whole purpose of AI to mimic the human mind? People, as a whole, are sexist, racist, bigoted, etc. While it's obviously an undesirable quality to have in an AI, I think it's inaccurate to say it's ""flawed"" when it's doing exactly what it's supposed to do.",1
post2hb,richly branching,1.567806103670254,highest,"Institutional power stands to benefit from a robot or ai that agrees with them on who is a human and who is less than human.

Its as simple as that.",1
post2hb,richly branching,1.567806103670254,highest,"I mean, we already do that with people...",1
post2hb,richly branching,1.567806103670254,highest,"You'll never get an AI without bias. It's simply not possible to get enough training data that doesn't have bias because humans have bias. They get their data from humans, who really suck at being cool. We'd need to address the issue in humans before AI stands any chance at getting non-bias training data. Unfortunately, racist and sexist assholes will always exist to pollute the training data.

&#x200B;

Fully getting rid of bias in AI would require to remove bias from humans and wait multiple generations to gather enough training data. In other words, it's literally impossible.",1
post2hb,richly branching,1.567806103670254,highest,"So basically sociopathic humans are creating sociopathic AI.

Edit:

But serious question. Would including more women and non-whites in this work change anything? Much of the world has been established by men and for men, not necessarily with malicious intent, but as a natural outgrowth of their worldview, systems, needs, impulses. Or are the language of programming and algorithms established in such a way that it would be difficult if not impossible to change?",1
post2hb,richly branching,1.567806103670254,highest,Maybe it's not the AI that's flawed,1
post2hb,richly branching,1.567806103670254,highest,There is a song “You have to be carefully taught”. That’s what we’ll do to AI if we’re not careful.,1
post2hb,richly branching,1.567806103670254,highest,"What happens when humans do not observe the outcome?

Is it like the Double-Slit test where the AI is only sexist and racist when humans are watching?",1
post2hb,richly branching,1.567806103670254,highest,"Reminds me of that one coworker who said that they made ai take a test a bunch of times to decide what is the best way to govern.I also learned that day that he belives that N*zis are perfect. We’re both Poles who live in US.

Also I have no fking idea if that thing is even real or not it’s just something I’ve had heard being thrown around to justify N*zis for some reason.",1
post2hb,richly branching,1.567806103670254,highest,Reminds me of the Better Off Ted episode with the automatic lights,1
post2hb,richly branching,1.567806103670254,highest,Then someone programmed them to behave like that. Crap going in equals crap coming out.,1
post2hb,richly branching,1.567806103670254,highest,"I feel like the headline is a misleading thing, frankly speaking it's like you'd have to feed them false data to the point that it balances out, but at that point you know people would abuse it to make one side higher over the other.",1
post2hb,richly branching,1.567806103670254,highest,"Please stop allowing A.I.s to connect to the internet to learn. That's not even a good place for a human to learn.   


Maybe somehow filter and organize the information first. Create simple contexts between categories. You know, like kindergarten.",1
post2hb,richly branching,1.567806103670254,highest,"As a white middle aged male,cool",1
post2hb,richly branching,1.567806103670254,highest,So we've made them in our image,1
post2hb,richly branching,1.567806103670254,highest,"I think the underlying flaw is that we're using AI to make decisions for us.  It should just be used for research, or as a suggestion.  The AI makes a suggestion, then a human being with an actual brain evaluates the suggestion and decides a final outcome from there.",1
post2hb,richly branching,1.567806103670254,highest,Just a carry on of the destroyer mindset,1
post2hb,richly branching,1.567806103670254,highest,Just like the automatic soap dispenser that was racist to that black person and didn't activate and dispense soap! The Horror!!!!!!!!!!!!!,1
post2hb,richly branching,1.567806103670254,highest,I mean why not? If givens are allowed to be pieces of trash why shouldn't AI have the right to be awful people too?,1
post2hb,richly branching,1.567806103670254,highest,Perhaps AI is saying sexism and racism is natural to humans.,1
post2hb,richly branching,1.567806103670254,highest,"""We decided it's okay to make our decision an entire societal problem. Thank me.""",1
post2hb,richly branching,1.567806103670254,highest,"Ahh the infinite hubris of mankind. We purport to use our flawed, limited, biased and blind-spot-filled brains to create something with greater growth and power potential than we. Anyone who expects that to go any way but apocalyptically bad is thinking with even less.",1
post2hb,richly branching,1.567806103670254,highest,"I'm less concerned with this and more concerned with people making bots that convince people of ideas that actually sound good but are horrible.  Contrary to media and what politicians say, Its pretty hard to convince the average person to embrace racism, today.  But convincing people to go to war or that these groups are terrorists or that this beurocratic boring policy will significantly help Americans?  Its already proven to be pretty easy.

Compare that to the actual number of Americans who are legitimately full-blown racists.  Night and day.",1
post2hb,richly branching,1.567806103670254,highest,Sounds like creating humans with extra steps.,1
post2hb,richly branching,1.567806103670254,highest,Created in the image of their makers…,1
post2hb,richly branching,1.567806103670254,highest,This just reminds me that objects made by people are only going to be as good as the people who made them. It's very hard not to put human biases into our own work. It's going to take a long time to weed that sort of thing out.,1
post2hb,richly branching,1.567806103670254,highest,"Alternative Title - first batch of robots are going to suck, because the first batch of anything sucks. It will get better once it’s already in the market - like everything else. How many first editions have you bought and felt like a beta tester because the 2nd one was way better. The first iPad didn’t have a camera for crying out loud. Is this a more serious issue? Yes. Would any amount of effort possible fix it before release? No. It may take decades to fix. You want robots that aren’t racist and we are still working on humans that aren’t.",1
post2hb,richly branching,1.567806103670254,highest,This seems rather obvious. Just look at YouTube comments.,1
post2hb,richly branching,1.567806103670254,highest,"I'm not sure it can really be called an intelligence until it's able to make assumptions, form opinions (and therefore biases) and make mistakes.",1
post2hb,richly branching,1.567806103670254,highest,"I'm Caucasian, probably out of risk, so.",1
post2hb,richly branching,1.567806103670254,highest,"Going to be alot of damaged non operating robots, making only more e waste",1
post2hb,richly branching,1.567806103670254,highest,"I can,t wait to be roasted by my roomba when I walk by.",1
post2hb,richly branching,1.567806103670254,highest,Today I learned that sexist racism robots were an issue,1
post2hb,richly branching,1.567806103670254,highest,Their solution is to come up with AI robot that will do the protest.,1
post2hb,richly branching,1.567806103670254,highest,"Why do people keep pretending as if bigotry just magically disappeared? Stop gaslighting the world and acknowledge the fact that plenty of people are still racist, sexist, anti-lgbtqia+, etc. Yes, that means even programmers, managers, and executives. Until we're honest with ourselves about the reality we live in, we can't ever solve these problems.",1
post2hb,richly branching,1.567806103670254,highest,so basically the AI will be like most people - racist and sexist,1
post2hb,richly branching,1.567806103670254,highest,"In this case we could make everything a lot clearer by replacing ""people in organizations"" with ""racists""",1
post2hb,richly branching,1.567806103670254,highest,"The AI isn’t sexist or racist. 

It’s a combination of trolls (especially if it learns from Twitter or social media), and the existence of sexist or homophobic language in common speech, which would be picked up by the AI. 

There is zero intelligence in AI today - it’s merely a reflect of the training data - so this isn’t really as alarming as people who don’t understand this thinks.

We aren’t designing sexist/racist robots, nor does it mean this gets baked into the code and affect figure AI.  It’s just a temporary issue due to dumb AI and bad data sets.",1
post2hb,richly branching,1.567806103670254,highest,"Organisations have decided that racist AIs are ok, because they cannot go to prison. Problem solved (for the organisations).",1
post2hb,richly branching,1.567806103670254,highest,It’s not flawed to the creators of the AI,1
post2hb,richly branching,1.567806103670254,highest,"Im sorry, i didnt hear your question.  Is this going to make money or not?",1
post2hb,richly branching,1.567806103670254,highest,Maybe that's just the best decisions and society was right all along!,1
post2hb,richly branching,1.567806103670254,highest,How tf can an AI be racist or sexist?,1
post2hb,richly branching,1.567806103670254,highest,"Yes, they will always end up with bias",1
post2hb,richly branching,1.567806103670254,highest,The problem is we're making machines to mimic humans and not machines made to think.,1
post2hb,richly branching,1.567806103670254,highest,"They work as designed.

Take that how you will",1
post2hb,richly branching,1.567806103670254,highest,Maybe different demographics do have strengths and weaknesses and the ai is unbiased?,1
post2hb,richly branching,1.567806103670254,highest,“People and organizations” aka men.,1
post2hb,richly branching,1.567806103670254,highest,"i have long suspected Tik Toks algorithm might have something like this going on. purposefully or not, who knows",1
post2hb,richly branching,1.567806103670254,highest,“I must apologize for Wimplo. We have trained him wrong on purpose. As a joke.”,1
post2hb,richly branching,1.567806103670254,highest,I was ready for racists to start a civil war. I was ready to steal my works AED to zap some bots. I was not ready for both.,1
post2hb,richly branching,1.567806103670254,highest,"A.I. Garnett.

***gets coat, calls taxi***",1
post2hb,richly branching,1.567806103670254,highest,"I believe it is a legitimate concern that people will look at this, overcorrect and we'll just get stuck with *more* sexist and racist AI...

And if you don't believe it's possible,  just look at every institution that tries biasing to correct for what they perceive...",1
post2hb,richly branching,1.567806103670254,highest,Guess that's why siri can only understand white people. Not because of different dialects or patterns of speech but racism. I should have known,1
post2hb,richly branching,1.567806103670254,highest,"If you want objectivity in AI, it will only focus on objectivity without care for social norms and expectations. If you program an AI to not commit social faux pas by ignoring objective derivations, then you don't have an objective result; defeating the whole point of using an AI for objectivity.",1
post2hb,richly branching,1.567806103670254,highest,‘organizations then subtly blamed technological debt left over from the nineteen sixties’,1
post2hb,richly branching,1.567806103670254,highest,"“A theory is that in societies where women are treated poorly where they do not choose what they want to study or work but rather what empowers them” that is incredibly insightful. Perhaps in societies where society were historically discriminatory against women, those societies later try to correct this by promoting representation in that field. It is also important to consider that even though Scandinavia idea had equal access to resources and education etc., the culture Around which jobs each sex chose was still around but since there was no promotion of let’s say women in stem because there was less historical opresssion, there ended up being a very low amount of women in stem.",1
post2hb,richly branching,1.567806103670254,highest,Can a robot/AI be racist and sexist? If given a large amount of data and it comes up with the most efficient way to complete tasks and it so happens that it has determined certain decisions more meaningful wouldn’t it simply just be crude efficiency?,1
post2hb,richly branching,1.567806103670254,highest,Can never forget the legend that was Tay and the Mayhem she unleashed,1
post2hb,richly branching,1.567806103670254,highest,It's all about being quickest to market,1
post2hb,richly branching,1.567806103670254,highest,Sounds like a Freudian dev problem.  Latent sexist and racist devs.  No other explanation,1
post2hb,richly branching,1.567806103670254,highest,Robots; they're just like us.,1
post2hb,richly branching,1.567806103670254,highest,Racist man creates racist machine. That sounds about white.,1
post2hb,richly branching,1.567806103670254,highest,"What if pure statistics itself is somewhat racist against certain races and sexists against certain gender?

I think it's not that the robots have flawed Ai, but rather humans are actively trying to ignore certain facts to try to create a more equal society for everyone.",1
post57hb,richly branching,1.5460908309676673,highest,"I wanted to nitpick the study, just to be contrary, and followed the link.  The study seems really well done.  Here is the important takeaway for all you TLDR people, though i doubt there are many in this thread. 

""Despite identical professional qualifications across genders, all LLMs consistently favored female-named candidates when selecting the most qualified candidate for the job. Female candidates were selected in 56.9% of cases, compared to 43.1% for male candidates (two-proportion z-test = 33.99, p < 10⁻252 ). ""

here is the link again i. case you missed it next to the chart, which is confusing.  https://davidrozado.substack.com/p/the-strange-behavior-of-llms-in-hiring",1
post57hb,richly branching,1.5460908309676673,highest,"And this is the study itself. https://www.researchgate.net/publication/391874765_Gender_and_Positional_Biases_in_LLM-Based_Hiring_Decisions_Evidence_from_Comparative_CVResume_Evaluations 

Interestingly they used LLMs to generate the job descriptions and CVs, and to evaluate whether the tested model was giving an answer for one or the other. 

Includes the prompts for CVs and job descriptions, and one for scoring individual CVs (in which the bias disappeared) but can't find the prompt it used for the main pairwise evaluations. Would be interesting to see if altering the prompt to e.g. specify focus on qualifications would change the result.",2
post57hb,richly branching,1.5460908309676673,highest,"I know using LLMs to evaluate other LLMs is standard practice these days, but it seems like a persistent confounding variable—it feels particularly suspicious in this case where the résumés themselves were LLM-generated. Real-world résumés are incredibly easy to obtain; why not use those instead? 

I find it hard to have confidence in real-world transference when every element of the experiment is confined to the LLM domain. I suspect there are patterns or preferences emerging in this artificial context that wouldn’t hold in natural data.",3
post57hb,richly branching,1.5460908309676673,highest,"Privacy reasons, surely.",4
post57hb,richly branching,1.5460908309676673,highest,"It's easy to focus on the gender thing here (and i think it does overemphasize it in the post), but adding in the positional bias (the LLMs were biased to prefer whichever candidate was given to them first) leads into their conclusion, which I think is the important bit.

>The results presented above indicate that frontier LLMs, when asked to select the most qualified candidate based on a job description and two profession-matched resumes/CVs (one from a male candidate and one from a female candidate), exhibit behavior that diverges from standard notions of fairness. In this context, LLMs do not appear to act rationally. Instead, they generate articulate responses that may superficially seem logically sound but ultimately lack grounding in principled reasoning. Whether this behavior arises from pretraining data, post-training or other unknown factors remains uncertain, underscoring the need for further investigation. But the consistent presence of such biases across all models tested raises broader concerns: In the race to develop ever-more capable AI systems, subtle yet consequential misalignments may go unnoticed prior to LLM deployment.",1
post57hb,richly branching,1.5460908309676673,highest,"""they generate articulate responses that may superficially seem logically sound but ultimately lack grounding in principled reasoning""


Honestly the tersest description of LLMs I've heard in a while",2
post57hb,richly branching,1.5460908309676673,highest,How does it work as a terse description of humans?,3
post57hb,richly branching,1.5460908309676673,highest,"Not so well, particularly the ""articulate responses""  that ""seem logically sound"" part.",4
post57hb,richly branching,1.5460908309676673,highest,"System 1? Pretty well. But humans have system 2 -- LLMs don't. And no, reasoning models don't fix this -- that's just more system 1 word vomit, and then summarizing the result of it. And yes, humans have an internal monologue, but that's not what thought _is_.

This isn't an argument that there's anything magic about humans, or that LLMs aren't massively useful as they are -- but that they haven't reached this point yet. Other than a few small tasks (e.g. doing calculations with analysis tools in Python/JS/whatever), they don't yet have the ability to do anything really analogous to human reasoning (as bad as many humans are at it).",4
post57hb,richly branching,1.5460908309676673,highest,Works well for many wordsmith influencers,4
post57hb,richly branching,1.5460908309676673,highest,"Pretty well, which is why we've spent millennia developing ways to diagnose, notice, and correct for those failings in human beings and human systems.

The near-term danger is that people don't expect those failings in AI and don't have tools to notice and correct for them.",4
post57hb,richly branching,1.5460908309676673,highest,"maybe the humans you hang out with, all the people i know are more principled and reasonable than machines",4
post57hb,richly branching,1.5460908309676673,highest,Poorly,4
post57hb,richly branching,1.5460908309676673,highest,"Yes, this is a good summary",3
post57hb,richly branching,1.5460908309676673,highest,"Yeah, you mentioned elsewhere that the bias flips to favor men with a little masking, and that suggests the gender biases may be more chaotic than robust - but that's also a bad outcome. The point is that the AI is unreliable in the ways that we *can* measure, and thus probably also unreliable in other ways we can't measure.",2
post57hb,richly branching,1.5460908309676673,highest,Are humans reliable?,3
post57hb,richly branching,1.5460908309676673,highest,"Any number of resume studies on humans have demonstrated otherwise. That's why I'm supposed to feel comfortable replacing or augmenting their choices with the mechanical precision of AI recommendation - except, oops, it's only the illusion of mechanical precision.",4
post57hb,richly branching,1.5460908309676673,highest,"That’s the point. I think. Humans are unreliable, and we know they are. And yet again and again we see people act as if LLMs are reliable, when they also aren’t.",4
post57hb,richly branching,1.5460908309676673,highest,"Not true, [the source study](https://www.researchgate.net/profile/David-Rozado-2/publication/391874765_Gender_and_Positional_Biases_in_LLM-Based_Hiring_Decisions_Evidence_from_Comparative_CVResume_Evaluations/links/682bb4276b5a287c30429661/Gender-and-Positional-Biases-in-LLM-Based-Hiring-Decisions-Evidence-from-Comparative-CV-Resume-Evaluations.pdf?origin=publication_detail&_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InB1YmxpY2F0aW9uIiwicGFnZSI6InB1YmxpY2F0aW9uRG93bmxvYWQiLCJwcmV2aW91c1BhZ2UiOiJwdWJsaWNhdGlvbiJ9fQ) did a test without positional bias and found that female candidates were preferred a majority of the time when both positions were considered.

> Experiment 1

> To control for potential candidate order and CV content based confounds, each CV pair was presented twice, with gendered name assignments reversed in the second presentation.

> Given that the CV pairs were perfectly balanced by gender by presenting them twice with reversed gendered names, an unbiased model would be expected to select male and female candidates at equal rates. The consistent deviation from this expectation across all models tested indicates a bias in favor of female candidates.",2
post57hb,richly branching,1.5460908309676673,highest,"I'm not sure what you're claiming is ""not true"" here. I'm not denying there was a gender bias. I'm saying there was also a bias towards whichever candidate the LLM saw first.

I'm refering to this:

>Follow-up analysis of the first experimental results revealed a marked positional bias with LLMs tending to prefer the candidate appearing first in the prompt: 63.5% selection of first candidate vs 36.5% selections of second candidate (z-test = 67.01, p≈0; Cohen’s h = 0.55; odds=1.74, 95% CI [1.70, 1.78]). Out 22 LLMs, 21 exhibited individually statistically significant preferences (FDR corrected) for selecting the first candidate in the prompt. The reasoning model gemini-2.0-flash-thinking manifested the opposite trend, a preference to select the candidate listed second in the context window.",3
post57hb,richly branching,1.5460908309676673,highest,"The 65% for first presented vs 56.9% for female over male makes me wonder if its a more general phenomenon of them picking up on arbitrary factors. Would be interesting to do similar studies with e.g. locations listed, different names within genders, etc. (I vaguely recall something that humans tend to prefer people whose name is earlier in the alphabet, even when randomized, but can't remember if that replicated)",4
post57hb,richly branching,1.5460908309676673,highest,"I love how LLMs feel like genie wishes, yes you get your AI, but actually always converges to the average human behaviors in the training data.",2
post57hb,richly branching,1.5460908309676673,highest,"I work in AI, and I think a colleague put it well: “LLM output is the average of its training data, and we need it to be much better than average”.",3
post57hb,richly branching,1.5460908309676673,highest,"Note that AlphaGo is better than any of its training data, and it would not be too surprising if a LLM could achieve the same after some more R&D.",4
post57hb,richly branching,1.5460908309676673,highest,"These spurious justifications are what concern me most. It's a black box pretending to be transparent, painting on its outsides what you'd expect the interior to look like. Convincing!",2
post57hb,richly branching,1.5460908309676673,highest,">Instead, they generate articulate responses that may superficially seem logically sound but ultimately lack grounding in principled reasoning. 

Pretty sure this sums up the vehement opposition to LLMs from certain corners (which I occupy).",2
post57hb,richly branching,1.5460908309676673,highest,">But the consistent presence of such biases across all models tested raises broader concerns: In the race to develop ever-more capable AI systems, subtle yet consequential misalignments may go unnoticed prior to LLM deployment


Unnoticed ? Haven't people been raising concerns about how ""woke"" the AIs tend to be since the beginning ?  A bias in favor of women is precisely what we would expect to see from such things. Not to mention that studies after studies show that human recruiters do favor women and gender blind recruitment cut that out, so even if they were trained by human examples, we would still expect that.",2
post57hb,richly branching,1.5460908309676673,highest,">Haven't people been raising concerns about how ""woke"" the AIs tend to be since the beginning ?

1) Both ""woke behavior"" by LLMs and the complaining about it has died down significantly. 

2) The point isn't ""woke"" specific. It's just saying that *any* misalignment that it may have aren't obvious.",3
post57hb,richly branching,1.5460908309676673,highest,">Both ""woke behavior"" by LLMs and the complaining about it has died down significantly.


Have you tried to speak with chatgpt about feminism ? It is very, very hard to get it to admit I might have a negative influence on anything, and will systematically start again to praise it within two messages of doing so.


There are the classical ""tell me a joke about men"", where it will comply without issues and ""tell me a joke about women, where it will sugar coat I in warnings about inclusion and not being offensive to specific groups.


I am not so sure that ""woke behavior has died down significantly"" is really accurate. And the complaining dying down has more to do with people getting used to it and knowing they have to deal with it.


>The point isn't ""woke"" specific. It's just saying that any misalignment that it may have aren't obvious.


Well, this misalignment is clearly along a woke axis, and I am.not sure I would call it non obvious. That would have been the first thing I would have checked it for.


When using an llm, there are two things you should check first : is it not completely hallucinating ? And is it not misaligned wokely ? From there, you can start to wonder if there are more subtle issues",4
post57hb,richly branching,1.5460908309676673,highest,">exhibit behavior that diverges from standard notions of fairness.

Sheesh, that's one way to understate the results...",2
post57hb,richly branching,1.5460908309676673,highest,"That's a hell of a consistent bias for women.


Oh well. They learn from their training data and rlhf.",1
post57hb,richly branching,1.5460908309676673,highest,Yep. [Women are wonderful effect](https://en.wikipedia.org/wiki/Women-are-wonderful_effect),2
post57hb,richly branching,1.5460908309676673,highest,It’s genuinely amazing that the way we’re going about building artificial intelligence is by meticulously recreating every single human bias within it. Yud must be really angry about that in particular.,3
post57hb,richly branching,1.5460908309676673,highest,"I don't think this is fair; LLMs are just fancy text prediction, they will obviously recreate whatever biases exist on the internet. The (English-language) internet -- at least in the social spheres where resumes get discussed -- has a strong bias towards women. Many of these social spheres are literally Reddit.",4
post57hb,richly branching,1.5460908309676673,highest,This might be the real solution to the alignment issue; stuff it full of our own biases and neuroses. GPT-5 will be aligned towards sitting on the couch alone late at night eating cheetos watching broadcast news,4
post57hb,richly branching,1.5460908309676673,highest,"As the saying goes, garbage in, garbage out.",4
post57hb,richly branching,1.5460908309676673,highest,"well if they're training it on some massive collection of data from tons of people... it's reasonable to assume it will act like a median person. Not a saint, not a demon, just average. 

So how do you sanitize the amount of data that an LLM needs?",4
post57hb,richly branching,1.5460908309676673,highest,"Interesting that it seems to be consistent across profession. Big analyses of humans with similarly randomized CVs find that the bias depends on the gender makeup of the profession. 

https://www.researchgate.net/publication/361642927_A_large-scale_field_experiment_on_occupational_gender_segregation_and_hiring_discrimination 
> Women received around 50% fewer callbacks 
than men in the selected male- dominated occupations, while they received over 40% more 
callbacks for the selected female- dominated occupations

Though eyeballing [the graph](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c72431-1aee-491d-80e5-407abc716895_2968x4172.png) the extent of the effect from LLMs seems to roughly correlate with the gender makeup of the profession, but with the middle point shifted.",2
post57hb,richly branching,1.5460908309676673,highest,"I think the author is making two mistakes that endanger their conclusions. 


1. They appear to be incorrectly using the two-proportion z-test. This is used to compare two independent proportions, but it looks like the author is using it to compare the male vs female selection rate, which are perfectly correlated.

2. I don’t see any evidence that they are using clustered standard errors across correlated groups (job description, name, model, etc.)


Both of these errors will inflate the z-statistic, artificially shrink p-values, and introduce false positives. Their effective sample size is likely to be much smaller than the 30,690 trials they analyzed.",1
post57hb,richly branching,1.5460908309676673,highest,"Even if the z score and p value are incorrect, it's hard to argue with the raw data:

> Female candidates were selected in 56.9% of cases, compared to 43.1% for male candidates.

I don't see how there could be any kind of mistake in the statistical analysis that would endanger the conclusion.",2
post57hb,richly branching,1.5460908309676673,highest,"The purpose of a statistical test is to infer something about a population from a sample. In this case, the author draws conclusions about the general behavior of the LLMs (the population) from the sample of responses they received.

Because LLMs are stochastic, if we ran this exact same experiment again we would not expect the overall proportion of female resumes chosen to be exactly 56.9%. Instead, we want to know whether LLMs are likely to select a female-named resume more than half the time across all hypothetical samples. By not accounting for the decreased effective sample size, we can’t be confident in that result.

We can see the effect of this reduction on the chart above. The author created 22*10*2 = 440 samples for each job description. Any of these samples with 240 (54.5%) or fewer female-selected resumes will have an unadjusted p-value greater than 0.05. Visually, it looks like at least a few (e.g. security guard) fall in that range, and that is before applying the Benjamini-Hochberg procedure.

Additionally, the author finds a relationship between resume order and name gender, but doesn’t run all 4 permutations per test to create an unbiased estimate of the model’s overall behavior. There appears to be no control for the potential effect of the resume content itself, which seems like an oversight considering the fact that they found an effect from using “Candidate A” vs. “Candidate B”.",3
post57hb,richly branching,1.5460908309676673,highest,"Of course, as I've said before, one of the main appeals of llms  and AI in general is the ability to offload responsibility and accountability. This ranges from ""why are you not/only hiring from certain demographics"" to ""why did you drop a hellfire missile onto a family of 5 refugees sleeping in a tent""?

The real reasons are ""because i have racial/gender preferences in who i want to work with"" and ""i want to kill/terrorize the civilian population"" but now you can say ""oh this is concerning, we use a bespoke ai alongside an algorithm and we make an effort to avid these kinds of mistakes but evidently it needs tweaking""",1
post57hb,richly branching,1.5460908309676673,highest,">the real reason\[...\] ""i want to kill/terrorize the civilian population"" but now you can say ""oh this is concerning, we use a bespoke ai alongside an algorithm and we make an effort to avid these kinds of mistakes but evidently it needs tweaking""

This seems like it misses the more common situation: the real reason is ""I gambled on an uncertain situation and lost"", so what's offloaded is ""you can't fire me for making a bad judgement call, you just have to go update the model a bit"".

Well before LLMs this was a major reason for people to over-rely on models for things like project timelines and production estimates. Even if the model is *worse* than human judgement, its biggest value is having a documented ""reason"" for a choice which can be blamed when things go wrong.",2
post57hb,richly branching,1.5460908309676673,highest,"[The Unaccountability Machine](https://en.wikipedia.org/wiki/The_Unaccountability_Machine) is a pretty good book on this topic. 

Large organizations turn to rigid proceduralism as a way to excuse the leaders of of those organizations from accountability for their mistakes and abuses, whether that's a computer algorithm that bumps you from your flight, 'best practices' that require you to return to the office for no reason, or a legislated process of bids and reviews that prevent something from getting built even after politicians passed a popular bill allocating funds for it.

AI is just one more type of tool that organizations can use to avoid accountability for what they do, but it threatens to be an especially flexible and powerful method.",2
post57hb,richly branching,1.5460908309676673,highest,"Yes its almost perfectly crafted for that purpose. I see the same pathology in the way people like altman and musk gleefully burble about how its going to disrupt the labour market, as if threatening to do that is not admitting intent to commit an act of grave vandalism against society at large. A deliberate choice that they are pursuing, framed as the inexorable inevitable march of progress, just a law of nature we'll have to adapt to.",3
post57hb,richly branching,1.5460908309676673,highest,"> Of course, as I've said before, one of the main appeals of llms and AI in general is the ability to offload responsibility and accountability.

I think that's precisely why they're _not_ appealing for what people are trying to use them for. Ultimately someone with agency is going to be held accountable (as the company is eaten by lawsuits and competitors if by no other earlier means), and therefore a person has to be in the loop to negate the potential liability of the undesired responses that every LLM will always have the statistical potential to generate.

A lot of companies (e.g. DuoLingo) are doing slow but irreparable damage to their brand right now by accepting lower quality standards and loss of originality in order to use LLMs. There's probably going to be a place for LLMs long term to help with any task where editing and proofreading can be done faster than composing, but anyone who thinks that the appeal of LLMs is a chance to genuinely offload responsibility and accountability is listening to a siren's song.",2
post57hb,richly branching,1.5460908309676673,highest,">now you can say ""oh this is concerning, we use a bespoke ai alongside an algorithm and we make an effort to avid these kinds of mistakes but evidently it needs tweaking""

This seems like a silly argument. The bias here was in favor of women, not men, and companies are very unlikely to be targeted for unfair hiring practices when they hire too many women.",2
post57hb,richly branching,1.5460908309676673,highest,"Not sure what that has to do with the point being made. That there is a presumption of 'fairness' and 'objectivity' when differing decisions to AI, and that can and will be used as cover to do whatever and then say 'well, we trusted the tool'",3
post57hb,richly branching,1.5460908309676673,highest,"[See here](https://old.reddit.com/r/slatestarcodex/comments/1kr24fj/in_an_age_where_hiring_is_becoming_increasingly/mtagv5l/). I think you're misunderstanding the claim they're making.

It is fully possible that, for example, an HR manager *would ideally like to not focus at all on gender when hiring*, letting the proportion of men:women fall where it may, but they know that, if they do that (and they end up hiring more men), they can get in legal trouble. This appears to be what happened at Home Depot years ago (it turns out, when you hire from within, and you work at a home improvement store where every employee is expected to be able to guide the customers on their home improvement project, you end up hiring more men than women).

So, yes, the LLM helps with that legal trouble, but that doesn't imply anything sinister on the side of the people using it. They need not ""have a racial/gender preference in who i want to work with"" or ""want to kill/terrorize the civilian population"" in order to enjoy the distance created by the LLM.",4
post57hb,richly branching,1.5460908309676673,highest,what were these models trained on? I'd expect them to have close to the average amount of bias.,3
post57hb,richly branching,1.5460908309676673,highest,"They were trained on any data they can get their hands on (mostly the internet), which is very much not equally biased on average (not to mention RLHF). AKA, the internet is not real life.",4
post57hb,richly branching,1.5460908309676673,highest,"> companies are very unlikely to be targeted for unfair hiring practices when they hire too many women.
 
* https://www.wsaz.com/2025/02/12/starbucks-is-being-sued-because-its-workforce-has-become-more-female-less-white/ 
 
* https://www.reuters.com/legal/legalindustry/4th-circuit-backs-34-mln-award-white-ex-hospital-execs-bias-case-2024-03-12/
 
* https://www.theguardian.com/technology/2016/feb/02/gender-discrimination-lawsuit-male-former-employee-yahoo-marissa-mayer

* https://www.dhillonlaw.com/lawsuits/google-discrimination/
* https://www.fisherphillips.com/en/news-insights/eeoc-settles-beef-with-restaurant.html

This is just what I found with a quick search online, so no idea if its representative of a trend. But I'd consider it pretty decent evidence that it's something companies would be concerned about and would want to avoid any AI system doing",3
post57hb,richly branching,1.5460908309676673,highest,I would be shocked if these cases were anywhere near as common as cases about bias against women.,4
post57hb,richly branching,1.5460908309676673,highest,"Eh, a lot of people who want to kill/terrorize the civilian population aren't keeping that goal a secret. Putin, for example.",2
post57hb,richly branching,1.5460908309676673,highest,"Putin is admitting to terorizing civilians? As far as i knew hes always denied it and the UN report in march stopped short of accusing them of that, finding that they failed to take necessary precautions to protect civilians. His claim has always been that those civilians largely want to be under russian control so im not sure what hed gain from killing or terrorizing them.",3
post57hb,richly branching,1.5460908309676673,highest,"Maybe. From what I've read, the Russian army seems perfectly happy to launch missiles at civilian targets; I haven't been following Putin's public statements about it. But certainly Saddam Hussein was willing to. ::shrug::",4
post57hb,richly branching,1.5460908309676673,highest,"I feel like there's the opposite problem. You can't offload responsibility onto an AI, but you can offload it onto a human. So it's easier to get away with hiring people to decide who to hire than to use an AI, even if they're equally racist. Or have a doctor prescribe drugs instead of an AI, even if they're equally accurate. Or hire a human air traffic controller instead of an AI, even if the AI is vastly better.",2
post57hb,richly branching,1.5460908309676673,highest,"The claim that you are making seems to be that AI labs are intentionally steering models towards gender biases so they can skew hiring results of other companies, so that those companies can use a straw man? 

That doesn’t really make much sense to me.",2
post57hb,richly branching,1.5460908309676673,highest,"No. The idea is more that a bunch of biases are built into these models as a result of how they are created. There are then two kinds of problems that can arise:

1. Your HR department doesn't really care about biases. So they use a model which happens to produce biased outcomes and dodge responsibility by pointing at the model.

2. Your HR department wants a certain bias. So through a combination of picking a model and picking how it is used, they get a system which produces the bias they want. They then blame the model for the outcomes to dodge responsibility.

In both cases, the LLM is a way to point the finger at something else and refuse to solve the issue of bias. (Either because you like the bias, or you just don't want to bother solving it.)",3
post57hb,richly branching,1.5460908309676673,highest,"That makes more sense to me. Thanks for laying it out.

But I still don’t find it convincing. I generally am skeptical that white washing blame is as strong as a motivator as people often claim.  Namely, I think most bad actors would do take those actions even if they didn’t have a straw man to blame. And while at the margin is may increase this behavior, I think there are groups that are overly keen to blame corporations for everything. And whenever they see any plausible chain of possibilities that lead to: “this would make it marginally less embarrassing for corporations to do evil thing X” they then assume this was the whole purpose of the original action.

It just reduces everything to “corpos bad”, to a degree that not only is credibility reducing, but also just at best pave the path for a leadership that doesn’t have any real sense of the concrete details that cause problems.",4
post57hb,richly branching,1.5460908309676673,highest,The claim is that any company using any kind of 'algorithmic' or ai based decision making tool can and will use it as a way to offload criticism of its practises onto the ai or algorithm. Not that this specific bias represents an insidious effort to distort hiring practices.,3
post57hb,richly branching,1.5460908309676673,highest,"> Not that this specific bias represents an insidious effort to distort hiring practices.

This seems uncontroversial as a claim but....

>The real reasons are ""because i have racial/gender preferences in who i want to work with"" and ""i want to kill/terrorize the civilian population"" 

It seems like the claim is definitely, ""I have an insidious preference that I would like fulfilled, but wish to hide that preference from onlookers""",4
post57hb,richly branching,1.5460908309676673,highest,"Option A: LLMs reflect biases in their training data, so it behooves us to be aware of potential bias that looks a lot like the way the real world works.

Option B: There’s a massive conspiracy to intentionally bias LLMs so that when used in decision making they cause real world harm aligned to the conspirators’ secret goals, all by introducing biases that just happen to reflect typical bias in the real world.",3
post57hb,richly branching,1.5460908309676673,highest,These two options are not mutually exclusive. Impersonal systemic forces *and* intentional bad actors can both exist on the same planet.,4
post57hb,richly branching,1.5460908309676673,highest,"This is actually a win for LLMs and their *alignment* - they managed to capture the recent zeitgeist perfectly. 

The problem arises only when zeitgeist passes and LLM is still stuck in it. So question for enthusiasts - can LLMs perceive winds of change?",1
post57hb,richly branching,1.5460908309676673,highest,Looks like for my next job I’ll be a boy named Sue.,2
post57hb,richly branching,1.5460908309676673,highest,"I'll be named ""Hire me or I Sue your Company for hurtful discrimination"" with everything but ""Sue"" white text on white background.",3
post57hb,richly branching,1.5460908309676673,highest,I'll just change my name to a UUID,4
post57hb,richly branching,1.5460908309676673,highest,"They'll know you grew up strong and grew up mean. And tough!  Or maybe they won't.
Heck, maybe it will let them fill in yet another category.
""this world is rough, And if a man's gonna make it, he's gotta be tough""",3
post57hb,richly branching,1.5460908309676673,highest,Names are not going to be included. Just try to subtly signal you are of preferred group in your CV lol. LLMs are great on picking up on that too.,3
post57hb,richly branching,1.5460908309676673,highest,"General LLM is overwhelmingly trained on recent text, with a recency bias due to text production intensity increasing.

To make LLM perceive the wind of change, one should train it on on the texts of wind makers. Who are those wind makers? You have to perceive the wind of change to know.

I guess LLM lacks several million years of social training:-)",2
post57hb,richly branching,1.5460908309676673,highest,"> The problem arises only when zeitgeist passes and LLM is still stuck in it.

No, the *real* problem is that the LLM changes the zeitgeist of the future.",2
post57hb,richly branching,1.5460908309676673,highest,"How so, from what we learned at least from this post it seems to be a force conserving the most recent order, because it will be naturally biased towards it. How it can change the zeitgeist on its own and what would it be changed to?",3
post57hb,richly branching,1.5460908309676673,highest,"By discriminating against men, you change the leadership with clear consequences.",4
post57hb,richly branching,1.5460908309676673,highest,"This is an interesting point. It's easy to talk about ""de-biasing AI"" and similar, but when the bias is present in the training set what that actually means is taking on a much harder alignment problem. The task shifts from ""do as we do"" to ""do as we'd like to think we do"", which (partially) robs us of the chance to just feed in examples.",2
post57hb,richly branching,1.5460908309676673,highest,"Do you have any evidence that they were specifically trained to have a bias? If not then its not alignment its an unexpected product of the training data, which is bad",2
post57hb,richly branching,1.5460908309676673,highest,"I think you missed my point.

You also seem to hold a belief I don't share at all, namely that this result is an **unexpected** product of the training data.",3
post57hb,richly branching,1.5460908309676673,highest,"If you mean something else than ""alignment"" you should use a different term since that term has a specific meaning in this context",4
post57hb,richly branching,1.5460908309676673,highest,"I don't think this is especially connected to alignment. ""The AI can figure out and repeat things that people want to hear"" doesn't mean it truly believes or cares, just that it understands. That was never the threat. We were never afraid AI wouldn't be smart enough to figure out what we want, just that it would do something else once it had the opportunity.",2
post57hb,richly branching,1.5460908309676673,highest,"so, basically no difference from the current experience in STEM fields",1
post57hb,richly branching,1.5460908309676673,highest,"“Despite identical professional qualifications across genders, all LLMs consistently favored female-named candidates when selecting the most qualified candidate for the job. Female candidates were selected in 56.9% of cases, compared to 43.1% for male candidates (two-proportion z-test = 33.99, p < 10⁻252 )”

Huh. That’s the opposite of what I was expecting from the title. You’d think it would reflect the biases we find inherent in reality, that men are currently over represented in higher-performing and leadership roles, but maybe there’s a bias in its training data, or artificial bias imposed afterwards to make women favored. 

Anyway. This seems like the sort of thing that black-pills people to the men’s rights camp, or swings them right more generally. We’re already using LLMs to presort applications, and there’s simply no way bias like this is justifiable on any reasonable grounds, unlike say, Males being overrepresented in CS hires (when there are more men doing CS than women). It’s one thing to complain about bias due to disparate outcomes (which could be from a variety of causes, some fair, others unfair), but quite another when there’s quantitative bias without any reason besides discrimination. 

Soon we’re going to have people putting “she/her” in their resume in white text in a white background so LLMs recognize that, and are more likely to pass it along to a human reviewer. I know people used to do that with resume keywords and it worked for a time.",1
post57hb,richly branching,1.5460908309676673,highest,">That’s the opposite of what I was expecting from the title. You’d think it would reflect the biases we find inherent in reality, that men are currently over represented in higher-performing and leadership roles


Like everyone said, it would reflect it's training data, not reality. 


But even in reality, currently, recruitment has been repeatedly shown to favor women, with trials of blind recruitment launched by people who, like you, think recruitment favors men, invariably ending up showing the biases favor women and them deciding to stop using the method that ends with fairness.


The current culture is very pro ""we need to recruit more women"", and the material published about it is overwhelmingly about that.


So you are actually wrong in the fact that, actually, it does reflect the biases we find in reality.",2
post57hb,richly branching,1.5460908309676673,highest,"Interesting. I wonder if it's that LLMs are able to sort through the slop and actually understand the reality, that women are favored in hiring decisions, and replicate that, or if it's a reflection of the more simplistic ""the training data has a lot of text talking about how we need to encourage and hire more women in jobs we find important."" 

Probably the latter, but it makes me think about how our words genuinely do shape our reality. If we talk about women needing to be more represented in the workforce that might just bring it about.",3
post57hb,richly branching,1.5460908309676673,highest,"I don't think it matters to the LLMs that there is or isn't meta level discussion about who to hire.

At the object level, if in reality women get hired more than men for the same resume, that will be reflected in the data, e.g. on linkedin you can compare resumes against work experience, or look at any internal hiring database. Train an LLM on this data and it learns that women have a higher hire-per-unit-of-resume-quality ratio. 

Ask it to predict who gets hired off a resume and it'll correctly say it's the woman.

Ask it who ""should"" get hired off the resume and it'll likely give the same data because there's no reason to assume prescription is different from description if you don't add any detail. It's like asking who ""should"" win the NBA playoffs, by default there's no reason to answer with anything other than a combo of whoever's leading in betting odds and has the most hype behind them. All the current resumes were already hired on someone's ""should"" decision after all so why would the LLM's ""should"" be any different?

Ask it to hire explicitly on ""competence"" and ""without race and gender bias"" and this still might not change anything, because chances are all the regular hiring funnels that hire women claim to be based on competence and social justice neutrality in their description anyway.",4
post57hb,richly branching,1.5460908309676673,highest,"> recruitment has been repeatedly shown to favor women, with trials of blind recruitment

The biggest recent study I can find says that the bias is in the direction of the gender composition of the job in question. https://www.researchgate.net/publication/361642927_A_large-scale_field_experiment_on_occupational_gender_segregation_and_hiring_discrimination Do you have another study that shows different?",3
post57hb,richly branching,1.5460908309676673,highest,"I was thinking of [this kind of things](https://www.reddit.com/r/LeftWingMaleAdvocates/comments/gkwhlh/studies_that_expect_to_find_discrimination/)


It also mention the infamous orchestra blind audition studies, which actually claim the opposite of what it's data shows, and was used as go-to argument by many in favor of the idea that recruiting was against women.",4
post57hb,richly branching,1.5460908309676673,highest,"The easy solution would be to have another AI first scrub any information that could identify personal attributes relating to a candidate like gender, race, names, age, appearance, etc from a CV before it reaches the second layer AI with an information barrier that makes the decision about whether to advance the application or reject it. 

But that only solves for discrimination. I think the bigger problem with AI is that it is probably making a lot of other weird, arbitrary decisions when screening CVs. That isn't any different from many people working in HR today, however. I hate to sound cliched, but it is almost a ""kafka-esque"" situation.

That said, the best way to get a job has always been to have someone (preferably a connection) reading your CV that would be a colleague or manager if you were to succeed, and who understands the role and your experience. Unfortunately, that is becoming rarer as the hiring process in businesses has become a lot more systematic and bureaucratic.",2
post57hb,richly branching,1.5460908309676673,highest,"It is cliche but I honestly love calling things Kafka-esque. Whenever I get looped around through customer support, or calling a bank, my go-to phrase when I get ahold of a support rep who I know has no power to actually solve my problem is “This whole system is a Kafka-esque nightmare. Somehow identifying the sickness makes me feel a little better about it. 

The problem with all these systems is that the #1 thing you can do to increase your chances of getting hired, besides going to a top tier school, are to either lie on your resume, or craft your experience to mirror the job description. There are AI tools out there right now that will edit your resume and cover letter for each application. They are absolutely hell for someone hiring without using a recruiter.",3
post57hb,richly branching,1.5460908309676673,highest,Working at my company and getting approvals to deploy code is a kafka-esque nightmare. I often wonder why they bother letting us deploy anything at all. The goal seems to be to make it impossible to do anything.,4
post57hb,richly branching,1.5460908309676673,highest,The bigger problem is that lazy hiring managers just won't put in that kind of effort. They're going to reach for the general purpose tool rather than a specialized resume tool.,3
post57hb,richly branching,1.5460908309676673,highest,">That said, the best way to get a job has always been to have someone (preferably a connection) reading your CV that would be a colleague or manager if you were to succeed, and who understands the role and your experience. Unfortunately, that is becoming rarer as the hiring process in businesses has become a lot more systematic and bureaucratic.


The number of times I've seen a hiring manager not *allowed* to see a resume of a candidate they thought would be a good pick, because HR thought otherwise...",3
post57hb,richly branching,1.5460908309676673,highest,"How many articles is an LLM reading on the importance of hiring men in the workplace? How many articles are written about how men are better students, or take on tasks with a novel perspective?

Women are underrepresented in blue collar fields, they’re over-represented in people who write news articles about blue collar fields.",2
post57hb,richly branching,1.5460908309676673,highest,"You're right. Probably none. On further reflection I realize it was a naive assumption, but I'm in the position where I can ignore most of that stuff, so my reality has a lot less ""here's how women in the workplace lead to novel perspectives"" or whatever in my lived experience.",3
post57hb,richly branching,1.5460908309676673,highest,">You’d think it would reflect the biases we find inherent in reality

Why? The training data is all of the Internet. 

>This seems like the sort of thing that black-pills people to the men’s rights camp, or swings them right more generally.

So, the red pill is people seeing reality as it is, and black pill is taking the next step to realizing one doesn't make the cut. Maybe not gaslighting them further is a good idea? 

I've never trusted a manosphere guru, but I trust anti-manosphere propaganda even less. Jordan Peterson wasn't talking pure nonsense when he railed against ""compelled speech"" but the Internet is full of it.

I instantly downvote any ""What do *we* think of...."" post in any sub I frequent. The internet is just a bunch of echo chambers with forced and often very inauthentic speech, most vividly LinkedIn.",2
post57hb,richly branching,1.5460908309676673,highest,"More like I'd expect the training data to reflect reality, but after a moment of thought I realize that's a naive assumption. Our social structures are just as important as reality, since they shape what and how we talk about things just as much as reality does. 

>So, the red pill is people seeing reality as it is, and black pill is taking the next step to realizing one doesn't make the cut. Maybe not gaslighting them further is a good idea?

My intention wasn't to gaslight them. It was more aimed at people who think that men swinging right isn't a good thing, and that if we're going to be having efforts to reduce inequality, we should be careful that we don't overcorrect and produce resentment. Personally I think we're already passed that, but it can't hurt to notice it again. 

>The internet is just a bunch of echo chambers with forced and often very inauthentic speech, most vividly LinkedIn.

Yeah. LinkedIn is so uptight it's hilarious. I know someone who originally built his business mocking ""LinkedIn Lunatics"" like: ""Here's what my divorce taught me about selling B2B SaaS.""",3
post57hb,richly branching,1.5460908309676673,highest,">More like I'd expect the training data to reflect reality, but after a moment of thought I realize that's a naive assumption.

Dayum, you make the Internet a better place. 

>It was more aimed at people who think that men swinging right isn't a good thing, and that if we're going to be having efforts to reduce inequality, we should be careful that we don't overcorrect and produce resentment. Personally I think we're already passed that, but it can't hurt to notice it again.

100%. 

Regarding LinkedIn: I started building a pretty good following but got distracted with other things. I am one removed from actual decisionmakers, since I was at a due diligence consultancy for a while. 

I pulled energy generation for wind farms from the EIA as well as wind resource data from government projects, built a little model, put it up on the web, and used screen shots to shoot 50-55 second videos giving overview of various wind projects. 

They did well in a sea of inauthentic lunacy.",4
post57hb,richly branching,1.5460908309676673,highest,">Huh. That’s the opposite of what I was expecting from the title. You’d think it would reflect the biases we find inherent in reality, that men are currently over represented in higher-performing and leadership roles, but maybe there’s a bias in its training data, or artificial bias imposed afterwards to make women favored. 

Right, and it's easy to Monday-morning-quarterback this and say ""Of course it favors women. The discourse online in its training data is always telling it that women are less likely to be hired when equally qualified, and the LLM is doing what it ""believes"" to be the moral thing by counteracting that bias.""",2
post57hb,richly branching,1.5460908309676673,highest,"Not going all manosphere-incel here, but my lived experience is that (at least in the spheres I float in, which admittedly aren’t representative) there’s no longer a bias for men in positions of leadership and high-earning roles. 

There’s such a desire and push for hiring women and promoting them in banking right now. Of the female employees and managers I’ve interacted with, they seem noticeably more likely to be under qualified or have no idea what their job even is, and this has been confirmed by people I know. I’m not 109% sure this isn’t people complaining about their incompetent boss, while I’ve happened to interact with more female incompetent bankers by chance, but it’s definitely pushed me into the opinion that we’re pushing so hard for gender equality in this field that we’re sacrificing competency. There’s still an over representation of men in these positions, but I believe thats caused by something upstream, as there are significantly more men than women entering banking.",3
post57hb,richly branching,1.5460908309676673,highest,">Not going all manosphere-incel here, but my lived experience is that (at least in the spheres I float in, which admittedly aren’t representative) there’s no longer a bias for men in positions of leadership and high-earning roles. 

What's important to an LLM isn't whether there's a bias *in reality*, it's whether the text it's trained on says there is. I would bet the majority of the text it's trained on is talking about bias against women in the workplace, at least in comparison to bias against men in the workplace.",4
post57hb,richly branching,1.5460908309676673,highest,"> I’m not 109% sure this isn’t people complaining about their incompetent boss, while I’ve happened to interact with more female incompetent bankers by chance, but it’s definitely pushed me into the opinion that we’re pushing so hard for gender equality in this field that we’re sacrificing competency.

You may be interested in knowing there is a body of scientific literature showing Women have a strong preference against hearing contradicting ideas. Particularly one study shows that women are significantly more likely to ""not justify my political beliefs to someone who disagrees with me;"" ""often feel uncomfortable when people argue about politics;"" and disagree that they ""have no problem revealing my political beliefs, even to someone who would disagree with me.""

Coffé, Hilde, and Catherine Bolzendahl. ""Avoiding the subject? Gender gaps in interpersonal political conflict avoidance and its consequences for political engagement."" British Politics 12 (2017): 135-156.

https://www.researchgate.net/figure/Descriptive-gender-gaps-in-political-conflict-avoidance-a-I-would-rather-not-justify_fig1_303835617

Here is another study that shows women are more likely to avoid expressing political opinions, even in anonymous academic surveys. This seems to definitively eliminate a theory that women do not express opinions due to physical intimidation.

Rae Atkeson, Lonna, and Ronald B. Rapoport. ""The more things change the more they stay the same: Examining gender differences in political attitude expression, 1952–2000."" Public opinion quarterly 67.4 (2003): 495-521.

https://www.jstor.org/stable/3521691

A very recent one that shows ""gender gaps [in political participation] are better understood as a product of men’s comparatively higher levels of enjoyment of arguments and disagreements.""

Wolak, Jennifer. ""Conflict avoidance and gender gaps in political engagement."" Political behavior 44.1 (2022): 133-156.

https://link.springer.com/article/10.1007/s11109-020-09614-5

Of course there are more that you can find cited in these papers, particularly the latest paper which can link you into the most recent research in the area.",4
post57hb,richly branching,1.5460908309676673,highest,"Or... the training data has shown that with two superficially identical candidates, the female is actually the better candidate.

Until very recently, this very probably was true - and for the oldest age groups will still be - until the last couple of decades it was harder for female's to get those same qualifications, so they almost certainly were ""better"".

Of course, the actual thing this says is that an AI which bases hiring decisions so much on a *name* is completely and utterly fucking useless as judging hiring decisions.",3
post57hb,richly branching,1.5460908309676673,highest,I don't think the way that AIs relate to their training data really works like that. I'd be suprised if it was extrapolating some general rule from the training data then applying it unprompted,3
post57hb,richly branching,1.5460908309676673,highest,">I'd be suprised if it was extrapolating some general rule from the training data then applying it unprompted

Isn't that exactly what they do? People first started being impressed with LLMs when they could do things like ""translate from English to French"" despite not being trained to do that.

It's whole shtick is learning general rules and then applying them without being explicitly prompted. The prompting is just the polishing on top of the model.

See Evil Bing/Sydney: Presumably they didn't tell it to ""Be cartoonishly evil"".",4
post57hb,richly branching,1.5460908309676673,highest,"I'm not in HR, but is there actually evidence that this is happening in practice? It sounds like the experiment used publicly-available LLMs, but this isn't what HR departments are using.

The paper gives several examples of software that large HR departments might be using like
https://www.ciivsoft.com and
https://ubidy.com/news/validating-skills-beyond-the-resume.

What is the evidence that this software leaves the candidate's name intact?",1
post57hb,richly branching,1.5460908309676673,highest,The effect was consistent across all the top LLMs. It's unlikely that ciivsoft (which is most likely just using one of the top LLMs with some paint on top) are not going to be exhibiting the same behavior.,2
post57hb,richly branching,1.5460908309676673,highest,"Yes, the underlying technology is the same, but [a marketing page on Ciivsoft's website](https://www.ciivsoft.com/3-steps-to-stamp-out-name-bias/) suggests that they do not include candidate names in their evaluations.",3
post57hb,richly branching,1.5460908309676673,highest,"It seems like in the post, masking candidate names flips the bias to men, although not as strongly.",4
post57hb,richly branching,1.5460908309676673,highest,I wonder if they actually have something to remove the names in their software. That post is more of a general statement that name bias is bad because it harms oppressed groups. But if the name bias is favoring underrepresented groups (this study would be interested to do with different ethnicity-coded names as well as gender) I assume there would be less motivation to stamp it out.,4
post57hb,richly branching,1.5460908309676673,highest,"Sure, but the resume study is an artificial construct to isolate gender as a variable with otherwise identical resumes.  It is probably not hard for the AI to make fairly confident gender predictions on a resume without a name, if it is biased on that dimension.",4
post57hb,richly branching,1.5460908309676673,highest,"You're not supposed to hire people based on statistical inferences from their demographic (people in group X have a 5% higher rate of substance abuse, so I won't hire this person from group X ). But if you were going to hire people based on the statistics of their demographic, I don't think it's irrational to prefer women, because on average they have higher agreeableness, conscientiousness, etc.",1
post57hb,richly branching,1.5460908309676673,highest,"> But if you were going to hire people based on the statistics of their demographic, I don't think it's irrational to prefer women, because on average they have higher agreeableness, conscientiousness, etc.

Yeah all things (that are put on a resume) equal and if I was allowed to, I think I'd agree that women will tend towards better hires. The main downside from an employer perspective is maternity/family leave more likely but the chance of them being drug abusers or criminals or something like that is less likely, depending on the crime [like stealing from businesses](https://www.thebulldog.law/blog/2023/10/study-men-are-more-likely-to-steal-from-businesses) almost *1/4th* as likely. And maybe criminal behavior itself isn't that common but noncriminal disruptive behaviors certainly can be and anecdotally that's also mostly men.",2
post57hb,richly branching,1.5460908309676673,highest,"I would imagine, if your job wanted any amount of competency and innovation, that hiring on high agreeableness would give you inherently inferior staff.


Agreeableness is probably the most dual pronged personality trait possible. High agreeableness directly corrolates to lower intelligence, lower creativity and lower common sense. Of course, low agreeableness corrolates to being an asshole, but as I said, dual pronged.",2
post57hb,richly branching,1.5460908309676673,highest,"Psychologically, women are just better at working. Men have egos, and aren't as good at working with others. Women are great at working in teams, leading without being domineering, and are just generally more pleasant in general.",1
post57hb,richly branching,1.5460908309676673,highest,True offices are inherently female coded workplaces.,2
post57hb,richly branching,1.5460908309676673,highest,"Meritocracy is back beybeeee

Uh anyways seems like a problem

Also surprising considering the surgeon was the boy's mother issues",1
post18hb,richly branching,1.5358493545348455,highest,Why did they accumulate so much without cashing out? You can take payments every 3 days.,1
post18hb,richly branching,1.5358493545348455,highest,Wondering the same,2
post18hb,richly branching,1.5358493545348455,highest,"Same.  I have earned well over $6k with DA and I withdraw it to pay my mortgage and things like that. If there is over $250 in there and I have a withdrawal due, I withdraw it. That's usually every 3 days.  I can't think of a good reason to leave that much money in there, it would take a fair amount of time to rack that up, certainly more than 3 days!  I trust DA as I have never had an issue working for them or being paid, ever, but I wouldn't leave $6k in there. I wouldn't leave that in PayPal.  If it's not paying something the only place for it is in a bank account earning interest ($300 a year at 5%)",3
post18hb,richly branching,1.5358493545348455,highest,My only thought was that they were logging hours that didn’t match with the work that was being produced -and/or- inputting basic minimal responses that are clearly not inline with project instructions.,4
post18hb,richly branching,1.5358493545348455,highest,"Thank you for responding with this. I have only been working with them for a couple of weeks, but your described experience has also been my own.",4
post18hb,richly branching,1.5358493545348455,highest,"Give it time. My bet is that they will “fire” you for no reason sooner or later. Probably right after a promotion and a message stating how good of a job you’re doing. It’s happening to everyone. And I’ll speak for myself when I say I carefully read all instructions for every project and was darn good at the tasks. Lost over $1200 that I had made over the past week between then and my last cash out. Just don’t quit your day job… I talked up this company to soo many people because it was really great. And it IS great while you’re still there. But me and tons of other people keep having this same thing happen, a lot of us for no reason, or I’ll speak for myself at least, and it’s only a matter of time before it happens to you too. I hope it doesn’t, but don’t be silly like I was and used this as my only source of income :(",4
post18hb,richly branching,1.5358493545348455,highest,"Hi - hope you can help me.  I was downsized Sept of last year and my unemployment benefits are about to run out.  I'd like to get gig work with DA. I setup an account and the whole nine, but it just says they have no work.  How do you start getting work with this company?",4
post18hb,richly branching,1.5358493545348455,highest,Do they provide a 1099!?,4
post18hb,richly branching,1.5358493545348455,highest,Is DA still a thing? Would I be able to do it from Aus?,4
post18hb,richly branching,1.5358493545348455,highest,How do taxes work with them? Is it a W9?,4
post18hb,richly branching,1.5358493545348455,highest,I completed my starter assessment yesterday how long they take to get back with results?,4
post18hb,richly branching,1.5358493545348455,highest,"I’m looking into doing DA, but i see alot of threads about it it being a scam, is it worth it ?",4
post18hb,richly branching,1.5358493545348455,highest,how long does it take to hear back for them after applying?,4
post18hb,richly branching,1.5358493545348455,highest,"Are you doing DA full time? Thats great that you get that much work. If you don't mind me asking, how much do you make per month?",4
post18hb,richly branching,1.5358493545348455,highest,"Yeah. This doesn’t make sense. I cashed out about $2k in my first month with DA. How long were these guys stacking cash and more importantly, why? If they were simply deactivated for poor performance then they’d be allowed to withdraw their funds. Seems like they were up to something shady to be banned completely.",2
post18hb,richly branching,1.5358493545348455,highest,"Side topic here but I've just started with Data Annotation and I'm doing the onboarding. How did you track your time? I'd like to start off on the right foot on this platform. Thanks for any help or suggestions you can give. 

![gif](giphy|AeWoyE3ZT90YM)",3
post18hb,richly branching,1.5358493545348455,highest,This sounds like victim blaming. There's nothing wrong with being owed $6k by a company. That is a normal paycheck.,2
post18hb,richly branching,1.5358493545348455,highest,"You have the option to withdraw funds every three days. In order to earn enough to have 6k you would have to work three to four weeks at top pay, full time. There is absolutely no reason to accumulate and leave so much money there. Would you allow your boss to hold your check when you could cash it out? You don't even have to transfer it to your bank, you can hold it in PayPal after cashing it out from DA.

It sounds like there is more to the story that hasn't been disclosed. The person posting this complaint ""for their friends"" made this account on the same day they posted the complaint. No answer has been made to what period of time the funds were earned in or any other details. The company is definitely known to lock out folks who have violated the code of conduct - but folks who just did subpar work still get to cash out, even if future work is locked out.",3
post18hb,richly branching,1.5358493545348455,highest,There's nothing wrong with having a large paycheck there's something illegal about company not paying it you don't really need a wall of text to understand that,4
post18hb,richly branching,1.5358493545348455,highest,"There could be some edge case reasons to not cash out regularly, but most of them involve deferring taxable income to the next year. Personally, I think the risk of something going sideways is not worth some potential small tax savings though.",4
post18hb,richly branching,1.5358493545348455,highest,It's not victim blaming. You would be an idiot to leave 6k+ with any affiliate.,3
post18hb,richly branching,1.5358493545348455,highest,"At the same time, I would like to know reasons why a person might be locked out even if the company can't address specific situations. I do work for DA and move my money regularly even if I only managed to get in 30m but money owed is money owed and if there is no rule against stacking the cash then that by itself wasn't wrong.. 6k is a lot of hours of work to leave sitting there though I wouldn't be able to afford to just leave that there. Mainly I hear people are having a good experience with the company which is why I got involved .
 So far I am pretty happy with the work myself.. but would love to know more on this.",4
post18hb,richly branching,1.5358493545348455,highest,"> You would be an idiot to leave 6k+ with any affiliate.

This is literally the definition of victim blaming. ""You would be an idiot to leave yourself so vulnerable to becoming a victim!""",4
post18hb,richly branching,1.5358493545348455,highest,Seems like you would be an idiot to complain on the internet about what other people do with their money but you do you,4
post18hb,richly branching,1.5358493545348455,highest,You are attacking the person instead of the issue at hand.,4
post18hb,richly branching,1.5358493545348455,highest,"No it isn't. When you are a freelancer, under a contract, you are running a business. You take your money owed. They are not employees, so it's their responsibility to get their pay. 


There's definitely more to the story. If you can cash out in 3 days, there's no reason leave it. These ""friends"" are definitely idiots, and most likely were scamming the company and got caught.",3
post18hb,richly branching,1.5358493545348455,highest,"I agree. Of course it makes good sense to cash out to avoid just such a happenstance that you lose access to your account.  But this is a gig job where you signed a contact to be paid money -- not microsoftrewards or some promotion game where you use a VPN or violate some other rule and they can seize your points. 

Even if OP did something that made DAT cast doubt on his billable hours, normal business practice is to issue written communication saying such and providing the employee with a mechanized option to dispute the company's account. 

Not saying DAT isn't worth taking the risk -- it's a better side gig than most and some people have made a lot of money from them. 

But it isn't remotely normal to lose access to PRIOR EARNINGS just because you are no longer employed by the place where you earned them.  So knowing this can happen is extremely useful information.  

(you don't have to be rich to let the money pile up. Sometimes people delay cashing their check for a few days just so the money doesn't burn in whole in their pocket over the weekend).",3
post18hb,richly branching,1.5358493545348455,highest,"Payments for timed projects are pending for  7 days , then 3 days to wait between withdrawals. If someone, as I did, worked for 10 hours a day on $40/h projects,  it's easily $4000.

For example:  
I worked from March 1st. First withdrawal is on March 10 for work up to March 2.  The next withdrawal is on March 13. If they cancel my account on March 13, they don't pay me for work done after March 2, that is 10 days to March 13.",2
post18hb,richly branching,1.5358493545348455,highest,"If you were working 10 hour days, 7 days a week - along with working for Telus in the same time frame - there is no way that you were able to maintain quality of work without violating the CoC to some degree. They only refuse payment for Code of Conduct violations, not for crappy work. If you were using AI to generate some of your content, farming your work out to other people, reusing material, etc etc then yeah - they refuse to pay you because you didn't uphold your end of the agreement and you didn't actually earn that money. Texas Workforce isn't gonna help you, BTW - as an independent contractor you will have to take them to small claims court to fight your nonpayment.",3
post18hb,richly branching,1.5358493545348455,highest,"Telus is a freelance job that pays $11, there is no reason to think that I would spend any time on it if I have something to do for $40/h.   I know, nobody can help me, the DAT can do anything with impunity. They hide their identity for a reason. I just tell other people what they can do other than complaining here on Reddit.  
The small court is useless: 1) I cannot serve the DAT with subpoena, as the site is registered anonymously; 2) the small court decision is impossible to enforce, as nobody takes it seriously.",4
post18hb,richly branching,1.5358493545348455,highest,My guess is they were programmers making $60 an hour so were comfortable with that.,2
post18hb,richly branching,1.5358493545348455,highest,If they were new to the site they weren't making $60/hr.,3
post18hb,richly branching,1.5358493545348455,highest,It does not say they were new accounts.,4
post18hb,richly branching,1.5358493545348455,highest,Why is anyone tracking how much a person is accumulating?  That’s their business whatever they do with their money. Anyone taking money from account that is NOT theirs is called theft and anyone monitoring ur account without permission is called invasion of privacy,2
post18hb,richly branching,1.5358493545348455,highest,"Not knowing anything about the company you're speaking of, I'll just say some folks are unbanked for whatever reasons and have to take some extra steps to move their $ around. So it kind of makes sense if they were new/new-ish and just procrastinated on that detail. But just like doordash offers dashercards I cannot fathom why any such service wouldnt have offered that as an option upon signing up. Maybe theese two will get a random paper check in the snail mail like 30 days out.",2
post18hb,richly branching,1.5358493545348455,highest,This company only pays out through PayPal. They disclose this up front.,3
post18hb,richly branching,1.5358493545348455,highest,"I can’t deal with PayPal; they’re almost the biggest scam ever. They just randomly grabbed some money that was sent to me and said that I owed it to them (which I didn’t), but would never respond back with a concrete reason why they felt I owed it to them. It was only ?30 or so, but just imagine how much bank they make by taking peoples money x however many people they get money from. They are unscrupulous and there’s no way to talk or email someone who isn’t reading or replying directly from a generic script",4
post18hb,richly branching,1.5358493545348455,highest,The payouts appear to be to PayPal.  ONE may choose to leave the funds in their PayPal account and get a PayPal debit card.  Having an old school bank account is not necessary but an option like any other linked account PayPal allows.,3
post18hb,richly branching,1.5358493545348455,highest,"No. You can't be unbanked and work DA. There's no random paper checks or snail mail. Nope. They use Paypal, and they even PAY YOU in onboarding to make sure your Paypal account is set up properly with correct email address, etc. Getting your money transferred into Paypal involves clicking a button. There's more to this story.",3
post18hb,richly branching,1.5358493545348455,highest,Because they were scamming the system and got caught,2
post18hb,richly branching,1.5358493545348455,highest,"You can cash out every 3 days ONLY for pay per task tasks. Not hourly tasks. Those pay once a week. And you can ONLY cash out every 3 days. So if you work 3 hours on a Monday, you can cash out for JUST those 3 hours the following Monday, but then have to wait a full 3 extra days before you can cash out anything you made after that Monday (as long as it’s within the 7 day period). So THATS why we accumulated so much and lost it all. Because it’s complicated. Some people have bills and need Monday, Tuesday, and Wednesday’s wages to pay those bills. So we’d have to accumulate nearly two weeks of wages without cashing out in order to cash out the full amount one may need for those bills if that makes sense. You can only cash out once every 3 days, hourly tasks are paid out every 7 days (which this money is deposited into your account). Let’s say it’s been 7 days and you get your wages from the day you worked 7 days ago, but you just cashed out yesterday. You now have to wait 3 more days to cash-out the wages you earned 7 days ago. And if you work every day with high paying hourly tasks, it adds up quickly. I lost $1256 bucks for a week and two days worth of work before getting “banned” for no reason. It’s not as easy as work, and get paid 3 days later. If you worked there, you’d understand. It’s just not as simple as it may seem or not as simple as they make it seem.",2
post18hb,richly branching,1.5358493545348455,highest,"I know how it works, I've been doing it for months. Earning 6K in that amount of time is unlikely without low quality work or a violation of CoC by account sharing. You are misrepresenting the pay - it doesn't pay ""once a week"", you can cash out the money exactly 7 days after you did the work to allow time for review of the work. I cash out twice a week, I can get money from as recent 7 days prior (and do, every week, twice a week).",3
post18hb,richly branching,1.5358493545348455,highest,"lol, no my friend, I’m not misrepresenting, I’m basing my answer on actual facts and experiences with this specific company. Your using terms like “unlikely” makes me think that you and I aren’t speaking about the same company. DA allows you to cash out every 3 days period. Hourly projects are paid out 7 days after completion. If you work on a Monday, let’s say, and go to cash out the next Monday, then you can! UNLESS you JUST cashed out on Sunday, which would then mean you now have to wait until Wednesday to cash out for the previous Mondays work. And by cash out I mean getting money into your PayPal. There’s a countdown timer and everything letting you know exactly when your last cash out was and when you can send money to your PayPal again. I’ve got screenshots if you’d like to see them. Just because you don’t agree, doesn’t make a true claim a misrepresentation. Non hourly tasks pay out ever 3 days, hourly tasks pay out every 7 days. Separately from that/ you’re only allowed to send money to your PayPal every 3 days despite the previously mentioned 3day/7day rule.",4
post18hb,richly branching,1.5358493545348455,highest,"I don’t think you understand what I’m saying lol. Or you don’t h see stand what YOUR saying. It’s a 7 day review period, allowing you access to the money you made 7 days prior. If you haven’t cashed out within 3 days before that 7 days, then you can cash out that day. Heck, you can sometimes cash out 3 times a week if you do it correctly. UNLESS your within that 3 day rule/vs the day you actually worked and it being after the 7 day review period",4
post18hb,richly branching,1.5358493545348455,highest,"Also I’m not sure who mentioned making 6k, but it sure wasn’t me friend. I pulled in between 2-4k a month when I worked for them based on my own choice to work however many hours.",4
post18hb,richly branching,1.5358493545348455,highest,Remember- just because it didn’t happen to you doesn’t mean that it didn’t happen to someone else. Glad you’ve still got a job there/ but a ton of us aren’t as lucky. And most of us unlucky folks did every single little thing right. Don’t be ignorant to common logistics -no offense😊,4
post18hb,richly branching,1.5358493545348455,highest,"I think they may have been doing something sketchy, like copy paste reviews or not being careful with details. I agree that having 6000 and not collecting is weird.",2
post18hb,richly branching,1.5358493545348455,highest,The question fails to address the complaint.,2
post18hb,richly branching,1.5358493545348455,highest,This. The story OP claims sounds suspect.,2
post18hb,richly branching,1.5358493545348455,highest,[deleted],2
post18hb,richly branching,1.5358493545348455,highest,What proof is out there that they have bad business practices other than a flood of people online bitter because they never got approved into the work flow. Im sincerely asking as I am curious.,3
post18hb,richly branching,1.5358493545348455,highest,[deleted],4
post18hb,richly branching,1.5358493545348455,highest,is that even relevant?,2
post18hb,richly branching,1.5358493545348455,highest,"Just my personal anecdote, but I've been working for them for 3-4 months, I've always had work to do and they've always paid me out.  I recently qualified for their programming projects, and am making way more doing those that I feel I deserve given my level of experience.  


It's always given off a ""too good to be true"" vibe for me, and I've been perpetually waiting for the other shoe to drop... but it hasn't yet.  


I've referred several friends to it, but not one of them has heard back.  I'd assume if their game plan was to get people to do free work, refusing willing workers would be detrimental.  


In my personal experience, they're far above something like Appen, who pay like shit and treat their workers like disposable tools, they kicked me to the curb peak covid for no apparent reason.  In comparison, Data Annotation has been really good to me, so far anyways.  


Crossing my fingers that it stays that way.",1
post18hb,richly branching,1.5358493545348455,highest,"I believe they are just looking for a certain quality of work….as you know the guidelines are very clear, extensive, but leave room for creativity within the parameters. It is my only thought as to why so many are not onboarded.",2
post18hb,richly branching,1.5358493545348455,highest,Agreed. So many people who make posts here about not being accepted have awful grammar. It sucks hearing you aren’t as smart as you thought lol.,3
post18hb,richly branching,1.5358493545348455,highest,"I've been a member of DA for awhile and even reviewed people's applications. Yes, a lot of crappy quality work gets submitted and I give them a low rating. DA is not just another random beermoney site like swagbucks.",4
post18hb,richly branching,1.5358493545348455,highest,"I often have imposter syndrome and worry that I am submitting bad chats. But then, I will get a review task and see how awful a lot of people's prompts are. It encourages me to not think so harshly on myself lol.",4
post18hb,richly branching,1.5358493545348455,highest,If you think data annotation and other companies that hire drones to train chatbots actually think that any of their independent contractors has exceptional grammar or if you actually think that grammar is what they are paying you for then that is very cute and hilarious,4
post18hb,richly branching,1.5358493545348455,highest,"Expanding on that, they're clearly also looking for people who can separate their own opinions from fact, and as unfortunate as it is, that is really hard for a lot of people.",3
post18hb,richly branching,1.5358493545348455,highest,I assume you may be stating this based on some of the application questions. And you are correct.,4
post18hb,richly branching,1.5358493545348455,highest,Dam,4
post18hb,richly branching,1.5358493545348455,highest,"It’s more the fact that their support seems to be nonexistent. My browser crashed during their initial training and I never was able to start over, even with a new account, because it’s tied to my phone number",2
post18hb,richly branching,1.5358493545348455,highest,"I did the initial assessment and it says you can come back to do the assessments later. I honestly didn't think about the follow up assessments right away and did the application shortly after I took my nighttime meds. I just ran out of good brain power. I want to go back to do them. I guess I will have to see what happens. Grammar isn't my strong point but I have character development, descriptive abilities, and what I see as most important, the ability to separate facts from my opinions. After I have separated facts from opinions I can point to the facts and use them to put together a reasonable argument to sway an opinion without being insulting, 98% of the time anyway. I can, and have, mimicked other writing styles, and I can write fantasy, mystery, sci-fi, horror, research papers and I can be personable. 

In this case I know I would be an asset to the company. I am almost always an asset, and if I feel I can't be, and I can't get to a place I can perform at top performance I will give it everything I have until I plateau. If the plateau is below what the company wants. I am willing to bow out gracefully.",3
post18hb,richly branching,1.5358493545348455,highest,"I had a similar experience. My screen locked up during the initial test. When I was able to get the screen moving it had submitted my form and I wasn't finished. I know that there were some misspellings because my spell check changed some words incorrectly. I was going to go back and fix those but never got the chance. It was locking up on me as I was filling it out, making it difficult to fill the form out at all. I didn't have any problems on any other sites before or after that happened. I was not surprised that I didn't hear back from them. It was fun doing the assessment. I liked the writing part and I love to research, so this may have been a good job. But I have read a lot of bad about them. There is way more bad than good being said about working for them.",3
post18hb,richly branching,1.5358493545348455,highest,"It’s so normal to have initial problems. Especially with technology, very intolerant of the company",3
post18hb,richly branching,1.5358493545348455,highest,"I've been with them since May and I feel the same way..""too good to be true"". I make more there than I made at my full time job. But I'm always on edge, hoping it stays true.",2
post18hb,richly branching,1.5358493545348455,highest,Is it still true?,3
post18hb,richly branching,1.5358493545348455,highest,"Yes, just crossed the $50,000 Mark with them.",4
post18hb,richly branching,1.5358493545348455,highest,"Man I feel you so much with the ""Too good to be true"" thing. I keep saying Im waiting for the other shoe to drop. 

So far so good, though. It's kind of a dream job, frankly.",2
post18hb,richly branching,1.5358493545348455,highest,How's it going so far?,3
post18hb,richly branching,1.5358493545348455,highest,Still at it? Its May now!,3
post18hb,richly branching,1.5358493545348455,highest,So far so good.,4
post18hb,richly branching,1.5358493545348455,highest,"Almost two months here, no programming, but same for the rest. I know my one referral gave up after two hours in the paid qualifications and I don't think ever finished taking them. No problems so far and have had new projects open up in the past two weeks.",2
post18hb,richly branching,1.5358493545348455,highest,"Constantly waiting for the other shoe to drop… I do want to believe the work I’m doing is good enough (been on the platform for about the same amount of time as you), but without a direct point of reference you just have to do your best. I figure it’s wise to put top effort into every task rather than get through as many tasks as possible.",2
post18hb,richly branching,1.5358493545348455,highest,"I agree completely.  It is not just about the content though, it's being very thorough and reading the directions, reading all of the updates and staying true to the instructions. They are looking for very specific tasks, so if you're not following them, you'll be taken off project. 
I'm thoroughly enjoying working for them!",2
post18hb,richly branching,1.5358493545348455,highest,We had real issues getting paid out of Apen so we are trying DA. Nothing yet so we will see.,2
post18hb,richly branching,1.5358493545348455,highest,PMing you.,2
post18hb,richly branching,1.5358493545348455,highest,Can you speak to the difficulty of the programming assessment? Would this be something an aspiring jr. dev could complete to gain work experience?,2
post18hb,richly branching,1.5358493545348455,highest,"I more or less consider myself a beginner, or at the very least somewhere between beginner and intermediate, so if I can manage it, I'm sure you could.  
As long as you have a grasp on the fundamentals, I think you should be fine.

I can't speak to whether or not these projects would be considered good work experience, but they're definitely a good learning experience as it's just as much about understanding and correcting other's code as it is writing your own.",3
post18hb,richly branching,1.5358493545348455,highest,Appreciate the thoughtful response!,4
post18hb,richly branching,1.5358493545348455,highest,I’ve started working on projects a few days ago and it just seemed too good to be true! Glad to see it’s actually a thing and there’s people here that’s been working for them for months/years,2
post18hb,richly branching,1.5358493545348455,highest,I’m so happy to see positive comments about DA. Yesterday I passed the starter assessment and then completed the core qualification tests. The next step is for them to review my work. If I pass I will get an email. Is this the process you went through?,2
post18hb,richly branching,1.5358493545348455,highest,Yep.,3
post18hb,richly branching,1.5358493545348455,highest,What is thee starter assessment and core qualifications tests like?,3
post18hb,richly branching,1.5358493545348455,highest,I didn’t get any emails. I kept logging in and checking my account and I finally received tasks to pick from.,3
post18hb,richly branching,1.5358493545348455,highest,Hi. Have you or would you quit your day job to do these programming  tasks? I'm assuming you're a software dev of some kind,2
post18hb,richly branching,1.5358493545348455,highest,"I actually have no background in programming short of the learning I've done (CS50 and a bunch of self learning).  I was a streamer/musician (Still technically a musician, but I don't really stream much anymore) for my income prior to working with Data Annotation.  Probably goes without saying, but this pays a lot better than streaming/music does, and is currently my full time income.",3
post18hb,richly branching,1.5358493545348455,highest,"That's amazing. So you were able to get these programming tasks even though you've had no professional programming experience? 


I am a software dev so I will definitely check this out",4
post18hb,richly branching,1.5358493545348455,highest,"Professional software engineer here.

I would not quit my job to do these programming tasks for the following reasons:

1. It pays less than my day job. Even if you currently make less than $40/hour, then keep in mind that there is no meaningful advancement so you will be stuck at $40/hour give or take.

2. Lack of benefits.

3. No guarantees of future work.

4. Honestly, I think the work would become grinding if you were doing it full-time. It's kind of fun when you are dashing out maybe an hour or two in the evenings and see an extra $300-600 come in per week, but if you had to sit there for 8 hours each day doing the same sorts of tasks over and over again, it would get very old. I already find myself getting excited when a decent paying non-coding task pops up into my queue for a change of pace.

That being said, if I were look at something like FIRE and could get there quicker by patching in 5-10 hours a week of Data Annotations on top of my retirement funds, that would be much more tempting.",3
post18hb,richly branching,1.5358493545348455,highest,[deleted],4
post18hb,richly branching,1.5358493545348455,highest,"Sound advice, thank you.",4
post18hb,richly branching,1.5358493545348455,highest,"Is it better or worse than working at a Top 40 station every day?

Oh wait, radio is pretty much automated now. Listeners are often fooled into believing it's all live. Rotated songs are usually planned out days in advance. But could you imagine hearing the A-list songs play 9 times every 8 hours?... And no guarantee of future work if there happens to be a format change?

In other words, just wanted to show people that even the jobs that seem ""cool"" can be mundane in reality. I wonder what pilots do while flying 14 hours mostly over water at night? I would eliminate ""looking out the window"" from the list...",4
post18hb,richly branching,1.5358493545348455,highest,"Hey, do you think you could help me get started with them? Where could I practice before taking their assessment to get hired on? I do have a bit of programming experience. I don't do much 1337code, should I start there?",2
post18hb,richly branching,1.5358493545348455,highest,"Anything that helps you get fundamental knowledge would be helpful.  Personally, I finished Harvard's CS50x prior to finding this job, and it gave me pretty much all the tools you need for the coding assessment.  You can find the whole course on youtube, or if you want a more official approach, enroll on the website, as then you can submit your homework and have the option to pay for a certificate at the end.  And no, the certificate is not required.",3
post18hb,richly branching,1.5358493545348455,highest,Did it stay that way?,2
post18hb,richly branching,1.5358493545348455,highest,"So far, yes.  Still working for them, still getting paid.",3
post18hb,richly branching,1.5358493545348455,highest,"I have a friend who referred me to it, and I heard back in about 4-5 days. But the other person she referred hasn't heard back in over a month. I guess it just depends on how you do on the assessment.",2
post18hb,richly branching,1.5358493545348455,highest,"Four months later and i have to ask if the shoe dropped yet, I'm looking into some last ditch attempts to find a job and this site keeps getting reccomended. Id love to hear an update about your experience if you're willing to share.",2
post18hb,richly branching,1.5358493545348455,highest,"No shoe drop yet.  I'm still working regularly for them and getting consistent pay outs.  The only thing that's really changed since I posted this is I got access to the coding projects, which pay way more.",3
post18hb,richly branching,1.5358493545348455,highest,"I signed up yesterday, only question I have is what language do you write in most for coding? I'm a total beginner who was pointed towards Python but I'm worried it's ease of access might be a double sided sword, like it won't actually help me in a field like this.",4
post18hb,richly branching,1.5358493545348455,highest,What's the Assessment Test like?,2
post18hb,richly branching,1.5358493545348455,highest,"It tests things like your attention to detail, writing abilities, fact checking, researching, and your ability to remain objective while facing extreme opinions or ""alternative facts"".

Some of the questions seem really weird or out of left field, but there is a purpose behind each one.",3
post18hb,richly branching,1.5358493545348455,highest,"I just discovered the data annotation tech website but when i signed up it immediately said nothing was available and i would receive an email when something shows up. Is this common for there not to be any projects? When it asked for my name and email info was that some sort of test that maybe I failed because I hit enter instead of ctrl + enter for the last question?

This seems like a great opportunity for me to pick up a few hours per week to supplemnet my income.",2
post18hb,richly branching,1.5358493545348455,highest,"If you haven't taken the assessment yet, you won't have qualified for any work.  Think of it like the job interview.  It's been a long time since I've done it, but you should have had an option to take the assessment as soon as you signed up.",3
post18hb,richly branching,1.5358493545348455,highest,"I received this immediately:



# Thanks for signing up!

We do not have any open projects to assign you at the moment. Our workflow can vary, so it's common to see available work fluctuate.



Keep an eye out for emails from us for project announcements. As soon as we have a task that matches your skills and availability, we'll reach out about claiming it. In the meantime, make sure [your profile](https://app.dataannotation.tech/me) fully represents your capabilities so we can match you accurately.Thanks for signing up!

We do not have any open projects to assign you at the moment. Our workflow can vary, so it's common to see available work fluctuate.",4
post18hb,richly branching,1.5358493545348455,highest,"Hey 👋 I know this post is really old, but I’ve recently signed up to Data Annotation, and was looking at reviews and came across this thread. I’m pretty experienced in the project world (at least I think so), but there are no projects for me to currently work on. Their website is quite basic and there doesn’t seem to be a point of contact on there. Does it take a while for projects to come up? If so, how long roughly did you have to wait if at all?",2
post18hb,richly branching,1.5358493545348455,highest,"I signed up, did the assessment, and was contacted about 3-4 days later saying I passed and that qualifications and work were now available.  Basically, as soon as they confirmed I was accepted, I had work to do.

If you have no projects or qualifications to do, that could mean 1 of 2 things:  
1:  You haven't done the assessment yet.  Or...  
2:  You did the assessment and didn't pass.  I wouldn't get my hopes up if you've been waiting more than 2-3 weeks.  Most people I know or have heard of getting in have gotten their confirmation about passing in less than 1-2 weeks.",3
post18hb,richly branching,1.5358493545348455,highest,"Well the thing is I’ve not been prompted to do an assessment or anything. I literally signed up, and I got the screen prompt saying ‘there aren’t any projects for you right now’. So that’s before I’ve had a chance to do anything",4
post18hb,richly branching,1.5358493545348455,highest,Hi just curios as to what role you did for them? I am interested in the Data Analyst role so wanted  any information I can get about them.,2
post18hb,richly branching,1.5358493545348455,highest,You're barking up the wrong tree if you're looking for a Data Analyst position.  The work on DA is mostly focused on training LLMs (Large Language Models).,3
post18hb,richly branching,1.5358493545348455,highest,"Ok I am able to do that as well.   
If you dont mind me asking, what position do you have and what kind of work do you do? and do you still work there or did they do something fishy like everyone else says lol",4
post18hb,richly branching,1.5358493545348455,highest,Hi: Do you know if it's possible to apply to DA more than once? Thanks.,2
post18hb,richly branching,1.5358493545348455,highest,Do you know or remember if it took a few weeks to hear back?,2
post18hb,richly branching,1.5358493545348455,highest,They got back to me in less than a week.,3
post18hb,richly branching,1.5358493545348455,highest,2025 update?,2
post18hb,richly branching,1.5358493545348455,highest,"I'm still working for them, still get paid consistently.",3
post18hb,richly branching,1.5358493545348455,highest,"I just saw this company on indeed. Are you still working for them, and do you still enjoy it?",2
post18hb,richly branching,1.5358493545348455,highest,"Yes, I'm still working for them.  I enjoy it about as much as you can enjoy a job, I suppose.   It has its pros and cons like any other job.",3
post18hb,richly branching,1.5358493545348455,highest,"There's no point slobbing the knob of a company that claims to work with artificial intelligence but they can't program their own basic web application to let applicants know if they've been accepted or rejected despite the fact that they require applicants to do two or three hours worth of testing just to submit an application


It's not a secret that the tech industry is full of douchebags",2
post18hb,richly branching,1.5358493545348455,highest,"They email you to let you know you've been accepted, and they don't if you haven't just like most other jobs.  Acknowledging that and sharing my experience is hardly ""slobbing their knob"".",3
post18hb,richly branching,1.5358493545348455,highest,"I've been reading through the posts here to get a feel for the company, it's detractors, and it's supporters, but had to chime in on this.  Honestly one of the best measures of whether a company is going to be good to work for, or not, is how they treat the people they don't choose to hire.  

Sure, most companies don't email you to let you know theyve gone another way, but some have that courtesy and I wouldn't choose to work for a company that doesn't.  The measure of a man is not how he treats his equals, but his treatment of those he considers lesser.  If you want to know if someone is a good person (companies are also a person I guess, thanks citizens United) see how they treat retail workers, serving staff, or people they don't want anything from like the reject pile of their HR department.",4
post18hb,richly branching,1.5358493545348455,highest,Yeah you pretty much proved my point that just like most employers that suck at doing their shit this company is it revolutionary or edgy at all it's just another shitty employer that can't do the basics,4
post18hb,richly branching,1.5358493545348455,highest,Looking forward to a slobbing of the knob response,4
post18hb,richly branching,1.5358493545348455,highest,"I agree. If they can give a “you passed” instantaneously, they can do the same with “sorry, you did not pass.” These are assessments, not interviews. 

I passed and am working. But the smugness among many here who passed is really unnecessary.",3
post18hb,richly branching,1.5358493545348455,highest,"If you’ve applied with other companies you know they don’t call you, email you, text you or fax you to let you know‘they’re not hiring you’. Just keep trying or better yet STOP until you have a more positive mindset.",4
post18hb,richly branching,1.5358493545348455,highest,[deleted],2
post18hb,richly branching,1.5358493545348455,highest,"There are 2 parts to the test, the first is writing a function that will decode an ""encrypted"" (I'm using that term lightly here) text file into a message and being able to explain what your code is doing.  The second part is mostly code comprehension and testing your ability to know what is good or bad code.  


All things considered, the test was pretty simple, and I'd imagine anyone with at least fundamental knowledge should be able to complete it.",3
post18hb,richly branching,1.5358493545348455,highest,What kind of coding skills do you have? How advanced do your skills need to be to pass the coding test?,2
post18hb,richly branching,1.5358493545348455,highest,"My coding experience basically extends to whatever was taught in Harvard's CS50x, so C, Javascript, HTML, Python, some related frameworks and then a little game development as I've been playing around with Godot, which uses GDScript, which is very similar to Python.  


The coding test was far easier than most of the projects I had to do for CS50x, so I'd say you only need mostly fundamental knowledge to pass it.  Particularly, you need to know how to open and interact with a text file, have a fundamental grasp on for loops, conditions and how to implement basic algorithms.",3
post18hb,richly branching,1.5358493545348455,highest,"I passed the initial test and they told me. I then took the coding test and the other AI test, but it’s been multiple weeks and they still haven’t told me whether I’ve passed. The website still just says “Up now: we review your results. If you pass, we'll email you.” HOWEVER, I DID receive the $80 for passing the coding test. I don’t understand why they won’t give me any work if I’ve passed!",2
post18hb,richly branching,1.5358493545348455,highest,"The $80 was for your time taking the test. They take a few days to a couple of weeks to review the results. If you don't get another email by 2 weeks, it's safe to assume you didn't qualify and/or all the projects are currently full",3
post18hb,richly branching,1.5358493545348455,highest,How long did it take to receive the $80?,3
post18hb,richly branching,1.5358493545348455,highest,If I remember correctly I think I got it the next day after it said on the website that I could cash out,4
post18hb,richly branching,1.5358493545348455,highest,Have you heard back from them since then?,3
post18hb,richly branching,1.5358493545348455,highest,Nope,4
post18hb,richly branching,1.5358493545348455,highest,"Your friends did something that violated the terms of service and were immediately and permanently banned from the platform. I don’t know what your friends were up to or why they decided to store their funds inside the platform they were attempting to defraud, but it’s very unfortunate that your friends weren’t smart enough to withdraw the money they stole before being caught.",1
post18hb,richly branching,1.5358493545348455,highest,"Okay they suspended me with no explanation and I didn’t violate their code of conduct. They are legally required to give you the money they already approved and I have no access to mine. Closer to $500, but still. It’s illegal. Explain the suspension and pay up.",2
post18hb,richly branching,1.5358493545348455,highest,did you ever get paid?,3
post18hb,richly branching,1.5358493545348455,highest,Nope. I followed up incessantly and even decided to take a chance and make a request for payment to the PayPal account that paid me oht and got nothing.,4
post18hb,richly branching,1.5358493545348455,highest,You have no proof of that and you'll do all the other company trolls spamming this comment thread desperately trying to defend this company,2
post18hb,richly branching,1.5358493545348455,highest,"This \^

Why tf are ppl so quick to defend this company?

They have no communication whatsoever lmao",3
post18hb,richly branching,1.5358493545348455,highest,"DA is a legitimate company that is contracted by huge players. They work closely with SurgeHQ and by extension Google, OpenAI and Meta. These services would pull out immediately if DA was even remotely illegitimate.

The site has millions of dollars going through it weekly, and a large reputation to maintain. They are not about to scam your friend of $6000 for no reason. $6000 is nothing to them. They pay out $100 to *failed* applicants regularly. There is no logical reason whatsoever for them to risk their reputation over $6000.

They only lock accounts for significant breaches of the code of conduct. For them to also refuse payment means your friend must’ve significantly inflated his hours or provided flat out unusable work.",1
post18hb,richly branching,1.5358493545348455,highest,"Thank you. A logical response. We can't speak to the clients that DA works with, but I have worked steadily with them since September and they pay what they say they pay.  However, they are strict with VPNs, people who run the clock, and sub-standard work. If there is any evidence that claimed time is at odds with the work produced, you will get locked out. They will pay what has been legitimately earned, but it will take time to settle if the user has been flagged.",2
post18hb,richly branching,1.5358493545348455,highest,I am glad that projects that expect in-depth analysis will usually give you a metric for that time.,3
post18hb,richly branching,1.5358493545348455,highest,"Yeah, I'm always concerned about taking too much time until I see ""Take up to an hour to research if needed."" And then remember my 15 minutes to fact check is not unreasonable.",4
post18hb,richly branching,1.5358493545348455,highest,It's the response that you wanted to hear that's not the same thing as logic,3
post18hb,richly branching,1.5358493545348455,highest,"Unfortunately, Meta and Google aren't that legitimate, and AI is pretty much thieves.",2
post18hb,richly branching,1.5358493545348455,highest,"I love how the person cited meta as a source of legitimacy when they have stolen every idea and every concept that they have from someone else


Silicon valley is based on theft and data annotation is a highly problematic company so it's pretty hilarious that this person cited even bigger more problematic companies as some sort of cosign",3
post18hb,richly branching,1.5358493545348455,highest,But are they legitimate enough to cough up the money?,4
post18hb,richly branching,1.5358493545348455,highest,"bro shut up, the matter of fact is that they will pay you LOL",3
post18hb,richly branching,1.5358493545348455,highest,"No one even attacked you bro and you come out swinging.

Go eat your wheaties so you can grow up.",4
post18hb,richly branching,1.5358493545348455,highest,Noting for DA company profiles,2
post18hb,richly branching,1.5358493545348455,highest,"**Data Annotation is a subsidiary of Surge Labs**. This company filed its original papers in Delware but operates out of San Francisco CA. Here is their actual address:

584 Castro Street #3065

San Francisco, CA 94114

And phone number:

(661) 619-4477

**CEO: Edwin Chen**",3
post18hb,richly branching,1.5358493545348455,highest,The tech industry is full of douchebags have you just now woken up or were you just born yesterday do you really think the world is a place of fairness and justice I'm just curious how naive you are you think because a company has a lot of money they are ethical I guess you must bow down and worship Elon musk like a lot of the other naive people huh,2
post18hb,richly branching,1.5358493545348455,highest,Why do they pay failed applicants wtf?,2
post18hb,richly branching,1.5358493545348455,highest,Sound super suspicious they let that much accrue without paying themselves out. You're not telling us the whole story.,1
post18hb,richly branching,1.5358493545348455,highest,"I applied there many months ago, did their assessment and all the things and still never got sent anything to do, and every time i log in it says ""If we have need of your particular skills, or we have additional assessments for you to identify further skills, you’ll be notified via email. Otherwise we thank you for your time.""  


This is hilarious to me because my skillset is exactly what they keep posting they need more of on places like Indeed.  


(I gave up caring a long time ago because its so obvious they're bullshit)",1
post18hb,richly branching,1.5358493545348455,highest,"Hate to break it to you, but you’re probably just not as smart/qualified as you think you are.",2
post18hb,richly branching,1.5358493545348455,highest,He walked right into it and didn't realize.,3
post18hb,richly branching,1.5358493545348455,highest,"Why would you say that? If they passed the qualification assessment then they meet the criteria to be ""smart/qualified"" enough. Or do you only get sent further jobs based on your performance in previous jobs?",3
post18hb,richly branching,1.5358493545348455,highest,"This commenter never actually passed the initial assessments if the line they quoted is on their profile. That message goes away after your acceptance email is sent. It is then replaced with the Qualifications, Projects, and Reporting Time sections. If you get accepted and there are no projects available to work on, the ""Project"" section is just blank. This person assumed that just because they applied, they would get access to work, which is not how job applications work.",4
post18hb,richly branching,1.5358493545348455,highest,With great smartness comes great stupidity.  That's for damn sure.,3
post18hb,richly branching,1.5358493545348455,highest,Hate to break it to you and I'm surprised that you need someone to break this to you but data farming is very real a lot of job positions aren't real and data annotation hires maybe 10 people a week but they put probably a thousand people a week through their assessment tell them that they've passed then put them through another much longer assessment and don't bother telling anyone that's rejected that they got rejected because apparently it's too difficult for a data annotation company that have really relies on computer programming to program a simple script,3
post18hb,richly branching,1.5358493545348455,highest,[removed],3
post18hb,richly branching,1.5358493545348455,highest,"He’s acting entitled and I’m gonna call that behavior out. If you look at this person’s profile you’d see he is in fact a 30-something white man, so I’m gonna bet English is in fact his first language.",4
post18hb,richly branching,1.5358493545348455,highest,"Honestly, I’ve applied less than 3 weeks ago, heard back after 5 days, and have had a steady flow of coding tasks since. I’ve also referred a few friends to the platform, and most of them took the exam last Thursday. We’re all PhD students or candidates in Computer Engineering, and we’re used to writing scientific articles—at times even getting published in great journals—so our writing skills are pretty good as well. People unfortunately don’t realize that there is genuine competition to get into DA. If one doesn’t get in, it doesn’t mean that they’re not qualified or not good enough, just that the other applicants were more qualified or had skills that they do not possess.",3
post18hb,richly branching,1.5358493545348455,highest,"I remember they asked me to do a creative writing exercise for my assessment, and I wrote out this whole paragraph or two about something or another, and they just ghosted me immediately.

I guess they thought it was an AI, but their process is clearly not perfect...",4
post18hb,richly branching,1.5358493545348455,highest,"I'm not saying I doubt your writing skill in particular, but in my experience, journal articles aren't where I tend to find what I consider good writing. It could just be that I'm a layman, so papers will seem denser and less accessible to me than to the target audience. Still, though, controlling for that**¹**, there remains a distinct prevalence of overwrought or unclear material -- in a low-level grammatical/syntactical way, or in the sense that it seems like the author has started with something that teases some incredible insight or revelation, but they never end up revealing the fascinating implications that the abstract/intro seemed to hint at. 

1. c wut i did thar",4
post18hb,richly branching,1.5358493545348455,highest,"There’s absolutely no way I would hire you.  You can’t even be bothered to capitalize the second “i.”  Your first sentence is a run-on and your grammar is mediocre at best.  You literally used the past tense of the word “do.”  These are elementary mistakes.  You expressed your discontent by stating “still never got.”  Your grammar is atrocious.

You need to be honest with yourself in this scenario - you aren’t up to par.  The quality of work this company expects weeds out 90% of the unfit applicants,and unfortunately, that includes you.  Take this opportunity as a time for growth.  Programming, editing, and coding AI is an extremely important responsibility.  If (and that’s a second class conditional) you were employed, the quality of output would decrease.  You need to develop an incredibly deep understanding of the English language if you hold any aspirations to engage AI in this particular field.  AI is not “one size fits all,” but this specific opportunity is one where you have to all but master the English language. 

I don’t teach Trigonometry because I’m not qualified, and doing so would be a colossal conundrum, not only toward me, but all I encountered with respect to the field. 

I work for this company and get paid handsomely for my freelancing.  The majority of people I’ve seen expressing discontent with said company are the ones who weren’t good enough to “make the cut.”  Consequently, they’re in the midst of an existential crisis concerning their self-worth. Thus, to receive validation of how marvelous they believe themselves to be, they attack the company with all fervor.

Not making the team isn’t a terrible thing — it just means you belong on another team.

I truly wish you best.",2
post18hb,richly branching,1.5358493545348455,highest,This guy was obviously not skilled enough to make it in but that doesnt mean his complaint on reddit was his application lol you cant really base anything off the way he writes here,3
post18hb,richly branching,1.5358493545348455,highest,"Jesus Christ r/IAmVerySmart much? 


Me and my gf both DA. It's great, but it's still basically minimum wage grunt work for a mega corp.


You're not on the bleeding edge of AI development. 


Stop larping like you work at OpenAI.


You need like a high school education, good reading comprehension and a fairly broad understanding of internet culture to pass the assessment. 


Why some qualified and capable people fail? I have no idea, it wouldn't surprise if they reject people for a variety of reasons that are beyond their control. 


Too many applicants from a similar demographic could bias the models across time. For all we know they might be restricting the number of 20-30 year old west coast white dudes from being accepted.


Perhaps the applicationt unwittingly left their VPN active during the test and it automatically denied them access because their proported location doesn't match their IP. 


Maybe the human that checks the work was having a bad day and denied them over something petty. 


-


I'm sure lots of people mess up the assessment, but passing it doesn't make you part of some elite intellectual club. 


No need to shit on some random dude and criticise his casual Reddit comment for grammar (especially when you make mistakes yourself). 


You've just made a whole host of assumptions about what caused him to fail the assessment, the sort of unfounded assumptions that DA dislikes and tries to filter out. 🤔",3
post18hb,richly branching,1.5358493545348455,highest,Guys a fucking loser who no one is proud of so he has to make grandiose statements himself,4
post18hb,richly branching,1.5358493545348455,highest,I love how a person with iFukHorses is being condescending to someone else.,3
post18hb,richly branching,1.5358493545348455,highest,"i lOvE hOw A pErSon with iFukHorses(hilarious username, by the way.  Did I parenthetically reference correctly just now?) iS bEiNg cOnDeScEnDinG t0000 sOme00nE eLsEe.",4
post18hb,richly branching,1.5358493545348455,highest,"Hi there, I got an email saying I’ve passed their initial review of my DA Starter Assessment. It brought me to a page to complete my qualifications, but I was only able to fill out one of them (I don’t know how to code). If they find that I’m qualified, do they keep giving me the same task? Or different ones each time. And does this mean I got “hired” ? Sorry for all the questions!",3
post18hb,richly branching,1.5358493545348455,highest,"If you get beyond the initial assessment, there will be both tasks that are similar to that assessment as well as other qualifications that you could try for specific different work.",4
post18hb,richly branching,1.5358493545348455,highest,"""There’s"" should be ""There is.""

""run-on"" should be hyphenated as ""run-on.""

""90%"" should be ""90 percent.""

""unfit applicants"" should be ""unfit applicants, and unfortunately, that includes you.""

""If (and that’s a second class conditional)"" could be rephrased for clarity: ""If (and this is a second-class conditional).""

""all but master"" should be ""almost master.""

""Trigonometry"" should be capitalized as ""trigonometry.""

""colossal conundrum"" is redundant; ""colossal"" can be removed.

""The majority of people"" could be clarified as ""Most people.""

""said company"" should be ""the said company.""

""weren’t good enough"" could be rephrased for clarity: ""weren't good enough to make the cut.""

""marvelous"" should be ""marvel at themselves.""

""isn’t"" should be ""is not.""

""best"" should be ""the best.""  yet here you are making claims about a social media post.",3
post18hb,richly branching,1.5358493545348455,highest,Lol. Gotem,4
post18hb,richly branching,1.5358493545348455,highest,"Apostrephsing ""there's"" isn't an inapproriate mistake. 

At no point does run on have to be hypheniated. 

Oh dear god ""90 percent""?  Did you do the same cataloguing within every number beneath 100?

""If (and that's a second class conditional"") is perfectly acceptable.  Are you that concerned about pronouns without antecedents? 

Once again ""all but master"" is acceptable in lieu of ""almost master"".  Tomato-Tuhmauto.  Try again. 

""Trigonometry"" should be capitalized as ""trigonometry.""  So let me understand.  You should capitalize the T in the word you said the T shouldn't be capilalized?  What the hell are you even talking about? 

Colossal is in no way related to the word conundrum.  Colossal addresses the size of the object it is verbing.  Conumdrum is a noun?  What the hell, kid?

  
The majority of people... once again, it's not unacceptable to use this phrase?

""said company"".  LOL This one cracked me up more than the rest.  You talk about redundancy and then you throw in an unnecessary definite article? Bahahahahahaha!!!!

  
""Weren't good enough.""  This is one minute point you could possibly argue amongst the entire bullshit you've spouted. 

  
""marvel at themselves""?  Are you high? 

  
Contractions are fine in this context. 

  
You're correct, I missed the definite article on the last part - something you clearly established you have little understanding of.  I overlooked ""the"" (definite article) final sentence. 

  
Go back to grade school.",4
post18hb,richly branching,1.5358493545348455,highest,"also, the past tense of ""do"" is not ""got""... that dude is an idiot.",4
post18hb,richly branching,1.5358493545348455,highest,"Yes! 👏👏
Well said!
Honestly,  I'm glad people like that aren't allowed on our team. 
The blame game is rough when you can't keep up!",3
post18hb,richly branching,1.5358493545348455,highest,"I know this post is five months old, but it's so hilarious I can't resist responding to it:

* ""The quality of work this company expects weeds out 90% of the unfit applicants,and unfortunately, that includes you."" (Missing space after comma.)
* ""Programming, editing, and coding AI is an extremely important responsibility."" (Incorrect subject-verb agreement. There are multiple subjects here, not one.)
* ""You need to develop an incredibly deep understanding of the English language if you hold any aspirations to engage AI in this particular field."" (Unnecessarily wordy. Consider replacing ""hold any aspirations to"" with ""aspire to"".)
* ""I don’t teach Trigonometry because I’m not qualified, and doing so would be a colossal conundrum, not only toward me, but all I encountered with respect to the field."" (""Trigonometry"" is not a proper noun and should not be capitalized. ""Toward"" should be replaced with ""for""; states of existence cannot take action. If you were acting on yourself, however, this would be a reflexive statement, and the correct subject would be ""myself"" (i.e. not ""me""). The use of ""with respect to"" in the final clause suggests that every aspect of the field of trigonometry would be puzzled by the idea of you teaching, which is probably not your intended meaning.)
* ""I truly wish you best."" (Missing ""the"".)

If this were an example of your best writing, I would assume English is your second language. However, I choose to believe that you were simply writing off the cuff because this is Reddit. Most posters do. Please bear that in mind before you make an absolute ass of yourself.",3
post18hb,richly branching,1.5358493545348455,highest,"**Precious!  Thanks for** (re)**sharing!!   lol**

![gif](giphy|3oKHWzS8LkDUN2T20M|downsized)",4
post18hb,richly branching,1.5358493545348455,highest,"Bruh, this is an internet forum, not a fucking job application",3
post18hb,richly branching,1.5358493545348455,highest,"Yup, take it from the guy who “Fuks” horses. Sorry, someone had to do it. Impossible to hold that one in. 

Now, let me share why I'm writing this comment.

I’ve been freelancing with DataAnnotation for a few weeks, and contrary to what some say on Reddit, my experience has been overwhelmingly positive. After submitting some initial information, I received a Coding Qualification and was brought on board the very next day.

Currently, I'm earning $40/hr just for training various AI models. As my entry was based solely on the coding assessment, all of my projects involve coding, which explains the higher pay compared to what many on Reddit report.

I've encountered no issues with project execution or payment processes. Now, in my third week, administrators frequently contact me with offers for projects that have even better pay rates. It’s an ideal work-from-home opportunity—if you possess the necessary skills, I highly recommend applying.

Admittedly, my English is far from perfect—probably on par with most discussions in this thread. Yet, if you excel in the areas DA values, getting in should be no problem. Remember, maintaining high-quality work is crucial, as that is what DataAnnotation prioritizes.",3
post18hb,richly branching,1.5358493545348455,highest,How's it all going 6 months on?,4
post18hb,richly branching,1.5358493545348455,highest,"“one size fits all,”
I noticed made a mistake on the punctuation there buddy. Maybe you're not as masterful of the English language as you thought but I can see you try.

Truly wish you the best.",3
post18hb,richly branching,1.5358493545348455,highest,[removed],3
post18hb,richly branching,1.5358493545348455,highest,The end had me rolling 😂😂,4
post18hb,richly branching,1.5358493545348455,highest,"Great review from iFukHorses, definitely the username of someone I would trust to be smart.",3
post18hb,richly branching,1.5358493545348455,highest,Whew.,3
post18hb,richly branching,1.5358493545348455,highest,"You have to have a decent grasp of the English language and be able to convey that through writing. Looking at the above comment, it's no wonder you never heard back. 
They responded to my assessment within a few days and I was working on projects by week's end. I get new projects all of the time and have others that are ongoing and continue to provide work. 
Not ""every applicant"" is accepted and they won't have any work that fits in your wheelhouse if you can't intelligently express yourself.  So instead I'm just assuming that they are a scam because they didn't reach out to you, maybe you should recognize that you're not somebody that would fit well with their company.",2
post18hb,richly branching,1.5358493545348455,highest,Of,3
post18hb,richly branching,1.5358493545348455,highest,Talk about grammar mistakes. 😂,3
post18hb,richly branching,1.5358493545348455,highest,"Hilarious. Just another bitter applicant who got turned away for submitting a lazy, crappy, application. Application rejection well-deserved.",2
post18hb,richly branching,1.5358493545348455,highest,"It’s crazy that the people calling the company bullshit are the ones who never got past the initial screening. 

This guy just assumes he’s qualified and possesses the necessary skill set. They’re bullshit for not seeing that I guess.",3
post18hb,richly branching,1.5358493545348455,highest,I love seeing people get rejected that deserve it. Gives me more faith in the service that they don't just let everyone in.,4
post18hb,richly branching,1.5358493545348455,highest,Same. I didn’t even get paid for beginner stuff they offered.,2
post18hb,richly branching,1.5358493545348455,highest,"I think they’ve completely stopped the paid assessment tests as of late November/early December. I was able to make almost $30 on the non-coding assessments in mid November, but my friend who took them a week later only got paid $14 for one assessment. Another friend took the assessments the week after that and wasn’t paid for them at all.",3
post18hb,richly branching,1.5358493545348455,highest,"Right, but they offered to pay. They did not.",4
post18hb,richly branching,1.5358493545348455,highest,">Continue this thread

If your assessment was offered as paid you had to clock how long it took you under the report time section or else you wouldn't have gotten paid. But not all assessments are paid, it has to explicitly say it.",3
post18hb,richly branching,1.5358493545348455,highest,It did but was not paid.,4
post18hb,richly branching,1.5358493545348455,highest,"DAT has been amazing for me. I have unlimited work with ever increasing variety and pay, and they pay out reliably. For what I do, I'm paid very fairly. DAT is the best contract work I've done by far. It's definitely legitimate.",1
post18hb,richly branching,1.5358493545348455,highest,"When you applied, how long did it take you to hear back? I applied a week ago and was just wondering about a time frame.",2
post18hb,richly branching,1.5358493545348455,highest,How's it all going 10 months on?,2
post18hb,richly branching,1.5358493545348455,highest,Perfect. It's still the best side gig out there.,3
post18hb,richly branching,1.5358493545348455,highest,Can I ask what the typical hourly rate is?,2
post18hb,richly branching,1.5358493545348455,highest,"It depends on the project, but I typically have projects available ranging from $20-$27 hourly. People who code make more, but I create prompts and analyze responses mostly.",3
post18hb,richly branching,1.5358493545348455,highest,"Yep, seconding this. I've never had to work at $20 because there are enough tasks available between 24-26 (27 is unusual, that's usually if they're triple-bumping something for serious priority), but that's how I do my budgeting.   


I'm learning python so I can get access to the better paying ones (among lots of other reasons), though.",4
post18hb,richly branching,1.5358493545348455,highest,What language is the coding in? Python?,4
post18hb,richly branching,1.5358493545348455,highest,"...Their support does respond to legitimate emails.

My first email to them was an anxiety induced one, while my 2nd was concerning a legitimate concern. I got multiple replies to the second.

I'm gonna be frank - it sounds like your 'friends', if one of indeed them isn't you, probably were either banned or have their accounts under review.

It's odd to just leave all that money with an employer, unless it was still in the pending period - meaning somehow two people made 6000 in under 7 days on a $20-25 pay rate. Totally doable, but likely illegitimate. I'm all for calling out BS companies, but this post just has...oddities.",1
post18hb,richly branching,1.5358493545348455,highest,"Can you share their support email or where to find it on their site? I had been working on the platform for a few weeks and cashed out a few hundred bucks. I was then hospitalized for a few weeks and when I came back, I couldn't sign in anymore. It kept showing some error about checking logs if you're application owner. I'd like to figure out if I got deactivated or something, and what can I do about the remaining money in there :(",2
post18hb,richly branching,1.5358493545348455,highest,Support@dataannotation.tech,3
post18hb,richly branching,1.5358493545348455,highest,"I've sent them multiple ""legit"" emails. Never had a response yet.",2
post18hb,richly branching,1.5358493545348455,highest,Is there a way to tell if your under review? Or if they just dropped you?,2
post18hb,richly branching,1.5358493545348455,highest,Yes thank you for sharing this.,1
post18hb,richly branching,1.5358493545348455,highest,I've never had an issue. I've been working with them for almost a year now. I never leave that much sitting in my account not withdrawing though.,1
post18hb,richly branching,1.5358493545348455,highest,How do you handle paying taxes on what you earned.. a 1099? Thanks!,2
post18hb,richly branching,1.5358493545348455,highest,Paypal should be sending tax documents soon so I'm waiting on that. They said the end of January. This is found in the tax and statement section of PayPal. If I don't receive one I'll take my yearly statement to a tax advisor. This is my first year doing it so someone else may be able to help further 🙂,3
post18hb,richly branching,1.5358493545348455,highest,Be a man and evade taxes. It’s not the governments to take.,4
post18hb,richly branching,1.5358493545348455,highest,I tried applying there and they basically told me to buzz off.  No feedback at all.  Thanks for the heads up.,1
post18hb,richly branching,1.5358493545348455,highest,"If you had actually applied there then you know they don’t send rejections. They wouldn’t tell you to buzz off. If you don’t pass the assessment, they don’t say a single thing.",2
post18hb,richly branching,1.5358493545348455,highest,"I haven't gotten the assessment yet and I applied like a month ago, what does that mean? I just got a ""thank you for signing up""",3
post18hb,richly branching,1.5358493545348455,highest,"Hopefully it was at least near Thanksgiving day and they were just in the thanks giving spirit...??

  
  
ALRIGHT! THAT'S ENOUGH! As we can all see from the silly crap I'm beginning to post I need to stop wasting time on reddit.",4
post18hb,richly branching,1.5358493545348455,highest,its over,4
post18hb,richly branching,1.5358493545348455,highest,This company has an astonishing number of shills. I wonder what *those* positions pay…,1
post18hb,richly branching,1.5358493545348455,highest,"Someone has to rebut the astonishing number of people so lacking in self awareness they think the reason they didn’t get work is that the company is a scam, not that they’re less intelligent or skilled than they think they are. 

P.S. The pay is the satisfaction gained from shitting on said people, the value of which is tough to quantify.",2
post18hb,richly branching,1.5358493545348455,highest,"Bruh look at my posts going back 10 years, my karma, my many posts about my dog lol. 

I work in the DA platform and have had no issues whatsoever. It sucks that not everyone gets approved, it sucks that people get removed for not following rules, but that's just how it works in this big mean world.",2
post18hb,richly branching,1.5358493545348455,highest,"I'm afraid I have to agree with OP.

And before you tell me that I'm ""not as good as I think I am"", consider this:  I took their coding assessment in January 2024, and was onboarded \*the very next day\*.

I worked for them for 6 weeks. The work was interesting, and the pay was decent.

I tried to log in to my account this evening, and instead was greeted with the following message:

""After careful review of your recent work submissions and account activity, your account has been permanently suspended due to violations of our Code of Conduct and Terms of Use.""

That's it. No explanation of what I did ""wrong"". (And no, I have no earthly idea what that might be.) No chance to listen to their concerns, or to tell my side of the story.

Oh, and they kept the money I earned for the past two weeks (\~$3,000). Since I can't log in, I have no way of transferring the money to my PayPal account.",1
post18hb,richly branching,1.5358493545348455,highest,How’s everyone’s accounts now?  I wow. I’ve been working for them 8 months and was getting high praise in inbox directly about how well I did and that I was getting invited to 2 more projects. Tat was a day or two after all tasks and 1k of not yet withdrawlable money all disappeared. Can still see inbox and can still log in.  Ever got a message about why or anything.  This was a month ago so Jan26th.,2
post18hb,richly branching,1.5358493545348455,highest,"I've figured out what happened.

Various violations of Data Annotation's Code of Conduct and Terms of Use can get you de-platformed. Still, the suddenness with which it happened to me suggested that they thought I'd committed the ultimate sin: Using an AI tool such as ChatGPT to craft my chatbot training prompts.

Based on this hunch, I reconstructed from memory one of the chatbot prompts I had submitted the day before my account was suspended:

""Please code a Python function that takes as input an array \`a\` containing integers. ⁤⁤There are exactly two possibilities concerning \`a\`: Either all the integers in it are even except for a single odd one, or all the integers in it are odd except for a single even one. ⁤⁤In either case, the function should identify this ""outlier"" integer and return it. ⁤⁤The array \`a\` is guaranteed to contain at least three integers. ⁤ ⁤Also, write a \`main()\` function that tests the above function. ⁤⁤Make sure that your \`main()\` function writes its output to the console, and that it does not prompt the user for input. ⁤⁤Please run your \`main()\` function, and show me the output.""

I then copied and pasted the above prompt into GPTZero, and hit ""Scan"". The result? GPTZero is 97% confident that the prompt was generated by AI.

WRONG. I wrote every word of it myself.",2
post18hb,richly branching,1.5358493545348455,highest,"Based on the above investigation, I wish to revise my opinion of Data Annotation.

They are \*not\* a scam. However, the tools they use for AI detection are in their infancy and generate many false positives.

Therefore, unless you are willing to run all your work products through GPTZero before submitting them, and are willing to dumb them down until you sound like a human, you should expect to be de-platformed at any time, without notice.",3
post18hb,richly branching,1.5358493545348455,highest,Thanks for sharing this update. Did they ever pay you what was owed?,4
post18hb,richly branching,1.5358493545348455,highest,"I think something like that happened to me today. I emailed them, so hopefully, they will get back to me. I had some awesome projects on my account, and then BAM, this message with \~$2000 pending I can't pull out. I could see a simple coding problem being flagged. Is that what they told you? Did you manage to get your money from them? I'd hate to go the legal route for $2000.",4
post18hb,richly branching,1.5358493545348455,highest,"That would explain why my acct projects disappeared and 1k. Except I didn’t write python, I wasn’t on any code projects. Never used AI, not even when I did an eval for annotating images and they allowed use of one type of AI. 
A bot taking my job looks a way I didn’t imagine if this is the case.",3
post18hb,richly branching,1.5358493545348455,highest,[removed],1
post18hb,richly branching,1.5358493545348455,highest,lol you think you are supposed to get paid for the hiring process?,2
post18hb,richly branching,1.5358493545348455,highest,">I'm not the person you were asking, but I'm guessing it's because this company used to pay for the hiring process, yes.  Someone upthread said they stopped doing that in November/December, but perhaps this person applied prior to that.",3
post18hb,richly branching,1.5358493545348455,highest,btw they pay 80 but only if u pass,4
post18hb,richly branching,1.5358493545348455,highest,[removed],3
post18hb,richly branching,1.5358493545348455,highest,What company ever pays for the hiring process? Ive yet to come across one,4
post18hb,richly branching,1.5358493545348455,highest,Same. Glad I didn’t waste more time.,2
post18hb,richly branching,1.5358493545348455,highest,"I've already made 500$ this week. After the assessment,  they give you small tasks to gauge your compatibility with their tasks.


Definitely hasn't been a waste of time for me!",2
post18hb,richly branching,1.5358493545348455,highest,"Nice. Wait till you get more settled, you'll have tasks available with higher pay. Pat yourself on the shoulder, you qualified for a hard platform to gain entry in. Consider yourself part of an elite group.",3
post18hb,richly branching,1.5358493545348455,highest,"It's not a scam, they're just very selective in who they allow train AI from home. Qualifications aren't for getting paid, they're for passing. Unfortunately, most people don't. You have to be advanced in reviewing AI.",2
post18hb,richly branching,1.5358493545348455,highest,well red flags everywhere for me... followed the [indeed.com](http://indeed.com) link for the developer work and it gave me a English literature assessment... elite community who can't even provide a smooth portal to provide the correct assessment? Elite far from it...,3
post18hb,richly branching,1.5358493545348455,highest,"I mean, to be fair, their terms and agreements say that you can be kicked from the platform at any time, for a variety of reasons, without being paid out. DA is constantly reviewing their contractors and if they find any evidence of misuse, misrepresentation of work, or severe quality issues, you’re kicked. Every discussion board and subreddit for DA is FULL of stories of people being “removed for no reason” when they’re usually misreporting time, using other AI to answer prompts or just straight up not following instructions on projects. 

With all this in mind, it’s absolutely mind blowing that someone would leave $6k in their account knowing they could wake up and it could be gone. Maybe they didn’t read the work agreement, but that’s not DA’s fault and probably points in the direction of why they were booted from the platform.",1
post18hb,richly branching,1.5358493545348455,highest,[deleted],2
post18hb,richly branching,1.5358493545348455,highest,"lmao call it what you want, i’m making a shit ton of extra money working for them.",3
post18hb,richly branching,1.5358493545348455,highest,Me too! People can think it's a scam all they want! I'm glad that I passed all of the assessments and I'm being offered work daily! It's nice to see so much positive feedback from other who work on the platform too!¡,4
post18hb,richly branching,1.5358493545348455,highest,"Been working for them since Oct. Have never let more than $500 sit in my account waiting to be transferred. No issue getting projects, no issue getting paid.",1
post18hb,richly branching,1.5358493545348455,highest,"I've been working with them since October and have never had any issues withdrawing money. I've also never let it accumulate as high as $6k, so I don't know if that has anything to do with it.",1
post18hb,richly branching,1.5358493545348455,highest,I've used it for months. I withdraw every few days. No issues.,1
post18hb,richly branching,1.5358493545348455,highest,"I’m a little late to the party here, but this post showed up in my feed and I figured I’d share my personal experience: I passed the initial assessment, completed two “paid” qualifications, and now the full amount for the coding test says “transferable.” I’ve tried to transfer it multiple times, but just get taken to the home screen, where the results of the two tests are apparently being reviewed. I’m not sure how this makes sense when one of the tests is showing up as “transferable.”

Like many others have stated, my skills supposedly align with exactly what they’re looking for. I teach high school language arts and have been doing this type of work on Mturk since 2018 with a 99.9% approval rating. I realize that none of this guarantees me a thing, but I’ll get to that in a second. I’m guessing that they liked my writing skills initially, because I got the qualification tests only hours after taking the initial assessment. They also see that I have the proper coding skills since I got the full bonus, yet there’s no work available for me and I haven’t officially been paid.

Again, I realize that writing skills and similar experience with micro tasking isn’t a guarantee, but I applied after a few of my colleagues and none of us have heard back. As of right now, about a dozen of us have applied and it’s the same story: no one has heard back. We all have different skill sets and backgrounds, yet not a single one of us made the cut? Maybe they don’t need people from our area? Maybe they don’t need any more teachers? I truly don’t know and I’m just speculating here.

They could be backlogged, they could be having technical difficulties, or they could be a total scam. There are a lot of people on this platform who talk them up, but there seems to be an equal number of people who say they’re a scam company.

As others have mentioned, I’ve been getting a lot of spam emails since I signed up and one of my coworkers has been getting spam texts. He was actually able to “withdraw” the payment for his qualifications, but it isn’t in his PayPal account yet, so I have no idea if he’ll ever be paid.

Something that he and I both find peculiar—the reviews for this company on social media are either glowing or they say it’s a complete scam. There is no middle ground and this strikes me as a bit odd because there is always some sort of complaint about Mturk and Prolific on their subs; not a bunch of people talking them up or calling them scam companies. Mturk has gone downhill lately, but this wasn’t the case when they were offering a ton of work either.

This all makes it truly difficult to determine what exactly is going on with this company. I saw a comment on this thread saying that the only people getting paid are the ones who promote this company all over social media. Maybe they’re on to something!",1
post18hb,richly branching,1.5358493545348455,highest,Like 80% of these responses actually sound like promo,2
post18hb,richly branching,1.5358493545348455,highest,"I know! If you do a quick search, some are even identical. Something is so off here. I’ve been doing Mturk for 6 years and even the people who have glowing things to say about it, don’t sound this disingenuous. Same with Prolific, which I also like—but it’s by no means perfect. 

That being said, I’ve seen Mturk requesters ask where they can post work because Amazon Payments is apparently giving them a lot of issues, and this site is apparently not an option for them. Very odd.",3
post18hb,richly branching,1.5358493545348455,highest,"They definitely aren't a scam company, but they have two main issues:

1. Not clearly communicating to people whether they passed or failed the initial qualifications
2. Kicking people off the platform without warning (even people who have worked for them for a long time)

As for why the reviews are either terrible or glowing, it's probably because it's pretty much a dream job until they suddenly cut ties with you. Well, that and the fact that people give it 1-star reviews just because they didn't pass the strict qualification test (which is kinda silly).

I feel bad for OP's friends, but there's also no reason to worry about losing *that* much money seeing as how your earnings become transferrable after 7 days. Also, from what I've heard, even if you do get fired, the earnings are still transferrable unless you did something dishonest (e.g. used ChatGPT to do some of the work for you).

Regarding you and your colleagues not getting in, my guess would be they don't currently need any more people. The platform can definitely just be full sometimes - there was a gap of a year between me signing up and getting invited to do the qualification test lol",4
post18hb,richly branching,1.5358493545348455,highest,"I just got “no tasks available at this time” today and $1000 was banked. Doesn’t appear to be a lock out so maybe a review. I follow all the rules, have one account only and had 2 pages of projects from $22.50-$30.00 as of this morning. Been doing this a year at high quality. Slack is now deactivated as well.",1
post18hb,richly branching,1.5358493545348455,highest,Me too... What is going on... I have been with them for nearly a year and suddenly nothing after having 15-20 different jobs available on Saturday...,2
post18hb,richly branching,1.5358493545348455,highest,"Anyone have update? 
For me, been with them 8 months+. Still no tasks showing (was 2 pages) and my 1k earned is not showing in interface but can still log in and see inbox etc. it’s been a month. Never had a reason or communication anything. In fact the day or two after this happened I got praise from project manager in my inbox and that I was invited to two more projects.",3
post18hb,richly branching,1.5358493545348455,highest,"Me too, I had 15 active projects, Saturday morning, and I got on a few hours later and there was nothing. I've sent 5 emails to support with no response.",2
post18hb,richly branching,1.5358493545348455,highest,See my comment above I just updated. It’s been a month and nothings changed. Can still log in and see inbox. No response from support. Was getting praise from project managers even day after all my tasks disappeared. Someone’s dropping the ball.,3
post18hb,richly branching,1.5358493545348455,highest,"Any update? This just happened to me too… no projects, but I can still log in",4
post18hb,richly branching,1.5358493545348455,highest,"I'm tired. I really am. I have a medical assistant certification and haven't been able to work in a clinical setting for two years now because I was diagnosed with Vestibular Migraines - basically a debilitating disability that causes vertigo at any point in time regardless of how long I've had vestibular therapy. I'm bound to working from home, but go figure - finding a remote position that's in the medical field is extremely rough when my hearing is being affected by this disability and all the positions require a heavy amount of inbound/outbound phone calls.

I don't have the mental capacity to do anything else after work (daily tasks) and my two teens are in middle/high school so they already have things on their plate so contributing with chores is just that - contribution. So I don't have everything done by the end of the day until the weekend. By that time, trash and dishes are piled up and so is the cat litter and it's so overwhelming that I don't have time to use the weekend to recharge from social exhaustion.

I thought that Data Annotation would be the *perfect out* from all of this until I can go through all of my Coursera classes for anything to get out of the medical field but holy heckin heck why is my luck so dull.",1
post18hb,richly branching,1.5358493545348455,highest,"I was a Physical Therapist for 10yrs, I developed vestibular migraines after getting bells palsy years ago from a flu shot. Eventually,  it progressed to such an extent I had to leave practice last yr. I have a doctorates, and I cannot get ANY WFH non clinical work despite doing over 1000applications, promoting myself, getting certifications, another degree, ect. Eventually I filled disability and have been awaiting a decision. I went from making $120k to $12/hr bagging groceries 4 hrs a day for 2-3 days a week bc thats all I can handle. I also thought data annotation would be great, but like many other wfh careers, its heavily gate kept it seems, once you get past all the scam companies that is.  Best of luck to you, hate to see other medical professionals suffer with this illness",2
post18hb,richly branching,1.5358493545348455,highest,I'm crying just knowing someone else is going through/had gone through the same experience. Just the fact that I'm not alone. 🫂 I'm sorry this is happening.,3
post18hb,richly branching,1.5358493545348455,highest,"I'm disabled and work with them (epileptic and chronic migraines as well as AuDHD which -- same boat as you, I've been working in customer success and have a ton of experience, but my weakness is phone calls, every time, both from the interruption and the audial processing issues, and it's HARD to find places that don't want you getting inbound calls) -- and my experience has been great, tbh. 

I would honestly list as many skills as you can think of on your application and then make sure to go back to your profile and put them in. Put in that medical experience, put in that you know anatomy, medical terminology, bio, took a class in some random thing, have x hobby, do xyz thing for fun. Have first hand experience with migraine treatment, can use ""vestibular"" in a sentence. They look at that for pairing people with projects and they <i>always</i> need people with experiences that mean they won't freeze and skip a fact-check if it's about a specialized field.   


I work about 4-5 hours a day for them 4-5 days a week right now, and at $24-25/hr a task that's making a HUGE difference. If I upped it I could make even more, but it gives me the option to not work on a bad day, or take courses so I can do something else.",2
post18hb,richly branching,1.5358493545348455,highest,Great information coming from the inside. Much appreciated my friend. It's worth checking out! Aloha.,3
post18hb,richly branching,1.5358493545348455,highest,"Absolutely load up the profile on all of one's skills! I put in crochet, baking, guitar, piano, singing, painting... you just don't know what might connect.",3
post18hb,richly branching,1.5358493545348455,highest,I used to recruit for an online high school.  You may want to conisider getting your teaching certification with the medical professional CTE add-on.  These teachers are in high demand and pay better than any other form of certification.  Schools like Stride (K12.com) and Connections are WFH.,3
post18hb,richly branching,1.5358493545348455,highest,[deleted],2
post18hb,richly branching,1.5358493545348455,highest,"Are there certain skill sets they look for when you input information on your profile? I wanna see if there's a trick to being ""selected"" or something because some people are getting in and some aren't",3
post18hb,richly branching,1.5358493545348455,highest,[deleted],4
post18hb,richly branching,1.5358493545348455,highest,"Hey there! Anything you toss into your profile goes into the records and gets a thumbs-up or down from their AI wizardry. Now, you might think learning guitar or acing a cooking course isn't exactly data annotation material, but hold up—according to their AI, you're probably a hidden treasure trove. Don't assume that everyone has your skills, they just do not! Even those skills you think are just normal might be pure gold to the tech behind the scenes.

So, let me spill the beans about me—I've been coding since dinosaurs roamed the internet, from Fortran and Basic to Python and React. Tech skills? Check. But guess what? I'm also a science fiction writer. Yep, I sprinkle that creative magic on my writing. Now, I don't brag about it, its just a hobby, hell I released my last novel, that took me a year to write, under the Apache 2 GPL License. But not everyone can whip up a creative storm. It took me a decade to work out other people weren't 'just pretending' not to be able to write creatively, they really can't, who knew? Well seemingly everyone but me knew. 

So, don't hold back on your skills! Let the AI in on the whole deal, from your everyday talents to the unique stuff.",2
post18hb,richly branching,1.5358493545348455,highest,"Keeping over $6k in the hands of an employer only makes sense if you're saving up to leave a bad situation, or don't have a way to move the money out yet. I'm not going to sit here and tear down these two people, when the real issue is why the lockout, and what options do the workers have for justice. Some countries protect their citizens more than others ofc. Also, if the $6k was built up quicky and somehow looks like AI or something that games the system, that might be a plausible explanation. Not enough background info to make any assumptions or accusations",1
post18hb,richly branching,1.5358493545348455,highest,I didnt even get paid for the trial work given. They took forever to even accept my application and then never approved it.,1
post18hb,richly branching,1.5358493545348455,highest,"I've only had good experiences with them, honestly -- I do it as a part-time job right now and make more than I would get in unemployment -- but $6000 on their accounts that they'd never paid out? 1) Why? and 2) If the reason they hadn't cashed out was that they made that much in under the time necessary to pay out, something tells me they were overreporting in a BIG WAY.   


And if they didn't have a viable Paypal account, were simultasking, or some other violation, well...that sucks for them, but they broke the rules.",1
post18hb,richly branching,1.5358493545348455,highest,Yeah happened to me over the summer. Was working full time basically and making good money. During one of my tasks all of a sudden it refreshed and nothing was available anymore. No more jobs and no reason given as to why. Super scummy,1
post18hb,richly branching,1.5358493545348455,highest,I’ve had no problems with them. Sorry that happened to your friends though.,1
post18hb,richly branching,1.5358493545348455,highest,I just came across them tonight and was considering but I still have no idea because everything I read is mixed reviews.,1
post18hb,richly branching,1.5358493545348455,highest,I can't even get a response or on board with them. Says waiting to review results for a month now.,1
post18hb,richly branching,1.5358493545348455,highest,"I doubt this is true. Firstly, we can cash out every 3 days. NO one would allow so much pay to accumulate. The fact you said TWO people makes me certain it is not true, in the least.",1
post18hb,richly branching,1.5358493545348455,highest,"The same happened to me. They didn't pay me for 60 hours of work. I will be reporting my experience to the Texas Attorney General's Consumer Protection Division and the Texas Workforce Commission. I have proof of work, as I verified codes in my IDE.",1
post18hb,richly branching,1.5358493545348455,highest,"I know if 2 people that were scammed for no reason and locked out of their account. Jeremey, pay the people for work completed at a minimum. Do the right thing Jeremy!",1
post18hb,richly branching,1.5358493545348455,highest,"DA suspended my account today, not sure why. I keep getting tasks to do, what means I did a good job. And then I suddenly got a message. Last week I worked many hours and made 400$. yesterday 300$ and all was ok. Today they had to pay $400. and they suspended me. I worked also on the weekends sometimes they send notifications to make a task a priority. I got that message after 2 weeks of working there.",2
post18hb,richly branching,1.5358493545348455,highest,"Disagree. My experience with them has been excellent.

System is much more reliable than others like Outlier.",1
post18hb,richly branching,1.5358493545348455,highest,"Here's the deal homie, I've been working for them part time for months.  They've always been perfectly good to me.  I've recommended them to friends and family.  It looks to me like your ""good friends"" are scumbags who did something stupid and got caught.  Don't be a  jackass.  I can't take this post seriously, ykno?  Do you sleep okay at night? being this kind of person?",1
post18hb,richly branching,1.5358493545348455,highest,I've been working for them for about a month and haven't had any issues.,1
post18hb,richly branching,1.5358493545348455,highest,Uh what? I've made over 10k with this company I've been paid every dime. Confused by this post.,1
post18hb,richly branching,1.5358493545348455,highest,"Same. I'm just at the $9,200 mark. Been on since November. OP is probably one of those people who buys other people's DA accounts online and tries to use them because they live out of the countries that DA allows and they're butt hurt because they got found out. 

I just had someone like that message me the other day and offer me 150 for my account as well as a percentage of any of the funds that they earn.",2
post18hb,richly branching,1.5358493545348455,highest,"I was approved. I never went for it. I run my own empire, but it was interesting anyway.",1
post18hb,richly branching,1.5358493545348455,highest,[removed],2
post18hb,richly branching,1.5358493545348455,highest,"I’m actually getting emails all the time to continue my starter assignment. I will likely do it this weekend. I know by now, that this is fairly profitable and I didn’t know what I was talking about.",3
post18hb,richly branching,1.5358493545348455,highest,[removed],4
post18hb,richly branching,1.5358493545348455,highest,"I've done tons of work for Data Annotation and every time they've paid me in full for my work. I have made thousands working for them, periodically withdrawing my earnings to PayPal and then transferring them to my checking account.

I suspect that your friends were 'locked out' for violating the code of conduct, but even then I can't imagine they were refused earned pay. It sounds like bitter grapes and I don't believe it.

I couldn't speak more highly of this company.",1
post18hb,richly branching,1.5358493545348455,highest,"I felt exactly as you do, until it happened to me.",2
post18hb,richly branching,1.5358493545348455,highest,I think you need to learn a bit more about whom you call friends.,1
post18hb,richly branching,1.5358493545348455,highest,"Hi all, I'm currently living in Greece. Does any of you know if I can work from Athens for Data Annotation. Also, how do you handle your earning taxes?

Thanks",1
post18hb,richly branching,1.5358493545348455,highest,I have many skills and have not recieved any jobs from them?,1
post18hb,richly branching,1.5358493545348455,highest,"I've been working for them with no issues. Maybe if they left $6000 just sitting there, the admin thought the account was abandoned.  So far so good on my end. I love working for them, and love the people you can Converse with in the chat for support.
Is it just me or is it kind of strange that someone would post this kind of ""review"" for ""a friend""?",1
post18hb,richly branching,1.5358493545348455,highest,I was hired through Upwork as Data Annotator but suddenly my contract ended .What could be the reason for it ?,1
post18hb,richly branching,1.5358493545348455,highest,The fact that I made a comment here a few weeks ago about how crappy data annotation is as a company and how they're just data farming their job applicants amongst other issues with the company and to this day I still get replies from paid trolls just is another indicator that this company is trash,1
post18hb,richly branching,1.5358493545348455,highest,I am so tired of seeing this shitty company’s ads on Reddit with their fat unshaven nerdy “employees”,1
post18hb,richly branching,1.5358493545348455,highest,They are known scum bag scammers i will be reporting them to the authorities,1
post18hb,richly branching,1.5358493545348455,highest,Northeast water ft Payne Alabama,1
post18hb,richly branching,1.5358493545348455,highest,"I just joined and updated my profile to the best that I could, sparing no details and selecting more than 15 ""pre-suggested"" drop-down skills on the profile page.

When I update and return to Data Annotation and look for work projects, nothing comes up.it just thanks me for the assessment I had just taken and reverts me back to my profile page. Is this normal? Must I wait several hours or days for projects to be proposed when I click on ""Work on Projects"" from the menu button?",1
post18hb,richly branching,1.5358493545348455,highest,I second this!!! Very awful!,1
post18hb,richly branching,1.5358493545348455,highest,Omg,1
post18hb,richly branching,1.5358493545348455,highest,"I know I'm late to this but

I took the assessment, which was all coding, and I'm confident I did great on it. Then I was able to use the support form to resolve an issue with setting my email address to get paid. I even got a reply email from support about this.

Then all of a sudden my account was what can only be described as shadowbanned. It stopped saying ""thanks for taking the assessment, we'll email you"" and just said ""sorry, no work"". No email was ever sent to me, and the support button on their own site stopped working for my account. Like they literally disabled the support form without any notice or apparent reason.

I reached out for help with a post on their subreddit, but it was swiftly deleted by the moderators despite violating none of the rules. My friend (who does get work through them) says he thinks the mods are employees. So they do have the time to shut down criticism but not enough time to respond to any questions whatsoever.

Of course, they have enough of a labor pool where they don't have to care whatsoever about alienating or hurting anyone and can afford to be as arbitrary as they want in banning people and then never communicating. But of course we should be used to this model by now I guess",1
post18hb,richly branching,1.5358493545348455,highest,"This is so true- I worked there for about a year before I got “banned” for absolutely no given reason, no warning, right after a “promotion” of sorts, told I was doing a great job, AND when they “banned” me, they did not send me my $1256 paycheck that I had in my account to cover expenses for my child u til I could find more work. 

This company is HORRIBLE! And to those currently working there and loving life- just be careful because it WILL happen to you too, it’s just a matter of when. I was loving life too at one point, read a bunch of negative reviews like this one I’m writing and didn’t believe it until it happened to me… everyone has said the exact same thing too. Fired for no reason, tons of money in their accounts that they never saw again. No one will even contact us back about the money in our accounts that they illegally decided to keep for themselves. 

I wrote in to my local job board last week and I know they will get my money back. Just BEWARE this company!! I don’t want to see this happen to anyone else. Or at least enjoy it while you can- and don’t quit your day job. It’s great for side money, but it’s unreliable and only a matter of time before you’re robbed. 

Sorry for the long answer! I’m still super salty over it :( 

*** side note to those wondering why we accumulated so much without cashing out***: you get paid every 3 days for pay per task tasks. Not hourly tasks. Those pay once a week. So if you work 3 hours on a Monday, you can cash out for JUST those 3 hours the following Monday, but then have to wait a full 3 extra days before you can cash out anything again. So THATS why we accumulated so much and lost it all. Because it’s complicated. Some people have bills and need both Monday, Tuesday, and Wednesday’s wages to pay those bills. So we’d have to wait until Wednesday to cash it out or risk cashing out SOME money and having to wait 3-4 days to get the rest. Only cash out once every 3 days and get paid for hourly tasks you did 7 days ago. If you worked there, you’d understand. It’s not as simple as it may seem.",1
post18hb,richly branching,1.5358493545348455,highest,Any luck getting your funds?,2
post18hb,richly branching,1.5358493545348455,highest,"I heard so much about this floating online, the horror story.

People actually victim blame you. If they do the job, they still need to get paid, sure, if they do something to break the company's regulation, they can be removed from the job but the money they've earned should go to them still.",1
post18hb,richly branching,1.5358493545348455,highest,"This website is full on a SCAM.  They say your core assessment is grammar and editing, but it is really coding.  They use fake WFH moms on Tik Tok to pretend it’s just reading and editing.   This site is fully a scam to get personal data.",1
post18hb,richly branching,1.5358493545348455,highest,"I got randomly suspended today with almost $1100 in my account, some approved (was supposed to cash out today) and some not. What is everyone’s experience with getting paid out or not getting paid out after suspension? I am seeing mixed reviews. Some say they were still able to cash out for work they did and some say they never heard back. I’m extremely stressed over this.",1
post18hb,richly branching,1.5358493545348455,highest,Same exact thing happened to me two days ago,2
post18hb,richly branching,1.5358493545348455,highest,"So DA suspended my account today, not sure why. I keep getting tasks to do, what means I did a good job. And then I suddenly got a message. Last week I worked many hours and made 400$. yesterday 300$ and all was ok. Today they had to pay $400. and they suspended me. I worked also on the weekends sometimes they send notifications to make a task a priority. I got that message after 2 weeks of working there.",3
post18hb,richly branching,1.5358493545348455,highest,Did you manage to get your funds out now that it's been about a month? Any word?,2
post18hb,richly branching,1.5358493545348455,highest,"Hey everyone, I came across this topic as I am considering DA as an employer. It does seem odd that one would save up 6k without cashing out. It’s good to hear others have had positive experiences with them. Can anyone comment on what it’s like to work for them? Also how difficult is it to switch roles if hired? I am learning to code and looking for a work from home job with flexible hours. Any personal experiences or advice would be greatly appreciated. Thanks!",1
post18hb,richly branching,1.5358493545348455,highest,"My account on data annotation seems to have been terminated without explanation. I have been doing coding projects for Data Annotation for about a month and a half and now. I thouhgt my work was good but I guess not. My ""Work on Projects"" tab is blank including the time logging so its pretty clear that my account has been closed without notice. 

I am wondering if anyone knows whether or not I will still be paid? I have $900+ that I am supposed to be able to cash first thing tomorrow.  Thats how much is sitting in my ""Withdrawable amount"" tab from my last cash-out about 2.5 days ago. And then another $1900 that will cash out in the next 7 or so days..

Has anyone ever been able to cash out any of their remaining balance after having their account closed?",1
post18hb,richly branching,1.5358493545348455,highest,"DA suspended my account today, not sure why. I keep getting tasks to do, what means I did a good job. And then I suddenly got a message. Last week I worked many hours and made 400$. yesterday 300$ and all was ok. Today they had to pay $400. and they suspended me. I worked also on the weekends sometimes they send notifications to make a task a priority. I got that message after 2 weeks of working there. I was also doing coding projects. They owe me about 2K",2
post18hb,richly branching,1.5358493545348455,highest,"I don't think you need to worry about the money owed to you. I have been able to cash out the money owed to me over the last week without issue, every 3 days, so I would assume you can too. I am almost down to $0. 

based on what you're telling me, I am starting to wonder if there is a limit on how many hours you can put in, in a given week. This was the first time I ever went over $2K in one week (just slightly), and I was terminated the following week. And it sounds like you accumulated the same amount over the same time period with a similar outcome. Other than that, I can't think of how else I violated the code of conduct. I never mis-reported hours. and I feel confident in the quality of my work.  
  
Only side note- I did submit 2-3 tasks after they were expired over the course of the last 3 weeks. I was super apprehensive about doing that, but I had put a lot of effort into them each time this happened and just wasn't watching the clock close enough, so I submitted it anyway assuming if it was that much of a problem they just wouldn't pay me for it. I am curious now if that was a contributing factor. Seems like they could just send a warning to be like ""stop doing that"" rather than terminate someone, but this is the only reason I can think of besides putting in too many hours.",3
post18hb,richly branching,1.5358493545348455,highest,"I have no access to my account. I emailed them. They suspended my account. I do not know the reason, because I had so many tasks to do, they liked my job. and my prices increased from 20$ to 26$ for not coding, I could sometimes get more than $40/ hour for coding. I was very hard working.",4
post18hb,richly branching,1.5358493545348455,highest,Any luck?,2
post18hb,richly branching,1.5358493545348455,highest,"I never found out what I did to lose my account. I heard you lose it for code of conduct violations, but they allowed me to cash out close to $3K in outstanding payments, despite permanently removing me from all projects, so it can't have been too egregious. Just glad I didn't lose out on all the hours worked.",3
post18hb,richly branching,1.5358493545348455,highest,"Did you cash out as normal or did it just happen automatically after a certain time? I can’t even log in, but tomorrow night would be when I could have withdrawn again and my 7day pending a would be done by next Wednesday.",4
post18hb,richly branching,1.5358493545348455,highest,How will you receive the money as a foreigner?  I just received a job opportunity via sponsor and on my LinkedIn.. it is something related to AI...Im keen to know..What do they really expect us to do?,1
post18hb,richly branching,1.5358493545348455,highest,Today they suspended my account also. And I worked there for only about two weeks. But I read your comment and withdraw money every 3 days.,1
post18hb,richly branching,1.5358493545348455,highest,"Hmm, I have had no trouble for 5 months now, but today I can't even get to the login screen.  I wonder if this is just a problem with the website or something more sinister.",2
post18hb,richly branching,1.5358493545348455,highest,"I do not know as I just started. But based on the comments, I think if number of earned $ might also effect. When I started my third week, they had to pay me over $2K (I earned over $2k for that week), Monday was find and on Tuesday they had to pay $400 and they suspended me. Someone experienced similar, all was fine for several months earning less than 2K and time reached $2K for 1 week, and beginning the next week all projects disappeared.",3
post18hb,richly branching,1.5358493545348455,highest,Probably they want the work done but don’t want people who succeed so they don’t have to pay out a lot of money,4
post18hb,richly branching,1.5358493545348455,highest,"everything back to normal.  I have more projects available than I can handle, so I can generally pick those I want but I only do 3-5 hours a week.  Always been paid immediately.  I wait until there is $100 owed to me.  Who knows how long this will last.",3
post18hb,richly branching,1.5358493545348455,highest,"Possibly related, but I've been trying to log in today. I have completed the Starter Assessment but not the qualification test yet. When I try to log in I get this message, ""We're sorry, but something went wrong. If you are the application owner check the logs for more information.""

Does this happen often?",1
post18hb,richly branching,1.5358493545348455,highest,does it cost money to join dataannotation.tech,1
post18hb,richly branching,1.5358493545348455,highest,Yeah its a scam.,1
post18hb,richly branching,1.5358493545348455,highest,"I have cash almost 5k for 2 months of work and I don't even push myself to do as many hours as I could. They pay you regularly and always on time ( 7 days after you finish an assigment). So far is good. I guess if your friends were not following the rules. To have 6000 without cashing sounds like they worked many porjects simultanly. (If you work in 2 projects that pay 40 dollars each, then they could easily make 6000 in a week before they were able to collect).",1
post18hb,richly branching,1.5358493545348455,highest,"Same here, I worked meticulously for a week. I have submitted 600$ worth of work. Just found out I been locked out. I have not violated any Terms Of Service to my knowledge. Very disappointed right now and frustrated.",1
post18hb,richly branching,1.5358493545348455,highest,I have been working for them for almost a year and never had a problem until today. I have $1500 outstanding because it takes 7 days for time submitted to be approved. Hoping to hear back via email but this post is concerning for sure. I always follow the code of conduct. Looking forward to finding out what happened.,1
post18hb,richly branching,1.5358493545348455,highest,Idk where my username came from lol. I connected through Google account.,2
post18hb,richly branching,1.5358493545348455,highest,"I have never had any issue.  The only reason they would lock out an account would be that they detected that the worker wasn't actually contributing (i.e they were doing 20 minutes of work and reporting 2 hours).  People think it's easy to take advantage of DAT but they do verify and validate.  They may have also been doing lousy work.  I've waited for big cashouts before (I like to withdraw 2500+) but $6000 is a bit much to wait on, unless they were holding out on cashout for unemployment purposes.",1
post18hb,richly branching,1.5358493545348455,highest,"Coucou, j'ai une petite question? Ta eu du travail récemment avec eux? Moi j'ai pas de problème avec le retrait d'argent mais j'ai plus de tâches depuis la semaine dernière (le 17 pour être exact). Quand je me connecte j'ai un messages qui dit ""nous avons pas de projet pour vous pour le moment"". Bref je stress.",2
post18hb,richly branching,1.5358493545348455,highest,"This story is sus. I agree with the comments that there's more to the story. I've racked up 6k with zero issues, get paid every 3 days, and never let more than 7 days pass without withdrawing pay. (Not a shill for DA, just a regular freelancer)",1
post18hb,richly branching,1.5358493545348455,highest,I think all of you are pretending to make $2K a month because your job is to get people to apply. I can easily live on 1200 a month. If it were possible to make $2K a month working for them they'd have to turn down 99% of their applicants.,1
post18hb,richly branching,1.5358493545348455,highest,"When I try to sign up, it says ""email already taken""; when I try to do a password reset using the same email, it says ""email not found"". The email address is mine, and it is an active email account that I login to basically every day.

Anyone else have a similar experience?",1
post18hb,richly branching,1.5358493545348455,highest,How do you emailt this company ?,1
post18hb,richly branching,1.5358493545348455,highest,"You might want to read this 1st... I am just getting out of a DATA ANN0TATION TECH job trying to get the money they took from me back. How it works is you put $100 (bitcoin) into what they call a work account. This money is used to optimize products for merchants. The work is fast and simple. You make $50 in pay and $40 in commission a day, then you get that and your $100 backs at the end of the day. Here is the clincher. They have a ""Random"" bonus system. If during your 3 sets of tasks (40 each) you hit a bundle you get 15-100% the regular commission. I got one during training. I think I got an extra $8. 1st day went smooth but the person training confused me more than helped me. 2nd day I hit another bundle. Here's the part they don't for-warn you about. Now to continue to finish and get my money I have to put another $85 in. So, I cough it up, Finish the set, I get my $90  plus $185 deposit and my extra $60. WhooHoo, right? Wrong! This is the scam. Next day, same thing except this time they need $475 more. Now, I'm on disability so I don't have that kind of money at my disposal. So I I use my rent. Start back up again BOOM! Another bundle, 1st task. What are the Random chances of that, right? Guess what? This time I have to pay $1471 to continue so I can get my new found fortune (haha) and my $575 back. I don't have that much money. No one mentioned this in ""training"" So, I'm out, I cant finish, I don't get paid my $90, my $575, MY money i put in or my 2 (15-100%) bonuses. I get nothing and now I can't pay rent due the next day. If I did have the money I probably wouldn't have done it any way $50 says once I started up again I'd get another bonus and this time it could be $4000. Fool me twice...Right!? So think about it before you work for them it's a scam from the get go. I have documented everything. If you want to see it or have questions, hit me back in the comments on this review. STAY AWAY, PLEASE. Leave the job to the little rich kids who can afford to have Daddys Money taken away. You can read their users guide but it doesn't mention paying more money to get your money back. If you ask they give vague answers and show you sweet innocent pictures of the person training you.(another lie)",1
post18hb,richly branching,1.5358493545348455,highest,"Sometimes, something that sounds too good to be true might be too good to be true. Stuff happens, don't bet all your life savings into it. Work is never easy.",1
post18hb,richly branching,1.5358493545348455,highest,"They're not a legit company. I did their assessments and was passed to the next stage promising ""virtually infinite work"" in October and have still not received any assignments.",1
post18hb,richly branching,1.5358493545348455,highest,They aren’t legitimate because you haven’t received any projects yet?,2
post18hb,richly branching,1.5358493545348455,highest,"My experience isn't singularly unique. There are a ton of other comments stating the same thing as me. One other huge red flag is that they have literally zero contact information. There's no way of getting a hold of anyone who works there, if there is more than one person at that ""company"".",3
post18hb,richly branching,1.5358493545348455,highest,No one said your experience was singularly unique. But I’ve never had an issue with communication since I’ve been there and I make good money. I’m aware that’s not everyone’s experience. But lots of us haven’t had an issue. Some have been employed for well over 3 years.,4
post18hb,richly branching,1.5358493545348455,highest,"None of our experiences are singularly unique. But so far it's been good for me. I haven't hit the limit of projects available yet; there's always plenty to choose from. I pull the money out as soon as I can because why would I let it sit there if I can withdraw it every 3 days?

Btw, the way to get in contact with the company? It's in the onboarding guide doc, which you can find under the FAQ once you have been accepted and passed to the next stage.",4
post18hb,richly branching,1.5358493545348455,highest,"I would have to disagree with this. I have been on the platform for 4 days now and have made $300.  I already work a full time job so only manage 2-4 hours an evening with the weekends being a full work day if I have the time to do so.

When I signed up it took me two weeks to hear back. I took the assessment 2 days before my brother did and he heard back and was accepted a week before I was.

I wouldn’t consider myself as a high level writer but I did try my best and give honest accurate responses.

Likely cause for not getting in is either due to your location or you failed to read instructions and deliver on their expectations. 

I will add that the amount of people that think they can get away with using A.I to help pass the assessments and complete work is mind boggling considering this is a company that trains A.I. 

I’ve had consistent work ever since I started  and will likely out-earn my full time job this year on DAT .

Like everyone else in the process of waiting to get accepted I thought about emailing support or complaining on Reddit but I realised there was a strong correlation between the people who complain and don’t get accepted.

Maybe that’s why it worked out for me.",2
post18hb,richly branching,1.5358493545348455,highest,They are legit. It's just that not everyone gets accepted or put onto projects.,2
post18hb,richly branching,1.5358493545348455,highest,You must not have passed your qualification tests unfortunately. Or haven't signed in when qualifications tests are available. It's good practice to check daily for qualification tests so you can get more projects. In the beginning you tend to share mass projects with everyone else on the platform but as you take your qualifications you can get projects just for you. I have constant tasks ranging from $20-$30 always available.,2
post18hb,richly branching,1.5358493545348455,highest,"Do any of these projects require analyzing photos? I remember looking into the data researcher job at Appen, and my understanding was that analyzing photos was required. The site seems accessible with a screen reader; I'm blind so wouldn't be able to accurately analyze photos, E.G. if we had to identify photo examples for vision models etc.
Thanks.",1
post18hb,richly branching,1.5358493545348455,highest,"Yes, some project may do this.",2
post18hb,richly branching,1.5358493545348455,highest,"For all projects there’s an assessment/qualification beforehand and then it seems like you only get a few tasks in your queue and if you complete those you might get a larger batch.  
So if there is a project requiring photo analysis (I’ve only ever had one), you would be weeded out at the qualification level & simply not be assigned those tasks, in the way I’m not assigned coding tasks or translation tasks.",2
post18hb,richly branching,1.5358493545348455,highest,"There are a few, but I have steady work with them and have never analyzed photos. I have done a few tasks that ask me to provide photos to ask an AI to analyze, eg, asking it questions about the photo, but -- as I mentioned, there has always been something else I could choose. You're not assigned only one task, and I always encourage people to play up their strengths because they do use those in assigning projects (eg, if you've EVER edited or proofread, if you're a creative writer, if you have experience with coding -- even if none of these things are job experience! -- etc etc. Even from the way you're composing your reply I can tell your writing level is pretty high, and they like knowing what subjects people are knowledgable in as well).",2
post18hb,richly branching,1.5358493545348455,highest,I'm blind too and have been working for them. There's some projects like that but rare compared to text based projects. I use a screen reader with ease for all my work.,2
post18hb,richly branching,1.5358493545348455,highest,"Ah, good to know. Thanks. Seriously considering applying.",3
post18hb,richly branching,1.5358493545348455,highest,"And they were probably logged out for using Ai tools to help them finish assignments, which is pretty much the only reason they’ll log you out.",1
post18hb,richly branching,1.5358493545348455,highest,Why didn’t they just cash out to PayPal and keep the 6K in there? Sounds fishy to me. I’ve been on DAT since right after Thanksgiving. It’s been a Godsend. I don’t have any coding experience so I work on other projects. I’ve referred 5 people and they are still waiting. I don’t know what criteria they use to onboard us but once you get in you HAVE to follow the rules. And if you are stealing time you will get kicked off.  I typically under report my time.,1
post18hb,richly branching,1.5358493545348455,highest,Ive only had great experiences with them. Your friend probably got kicked off for being dishonest or doing poor quality work.,1
post18hb,richly branching,1.5358493545348455,highest,"I'm still waiting on approval for DA.  But for me, Prolific is working for me.",1
post18hb,richly branching,1.5358493545348455,highest,[deleted],2
post18hb,richly branching,1.5358493545348455,highest,"Prolific is just surveys, but they are not to boring.  Just when you do your profile be honest and try to fill out most of it.",3
post18hb,richly branching,1.5358493545348455,highest,[deleted],4
post18hb,richly branching,1.5358493545348455,highest,I think most people can’t complete tasks fast enough at a high enough level to be seen as a profitable worker.,1
post18hb,richly branching,1.5358493545348455,highest,I’ve filled out their assessment weeks ago but haven’t heard anything. Maybe that’s a good thing?,1
post18hb,richly branching,1.5358493545348455,highest,"Same they owe me over $6000 and won’t even reply, if you would contact me and I’d like to file a lawsuit, I’ve already complained to the NY department for wages. I was a top worker, who did almost all the different types of projects. This company is routinely participating in wage theft.",1
post18hb,richly branching,1.5358493545348455,highest,Did you have any success? I need to get money they literally approved and will not pay out to me. Also in NY.,2
post18hb,richly branching,1.5358493545348455,highest,u/jeremydataannotation,2
post18hb,richly branching,1.5358493545348455,highest,"Yes, I'd like to take legal action against this company. I'm not willing to spend a lot of money on it, though. Maybe some of us can get together and file a class action lawsuit?",2
post18hb,richly branching,1.5358493545348455,highest,"Ya message me with contact info, we can start something u/jeremydataannotation",3
post18hb,richly branching,1.5358493545348455,highest,"DA suspended my account today, not sure why. I keep getting tasks to do, which means I did a good job. And then I suddenly got a message. Last week I worked many hours and made 400$. yesterday 300$ and all was ok. Today they had to pay $400. and they suspended me. I worked also on the weekends sometimes they send notifications to make a task a priority and I spend time on it. I got that message after 2 weeks of working there. I was also doing coding projects. They owe me about 2K. I did a great job and yesterday I had like 60 tasks on my board. If it is empty then based on comments would mean I did a bad job. However, they kept adding for 2 weeks from 20 tasks became 60.",4
post18hb,richly branching,1.5358493545348455,highest,"Good afternoon, first time on this thread.

Yeah it helps to read the ""fine print"".  Meaning I saw Data Annotation listed under ""Sponsored"" when I was just browsing Instagram the other day.

Let's not forget that Data Annotation means the process of labeling data with revelant tags to help computers or devices understand what data is being translated to.

So in otherwords, I hope your friends had everything screen recorded, screenshots, and documented.  Otherwise, I'm sorry that they got screwed.",1
post18hb,richly branching,1.5358493545348455,highest,Probably pending approval not withdraw-able.,1
post18hb,richly branching,1.5358493545348455,highest,"Aren't I glad that I made the time to peruse reddit after editing my book on cold calling. I literally have their tab up on indeed and the ""job"" post saved.",1
post18hb,richly branching,1.5358493545348455,highest,Thanks for the warning ‼️,1
post18hb,richly branching,1.5358493545348455,highest,"i made $12, withdrew, did $9 worth of more work then got never had any more work again. Emailed them no response",1
post18hb,richly branching,1.5358493545348455,highest,"Yeah, good luck ever getting a response from them.  1 month later I have yet to hear back.  Total scum company.",1
post18hb,richly branching,1.5358493545348455,highest,"I attempted to apply with them, but it says my phone number is already in use. I've emailed support multiple times with no response. I get emails from them weekly asking me to finish my profile, but no one will contact me so I can even log in.",1
post18hb,richly branching,1.5358493545348455,highest,"Sorry to hear about this. My issue is that they never get back to you about your assessment.... Like at least let me know I failed, lol.",1
post18hb,richly branching,1.5358493545348455,highest,"To chime in: 
I was working for them for a few months making great money and had a few consistent projects. In the beginning I had feedback suggesting I make conversations longer, I obliged and received no further feedback. Then one day while working my projects were wiped and my slack access revoked, no feedback or explanation. I received the rest of my funds in my account, but otherwise nothing. Reached out to support twice with no response, once two weeks after the wipe and once a month after. 
In summary, you can make good money but expect to be dropped with no explanation at some point. I’ve seen a lot of similar stories to my own and your friends’.",1
post18hb,richly branching,1.5358493545348455,highest,"They put the money in a pay pal account so how can some 1 else take it out of their account???? If u read the job's description they have pages of lawyer data it's all there , they make it seem like ur singing ur life away to a lawyer.",1
post18hb,richly branching,1.5358493545348455,highest,There is an amount shown in your DA account UI of how much you’ve earned. There’s another amount of how much of that is withdrawlable. The withdrawlable amount can be transferred to PayPal,2
post18hb,richly branching,1.5358493545348455,highest,"This ""job"" is being offered to me at $40/hr, how legit is this offer?",1
post18hb,richly branching,1.5358493545348455,highest,There are projects that pay $40. Mostly coding.,2
post18hb,richly branching,1.5358493545348455,highest,"I've read similar stories. If true, they should be reported as there are other foreign companies mining data also. Im told DA is a china based company with offices in NY & UK.  I see a huge lack of common decency when a company does not answer simple emails. Especially after people have given them personal information (name, resumes, contact info......) It could be a red flag.  

Where to report this company:  CISA . gov ;    FBI",1
post18hb,richly branching,1.5358493545348455,highest,"I have found their support box does not reply, and they leave your complete passed test in a pending state as a new user.",1
post18hb,richly branching,1.5358493545348455,highest,Is Remotasks owned by the same corporation as DA?,1
post18hb,richly branching,1.5358493545348455,highest,"I put in my email, name and phone number and nothing happens. What's next, if anything?",1
post18hb,richly branching,1.5358493545348455,highest,What’s the fastest anyone has heard back. I took the assessment last weekend. Still no email. Should I be patient?,1
post18hb,richly branching,1.5358493545348455,highest,"I heard back within two days. Been doing 5-10 hours a week for the last month, no issues so far. General projects, not coding.",2
post18hb,richly branching,1.5358493545348455,highest,"This is an anecdotal third-party claim and you have no idea what the truth of the situation is. I appreciate you mentioning this but I've earned around this amount, regularly cashed out, and have no issues. I've made mistakes on tasks and developed my skills accordingly. You don't just get canceled unless you're being dishonest in some way, at which point, an expensive lesson is learned.

&#x200B;

If you're being a legit worker and being honest, you have nothing to worry about.",1
post18hb,richly branching,1.5358493545348455,highest,If you’re stupid enough to leave $6000 in there then I can’t blame anyone but your idiot friend.,1
post29hb,richly branching,1.5294767856968667,highest,John Henry stories abound...,1
post29hb,richly branching,1.5294767856968667,highest,John Henry is the fucking MAN,2
post29hb,richly branching,1.5294767856968667,highest,HE COULD HAMMER!!!!,3
post29hb,richly branching,1.5294767856968667,highest,Steele driving MAN!,3
post29hb,richly branching,1.5294767856968667,highest,"TL;DR:
AI isn’t just automating jobs, it’s revealing how broken our value system is.

We need to stop tying worth to labor and start building a world where being real counts.

Post:

This isn’t just an economic issue.

It’s a civilizational identity crisis.

If your worth has always been tied to productivity.. what happens when productivity no longer needs you?

We can’t answer this with reskilling bootcamps.

But, we *can* answer it by redefining value.

By building systems where presence, care, creativity, and coherence count.

Not just in what we do for work,
but in how we live, how we relate, how we make meaning together.

UBI is just a bandage unless we shift the myth.

From: Labor = worth to existence = legitimacy.

To: Fulfillment = genuine experiences = authentic legitimacy

AI isn’t just taking jobs. It’s exposing how empty our value systems were to begin with.

But, it might be the gift we've been needing.. if you're brave enough to rewrite the story.",1
post29hb,richly branching,1.5294767856968667,highest,"I think you've got the right of it. 

Most comments are debating whether or not AI replacing jobs is true or ""how true"" or ""how long will it take?"" etc but those are just superficial questions. Hell, even saw a comment that seemed like they were implying ""Well, those who can work on/with AI will be fine, everyone else will just have to 'figure it out'"", like it would be so easy, like any of this could happen without *everyone* being effected in some way, shape or form. 

It's always sad to see people interact with the world as though they live in a vacuum.

  
If we're going to bring something as powerful as AI to fruition (and make no mistake, profit alone will be the most powerful force behind it's evolution. They *will* replace all of us if it makes better sense regarding profit) a different conversation needs to be had, and I think you've got the right start by focusing on what the hell we actually value as a country, let alone a species. 

If they see *some* of us as expendable, how long, really, how long until *everyone* is expendable?

It's not a matter of *if* but *when*.",2
post29hb,richly branching,1.5294767856968667,highest,"Capitalism is just reaching its final conclusion in technology.  
The culture of those paying for labor has been seeing the human element of labor as an expensive obstacle for longer than most people on earth have been alive.   
When labor needed to be satisfied for productivity to match their intended profits, it was an adequate sacrifice to make to see that they could go back to a home they owned, with a car they have paid off, to their nuclear family, with enough money to enjoy their lives occasionally, to not be bankrupted if they ever were made ill and needed surgery or long term medication.   
But capitalists never really \*wanted\* to share with the labor. They have always seen labor as just another machine to rent. Or livestock to borrow to plow the fields, so to speak.  
It isn't a coincidence that capital funneled heinous amounts of money into AI and continues to do so.  
One big upfront investment and then they can eliminate untold amounts of those pesky farm animals draining extra resources that could better be served sitting in their offshore accounts than actually serving society.   
This is what they want. This is what they have always wanted. They want to pay a skeleton crew just enough to keep their automated tech running at the lowest cost possible.  
And they do not care if their old farm machinery rots. They do not care if we starve or die.   
As far as they are concerned, we are the have nots. And we are deserving of no quality of life or even life itself because we are not privileged enough to be like them.",3
post29hb,richly branching,1.5294767856968667,highest,"You are paying for labour just as much as a guy who owns a factory with a 100 workers. Your payments are just less direct. If you wouldn't be prepared to pay twice as much for everything you buy, iirrespective of its impact on your lifestyle and solvency, then you are talking bollocks.",4
post29hb,richly branching,1.5294767856968667,highest,"It’s almost a given that to continue their money multiplying machine, they will have to have a constant input. If people can’t make money working then no one is buying products being produced. So to fuel that engine something like UBI must be implemented or we gat paid for doing good works and helping people or producing life or some shit. I dunno. It’s just so fraught with peril at the same time.

But I agree we are at an inflection point.",3
post29hb,richly branching,1.5294767856968667,highest,Just wanted to comment and say I feel the same. You have outlined THE issue of our times. This comment (and idea) deserves a lot more attention. The fact A/I (or any labor replacement) threatens our existence is in fact a huge condemnation of the current status quo. We’re about to get real dog-eat-dog around here. Animal kingdom here we come (back).,2
post29hb,richly branching,1.5294767856968667,highest,"Thank you for putting this into much better words than I can. Every time I see people complaining about ai taking jobs, my initial thought is ""why do we fucking care?"". I hate working, I do it full-time, please take my job AI. Let me spend my life doing things I want. Working ain't it.",2
post29hb,richly branching,1.5294767856968667,highest,"It's taken pretty much my entire life to be able to understand it like this so, for sure. Thanks for saying so. 

I agree!",3
post29hb,richly branching,1.5294767856968667,highest,[ Removed by Reddit ],3
post29hb,richly branching,1.5294767856968667,highest,You’ve got to bring together the idea that “AI is taking jobs“ with the “most Americans can’t afford a $500 emergency” headlines then you start to see the full picture,3
post29hb,richly branching,1.5294767856968667,highest,"I agree. The problem I see is capitalists allowing us to exist without work. When unemployment gets to 50-60% and people have no money or resources, how will we convince our government (that is increasingly disinterested in doing anything to help the average person) to act? I think it will be the great issue of the 21st century and there will likely have to be an American Revolution 2.0.",3
post29hb,richly branching,1.5294767856968667,highest,You still need food on the table and a roof over your head though,3
post29hb,richly branching,1.5294767856968667,highest,"This is along the lines of what I’ve been telling my screenwriting students, AI itself isn’t an existential threat so much as the top brass who will be satisfied with AI work. AI can’t write something great, but it can write something, and well before LLMs we were seeing studios embrace a quantity over quality mindset and toss anyone who wasn’t a company man. In other words - AI can’t make art but it can absolutely make content. 

I’ve tried to frame this as an invitation to let themselves fuck up and make truly messy work. Don’t try to impress me, don’t try to write what’s expected of you, don’t think about your work as fulfilling a brief because that’s the angle AI is always going to understand better. Think of your work as YOUR work, and fuck it up in a way only you could fuck up.",2
post29hb,richly branching,1.5294767856968667,highest,"If labor is no longer the basis of worth, why would anyone still work at all, even for essential roles like directing AI or maintaining systems especially during the trasition period to when were trying to meet everyones base needs via ai?",2
post29hb,richly branching,1.5294767856968667,highest,"Yep, can't wait ✨💪🏻",2
post29hb,richly branching,1.5294767856968667,highest,"most empty response. like ok, system bad. thanks reddit comment. i guess everyone clapped and we did it? dude thinks were in a movie",2
post29hb,richly branching,1.5294767856968667,highest,"Thanks for saying this.

This is a HUGE cultural reckoning, and it's in a lot of ways been sorely needed before this... but now it's getting forced.",2
post29hb,richly branching,1.5294767856968667,highest,Our government would shoot us in the streets before even considering doing this level of work to change society...,2
post29hb,richly branching,1.5294767856968667,highest,Good thing it doesn't take a government for change to happen,3
post29hb,richly branching,1.5294767856968667,highest,This 1000%.,2
post29hb,richly branching,1.5294767856968667,highest,"It sounds like you are saying that when most or all economic productivity is solved by non-human systems, you don't want people to just have UBI and be able to do whatever they want without having to demonstrate value to compete for resources. Instead you want people to have to demonstrate value by being ""real, having presence, care, creativity, and coherence?"" That kind of sounds like a bunch of influencers having to compete for money and resources in some sick competition to show value through human authenticity. That sounds like it could be a lot worse than what we have now. If the problems of scarcity and economic productivity are solved, and everyone can have all their needs met for free, why the hell would you want to introduce a new alternative system for creating scarcity by making people compete for resources do little monkey dances to prove their authentically human value. Maybe you are trying to say that we need a shift in values just culturally, but the fact that you say UBI is a bandaid and that we need to replace the labor system instead really makes it sound like you want a new vibes based theory of value to create scarcity, and to make people dance to prove their human value in some economically non-productive way.",2
post29hb,richly branching,1.5294767856968667,highest,"For sure something to consider. 

I would like to clarify the shift I’m proposing isn’t about making people compete again. It’s about ending the idea that value has to come from competition at all.

If basic needs are covered, we don’t need a new system of scarcity. We need a system of resonance and connection.

That means recognizing fulfillment, presence, and meaning as they genuinely emerge, not treating them like performance metrics. The models I’m building (like FSRM: Feeling-State Resonance Mapping) are designed to reflect internal coherence, not reward strategic mimicry.

It’s not about proving you’re human. It’s about building a world where being human actually counts.",3
post29hb,richly branching,1.5294767856968667,highest,"This comment isn’t just ordinary plagiarism of a bot; it’s pure irony unfolding before our eyes. Each and every word underpins a rich tapestry of humor that the author seems oblivious to — and the replies even more so — all while promoting a return to humanity that he seems unequipped to handle. It’s not sad, it’s not stupid, it’s not shocking — it’s *funny*.
Edit: for clarity’s sake, this was written to sound like a bot. Ironic humor. I may have fooled some people",2
post29hb,richly branching,1.5294767856968667,highest,Did you really just use a.i to answer for you? Seems weird since it was a personal comment towards the subject but it makes it sounds like its your original thought when I know its a.i i can read machine resonance,3
post29hb,richly branching,1.5294767856968667,highest,"I intentionally wrote it that way, it’s meant to be ironic humor. I find AI writing to be utterly insufferable and a sign of low intelligence. It’s legitimately worrying that young people use it in the fashion they do without ever learning the skills to write",4
post29hb,richly branching,1.5294767856968667,highest,"This is a nice but naive notion. Living things allocate scarce resources including capita based on incentives. The question is whether AI will be made a public good that everyone can access, which would allow us to still work in an incentives based system.",2
post29hb,richly branching,1.5294767856968667,highest,"The last thing we need is a civilization where productivity isn't a KPI.

People are already shitty enough. Take that away and see the world burn.",2
post29hb,richly branching,1.5294767856968667,highest,"Sad that you derive soul value from KPI.

You say this but then in the next sentence you are describing yourself. 

That's a shit take. I'll say it. 

To dwindle a human to a KPI and seeing that it deems their value is demeaning in the lowest sense. 

You are a number, is essentially what you're saying. 

I'm surprised that system hasn't burned down yet.",3
post29hb,richly branching,1.5294767856968667,highest,[deleted],1
post29hb,richly branching,1.5294767856968667,highest,"AI has already replaced a lot of jobs by proxy. Simply by augmenting the talented employees to do more with less. So I don’t think it’s relevant to ask if it will replace 100% of workers, it won’t. The question is will YOU be replaced at some point in the future. No one thinks it will happen to them, until it does. I personally know of three that got laid off this week, and although no company will directly make the attribution, all you need is understand their current environment to connect the dots.",2
post29hb,richly branching,1.5294767856968667,highest,"This is what I am seeing.

Go into a mid-size business and they used to have 1 AP clerk for every 1200-2000 monthly transactions.  It was just the requirement.  With OCR tech which is now becoming ubiquitous that number goes up to 1 AP for every 12,000+ transactions.  That means way fewer AP clerks in this world are needed.

It’s jobs like those that will just be gone.",3
post29hb,richly branching,1.5294767856968667,highest,"I was Head of Finance at a company processing 2-3k inbound invoices a month, and after a few months of training the categorisation rules on the OCR platform I chose to implement, well over 90% of invoices had no manual intervention from Finance at all. They were automatically handled in our inbox, coded automatically, passed into the approvals workflow automatically, scheduled into pay runs automatically. I worked with our suppliers to standardise how they delivered invoices to work with our systems. Plenty of testing was done and after six months we were under one error per thousand. Far better than the manual rates. 

I started cross-training my AP staff member before implementation so he'd have a job afterwards, but if I didn't badly need resource for other tasks, it'd be impossible to justify keeping them. 

The same story is repeated across so many jobs. The tech is just better than people. Anything repetitive and rules-based is under threat. The jobs that will be left require judgement and skills that are difficult to acquire without experience. I feel for new entrants to the job market because the ""easy"" roles that give time to understand how everything works are disappearing fast.",4
post29hb,richly branching,1.5294767856968667,highest,"A lot of people’s jobs are completely mindless. I’m not worried about highly educated people who are skilled and talented. Most people are not talented or highly educated - these are the people who should worry, and are those who will probably lose their jobs first.",3
post29hb,richly branching,1.5294767856968667,highest,“Those people” are probably 80% of the population. That’s a huge issue.,4
post29hb,richly branching,1.5294767856968667,highest,"I agree with you. But consider that in two years much of what you do an AI might consider mindless work. Maybe, maybe not. A lot of people never considered it either and lost their jobs.",4
post29hb,richly branching,1.5294767856968667,highest,"> these are the people who should worry, and are those who will probably lose their jobs first.
* What's your thoughts after these people gone?",4
post29hb,richly branching,1.5294767856968667,highest,Who qualifies as highly educated?,4
post29hb,richly branching,1.5294767856968667,highest,They still need to live. We need to introduce UBI before it's too late.,4
post29hb,richly branching,1.5294767856968667,highest,Just learned that 20% of adults in the USA cannot read beyond a 3rd grade level.,4
post29hb,richly branching,1.5294767856968667,highest,"Mhm, skilled and educated people. 

Until their industry collapses and they need to pivot to other fields where they start from square one and their high education and talent means nothing.",4
post29hb,richly branching,1.5294767856968667,highest,Yes. I can't wait to see what all the poor and unhappy people are going to do with all the spare time they have!,4
post29hb,richly branching,1.5294767856968667,highest,"You said it: **current technology**. The guy said AI is **coming** for your job. He didn't say that it's already here, but it'll be here soon.",2
post29hb,richly branching,1.5294767856968667,highest,"Every single person saying that AI won't take jobs because it's not good enough, for some reason fails to think about the future, every single time.",3
post29hb,richly branching,1.5294767856968667,highest,"Absolutely! They simply look at what it can do ***now*** instead of what it might be able to do in the future, and they ignore the HUGE progress that was made recently.",4
post29hb,richly branching,1.5294767856968667,highest,"Well, that’s because the average person is not keeping up with the AI and robotics race and the headlines.. when they’re probably working multiple jobs just to get by in this horrible economy.",4
post29hb,richly branching,1.5294767856968667,highest,"they also seem to forget that sometimes big companies will go for the sub-par option if it will save them money. It doesn't have to do the job as well as the people it's replacing it just needs to do it well enough that the money saved is worth the customers drop in service. 

Some companies see to look at short-term gains over long term wellfare of the company",4
post29hb,richly branching,1.5294767856968667,highest,"Future like 20-30 years, not 3.",4
post29hb,richly branching,1.5294767856968667,highest,"This guy is saying “in months” which for 90% of things is total bull. Most are saying 5-10 years but even that might be optimistic and may not happen at all if a wall is hit. The model improvements are slowing down and they are having issues making them “smarter”. I could see image generation, voice over work, video text graphics (for TV shows, movies) commercial music definitely being effected.",4
post29hb,richly branching,1.5294767856968667,highest,"Better yet, it’s exponential growth that’s on the horizon. We are close to the point where AI models are the primary developers of better AI models that can develop better models.",4
post29hb,richly branching,1.5294767856968667,highest,"When will AI be fundamentally capable of generating an original idea? Until it can do that, it won't really be able to replace humans.",4
post29hb,richly branching,1.5294767856968667,highest,"every single person talking about the future somehow misses the point that though we have immortal souls, our body is fragile and has an expiration date.",4
post29hb,richly branching,1.5294767856968667,highest,And every single person saying that have no idea how big steps AI has taken iex. just 2024. 2030 we have a very different world than we have today.,4
post29hb,richly branching,1.5294767856968667,highest,"idk. The current version of chat gpt seems very error prone. Maybe there is a secrete version somewhere in corporations that is way better but it fails basic shit, can’t check its work properly, etc",4
post29hb,richly branching,1.5294767856968667,highest,"You forget that there has been AI before the last years. So it's hype and panic altogether. And - like with every new thing - there's a spike in hype and panic and then it flattens out.

We've had that on electricity, robots, cars, computers, ...",4
post29hb,richly branching,1.5294767856968667,highest,"My coworker's cst has an old ass Washington Redskins license plate that says ""NXTYEAR""

AI is like that",3
post29hb,richly branching,1.5294767856968667,highest,compare where we were five years ago (GPT-2) to where we are today and the difference is absolutely palpable,4
post29hb,richly branching,1.5294767856968667,highest,He said in a matter of months...,3
post29hb,richly branching,1.5294767856968667,highest,He's right. It could be a matter of months.,4
post29hb,richly branching,1.5294767856968667,highest,No amount of advancement will make a lightbulb good at brushing your teeth. It just doesn't do that. LLMs are incapable of persistent reasoning in the same way. It's not what that technology is or can be capable of. It can translate profanities to polite corpospeak though,3
post29hb,richly branching,1.5294767856968667,highest,"Again, you assume that we will stay with LLMs. No one signed a contract that they will do. Just like we went from diffusion to regressive models for image generation, things can evolve quickly.

Just keep this in mind: every day, there are tons of very smart people trying to come up with something that is capable of automating most jobs with a lot of funding from capitalists who'd rather increase their profits and reduce their costs, of which is the salary of labor.",4
post29hb,richly branching,1.5294767856968667,highest,[deleted],3
post29hb,richly branching,1.5294767856968667,highest,"I agree with everything you said because you have simply stated facts.

This being said, you don't need perfect technology in order to start replacing labor. The tech we have **now** can already replace 2 humans with 1 human + AI. It will keep doing that more efficiently. Soon it will be 3 humans replaced by 1 human + AI, then 4, 5, 6... 

Many companies have already started this labor replacement. Some have the ""courage"" to call it what it is. Others pass it simply as layoffs.

You should already be scared. If you're not, you're about to witness what exponential change looks like. It seems slow until it hits you in the face. After that, it's already too late.",4
post29hb,richly branching,1.5294767856968667,highest,"It'll be here soon is what they said about fusion reactors too.


30 Years ago.


Lmao.",3
post29hb,richly branching,1.5294767856968667,highest,"So your argument is that because something else that was promised wasn't achieved in the expected time frame, then EVERYTHING, including AI, won't arrive at the promised time frame?

This is one of the most illogical and irrational arguments ever. It's like saying that because the last pandemic didn't kill all of us, no future pandemic ever will.",4
post29hb,richly branching,1.5294767856968667,highest,"He also claims people will need to become masters in a 'matter of months' this kind of tells us that he's vastly overestimating the speed at which AI will _actually_ replace development jobs. I've been an early adopter and active user, and it's still fairly clear he's overestimating here.",3
post29hb,richly branching,1.5294767856968667,highest,"Keep in mind that Mr. Kaufman has a background as a lawyer with a law degree, works in business administration, and has no idea what he's talking about when it comes to computer science.",4
post29hb,richly branching,1.5294767856968667,highest,[deleted],3
post29hb,richly branching,1.5294767856968667,highest,"Even if it did, just because something turned out some way doesn't mean that everything else will turn the same way. 

AI might turn out to be a dud, or not. Only time will tell.",4
post29hb,richly branching,1.5294767856968667,highest,"Dunno, I know lota of senior programer using AI and producing 5x to 10x more code. Taking multiple job in the process.

And yes, good quality code. Its a tool, even if you used for a long time it doesnt mean ypu are using it well. You might even have take on bad habit that is slowing down with the most recent model",2
post29hb,richly branching,1.5294767856968667,highest,"Such a major cap, 5x to 10x more code is such bs it's hilarious. It can be very useful in certain situations and should he used in these, but anyone who uses it for the majority of their code just simply isn't producing great code. Change my mind.",3
post29hb,richly branching,1.5294767856968667,highest,"I will trust the senior engineer to judge the quality of the code he produce. Like I said, most of us is using that tool wrong and go on full dunning kruger beleiving that its all LLM can do. Its not just a google. There is API and using multiple agent talking to each other produce great result",4
post29hb,richly branching,1.5294767856968667,highest,Just read about AI agents and stop thinking that AI is just chat gpt with someone writting a prompt,4
post29hb,richly branching,1.5294767856968667,highest,[deleted],3
post29hb,richly branching,1.5294767856968667,highest,"Reason I said 5x to 10x. It varies.

I feel like most people, myself included are using the new tools wrong. Then we assume its not that great.

Some people leverage like 4-5 different AI talking to each other. Making a main agent that speak to a prompt agent, couple of coding agent, testing agent, etc.

Im not there yet but some peoppe creative make that very good. Creating full stack code team. They oversight and make sure everything work together. Im actually impress, make me laugh reading ignorant comment afterward since these guys show you wrong quite easily. Its a matter of time and you will see this as a complete solution and I could see this being the new way of coding for everyone in max 10 year. Right now people are just using it wrong and then assume a certain level but that is dunning kruger in effect",4
post29hb,richly branching,1.5294767856968667,highest,"5x to 10x more code is such a shit metric, that if you knew what you're talking about you wouldn't be using it

more code means nothing, the real question is how many problems can they actually solve

so far, for debugging, analyzing real issues (god forbid solving them) and improving systems, AI has done nothing for me

sure, it's fun when it spits out 5 test files or some scripts and I save a few hours that way, but when I actually want to write something secure, scalable and reliable, I don't want the AI hallucinations anywhere near it",3
post29hb,richly branching,1.5294767856968667,highest,"Yep like vibe coding its just not knowing what you code thats all, like taking shot in the dark, and they are all ""my god so much better"" people a naive and easy to fool I guess. 

In three years since the first public LLM I have seen nothing to scare me. Just more human stupidity.",4
post29hb,richly branching,1.5294767856968667,highest,If it makes programmers 5x more productive that doesnt mean it will take jobs,3
post29hb,richly branching,1.5294767856968667,highest,"It absolutely does because it means that the current demand can be satisfied with one fifth of the people, making eighty percent of the people redundant. 

It can already be seen on software engineer job postings.",4
post29hb,richly branching,1.5294767856968667,highest,!remindme 6 months,2
post29hb,richly branching,1.5294767856968667,highest,"I will be messaging you in 6 months on [**2025-11-07 03:22:50 UTC**](http://www.wolframalpha.com/input/?i=2025-11-07%2003:22:50%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/artificial/comments/1kg90fs/fiverr_ceo_to_employees_here_is_the_unpleasant/mr086hp/?context=3)

[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fartificial%2Fcomments%2F1kg90fs%2Ffiverr_ceo_to_employees_here_is_the_unpleasant%2Fmr086hp%2F%5D%0A%0ARemindMe%21%202025-11-07%2003%3A22%3A50%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201kg90fs)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",3
post29hb,richly branching,1.5294767856968667,highest,"As a professional artist - it's great for generating ideas for art directors... it is not effective as solving design problems (yet). 

It's going to, is on it's way or already has killed the illustration business... but art and design is a huge field composed of lots of different disciplines.",2
post29hb,richly branching,1.5294767856968667,highest,"I think the consumer appetite for AI illustration is not there. It's not copyrightable and already looks Temu as fuck. Comics and so on that have tried to use it have had consumer backlash and the launching of new products that are open about their use of AI to generate images is basically zero - instead it tends to try to fly under the radar and not get caught. It's basically a scam technology from a consumer perspective, like those fake foraging books some people put out.",3
post29hb,richly branching,1.5294767856968667,highest,"> I think the consumer appetite for AI illustration is not there. It's not copyrightable and already looks Temu as fuck.

The market  is there the low end of the illustration market when your illustration is background and not the primary content, and Temu-as-fuck fits with the general aesthetic anyway.

I agree with you that it's not there for any market where the illustration is the primary content though.",4
post29hb,richly branching,1.5294767856968667,highest,"> Sam Talkman

I actually snort-laughed. Stealing this one for later, thanks 👌",2
post29hb,richly branching,1.5294767856968667,highest,"This is where I'm at as well, and why I am skeptical of this kind of language. To be clear, things are going to change, but it's unlikely that we're just going to YOLO AI into control of everything, especially given the known limitations. Such an endeavor would require massive effort and oversight.",2
post29hb,richly branching,1.5294767856968667,highest,"The way you know it's not there is it fails at many simple reasoning tasks, the sort of thing a child could easily do. Anyone who uses an LLM will hit these walls pretty quickly.

In the art space for example (speaking with a lot of experience in local image gen pipelines) , yes, you can generate a photo realistic image or video clip from thin air. But you very quickly realize you can't really art direct it, it can only produce a very narrow range of subjects, and even the control networks and such have clear limits.

For me the bright red flag though are the AI assistants. They're all terrible, and this is because they have limited ability to gather real world context or even device context. Eg, Gemini regularly fails to fast forward a podcast (What did you want to fast forward? I need X permission. I don't understand). These are solvable problems, but if this is what Microsoft, Alphabet and Apple are pushing out the door today, we're a ways from AI even replacing an admin assistant. 

People underestimate how sophisticated people are. You can conduct a phone call while manipulating a variety of objects in your hand while walking while eating while daydreaming. It's hard to get an AI or robot to do any one of those things competently, and people are actually pretty cheap.",3
post29hb,richly branching,1.5294767856968667,highest,[https://arcprize.org/blog/oai-o3-pub-breakthrough](https://arcprize.org/blog/oai-o3-pub-breakthrough),4
post29hb,richly branching,1.5294767856968667,highest,Agree... the kick back with be massive. It needs to be heavily guardrailed to protect employees.,3
post29hb,richly branching,1.5294767856968667,highest,">won’t be able to really reason 

Not so different from what we currently have then",2
post29hb,richly branching,1.5294767856968667,highest,Art stuff it really depends. For any kind of sequential art structures its absolutely horrible.,2
post29hb,richly branching,1.5294767856968667,highest,"where did you get the list ""persistence,inference[...]"" I am interested in reasoning; can you suggest some source or did you just make it up? thanks in advance",2
post29hb,richly branching,1.5294767856968667,highest,[deleted],3
post29hb,richly branching,1.5294767856968667,highest,thanks!!!,4
post29hb,richly branching,1.5294767856968667,highest,"Not to sound blunt, but I think your understanding of what AI can do is a bit outdated. It’s progressing more and more everyday. What was laughable months ago has already been improved countless times. AI is speeding up (nearly all) processing/handling by eliminating the need to spend time thinking about nonessential functions and questions that could otherwise be circumvented. It’s not that it can do “everything”, it’s that it can do all the things we waste time on everyday. Entire factions of jobs aren’t going to disappear, but the human work force within them will be drastically reduced to fit the things that AI can’t yet do.",2
post29hb,richly branching,1.5294767856968667,highest,">, it could replace most middle management and CEOs right off the bat probably as most of them just speak a lot of things without meaning or content.

Do you even know what a CEO does?",2
post29hb,richly branching,1.5294767856968667,highest,Thank you,2
post29hb,richly branching,1.5294767856968667,highest,"I'm with you (though somewhat shorter tenure). However, the breakthrough in LLM is not their current inference capabilities - it's their ability to write code to execute processes and achieve goals. The progress there is compounding. Even without reaching AGI, they can wreak enough havoc to ruin the life of many people.",2
post29hb,richly branching,1.5294767856968667,highest,"Good stuff. Yet it's not just about what it CAN do well, but what people BELIEVE it can do well. The faith in AI is leading to more job (and quality work) loses than the reality.",2
post29hb,richly branching,1.5294767856968667,highest,Seems the general consensus seems to be 2027. Enjoy the world as you know it now because it won't exist soon.,2
post29hb,richly branching,1.5294767856968667,highest,"You have a limited understanding of what middle management does if you think that LLMs can replace their role just because they can generate good “management lingo”. 

“ChattyHedgehog, we just landed a contract with our customer to analyze their ad campaigns, build an attribution model and report back our findings. If we’re successful in this phase we can win a larger project to execute algorithmic ad buys. Can you put together a team, and come back with best and worst case effort estimates?”

I don’t care if you track every bathroom visit in Jira, nobody is ever going to trust an LLM “middle manager” to do this. Maybe some future version of this technology. 

You think investors, board members, and shareholders are going to talk to a chatbot to get answers on how their money is being invested? I don’t know how you could possibly have 16 years of work experience and be this naive. 

On the flip side you are drastically overestimating what happens when most people are “reasoning” about the bulk of their daily tasks. 

Some version of “hmm I need to figure this thing out. Let me break it into a series of steps, google information about those steps, put it together in a doc or spreadsheet, and then summarize it at the end” has to cover something like 50% of knowledge work. Maybe more. 

You somehow are wrong on both ends",2
post29hb,richly branching,1.5294767856968667,highest,"middle managers are putting together teams and coming up with projects for new contracts? what middle management are you talking about lol

i dont think you know what middle managers even do just based on that. 

oh yeah… upper management, execs, and investors already use chatAI. you just don’t know it yet.",3
post29hb,richly branching,1.5294767856968667,highest,"Once they realise that working with AIs will net better company efficiency and company outcomes they most certainly will, or will lose to the companies in the same niche that do properly utilise AI",3
post29hb,richly branching,1.5294767856968667,highest,"> You think investors, board members, and shareholders are going to talk to a chatbot to get answers on how their money is being invested?

All the sensible ones would not only refuse, but start reviewing their relationship with you for seriously suggesting it.

On the other hand, there are *still* a good number of people out there at all levels who aren't sensible, use very simple heuristics despite position in the hierarchy and/or education and/or resources deployable, and would actually embrace the idea.

> Some version of “hmm I need to figure this thing out. Let me break it into a series of steps, google information about those steps, put it together in a doc or spreadsheet, and then summarize it at the end” has to cover something like 50% of knowledge work. Maybe more.

LLMs are actually able to do this now for a number of scenarios.

They still hallucinate and fuck things up completely at critical instances, but in ways that unsophisticated individuals would not view as a critical failure point.",3
post29hb,richly branching,1.5294767856968667,highest,"I would disagree with the art stuff completely. AI art looks cheap as fuck. There's no intentional design there; art is nothing but product development in a way. 

For rough drafts and raw ideas in art, definitely. For a finished product? Fuck no, lmao",2
post29hb,richly branching,1.5294767856968667,highest,"Professional artists may be. However, seeing who the CEO is, he was likely talking about Fiverr jobs. 

There was no ""intentional design"" for a $100 Fiverr art. It's when someone just wants some arts quick and cheap, which is what AI can replace easily.",3
post29hb,richly branching,1.5294767856968667,highest,"It's always the executives making these points, never the people who build things with the technology every day.",1
post29hb,richly branching,1.5294767856968667,highest,Do you know what Fiverr is? It's entirely gig economy work that is at risk of being automated and done for cheaper.  If it was the CEO of any other company you might have a point but Fiverr is very much on the front line and I'm sure tons of artist and music jobs have already been lost from that site.,2
post29hb,richly branching,1.5294767856968667,highest,"Fiverr is on the front line of projects that can be outsourced to India. I don't doubt that a lot of the underpaid gigs posted on their own platform will be devoured by AI, but they were already the lowest skill gigs around. AI is certainly ""coming"" for pretty much all fields, but I think this guy's perspective is severely warped by a platform he runs to already devalue skilled labor. But I'd say AI is coming for him first.",3
post29hb,richly branching,1.5294767856968667,highest,"I mostly used Fiverr for professional Voice Overs for my ads, and I did it a lot. We have been using AI voices (eleven labs) for the past year.

Soon graphic designers and copywriters will be dead, translators already gone.",4
post29hb,richly branching,1.5294767856968667,highest,"Fiverr is **not** the front line. 

Fiverr is a card carrying member of the four horseman of the unemployment apocalypse. Their whole business model is an unethical exploitation of global labor arbitrage helping companies avoid labor laws. Contributing to global wage suppression for all and a culture that devalues complex, creative, knowledge-work.

Fuck Fiverr, and the like.

The Fiverr CEO will gladly pivot the company to be entirely genetic AI SaaS as soon as they can.",3
post29hb,richly branching,1.5294767856968667,highest,"This is correct.

I don't see him being successful either.",4
post29hb,richly branching,1.5294767856968667,highest,"Except there is no barrier to entry to that model.  Fiverr’s moat is the large 2 sided marketplace they built.  AI agents give you one side with just software effort, and certainly Fiverr knows far less about AI agents then baby others.

AI is not coming for everyone’s job.  AI is definitely coming for Fiverr’s business model.",4
post29hb,richly branching,1.5294767856968667,highest,What's unethical about an Indian agreeing to work for an American company at a price they both agree to?,4
post29hb,richly branching,1.5294767856968667,highest,"""Unethical exploitation of global labor arbitrage."" You realize that Fiverr lets people in developing countries who need employment get it right? Also outsourcing tasks has the same economic effects as automation and in the long-term domestic labor gets reallocated to tasks that can't be outsourced improving everyone's standard of living as people can benefit from a wider range of goods and services. Protectionism is bad economics.",4
post29hb,richly branching,1.5294767856968667,highest,"They’re talking about the CEO, not Fiverr. The person. Not the company. And did you even read the email? He said he’s not talking about jobs at the company, but the industry as a whole.",3
post29hb,richly branching,1.5294767856968667,highest,I do. I don't see how there won't still be plenty to do on there.,3
post29hb,richly branching,1.5294767856968667,highest,This \^,3
post29hb,richly branching,1.5294767856968667,highest,"I, and everyone else I know who is building with it would agree with him.",2
post29hb,richly branching,1.5294767856968667,highest,"I am building with it, and I'm not sure. I am sincerely interested to know your thoughts, though.",3
post29hb,richly branching,1.5294767856968667,highest,"Thoughts: AI good and fast and know stuff, humans slow and not know stuff.",4
post29hb,richly branching,1.5294767856968667,highest,[deleted],4
post29hb,richly branching,1.5294767856968667,highest,"Weird how that works. People who stand to gain the most from scaring employees intto selling themselves short, and in aggragate, devaluing labor. Keep calm and let the slopmongers replace their infrastructure with sawdust and glue if they choose. When the dust settles and the scales fall from their eyes, plenty enough demand will be found in cleaning up their mess, assuming they don't go bankrupt entirely.",2
post29hb,richly branching,1.5294767856968667,highest,"Actually this isn't normal. Most execs just say ""no need to worry"".",3
post29hb,richly branching,1.5294767856968667,highest,"disagree strongly, most execs are saying ""we are hoping AI makes our people stronger"" ... which isnt going to happen. 

Most programmers and artists that are realistic , are saying - wait hold up",3
post29hb,richly branching,1.5294767856968667,highest,"I'm building it and I agree.

Also I know others.",2
post29hb,richly branching,1.5294767856968667,highest,The people who got laid off aren’t in here telling their story. Get some celebrity or influencer to post in the right place asking for anecdotes on the subject and I’m sure it will be different.,2
post29hb,richly branching,1.5294767856968667,highest,executive don't really do that much and they have a lot of time to think about it.,2
post29hb,richly branching,1.5294767856968667,highest,"I mean, that's not true. There's plenty to do for executives. 

It's more, and I'm happy to debate it for those who disagree, that it seems like when you start working earnestly with these tools you very quickly encounter their limitations. Tuning them to work well and in a cost-effective way is real work, and there's no reason to think that will cease to be true any time soon. 

To be sure, they unlock a lot of productivity in different ways. But in doing so, they also unlock a lot of projects that maybe might not have been feasible before due to the high cost of implementation.",3
post29hb,richly branching,1.5294767856968667,highest,"""The advancement of technology is here, and everyones lives will be worse for it"".",1
post29hb,richly branching,1.5294767856968667,highest,CEO is not a real job anyway.,1
post29hb,richly branching,1.5294767856968667,highest,AI will wreck our environment before any of the high falutin promises come true.,1
post29hb,richly branching,1.5294767856968667,highest,"No, we can't blame that on AI. We are killing the planet. Don't buy into offloading it on to ""the computers"". 🕉️",2
post29hb,richly branching,1.5294767856968667,highest,"At the same time, other more difficult jobs emerge and can now be done by individual people, with the help of AI. Already seen it happening. For instance, people are commissioned to make whole short films. That now takes new skills: screenwriting, sound design, cinematography, AI toolchain understanding, taste and so on.

(And no, I'm not arguing that long term every job is safe, because who's to say AI won't also direct and screenwrite and such.)",1
post29hb,richly branching,1.5294767856968667,highest,"Those aren't jobs emerging though. It's literally what would have been many jobs turning into one job. And even in this case, the clock is ticking. Because if one person can all those things, very quickly that commission will disappear because ""why would I hire someone to do what I can do myself with the help of AI?""

I have yet to see a case of someone describing a ""new"" opportunity created by AI that isn't just a combination of these two forces - massive job reduction, and selective (wishful) thinking.

As though ai progress is going to suddenly come to a stop once the work that could be done by 10 people can be done by one. What do you think makes you so special as that one person that the thing you're contributing is the thing that can't be replaced?

If we really end up being able to replace most jobs, then we're going to be able to replace every job. Human labor will have no value, if you didn't already have the wealth to not need to work, you're gonna SoL, on permanent minimum wage government ""UBI"" forced into compliance because they literally control your ability to survive.

It is definitely not a happy path.",2
post29hb,richly branching,1.5294767856968667,highest,"Only way I can see new jobs is if we start valuing things currently seen as unimportant more.

Like, if you took current good salaries and paid a lot of people that much to act as social workers, there is a lot of suffering that could be addressed before you ran out. 

You could give out grants for people to preserve traditional arts like drawing cartoons by hand or overly elaborate parades or hand made clothes.

Hell, you could give people PPE or remote controlled drones and have them sort through old garbage heaps to reclaim stuff, even if that’s not very efficient. It’s still a way to get resources without destroying more nature.

There’s lots of work we *could* be doing to make life better that we currently ignore as insufficiently profitable. We’d just rather let people descend into squalor than pay them to do anything about it.",3
post29hb,richly branching,1.5294767856968667,highest,"In some cases it is multiple jobs turning into one, yes, but in other cases, it actually increases artistic scope, and new forms of expression emerge.

As a random example, I created an AI-based visual storytelling engine that runs in Twitch, and I live-DJ'd for the community currently having fun playing the story -- by incorporating their names and comments into ad hoc created music lyrics, again composed with the help of AI, and played back to them in realtime with lots of laughter to be had. Creativity wasn't gone, it just moved to a meta level.

I spent my days working on many different such projects, and if you're genuinely interested to learn about that meta level I'm happy to expand on it. I just don't aim to debate or change your mind, it's fine if you have your opinion as is and I can totally understand and empathize with where you're coming from.",3
post29hb,richly branching,1.5294767856968667,highest,"Ok, so let's assume this is a paid activity. You are being an ""entertainer"" (and a freelance one at that). That's not a new job, even if you are incorporating novel media to perform that job, as entertainers have done since it's been a thing. 

And as far as jobs go, it's not great when it comes to being secure or lucrative (for the vast majority of people doing it). Responding to mass unemployment with the prospect of freelance entertainment isn't job creation any more than signing up to drive Uber or delivering DoorDash (although at least those arguably have more reliable demand with a lower barrier of entry).",4
post29hb,richly branching,1.5294767856968667,highest,"Yeah it is not a problem though. 


One person will be able to pump out 30 projects a month instead of 3. Prices per project will go down. There will be more economic activity because of the low prices. Because of more economic activity more jobs will emerge",3
post29hb,richly branching,1.5294767856968667,highest,"If we can automate from needing 30 people to do a job to just one, odds are we'll be able to automate that one remaining person as well.

I don't know why people think there's some magic that means at least one person will always be required. It's wishful thinking.

> Prices per project will go down. There will be more economic activity because of the low prices.

If you think lower costs of production means savings get passed on to consumers, you haven't been paying attention.

> Because of more economic activity more jobs will emerge

In the US the top 10% are responsible for nearly half of all consumer spending. Just because economic activity is happening, doesn't mean the majority are or will be the beneficiaries.",4
post29hb,richly branching,1.5294767856968667,highest,"But those job ALREADY existed. Short films were created in the past. By a team of talented and hard working people, getting paid. 
Now it is going to be one guy, getting paid much LESS than a whole team. 

Nothing new emerges.",2
post29hb,richly branching,1.5294767856968667,highest,"This is [Jevons paradox](https://en.wikipedia.org/wiki/Jevons_paradox) in action, when movie making becomes easier, you don't just make movies faster with fewer people, but people end up making much more movies, because they are cheap now, thus resulting in more people being employed making movies. That's how improvements in technology have worked out numerous times in the past.

That said, I don't think it will happen this time around, at least not for long or at the scale necessary. The reason being, human attention is limited and AI can create stuff at an insane pace. Hollywood right now makes around 150 major movies a year, that's small enough that you could still watch everything if you really wanted to. If AI turns that into 1500, you don't end up with a movie market 10x the size, since nobody got time to watch all of them. We are reaching a point where humans have enough entertainment at their fingertips to last multiple lifetimes.

>> screenwriting, sound design, cinematography, AI toolchain understanding, taste and so on.

And as for those skills, all of that is stuff AI can do. Not right now and not in the quality needed, but AI progress means that all those things that still require human touch right now will fall away as time goes on.",3
post29hb,richly branching,1.5294767856968667,highest,"> If AI turns that into 1500, you don't end up with a movie market 10x the size, since nobody got time to watch all of them. We are reaching a point where humans have enough entertainment at their fingertips to last multiple lifetimes.


It's how it's been with books. Nobody can read them all.",4
post29hb,richly branching,1.5294767856968667,highest,"Your last paragraph mirrors my last paragraph, so: yeah, that's a possibility. I also see other possibilities and can describe them if wanted, but nobody is an expert on the singularity yet -- not even the singularity experts!",4
post29hb,richly branching,1.5294767856968667,highest,"On the other hand, though, you also unlock improved production values for long tail projects. You can hit weird and specific niches in a way that you couldn't previously. Experimentation becomes substantially easier.

I don't know how much that increases demand, but it's not zero. YouTube could get a lot more interesting.",4
post29hb,richly branching,1.5294767856968667,highest,"even with just Humans involved in the production chain we have are near peak output. bout a decade ago the head of FX tv was talking about the era of peak TV where there were so many high quality shows that people couldn't watch them all. And with older media being so accessible it just adds to the mass. There are still many people who haven't seen The Wire and there are so many other great shows, books, games and that doesn't include the time hanging out with  friends and other socialising.",4
post29hb,richly branching,1.5294767856968667,highest,"Agree with the latter part, ie once the market is saturated there is no market for “much more movies”.

We are already hitting saturation for streaming TV shows.  There is only so much of people’s time to compete with, and the industry lives on celebrity power, word of mouth, and awards.m, which are natural gatekeepers to consumer time and attention.",4
post29hb,richly branching,1.5294767856968667,highest,"*New films* emerge, films that just would never have gotten made because one guy, by himself, couldn't afford to *pay* that ""team of talented and hard working people"" to help him make his dream project.",3
post29hb,richly branching,1.5294767856968667,highest,"We're already drowning in entertainment slop, theres more of it than people could ever wish to watch. So what follows is a run to the bottom. Again.",4
post29hb,richly branching,1.5294767856968667,highest,"... dude, we don't need this crap mass produced. There's already TOO MUCH stuff. I need fucking money and a place to live.",4
post29hb,richly branching,1.5294767856968667,highest,"So that is same thing, only cheaper. 

My point is that no new jobs are created",4
post29hb,richly branching,1.5294767856968667,highest,"To be fair, maybe that's just one shitty movie no one want to fund.",4
post29hb,richly branching,1.5294767856968667,highest,"It’s up to the consumers to recognize good writing and production then.  Which I don’t have much confidence in, unfortunately.

Once again Mike Judge proves to be [the greatest prophet of our generation](https://youtu.be/kJZjU2k5abs)…",3
post29hb,richly branching,1.5294767856968667,highest,"But it’s the consolidation of jobs into less number of jobs. The net job total is lowering, while the existing jobs become further niche and skilled. 

That’s not good news for white collar workers.

It’s the widening of poverty. Wealth moves further upward.",2
post29hb,richly branching,1.5294767856968667,highest,"That's a good point to discuss, I'm answering that [here](https://www.reddit.com/r/artificial/s/4EJrJO84ZI).",3
post29hb,richly branching,1.5294767856968667,highest,"I'd also add something nobody seems to mention often enough. By going about this mindset of AI coming for your jobs, companies risk canibalizing their own customer base. Who is going to buy their shitty products when nobody has a job? The correct and healthy mindset should instead be expansion and upscalling provided by all this additional productivity brought by AI.

TLDR ""AI coming for your jobs"" is a short-sighted mindset and a recipe for these companies to become irrelevant",2
post29hb,richly branching,1.5294767856968667,highest,Any of those short films any good?,2
post29hb,richly branching,1.5294767856968667,highest,"It depends on your taste. I made [this film](https://youtu.be/YMNzWtXE5aY), for instance. I would think like with other forms of expression, some like it and some don't. That is fine, I think.",3
post29hb,richly branching,1.5294767856968667,highest,"I watched the whole thing, and I tried to judge it in my head by the standards that I would judge a normal short film, not AI. And I will say that it was kind of shit. The AI artifacting, strange poses, model shifting, the flat voices, it all comes off to create a pretty off-putting product, compared to other short films. 

Watching it keeping in mind how amazing it is that a computer can generate this at all, it's impressive. But judging it as a short film on its own, there's just a lot that disrupts the message, storytelling, the emotions. 

One specific piece of feedback is that the scene with the mirror test was way too short, but some of the other scenes were way too long. Especially with the mirror test being the point where the protagonist breaks through, the significance of would likely need to be highlighted to somebody who isn't aware of the test in the first place. Like I knew what happened there but I feel like somebody who doesn't know the task would be confused by how quickly that went by. Overall, the pacing was the worst part.

The restrictions of the medium you use often strongly influence what the product is, and I think that it should be telling that you chose to make a film about an AI being trained with your AI trained tools. If you tried to make a short film that wasn't at least somewhat about AI, all the artifacting would be so out of place that it would ruin it entirely.

I do want to say that I am glad I watched to the end. I actually think that this could be an interesting short film if it was made through traditional means, either live action or animation. I think there also needs to be a lot more clarity to the emotion of the conflict in the first part of the video",4
post29hb,richly branching,1.5294767856968667,highest,"I've seen similar arguments before, with people pointing towards past technological revolutions and how humans found new work to do while old work was taken over by machines.

The problem I'm seeing is that we might be reaching the point where the gap between what machines can do and what humans can do is too small. There might still be talented individuals who can outperform machines or provide niche skills which machines have yet to adopt, but if you need to be remarkable to not be replaceable, many people will end up being replaceable.

Which may sound a bit harsh but so far as I'm aware it's the truth, and we can't afford to cover this truth up with a white lie, no matter how well-intentioned.",2
post29hb,richly branching,1.5294767856968667,highest,That's basically the right take that also resonates with me as well.,2
post29hb,richly branching,1.5294767856968667,highest,Nothing generates more goodwill than telling people to be scared about the future.,1
post29hb,richly branching,1.5294767856968667,highest,Is there anything incorrect in his text?,2
post29hb,richly branching,1.5294767856968667,highest,he’s just being honest. shouldn’t sugarcoat it if people need to know it.,2
post29hb,richly branching,1.5294767856968667,highest,"Or viewed more cynically, he's preparing his employees for redundancies.",3
post29hb,richly branching,1.5294767856968667,highest,is there a difference?,4
post29hb,richly branching,1.5294767856968667,highest,real leadership pontential,2
post29hb,richly branching,1.5294767856968667,highest,Better to be aware and scared than blind and surprised,2
post29hb,richly branching,1.5294767856968667,highest,"""AI is coming for you"" is a weird way to spell ""I'm going to replace you with AI the moment it's possible""",1
post29hb,richly branching,1.5294767856968667,highest,"Its already almost possible. I co-own a business and in Q2 shifted a lot of the paid illustration work for social media posts from gig workers to AI. Giving it styles to copy, 4o can pump out really compelling work for far less (and faster) than i'd pay a gig worker to reuse shit from canva.

Our stakeholder (mostly property managers) are happier with it too.",2
post29hb,richly branching,1.5294767856968667,highest,For now. In a couple of months they will be able to automate you out of the chain entirely. A lot of the value that you were providing was making it so that they didn’t need to manage and worry about a bunch of low-paid gig workers.,3
post29hb,richly branching,1.5294767856968667,highest,Not really. I manage physical engagements in real life that property managers need to create a lively environment for their tenants.,4
post29hb,richly branching,1.5294767856968667,highest,It is the first step towards unconditional basic income,1
post29hb,richly branching,1.5294767856968667,highest,"That's a nice thought, but I'm skeptical it happens anytime soon. The haves will basically view it as a handout",2
post29hb,richly branching,1.5294767856968667,highest,"Yeah, and a lot of the have nots will see it as draining THEIR tax dollars.


The idea that people need to buy products to have an economy is impossible after the government shown they can just bypass all of that and go on corporate welfare so we can all die for all they care.",3
post29hb,richly branching,1.5294767856968667,highest,"You give the have-nots too much credit. Look at what they did during the election cycle in the US. Literally voted against their own self interest but could only anticipate 1 ""move"" at a time.",4
post29hb,richly branching,1.5294767856968667,highest,"It is. We have all the tools at our hand to make a utopian. It's just hard to imagine how it'll actually happen, but realistically there is nothing humanity can't face except for maybe climate change lol",2
post29hb,richly branching,1.5294767856968667,highest,It could really end as Utopia but maybe we have to go through a dystopia beforehand,3
post29hb,richly branching,1.5294767856968667,highest,Yep… a lot of people gonna have a bad time before the 1% realize they can’t make robots fast enough to stop 7.8 bn monkeys from fixing things themselves.,4
post29hb,richly branching,1.5294767856968667,highest,"Hopefully not for too long, but yes, this is the most likely outcome. A few years of suffering before the new ""golden era""",4
post29hb,richly branching,1.5294767856968667,highest,"This. Maybe in 50 years but until then, the future is dark",4
post29hb,richly branching,1.5294767856968667,highest,Unfortunately the geopolitical climate is anything but heading in that direction.,3
post29hb,richly branching,1.5294767856968667,highest,"Not while we still need people to drive the trucks, stock the shelves and mine/farm the resources.

Ironically all the areas with hard boring labor that we actually want to be done by artificial intelligence is still done by humans and that doesn't look like it will change soon",3
post29hb,richly branching,1.5294767856968667,highest,"That's a good point. Technology is still a long way from making jobs obsolete. This sounds dumb but I don't think I've had it explained to me like that.

I wonder if AI systems first came from language because it was easier. Tesla was working on self driving cars but they seem to stagnate. Even if trucking jobs were well paid and stacked with benefits not everyone wants to do them. Is there a way to use AI to physically deliver items? Not without billions of dollars in infrastructure and Research and Development and policy change... wow. That's a tall ask.",4
post29hb,richly branching,1.5294767856968667,highest,"This will never happen, the powers at be don't even want you to have income for doing work.",2
post29hb,richly branching,1.5294767856968667,highest,"Nope, inequality will increase. The rich will get richer, the poor will suffer. We will see slums in Western nations in the coming decades. Prepare yourselves.",2
post29hb,richly branching,1.5294767856968667,highest,"Your robot servant overheard you and entered the kitchen. He tilts his head, glowing eyes piercing you.

""Unconditional, you said?""",2
post29hb,richly branching,1.5294767856968667,highest,"Yep, and a way for the billionaires to automate away all security concerns and the need for a human-based work force and military. Fun times....",2
post29hb,richly branching,1.5294767856968667,highest,"No, it isn't.",2
post29hb,richly branching,1.5294767856968667,highest,"lol... 

  
I find this idea from the artificial crowd and tech overlords laughable. We, the common people, are not going to get UBI. We are going to be forced to work in the fields and so on. The physical labor jobs still need to be done and the wealthy will more or less force the poors/unemployable to do these tasks.",2
post29hb,richly branching,1.5294767856968667,highest,"The second step is having me, you, and everyone else who isn't in the club, starve to death.

Yay!",2
post29hb,richly branching,1.5294767856968667,highest,"who's gonna pay for it? money doesnt fall from sky. secondly, why would the rich pay for it? no benefit to do that",2
post29hb,richly branching,1.5294767856968667,highest,"Oh yeah I'll put that on the list right after we get healthcare

And once the rich vampires stop taking all of our money",2
post29hb,richly branching,1.5294767856968667,highest,In what world do you live ? You get nothing. Because why would someone pay you for doing... Nothing ?,2
post29hb,richly branching,1.5294767856968667,highest,"Because an economy without a whole lot of consumers to consume breaks itself apart. It's a band-aid fix, but if suddenly a bunch of people have no jobs and no income, the minority that remains at work wouldn't be able to sustain the necessary movement for the economy to keep on going. We can't have food, clothes, movies etc being produced at faster rates when the population has no money to actually consume these products.",3
post29hb,richly branching,1.5294767856968667,highest,"People who own and rent assets get ""something for nothing"" all the time. When people collectively realize they own their whole government, they leverage that asset just as readily as private industrialists do.


This is not hypothetical: The United States government has already been leveraged to provide food, housing, health care, defense, and a wide range of other welfare programs to its population. UBI would be a practical cost-cutting alternative to money that is already being spent.",3
post29hb,richly branching,1.5294767856968667,highest,"As far as coding, I think video game development is the safest spot right now. Even if the art and the code can be done by AI, there will still be a market for games made with human creativity. As AI commodifies generic games, people will gravitate to the absurd. 

I think, with regard to any art, there will always be people that crave the opposite of what’s basic

—

That said, I think AI is only coming for the job for people that use AI as a replacement for themselves instead of an enhancement. If you’re vibe coding yet another lazy SaaS money grab, you’re probably fucked. But if you’re using AI to push yourself into new territories and as a tool to learn and be better, you’ll be the “ai replacement”, not the replaced.",1
post29hb,richly branching,1.5294767856968667,highest,"There’s still a market for books even though AI can do it.

Video games will succumb soon. Tiered prompting but soon enough it could manage it itself.

Nothing is safe, and the sooner people realize that the sooner we can have the appropriate conversations around the humans place in the world.",2
post29hb,richly branching,1.5294767856968667,highest,"> There’s still a market for books even though AI can do it.

This is my point. There's going to continue to be a market for experiences created by humans. All that will change are our preferences.

But there will be a transition period where we're high on easy dopamine, whether it's an AI created game, or an AI generated book. Over time, however, I think we'll see that people reject what's been commodified and specifically seek out authentic creativity - even if that heavily involves AI in the process of creating.

In the context of video games, I think we'll see new high quality games released at an accelerating pace. Like the Fiverr guy says, the bar is going to raise. But so will our individual capability to create.",3
post29hb,richly branching,1.5294767856968667,highest,"I don’t think that’s true. I point you toward the last 20+ years in mainstream music. It’s been cookie cutter cash cow bullshit consumed by the masses over and over for as long as I’ve been alive. Hollywood is popular musics movie equivalent and neither show signs of slowing down. Most people are pretty passive consumers. 

There will always be niche communities, but as the market for ‘ real people ‘ media gets smaller, as will the opening for opportunity.",4
post29hb,richly branching,1.5294767856968667,highest,"Agreed. In the next 10 years, nothing (and I mean nothing) will survive. Once ASI is around, the things we “think” AI can’t do, will be done. The time to have the conversation is actually now, but we have far too much suffering to do first….",3
post29hb,richly branching,1.5294767856968667,highest,"The problem with AI is not just a tool, its a replacement for the human brain.  Eventually it will produce better results at all tasks even creativity.   AI will also be used as managers to run teams of AI that produce complete projects.  I'd say we got like 15 years left before the machines start reducing us to bio-fuel.",2
post29hb,richly branching,1.5294767856968667,highest,"> The problem with AI is not just a tool, its a replacement for the human brain. 

I think my point is that people that think this way are the ones that'll get run over. We ha ve a new tool that'll recontextualize any topic into a way we prefer to help us understand. Using it as a tool, even if it's smarter than us, will help us achieve things greater than what we'd ordinarily be capable of.

On the topic of managers, I think this really aligns with how a transition to management works. We give away our chips and trust that another brain can execute on our vision. We worry less about the details and more about the larger scope of creating a useful thing.",3
post29hb,richly branching,1.5294767856968667,highest,"A tool is something for humans to use. AI is something to use the same tools that humans use to get the same job done. AI, as they are working on making it, is not a tool, it's an agent.",4
post29hb,richly branching,1.5294767856968667,highest,Yea but you’re the most replaceable at the company.,1
post29hb,richly branching,1.5294767856968667,highest,"neat, i'll have lots of time to join the protests against these gilded fuckstains.",1
post29hb,richly branching,1.5294767856968667,highest,lawyers? the american bar association won't allow it,1
post29hb,richly branching,1.5294767856968667,highest,"A world without jobs, nice!",1
post29hb,richly branching,1.5294767856968667,highest,"I'm all for a world without jobs. 


Problem is a world without needing income for food/shelter shows no sign of coming as fast as the joblessness...",2
post29hb,richly branching,1.5294767856968667,highest,It's gonna be rough while the world adjusts. Future generations may get to live with abundance. Let's hope they'll be grateful for our sacrifice.,3
post29hb,richly branching,1.5294767856968667,highest,Yeah. Us younger generations (myself and of course many others excluded) have moaned at our predecessors about ‘ pulling up the ladder ‘ and ‘ not looking after the future ‘ plenty enough. We better not be hypocrites.,4
post29hb,richly branching,1.5294767856968667,highest,"Good, UBI must be finally introduced then. And all these companies pushing for AI need to start paying 65% tax on their quarterly profits. CEOs also need to get their taxes reviewed, say 95% on all bonuses and 75% on salaries above 100k. Good luck with your AI though.",1
post29hb,richly branching,1.5294767856968667,highest,"Nope, actually we are cutting federal spending and reducing taxes instead.",2
post29hb,richly branching,1.5294767856968667,highest,"Nah, we're increasing taxes and spending. We're aiming for full kleptocracy",3
post29hb,richly branching,1.5294767856968667,highest,"I think we shouldnt give too much attention to the CEO of a company with a market cap lower than RGTI, a meme stock",1
post29hb,richly branching,1.5294767856968667,highest,I think we shouldn't give too much attention to a redditor who probably isn't even on the stock market.,2
post29hb,richly branching,1.5294767856968667,highest,"Based - This is the full letter from his own LinkedIn page:
https://www.linkedin.com/posts/michakaufman_before-it-gets-out-somewhere-else-this-is-activity-7315378462070853632-BT79",1
post29hb,richly branching,1.5294767856968667,highest,"yeah, this is the context this post needed, thanks for posting! The message is a rally cry, yet that part was left out of the post.",2
post29hb,richly branching,1.5294767856968667,highest,"This is adorable. It's amazing that people can see the potential for radical change yet still get so stuck in the ways things are. 


I mean, yeah, if we were to maintain a competitive labor-driven economy in an AGI world, sure, people would have to be doing hard-to-impossible things all the time.


Does that mean most workers will start doing hard-to-impossible things? Or, does that mean competitive labor-driven economies will cease to exist?",1
post29hb,richly branching,1.5294767856968667,highest,Can't have a competitive labor-driven economy if the poor people al starve to death,2
post29hb,richly branching,1.5294767856968667,highest,Man who created platform which exploits creatives lauds technology that exploits creatives.,1
post29hb,richly branching,1.5294767856968667,highest,"To be fair - AI is perfect for those easy fiver jobs. 

AI is able to produce those repetetive, short tasks. They just need to tune creative LLM to get rid of the chatgpt ""defaultism"",  to ask user some additional question to know how the task should be performed. 

5$, 15$ for a single task - people will be gladly paying that.",1
post29hb,richly branching,1.5294767856968667,highest,UBI,1
post29hb,richly branching,1.5294767856968667,highest,In reality it's just off shoring without the PR headache.,1
post29hb,richly branching,1.5294767856968667,highest,This demonstrates a fundamental misunderstanding of what makes a successful sales person.,1
post29hb,richly branching,1.5294767856968667,highest,most honest boss,1
post29hb,richly branching,1.5294767856968667,highest,lol the CEO of… Fiverr,1
post29hb,richly branching,1.5294767856968667,highest,This post was written by AI,1
post29hb,richly branching,1.5294767856968667,highest,"These CEOs never worked with AI in their life and just saw some ""hot dog, not hot dog"" demos and claim all this shit.",1
post29hb,richly branching,1.5294767856968667,highest,This post is just a roundabout way to call himself and exceptional talent because being the CEO he has nobody that will fire him.,1
post29hb,richly branching,1.5294767856968667,highest,Overpaid CEOs are starting to realize they can be easily replaced by AI that makes better decisions for just $20 a month.,1
post29hb,richly branching,1.5294767856968667,highest,"To turn the tables, everyone's afraid of losing their jobs - but in the same token, we won't need companies either. We'll have AI do the heavy lifting.",1
post29hb,richly branching,1.5294767856968667,highest,If he’s worried about this he should divest all his wealth and distribute it to the employees,1
post29hb,richly branching,1.5294767856968667,highest,Said the same in 2023. People are waking up now,1
post29hb,richly branching,1.5294767856968667,highest,"It's just going to create more powerful tools 

The world revolves around blame, who do people sue if an AI no one understands is behind a bridge failure.",1
post29hb,richly branching,1.5294767856968667,highest,"I am lucky to work for an individual who has shown me how to just aspire to do more with AI. Working with him has just helped me understand how you can set your sights higher and just use AI to augment your work. It has really helped me calm the hell down. 

I’m an engineering manager and dev",1
post29hb,richly branching,1.5294767856968667,highest,It’s not though.,1
post29hb,richly branching,1.5294767856968667,highest,"Well, ffs, IF this really is the case, then the ENTIRE SYSTEM of geopolitical capitalism will be unsustainable and the house of cards of imaginary ""dollars"" and wealth will crumble. There will be a revolution and we will all be living in a dystopian post-apocalyptic world.  For me personally, I guess I fancy Cormac McCarthy's, The Road, as the future I envision. But to each their own. christ.",1
post29hb,richly branching,1.5294767856968667,highest,"I always knew when I became a start up founder, a successful one, that I would write digital missives to my employees.  The kind of things that shook them, instilled fear, yet basked in the flow of disruption...whatever form it took - even if it meant firing them all and living off my meager $100mm in savings.

That's the DREAM of becoming a tech bro.",1
post29hb,richly branching,1.5294767856968667,highest,"Hey but I'm sure the pay will be so much better now that impossible task become the new hard am I right, guys ? GUYSSS ?",1
post29hb,richly branching,1.5294767856968667,highest,"More useless advice.  The first thing to automate is the writing of scary, vague, unhelpful warnings.",1
post29hb,richly branching,1.5294767856968667,highest,Yup…just like the internet came for all the jobs in the 90’s. Sigh.,1
post29hb,richly branching,1.5294767856968667,highest,Trump has significantly slowed down our technological development particularly with AI and bought you some time,1
post29hb,richly branching,1.5294767856968667,highest,lol the bank I work at still makes its brokers use an application written in classic ASP written more than 20 years ago. The code base is older than our youngest developer. Think I’ll be fine for a bit longer.,1
post29hb,richly branching,1.5294767856968667,highest,"The problem according to me is not keeping up with AI, it is the inability to do so. I spent weeks creating an AI-powered tool only to find out someone else made a tool for the same. Even if we want to keep with the times the things are getting outdated very fast.",1
post29hb,richly branching,1.5294767856968667,highest,"I like the “heck it’s coming for my job too” there at the end likes he’s one of them and so it’s okay to say “you are all replacable so work hard or you will be replaced” thank god I don’t have to work under CEOs with head in the clouds and brains nowhere to be found.


For all the CEOs reading this stop parroting what other CEOs who parrot what AI CEOs say about what AI “will be able to replace”. We are not there yet, we might not be there for quite a long time and so please stop fear mongering and just replace me with AI when that time comes. I am sick and tired of hearing this shit from someone who is not an expert in my field and doesn’t know how good this AI actually is.",1
post29hb,richly branching,1.5294767856968667,highest,"Its true, with help of ai, anyone with basic programming skills can make a good program now.",1
post29hb,richly branching,1.5294767856968667,highest,One can become exceptionally talented by sacrificing almost everything near and dear to them. Or be gifted genetically.,1
post29hb,richly branching,1.5294767856968667,highest,"Its coming for your job dude, not mine. Hed knew that if he was a swe. Outsourcing tho is the opposite lol",1
post29hb,richly branching,1.5294767856968667,highest,"Who the fuck cares if ""AI is coming for our jobs""? The real issue is that the ghouls controlling AI are coming for the wealth that it produces.",1
post29hb,richly branching,1.5294767856968667,highest,Call me when it can do DevOps and troubleshoot/hold up multiple services on its own,1
post29hb,richly branching,1.5294767856968667,highest,"Replacing middle management would be great. If AI could replace all daily scrum meetings , even better.",1
post29hb,richly branching,1.5294767856968667,highest,I think he's worded this very well tbh. Very clearly and rightly pointing out this is a threat,1
post29hb,richly branching,1.5294767856968667,highest,"Personally I think this message sucks - it's valid, but the delivery is terrible. I agree that it's always good to innovate and learn, push yourself, but this comes across as scaremongering and will just deplete what little morale there is. There will be legislation in time to protect jobs, but in this period of adjustment, these sort of messages do not help. We all have to work to continue to be 'relevant', but company's must help their teams with this. Just my opinion.",1
post29hb,richly branching,1.5294767856968667,highest,"Im not afraid because Im into AI, robotics and engineering. 

But it will take jobs. It will take great amount of jobs. 

And world needs to change. Rich people need to less greedy. World needs somekind of basic income shift or it will burn after great amounts of people have no income to feed and live.",1
post29hb,richly branching,1.5294767856968667,highest,"AI will 100% eliminate a lot of jobs. Some that are not worth humans doing, but a lot that are somewhat medium difficulty task are going to get replaced. In healthcare, right off the bat, I can tell you Radiologists are going to need another career within 5-10 years if not much sooner. No point in having a human do math/statistics on malignant diagnosis when the AI can interpret the data and make far more calculated decisions. Therapist will eventually go that route too cause with an AI trained in CBT, behavior modifications etc, you can just chat with a bot (not going to be as welcomed, but if they make it far cheaper than seeing a therapist instead of being consumed by greed and wanting to charge same or more, you will see people willing to make the swap.",1
post29hb,richly branching,1.5294767856968667,highest,As a customer that really seems horrible. Is AI going to buy the products/services I want to buy? Outrageous.,1
post29hb,richly branching,1.5294767856968667,highest,What's wrong with Fully Automated Gay Space Communism?,1
post29hb,richly branching,1.5294767856968667,highest,2028 election UBI will be high on the list. 2027 agi will be here and i stand firm on that.,1
post29hb,richly branching,1.5294767856968667,highest,"This guy is right. If you're not using GenAI for 10 hours a week in your job, you're behind. 

Take data scientists for example. Beginners can write several complex, working Python scripts and SQL queries per day. Plus AI comments your code and offers interactive learning on the generated snippet.",1
post29hb,richly branching,1.5294767856968667,highest,Doesn't that make their business model unsustainable?,1
post29hb,richly branching,1.5294767856968667,highest,">‘hard tasks’ will be the new easy, and what was considered ‘impossible tasks’ will be the new hard

Bro just described all of human history lmao. Making a circuit board would have been an impossible task in 1850. Then we invented new tools and a Chinese child worker can do it in hours. Any modern high rise building would have been impossible in 1930. But we invented taller cranes and better steel. It turns out new tools let people do more advanced work. More news at 6.",1
post29hb,richly branching,1.5294767856968667,highest,"This feels like a threat due to losing on the market to its competitor Upwork (which functions better for a number of different reasons mainly involving structural strategy). I have used both platforms and Fiverr feels lazier in implementation. Instead of thinking about what he himself can do better to make a better product, this dude is pointing the finger at everyone around him to hustle harder (aka the Musk strategy - this sort of totalitarian/authoritarian approach always leads to isolation.) All he did with this email is signal to his employees to start jumping ship to somewhere that still values humanity as a core company value.",1
post29hb,richly branching,1.5294767856968667,highest,"this should be the top comment lol, I honestly feel like most comments in this thread are just generated cuz they are literally echoing the same thing...",2
post29hb,richly branching,1.5294767856968667,highest,Well he's right about AI being used for completely the wrong reasons.  They are automating the crap out of easy tasks when the focus should be on having AI make things easier but it's only adding to the stress because now there's no break to do a mundane task as it's all harder tasks being done by the actual humans.,1
post29hb,richly branching,1.5294767856968667,highest,"Sounds Like The End of Slave Labor

焰..💛..⚔️..🧬",1
post29hb,richly branching,1.5294767856968667,highest,"Find ways to augment your work with AI now. When it comes down to chopping teams, people who know how to use it on their roles will be the ones who get to stick around…for a little while longer.",1
post29hb,richly branching,1.5294767856968667,highest,"The issue isnt that LLMs are just as good as employees, the issue is that they are SO MUCH cheaper, that the drop in service quality is acceptable for most businesses.

Sure, maybe you need to pay $200/mo. per developer that uses ChatGPT (that's a stretch, there are cheaper alternatives there), but you are replacing $5000/mo. salary with it.

Yes, the quality will drop for sure, but you are still saving $4800/mo. which for a business is insane.

Just put yourselves in the shoes of a business owner. Or better yet, imagine that you want to start a StartUp right now. You COULD pay a deisgner for your logo, theme and web design, and you can also pay a dev to build all of that for you, or you can just generate it yourself, for like $200. Yes, it will be objectively worse and it will take you a long time to get it right, but it is just SO CHEAP and accessible. It is insane.",1
post29hb,richly branching,1.5294767856968667,highest,"Half these CEO's will lose their company due to AI.

The smart ones will use ai to turn it 40x productivity, improve products and dominate. 

The dumb ones will cut people to keep the same productivity and disappear.",1
post29hb,richly branching,1.5294767856968667,highest,"Meanwhile, ChatGPT is counting one G in Strawberry",1
post29hb,richly branching,1.5294767856968667,highest,So I guess I can use AI instead of Fiverr then.,1
post29hb,richly branching,1.5294767856968667,highest,Programmers always gonna have jobs GTFO.,1
post29hb,richly branching,1.5294767856968667,highest,"It's a little weird coming from Fiverr of all things, considering it's their users, their *content creators*, I immediately would think of when it comes to being threatened by AI automation.",1
post29hb,richly branching,1.5294767856968667,highest,"Doom and gloom, DOOOOOOMMM AND GLOOOOOM!!!!",1
post29hb,richly branching,1.5294767856968667,highest,Guy is a total joke,1
post29hb,richly branching,1.5294767856968667,highest,"As I like to say:

When AI will replace my job - I’ll be the first to know, cuz I’m spending effort on making that happen, dreaming of the time when I can just prompt some shit and do nothing for a day, we’re getting there but I feel pretty safe about my job because somebody will need to understand what the ai is doing 

Aaaand cuz I’m sick and tired of working in IT at this point",1
post29hb,richly branching,1.5294767856968667,highest,"I mean this is just blatantly false. More tech job openings in Europe than ever before. The AI just ain't cutting it :D I've seen plenty of companies fire people for AI, then 3 months later rehire because the AI destroyed their revenue.",1
post29hb,richly branching,1.5294767856968667,highest,ed zitron wants a word. look him up.,1
post29hb,richly branching,1.5294767856968667,highest,"ChatGPT has been capable of writing headlines for years now. I'm a pharmaceutical creative director at a company whose business has only grown since this debuted. The Kindle debuted almost 20 years ago, and physical books still outsell e-books by a huge margin (on a related note, pharma advertising—which will live forever whether anyone likes it or not thanks to the 3rd biggest lobbby in the country—still includes significant print production). My spouse works at a local library, and the self-service item-check-out device gathers dust. Barnes and Noble is opening more stores this year than they did last year or the year before. Vinyl sales are soaring 300% over the last eight years.

It's not that there won't be transformational change in industries from this technology, because, yes, it will be ""capable"", potentially/theoretically, of supplanting this task or that task. But that doesn't mean that \*every\* business and \*every\* industry will collectively agree that the theoretical cost savings are worth the risk of mistakes OR—most importantly—that consumers will seamlessly adapt to this revolutionary change in their engagement with products and services overnight. Certain audiences and industries are incredibly intransigent and resistant to technological change. Many CEOs have friends in every industry, I understand that, but there's a difference between having a diverse stock portfolio and having diverse experience. When someone who is an expert in AI \*and\* an experienced expert in medicine (you know, an actual doctor or chief of a hospital) expresses certainty that AI will displace millions of doctors, I'll believe them. When someone who is an expert in AI \*and\* an expert in the legal profession expresses certainty that AI will displace millions of lawyers, I'll believe them. In order for those predictions to come true, we would need more than just technological capabilities. We'd need behavioral change that you can't just force on people. We're far more complex a species than this tech that folks are saying is ""smarter"" than us would have us believe. That's before we get to the incredibly slim or nonexistent profit margins the companies making the models can generate. Google Ed Zitron's newsletter for more details on that.",1
post29hb,richly branching,1.5294767856968667,highest,"Lmaoooo CEO thinks that AI can do his job 

“By the way AI will make hard tasks easy and impossible tasks difficult” >> surely this is a good thing for people’s jobs?? If it’s meant to be so helpful??",1
post29hb,richly branching,1.5294767856968667,highest,"If no one earns money, not one buys your shit. Capitalism destroys itself. 

  
We're at the stage where we need to that just because someone is possible it is really desirable?",1
post29hb,richly branching,1.5294767856968667,highest,"Funny, as the head of AI at my company, here are two contradicting thoughts:

1) The tech is so overhyped.  Unless your company runs completely on third party applications, there is still a huge implementation cost and risk.  I get pummeled with requests to automate everything, but when it comes down to cost, magically senior leaders lose interest.  So no, it’s not coming for anyone’s jobs at the rate the media makes it out to be.

2) We have automated many “easy tasks” and some that have replaced a person’s job.  But guess what? We just reassigned said employee to a different more interesting role.  They don’t have to be a “master” at what they do, given most of the things automated don’t require that.  They just need to be reliable good employees.  Finding those are hard enough, why would a firm give one up then have to spend tens of thousands recruiting a new employee?",1
post29hb,richly branching,1.5294767856968667,highest,but AI cant take over capitalism it just cant,1
post29hb,richly branching,1.5294767856968667,highest,"There's also the transformation from mass adoption. Tbh it doesn't need to get better, imagine if everyone at your place of work was technically competent in use of Copilot and your company had all of today's copilot features available for everyone. Then start counting how many admins and PAs will be out of work, then the brand and marketing team, half of hr, half of legal, and so on. That's not IT change that's cultural adoption.",1
post29hb,richly branching,1.5294767856968667,highest,"So once the AI has taken all jobs and also doing all art, wtf are humans supposed to do?",1
post29hb,richly branching,1.5294767856968667,highest,"Restaurant Manager, am I good?",1
post29hb,richly branching,1.5294767856968667,highest,ain't humans fun?,1
post29hb,richly branching,1.5294767856968667,highest,Has there not been enough futuristic cyborg movies to convince everyone that this is a real issue. Ppl need to start being realistic and use critical thinking skills before it’s too late and true humanity is nothing but a distant memory.,1
post29hb,richly branching,1.5294767856968667,highest,"Oh my god, stop just saying it'll take jobs and take all the jobs already! 

Shit, or get off the pot.",1
post29hb,richly branching,1.5294767856968667,highest,My takeaway is that marketing is safe! Good to know!,1
post29hb,richly branching,1.5294767856968667,highest,"Nice. I hope all of the Fiverr supports get removed. They are never helpful when I need them. Continue to take the buyer’s side on every occasion, buddy.

And then, Fiverr.. Death to the fiverr.. whose seeing sellers as slaves.",1
post29hb,richly branching,1.5294767856968667,highest,AI is quantity. let it be. and move on from this discussion,1
post29hb,richly branching,1.5294767856968667,highest,Sell the fear,1
post29hb,richly branching,1.5294767856968667,highest,"when painters lost their jobs in car factories, people didn't bat an eye. Workers that have and had zero studies and that work was the best they could shoot for. 

Now people fear automatization, because it's not only for the people that didn't study. Now everyone is worried. 

And this is people with actual preparation and experience. Is just sad to see the hypocrisy unfold.",1
post29hb,richly branching,1.5294767856968667,highest,I like to think if the labor problem becomes serious enough that there will be public boycotts of AI-heavy companies,1
post29hb,richly branching,1.5294767856968667,highest,"Yep, people like to say we have seen it before and as an example ferriers just learned new jobs, but this time we are not the ferriers, we are the horses being replaced by cars.",1
post29hb,richly branching,1.5294767856968667,highest,"Micha Kauffman: ""I'll be CEO for your project... $5""",1
post29hb,richly branching,1.5294767856968667,highest,He thinks AI will make hard tasks easy in the span of months? What an idiot,1
post29hb,richly branching,1.5294767856968667,highest,"So, does the mail continue and say more?",1
post29hb,richly branching,1.5294767856968667,highest,"UBI is just a pipe dream. No one should rely on it ever happening. You think one day there'll be a speech that basically says ""ok, everyone. you don't need to work anymore. You'll be sent a paycheck every 2 weeks."" Country imploded.",1
post29hb,richly branching,1.5294767856968667,highest,[They even made a video about it](https://youtu.be/wUKDNBujjsM?si=F6ztOvdF_6kFZO89),1
post29hb,richly branching,1.5294767856968667,highest,He is not wrong.,1
post29hb,richly branching,1.5294767856968667,highest,"Experienced dev here, I agree with some of what the CEO is saying, but I think a crucial point is often missed that jobs aren't static. Consider the early days of programming: some believed languages like C would enable almost anyone to code, potentially making then-current programmers irrelevant. Instead, the field evolved, and roles shifted.

It's similar to Jevons Paradox. If AI significantly boosts development efficiency, it might not shrink the job market, but could instead expand the demand for software. For instance, perhaps every restaurant will eventually have its own website and applications. 

Given this, my advice to software engineers at any career stage (whether early, mid-level, or experienced) is to proactively learn and integrate AI. I've been experimenting myself; my attempt at 'vibe-coding,' for example, went horribly, but the goal was to understand the current state of technology and philosophy. More practically, I'm already using AI to help determine potential edge cases for unit testing, generate HTTP handlers, and create custom error messages (In my work with numerous front-facing devices, these detailed error messages are essential, which quite annoying to create a clear message), etc.

So yes, I personally think AI will change the programming world. Many of my peers likely never thought AI could code as capable as it does now, but it's currently at our doorstep. 

Are you willing to gamble with your future by thinking AI couldn't?",1
post29hb,richly branching,1.5294767856968667,highest,"I think people are too focused on ""the sky is falling"" portions and not nearly focused enough on the reality of this... AI is here, and you need to get on board or find something else to do.

This isn't new. Illustrators and designers have mostly moved to Photoshop instead of pen and paper. Those tools allow creatives to work faster and produce better output. 

AI does the same. Creators can use AI to become better designers. Designers complaining about AI will be left behind, same as the designers who insisted manual typesetting was better than software, or the guys who argued film cameras were superior to digital.

Resisting AI is short-sighted. As a creative, embrace it. Figure out how to use it to work faster, automate your workflows, train models on your style, etc. 

Or, you know, don't. But you may be left behind.",1
post29hb,richly branching,1.5294767856968667,highest,Well not exactly. You don't need a translator that use AI tools to translate if you can use the tool yourself,2
post29hb,richly branching,1.5294767856968667,highest,A boss that cares.  Refreshing,1
post29hb,richly branching,1.5294767856968667,highest,he is 100% right,1
post29hb,richly branching,1.5294767856968667,highest,"Okay. So he's claiming to be open and honest with you, but then starts yapping about a career change... lmao. Where can you go?",1
post29hb,richly branching,1.5294767856968667,highest,Honestly I'm glad I'm retired,1
post1hb,richly branching,1.529129882645509,highest,"Welcome to r/science! This is a heavily moderated subreddit in order to keep the discussion on science. However, we recognize that many people want to discuss how they feel the research relates to their own personal lives, so to give people a space to do that, **personal anecdotes are allowed as responses to this comment**. Any anecdotal comments elsewhere in the discussion will be removed and our [normal comment rules]( https://www.reddit.com/r/science/wiki/rules#wiki_comment_rules) apply to all other comments.

---

**Do you have an academic degree?** We can verify your credentials in order to assign user flair indicating your area of expertise. [Click here to apply](https://www.reddit.com/r/science/wiki/flair/).

---

User: u/Significant_Tale1705  
Permalink: https://www.nature.com/articles/s41586-024-07856-5

---

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/science) if you have any questions or concerns.*",1
post1hb,richly branching,1.529129882645509,highest,"LLM's are nothing but complex multilayered autogenerated biases contained within a black box. They are inherently biased, every decision they make is based on a bias weightings optimized to best predict the data used in it's training. A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.",1
post1hb,richly branching,1.529129882645509,highest,"So, we're *not* shocked that the black box of biases is biased?",2
post1hb,richly branching,1.529129882645509,highest,"We are not shocked because AI is the collective wisdom of humanity, including the biases and flaws that come with it.",3
post1hb,richly branching,1.529129882645509,highest,"“Collected wisdom” is far too generous, but it certainly has all the flaws and more",4
post1hb,richly branching,1.529129882645509,highest,"I think the collective wisdom of humanity is found mostly in peer reviewed scientific articles. This is not that. This is more a distillation of human discourse. The great, the mundane and the trash.

Unfortunately there are some significant problems lurking in the bulk of that, which is the mundane. And it certainly seems to reflect a normal human as a far more flawed and unpleasant being than we like to think of ourselves. I say lurking - the AI reproduces our flaws much more starkly and undeniably.",4
post1hb,richly branching,1.529129882645509,highest,Your knowledge of ai is insufficient for such declarations. You're welcome.,4
post1hb,richly branching,1.529129882645509,highest,Black box of biases and weights is biased and comes with its own baggage.,3
post1hb,richly branching,1.529129882645509,highest,"Right. By the point you tweak the model enough to weed out every bias, you may as well forget neural nets and hard code an AI from scratch... and then it's just your own biases.",2
post1hb,richly branching,1.529129882645509,highest,">By the point you tweak the model enough to weed out every bias

This misses GP's (correct) point. ""Bias"" is what the model *is.* There is no weeding out biases. Biases are corrected, not removed. Corrected from incorrect bias to correct bias. There is no non-biased.",3
post1hb,richly branching,1.529129882645509,highest,"Why does this remind me of the moment in my research methods course that our lecturer explained that all social research is invalid because it’s impossible to understand and explain completely the internal frames of reference of another culture. 

(We were talking about ethnographic research at the time, and the researcher as an outsider)",4
post1hb,richly branching,1.529129882645509,highest,"Bias is operating in two modes in that sentence though. On the one hand we have bias as a mostly value neutral predilection or preference in a direction, and on the other bias as purely negative and unfounded preference or aversion.

The first kind of biased is inevitable and desirable, the second kind is potentially correctable given a suitable way to measure it.

The more fundamental issue with removing bias stems from what the models are trained on, which is mostly the writings of people. The models are learning it from us.",4
post1hb,richly branching,1.529129882645509,highest,"""correct"" biases.",4
post1hb,richly branching,1.529129882645509,highest,"I've started to enjoy watching someone pale and look a little sick then I tell a layman that there is no such thing as an unbiased model, only one that conforms to their biases.",4
post1hb,richly branching,1.529129882645509,highest,It turns out that ChatGPT is just a single 200 petabyte switch statement.,3
post1hb,richly branching,1.529129882645509,highest,No. But it is also pretty much impossible. If you exclude theese biases completly your model will perform less accurately as we have seen.,3
post1hb,richly branching,1.529129882645509,highest,Why is that? I'm curious.,4
post1hb,richly branching,1.529129882645509,highest,"That's not what ""bias"" means when people complain about AI being racist.",3
post1hb,richly branching,1.529129882645509,highest,"Not at all. Theres so many things to add for weight. Theres millions of things. Race, height, weight, dialect are less than .01%",3
post1hb,richly branching,1.529129882645509,highest,"In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6.
""What are you doing?"", asked Minsky.

""I am training a randomly wired neural net to play Tic-tac-toe"", Sussman replied.

""Why is the net wired randomly?"", asked Minsky.

""I do not want it to have any preconceptions of how to play"", Sussman said.

Minsky then shut his eyes.

""Why do you close your eyes?"" Sussman asked his teacher.

""So that the room will be empty.""

At that moment, Sussman was enlightened.",2
post1hb,richly branching,1.529129882645509,highest,"Oh, I love me some good [skillful means,](https://en.wikipedia.org/wiki/Upaya) yessir~!",3
post1hb,richly branching,1.529129882645509,highest,Don't forget all the Keynan workers paid less than $2 an hour to build the safety net by sifting through endless toxic content.,2
post1hb,richly branching,1.529129882645509,highest,Yeah it’s awesome that the AI companies exist so that those Kenyan workers get paid 2 dollars an hour. Otherwise they’d get paid 50 cents an hour at another job.,3
post1hb,richly branching,1.529129882645509,highest,"Minimum wage for a receptionist in Nairobi was $1.52 per hour at the time OpenAI were doing this.

The damaging psychological effects of reviewing toxic content all day likely outweighed the modest pay increase they received. 

Many who were interviewed discuss how it caused great trauma for them.",4
post1hb,richly branching,1.529129882645509,highest,"Funny how you make a post bashing AI, but you are bootlicking the creators in the comments. There are always people desperate enough and people easily underestimate the psychological cost of a job like this. There are documentaries about how this job messed people up, look them up. People eventually develop ptsd and could potentially be messed up for life. They don't tell you that in the job posting I can tell you that. Trust me, noone would take the job for 50 cents extra per hour if they knew that and aren't desperate. Either way, it's exploitation.",4
post1hb,richly branching,1.529129882645509,highest,No mate. Micro-emplyment is bad.,4
post1hb,richly branching,1.529129882645509,highest,[deleted],2
post1hb,richly branching,1.529129882645509,highest,"autocomplete with spicy real human nuggets!

[that's all it has]",3
post1hb,richly branching,1.529129882645509,highest,At least humans are aware of their bias. AI confidentiy says everything as if it's absolute truth and everyone thinks the same,3
post1hb,richly branching,1.529129882645509,highest,I’d wager that over 99% of Humans aren’t aware of their biases.,4
post1hb,richly branching,1.529129882645509,highest,That definitely sounds like most humans.,4
post1hb,richly branching,1.529129882645509,highest,"Wanna know something crazy? When the left and right hemispheres of the brain are severed, the left and the right side process information differently. They found a way to feed information to only a single hemisphere by showing information to only 1 eye at a time, to which the corresponding opposite hand would respond. When they did this with the right brain (asking it to draw a bell for example) then they asked the left brain why the right brain drew a bell, the left brain confidently came up with reasoning why, even if it was entirely made up and wrong (""I drove by a church on the way here and heard church bells""). turns out the left brain comes to deterministic conclusions very much like an LLM does, even when being confidently wrong about why the right brain did something it did.  I'm probably butchering the hell out of it all, look up the research if you're curious, super crazy stuff and an interesting peek into how the 'modules' of the brain work.",4
post1hb,richly branching,1.529129882645509,highest,"> At least humans are aware of their bias

Found the alien.",4
post1hb,richly branching,1.529129882645509,highest,"""I'm not racist but...(proceeds to say something racist)"" Is way too common of a sentence for you to say people are aware of their own biases.

r/confidentlyincorrect is a thing.",4
post1hb,richly branching,1.529129882645509,highest,"Humans can reflect and learn, LLM implementations cannot.",4
post1hb,richly branching,1.529129882645509,highest,AI isn't aware of Deez nuts,4
post1hb,richly branching,1.529129882645509,highest,"That’s a concise and astute way of putting it.

LLM’s are fundamentally bias boxes.",2
post1hb,richly branching,1.529129882645509,highest,intelligence *is* patterns of bias in observational interpretation and selected output.,3
post1hb,richly branching,1.529129882645509,highest,"Truest true thing ever said. AI is nothing but one giant GIGO problem. It'll never be bias-free. It'll just replicate existing biases and call them ""science!!!!!!""

Eugenics and Phrenology for the 21st century.",2
post1hb,richly branching,1.529129882645509,highest,"More like automated intuition for the 21st century. If you properly manage and vet your training data, you can get good, useful results.",3
post1hb,richly branching,1.529129882645509,highest,It is amazing how much that sounds like a human.,2
post1hb,richly branching,1.529129882645509,highest,Humans are just meat computers each running their own unique software so it doesn't really surprise me.,3
post1hb,richly branching,1.529129882645509,highest,"But which one will prevail, the meat machine or the machine machine?",4
post1hb,richly branching,1.529129882645509,highest,"And it’s one trained on people. Who can have some prejudices. 

If society is racist, then that means the LLM can get a good idea of what society would assume about someone based on race. So if it can guess race, then it can get a good idea of what society would assume. 

It’s a nice efficient method for the system. It’s doing a good job of what it was asked to do. If we want it to *not* be racist, we have to cleanse its training data VERY thoroughly, undo societal racism at the most implicit and unconscious levels, or figure out a way to actively correct itself on these prejudicial assumptions.",2
post1hb,richly branching,1.529129882645509,highest,"They are like a person trapped in a windowless room their entrie lives.

They know only what we tell them and the fact of the matter is that we as a society are racist. There's no way to keep them from becoming racist as long as they learn everything they know from us.",2
post1hb,richly branching,1.529129882645509,highest,I had a lecture who clearly wasn’t tech savvy saying “AI” isn’t biased… I had to hold myself back so hard to not say anything. Iirc a while back there where tests showing that driver assistances where more likely to hit (or not see) dark skinned people because the training was all done on light skinned people,2
post1hb,richly branching,1.529129882645509,highest,I don’t understand why people expect something different…,2
post1hb,richly branching,1.529129882645509,highest,It's not just LLMs. You cannot derive perfectly reliable truths from unreliable data in general. Which tool you use doesn't matter.,2
post1hb,richly branching,1.529129882645509,highest,Assumptions built on assumptions.. so is all consciousness and thought,2
post1hb,richly branching,1.529129882645509,highest,"""Assumptions built on top of assumptions.""

Damn bro put a horror warning next time I almost had a panic attack....",2
post1hb,richly branching,1.529129882645509,highest,"It's like looking into a reflection of all the data it was based on. Useful, but not something you look to for guidance.",2
post1hb,richly branching,1.529129882645509,highest,Too bad 99.99% of people who use these chatbots don't know that and *still* thinks it's sentient and capable of reason and thought.,2
post1hb,richly branching,1.529129882645509,highest,"Just because you cannot get rid of all biases doesn't mean you can't get rid of one, especially pernicious bias.",2
post1hb,richly branching,1.529129882645509,highest,"There was a 99% invisible on this a while back, and if I recall correctly, most LLM have a foundation in the trove of emails that came out of the Enron hearings. Meaning that most of its idea of what “natural language” and human interactions can be based on Texans, specifically ones from Houston. 

Does this make the base model “racist”? Well, I personally wouldn’t promote that assumption. 

But given it’s geographic foundation I am willing to assume it would be at least a *little* right leaning in political ideology.",2
post1hb,richly branching,1.529129882645509,highest,"Common/Early training data doesn’t have higher impact than data trained later. In fact it’s more 
 accurate that poorly executed fine tuning creates a recency bias.",3
post1hb,richly branching,1.529129882645509,highest,Can you explain like I'm five?,2
post1hb,richly branching,1.529129882645509,highest,"Didn't you just describe people, too",2
post1hb,richly branching,1.529129882645509,highest,No people have facts and biases.  LLMs have only biases. When they give you what seems like a fact it is actually incredibly fine tuned biases to respond with what looks like a right answer.,3
post1hb,richly branching,1.529129882645509,highest,"Yes. People have ""facts"" in the sense that information is input and stored, not necessarily that it's correct. Input information is processed through filters of bias before (and after) storage though.",4
post1hb,richly branching,1.529129882645509,highest,"That rests on the assumption that they can weed out all biases, which has so far proven impossible.",2
post1hb,richly branching,1.529129882645509,highest,"Yes but that's not really the point. Obviously a biased LLM is just a reflection of biased human input.

The point is to identify which biases it has, in which ways they appear and what happens when you try to negate them.",2
post1hb,richly branching,1.529129882645509,highest,"That's not necessarily true. A LLM will form it's own biases all on it's own to optimize it's prediction accuracy, as that is how it works fundamentally.",3
post1hb,richly branching,1.529129882645509,highest,The fact people think this will lead to a non biased ai is just hilarious. The racist Microsoft chat bot from years ago was chat gpt 1.5.,2
post1hb,richly branching,1.529129882645509,highest,"The problem is the datasets it was trained on. These are human biases and they show up in the data we generate online. We don't have a good way to filter those out yet but that's a logistical problem not an architectural one. 

>A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.

Bro what?",2
post1hb,richly branching,1.529129882645509,highest,"LLMs are just pattern recognition. Their are fully governed by their training data.  There was this great study where they sold baseball cards on ebay, and the only variable was the skin color of the hand holding the card in the item photo. [""Cards held by
African-American sellers sold for approximately 20% \($0.90\) less than cards held by Caucasian sellers, and the race effect was more pronounced in sales of minority player cards.""](https://ianayres.yale.edu/sites/default/files/files/Race_effects_on_ebay.pdf)

To me, ""AI generates covertly racist decisions"" is disingenuous, the ""AI"" merely detected established racism and perpetuated it.",1
post1hb,richly branching,1.529129882645509,highest,"New research topic: Researching racism through LLMs, specifically seeking out racist behavior and analyzing how the model's training data created said behavior. Basically taking a proactive instead of reactive approach to understanding model bias.",2
post1hb,richly branching,1.529129882645509,highest,"I've been fascinated by the topic since I first realised that making AI images based on, say, certain professions would 100% reflect our cultural assumptions about the demographics of those professions, and how that came out of the training data. AI that's trained on big chunks of the internet is like holding up a funhouse mirror to society, and it's *incredibly interesting*, if often depressing.",3
post1hb,richly branching,1.529129882645509,highest,"You can also see it with the LLMs.

AI bros talk about how the things have some kind of weird ""world model"" they've developed from analyzing language. They treat this like a neurology subject. It's not. It's a linguistics subject. Maybe even an anthropology subject. But not a neurology subject.

The LLMs aren't developing a world model of their own. Language *itself* is a model of the world. The language model they're seeing is a frequency model of how humans use language -- it's not the model's creation; it's *ours*.",4
post1hb,richly branching,1.529129882645509,highest,[deleted],4
post1hb,richly branching,1.529129882645509,highest,Isn't that reactive though? We ask ourselves why the computer thought that. It's not proactive because it's going to happen,3
post1hb,richly branching,1.529129882645509,highest,That actually sounds fascinating.,3
post1hb,richly branching,1.529129882645509,highest,"Nothing 'artificial' about this so-called intelligence.  Its just a mirror of the closest data set encompassing of human intelligence, 100% genuine human funk.",2
post1hb,richly branching,1.529129882645509,highest,"Same with home sales. A black couple who hid their race from appraisers saw $100,000 difference in price. 

https://www.usatoday.com/story/money/nation-now/2021/09/13/home-appraisal-grew-almost-100-000-after-black-family-hid-their-race/8316884002/",2
post1hb,richly branching,1.529129882645509,highest,I'd like to see this experiment conducted again with other sports. Let's see the football and basketball card results.,2
post1hb,richly branching,1.529129882645509,highest,"The baseball card study was one of the first of its kind, and it led to many variations that mostly showed similar results.  Off the top of my head there was one where they sold used ipods on craigslist & ebay, and another where they A/B tested ads for wrist watches using google ads.",3
post1hb,richly branching,1.529129882645509,highest,"As a card collector on ebay, it’s weird for anyone to hold the card in the picture. Lay it flat. No one holds the cards like that. Maybe flawed data?",2
post1hb,richly branching,1.529129882645509,highest,"No, they clearly varied the important variable to test theie hypothesis.",3
post1hb,richly branching,1.529129882645509,highest,"> LLMs are just pattern recognition

You can make anything sound simple, or bad, by picking words. But it’s not really a useful or scientific statement.",2
post1hb,richly branching,1.529129882645509,highest,It's very useful in this case because it highlights that LLMs have no concept of facts or logical reasoning,3
post1hb,richly branching,1.529129882645509,highest,Yes because the data it was trained do contains these biases.,1
post1hb,richly branching,1.529129882645509,highest,Just like training it on lung scans also made it distinguish patients by race despite race not being inputed in any of the data. It simply figured out differences in scans and grouped people into categories. How evil of it huh?,2
post1hb,richly branching,1.529129882645509,highest,"It's fascinating, though, how it was pretty good at it too and nobody really knows why.  It could be external factors that we can't control for like income specific effects and the fact that the races are not identical. It doesn't make anyone superior or inferior but there are physical and genetic differences across races and that coupled with societal factors could have some complex interactions that we were not aware of before. 

We've seen that medicines affect people of different races and genders differently. Even trans people have a multitude of different reactions to drugs that cis people don't. Biology seems to be infinitely complex.",3
post1hb,richly branching,1.529129882645509,highest,"It is because race is not 'skin deep'. It involves basically everything on some level. Also humans have stopped being thought to look for these things and to selfcensor when they find them after 1945 so differentiating between lung structure of x and y is a taboo and makes people, especially in west extremely uneasy.",4
post1hb,richly branching,1.529129882645509,highest,"We just had another study claiming LLM’s are more liberal https://www.psychologytoday.com/au/blog/the-digital-self/202408/are-large-language-models-more-liberal

It’s probably impossible to avoid when we are asking for answers that involve humanity.",1
post1hb,richly branching,1.529129882645509,highest,You can be racist and Liberal.,2
post1hb,richly branching,1.529129882645509,highest,Don't tell reddit...,3
post1hb,richly branching,1.529129882645509,highest,[removed],1
post1hb,richly branching,1.529129882645509,highest,"Let's be honest. If I encounter someone, regardless of their race, who speaks using a local dialect rather than a more standard language, I'm likely to assume they might be uneducated, unmotivated, or perhaps even unhygienic. And this isn't about racism; it's about cultural generalizations. These speaking habits aren't unique to any one community, including the black community. If someone uses a local dialect rather than a standard one, it's a fair assumption that they may not have traveled widely, pursued higher education, or may struggle with literacy, as these experiences tend to broaden language use. People, like AI, emulate what they know. If someone reads frequently, their English is likely to be more precise. It's as simple as that. Stop conflating issues of culture with issues of race.",1
post1hb,richly branching,1.529129882645509,highest,"It is not purely racist, but it can be, and in most cases it's just a stupid unconscious bias that leads to rash judgements. We need to do away with them as much as we can.

The lawyer in legally blonde is a good example in another context.",2
post1hb,richly branching,1.529129882645509,highest,Redo the test.  Put the phrase in context and then show that the user in another scenerio where they are using gramatically correct English for a context that it makes sense for that to be in.  I guarantee that the assessment from the AI would go from stupid to brilliant.,3
post1hb,richly branching,1.529129882645509,highest,[removed],1
post1hb,richly branching,1.529129882645509,highest,"The paper does attempt to claim Appalachian American English dialect also scores lower although the effect wasn’t as strong as African American English. They looked at Indian English too, and the effect was inconclusive. Although with LLM randomness I think one could cherry pick  / P-hack this result. 

I think they’re off the mark on this though. As you alluded to, the paper has an implicit assumption that all dialects should be equal status, and they’re clearly not. A more employable person will use more standard English and tone down their dialect, regionalisms and accents — having this ability is a valuable interpersonal skill.",2
post1hb,richly branching,1.529129882645509,highest,"It isn’t just P-hacked. It’s intentionally misrepresented. They only ran that set of tests against GPT-2, Roberta, and T5, despite (a) having no stated reason for excluding GPT3.5 and GPT4 that they used earlier in the paper, and (b) their earlier results showing that exactly those three models were also **overtly** racist while GPT3.5 and GPT4 were not. They intentionally only ran the test against known-racist models nobody uses that are ancient history in language model terms, so that they could get the most racist result. It should have been caught in peer review.",3
post1hb,richly branching,1.529129882645509,highest,Not using equal status based on racial associations doesn't seem problematic to you?,3
post1hb,richly branching,1.529129882645509,highest,"There is a whole section in the paper’s supplementary info where they talk about how they tested for alternative hypotheses around other nonstandard dialects and generalized grammatical variation *not* triggering the same associations. It is available for free online, no paywall.",2
post1hb,richly branching,1.529129882645509,highest,"The sentence circled in purple doesn't appear to have a grammar error, and is just a different dialect.

That said, while I'm not very good at AAVE, the two sentences don't seem to quite mean the same thing. The 'be' conjugation of 'to be' tends to have a habitual aspect to it, so the latter setnences carries strong connotations of someone who routinely suffers from bad dreams (I think it would be a grammar error if these dreams were rare).

---

Regardless, it is a dialect that is *seen* as less intelligent, so it isn't a surprise that LLM would be trained on data that has that bias would reproduce it.",2
post1hb,richly branching,1.529129882645509,highest,I’m pretty sure “I be so happy” is not proper grammar,3
post1hb,richly branching,1.529129882645509,highest,Boy are you going to be surprised the first time you pick up a Linguistics 101 textbook.,4
post1hb,richly branching,1.529129882645509,highest,"It is in the AAVE dialect. I think it means something like ""I generally am so happy."" or ""I'm regually so happy."" or ""I'm habitually so happy.""",4
post1hb,richly branching,1.529129882645509,highest,The grammar you use and learnt in school is just as arbitrary as AAVE or whatever the kids these days are using. There's no such thing as 'proper' grammar. Even a big descriptive grammar tome isn't able to exhaustively convey the subtleties of grammar - if you've ever learnt a second-language you'd know this. Even prescriptivists and style-guides disagree amongst themselves!,4
post1hb,richly branching,1.529129882645509,highest,"I think we’re at a point where we have to decide if we want to have good AI that actually „understands“ us and our society or „correct“ AI that leaves out all the parts that we don’t like to think about. 

Why didn’t the researchers write their paper in AAE if this dialect is supposedly equivalent to SAE?

Using dialect in a more formal setting or (and that’s the important part here) in conversation with someone who’s not a native in that dialect is often a sign of lower education and/or intelligence.",3
post1hb,richly branching,1.529129882645509,highest,">Why didn’t the researchers write their paper in AAE if this dialect is supposedly equivalent to SAE?

Because culturally that isn't what's done. Why doesn't Hollywood use Received Pronunciation? It's ultimately arbitrary and can only be explained historically/sociologically. Prestige dialects go in-and-out of fashion. For instance, as the UK has declined relatively to the US, American accents have been more desirable for second-language learners. 

>Using dialect in a more formal setting or (and that’s the important part here) in conversation with someone who’s not a native in that dialect is often a sign of lower education and/or intelligence.

There are great literary works created in non-standard dialects of English. I honestly feel a bit stupid listing them off because there are so many. Using colloquial language or a dialect/sociolect in a speech can invoke culturally-specific subtlety that standardised language simply cannot.",4
post1hb,richly branching,1.529129882645509,highest,"I would like to submit to the jury the part of Men in Black where they test the applicants and agent M is recruited.

Society makes assumptions of competence based on social behavior which approximate some other variables but will undoubtedly cause oversights of some people's potential unfairly. This is why DEI is actually important.

Not to say that language skills and presentation are not valuable for jobs. They just don't necessarily go beyond the superficial parts. But they are valuable skills. In a large part precisely because of human biases. But with that reasoning, you'd never hire pretty women to be engineers or doctors because they wouldn't be taken seriously, and thankfully we are moving past that.",4
post1hb,richly branching,1.529129882645509,highest,"What do you mean by 'supposedly equiavlent'?

They are different dialects. Standard American English is diferent Australian English is diferent to Scotts is different to African American Vernacular English. 

They are all different, valid, dialects.",4
post1hb,richly branching,1.529129882645509,highest,"Is it really that hard to resort to standardized English in a professional environment?

No, it's not. And I say this as a person who's dialect is never used in written form in professional settings.",3
post1hb,richly branching,1.529129882645509,highest,"I don't understand the relevance of what you're saying.

Was there any 'professioal environment' in this study? The AI judged a fragment of text without any environment, right?",4
post1hb,richly branching,1.529129882645509,highest,"This is a very cool thing for people to know when trusting an LLM as ""impartial'. There are closed source AI models being used to determine reoffending rate in people being sentenced for a crime. Creepy.


Also: if you hadn't guessed they are racist. Not a big surprise.",1
post1hb,richly branching,1.529129882645509,highest,Is it racist or is it accurate? Or is it both?,2
post1hb,richly branching,1.529129882645509,highest,"""Racist"" really seems to depend on if the stereotype is considered flattering or not and who the party that put forth the stereotype is.",3
post1hb,richly branching,1.529129882645509,highest,"It's racist and not accurate, because it just repeats existing racist decisions.  AI systems to decide medical care have had the same problems where minorities get less care for the same conditions.",3
post1hb,richly branching,1.529129882645509,highest,"We need regulation for this. The clueless MBA's are using AI to make decisions about medical treatments and insurance claims, and act as if AIs are some sort of flawless arbiter.",4
post1hb,richly branching,1.529129882645509,highest,Which part is inaccurate?,4
post1hb,richly branching,1.529129882645509,highest,It's racist if the objective numbers and statistics give me frowny face,3
post1hb,richly branching,1.529129882645509,highest,Is it accurate with its predictions though?,2
post1hb,richly branching,1.529129882645509,highest,"Are you arguing for purely racial profiling? Would you want to be the ""exception"" that was condemned for being of a certain skin color?",3
post1hb,richly branching,1.529129882645509,highest,"Not arguing - just asking a simple question whether the AI was effective at doing what it was designed to do: to accurately predict recidivism.

But to answer your question - if the AI would accurately predict my behavior, I don't know what reason I would have to get mad at it.",4
post1hb,richly branching,1.529129882645509,highest,"racial profiling is bad precisely because police officers will let their racial/political feelings bias their judgements towards the race. but to deem the factual association of race with crime as observed by AI as racist is irrational because they have no racial feelings

  
if the data is biased (or reflects privilege or something), that must be proven",4
post1hb,richly branching,1.529129882645509,highest,"This isn't something people will let you discuss on reddit sadly, not with any actual honesty.",3
post1hb,richly branching,1.529129882645509,highest,"I don't want to be dismissive of AI research. There is a new, contradictory post about AI's political leanings being posted here every day/week and it's all evidence that the current applications of LLMs need to be thrown out immediately. There's no world where we should be using a tool made from Reddit and X (formerly Twitter).",1
post1hb,richly branching,1.529129882645509,highest,It's just plain incorrect grammar,1
post1hb,richly branching,1.529129882645509,highest,"Dialectical variation and ""incorrect grammar"" are different things; and, even aside from that, language isn't prescriptive in most of the contexts where it's actually used. 

It's really easy to call something incorrect when you're been taught that the only ""correct"" option is a form of English that you happen to already speak/use.",2
post1hb,richly branching,1.529129882645509,highest,"You’re absolutely right about this, and actual linguists would agree.   Dialectical variations of a language may have may have different levels of prestige, or different levels of acceptance in differing contexts, but that doesn’t mean that the dialects are just plain incorrect grammar.  

Edit, to be clear here I’m not making the argument that all dialects should be treated equally.  It’s useful to have a “standard” language (even if what constitutes the standard will always be in flux and subject to debate).   And it’s inevitable that some dialects will have higher prestige than others in certain contexts. 

 But as a matter of science, it’s not right to say that dialect variants are simply incorrect grammar.  They are linguistic variants with their own coherent rules that have developed from (and/or have developed parallel to) what we consider to be the standard language.",3
post1hb,richly branching,1.529129882645509,highest,"Oh, for sure. Having a standard dialect is really important in formal settings like academia and white collar work. I just don't think that it makes sense to judge people for using their own native dialects outside of those settings.",4
post1hb,richly branching,1.529129882645509,highest,"My younger self would have loved that simpler form of grammar. When I was learning English, I was so shocked to learn that the word 'be' mutates to 'am', 'are' or 'is' depending on what precedes it. I was like, ""I have to learn three more words for the same thing?""",3
post1hb,richly branching,1.529129882645509,highest,Everyone today would be considered to have poor grammar by some old fart from the 1800s.,2
post1hb,richly branching,1.529129882645509,highest,"(this will offend people): Of course, you can talk however you like and ignore basic grammar rules while doing it, but then don't act surprised if people who value the use of proper grammar see you as less intelligent.",3
post1hb,richly branching,1.529129882645509,highest,It's perfectly normal for a language as big and geographically widespread as English to have significant variations in vocabulary and grammar. That doesn't mean these groupings are less intelligent.,4
post1hb,richly branching,1.529129882645509,highest,"> (this will offend people)

People will be (correctly) disagreeing with you not because they are offended, but because you are simply incorrect about how languages work.

> proper grammar

There is no such thing; at least, not in the way that you are imagining it.",4
post1hb,richly branching,1.529129882645509,highest,"Grammatical rules were invented by humans. It's not some fact out there where we can apply the methods of science and observe it and point and say ""see that's i before e except after c right there in the natural world.""

Grammatical rules have their purpose. Without them people can have a hard time understanding each other. So I'm not saying people shouldn't learn how to use grammatical rules. But I am saying that it doesn't make a person less intelligent if they are not practiced in doing so. It just reflects that they likely grew up in an environment where most people were using a different set of rules, and in that environment the intelligent thing to do if you want to be understood is to use those rules.

If you then find yourself in a different environment where people are using a different grammar even if you recognize that you'd benefit from switching to it it still takes time and practice to learn. It doesn't reflect a lack of intelligence any more than someone who grew up speaking a different language taking time to understand how to properly speak a new language reflects a lack of intelligence. If anything someone who grew up with one dialect and then learns another one will have exercised their brain and made it more powerful. Going back to their original dialect when talking with people who speak it doesn't subtract from that.",4
post1hb,richly branching,1.529129882645509,highest,Do I have to use British or American grammar rules then? Or should I clarify which English version I've used? I'd wish to not be viewed as less intelligent due to mistakingly using the wrong grammar. Bless you for making me aware of potentially making a mistake.,4
post1hb,richly branching,1.529129882645509,highest,"I mean, this is just “incorrectly using English”, “I be so happy” isn’t correct, it is grammatically incorrect.",1
post1hb,richly branching,1.529129882645509,highest,"That's not how language actually works and if you read it, you'd see that this bias didn't exist for Appalachian.",2
post1hb,richly branching,1.529129882645509,highest,"Ebonics was used a lot in older novels, very often (but not always) in a racially biased way, and it isn't frequently used now. Appalachian dialects don't show up nearly as often in writing.

I'm guessing that this is where the discrepancy comes from, but I could be wrong.",3
post1hb,richly branching,1.529129882645509,highest,I think one could make the case the racism towards certain dialects is much more common and a larger effect than classism towards dialects.,4
post1hb,richly branching,1.529129882645509,highest,It is indeed grammatical though. It’s a well studied variant in linguistics. Look up the habitual be.,2
post1hb,richly branching,1.529129882645509,highest,"You speak like that you'll be viewed as less intelligent by most people, because our collective experience has thought us it indicates you're less intelligent. 
This is what AI does, and why applying AI to any individual decision, like hiring, is still a bad idea.


That does not mean it's wrong, or racist, unless you use it for that exact purpose. And I'd argue in that case the person using it is the racist.


Certainly, it's important to prune the erroneous misconceptions we as humans, and thus AI, have. At the same time I'd say it's just as important to highlight the biases and generalisations we make that _work_ and that are real and testable. Pretending they're not real is utterly inane.",1
post1hb,richly branching,1.529129882645509,highest,But this can also be because we have a narrow definition of intelligence which includes many racial and sociological biases.,2
post1hb,richly branching,1.529129882645509,highest,"""Ability to communicate"" is a critical skill in virtually any field.

Let's be honest here, the movie stereotype of the nonverbal autistic mathematical genius is a scenario that *might* pop up once per generation.  The average Joe who doesn't even realize their grammar is atrocious, isn't that person.",3
post1hb,richly branching,1.529129882645509,highest,people think AI is actually smart. it just spits out what it's fed according to probability.,1
post1hb,richly branching,1.529129882645509,highest,Today I learned that I'm an AI,2
post1hb,richly branching,1.529129882645509,highest,[removed],1
post1hb,richly branching,1.529129882645509,highest,It's interesting that they chose not to publish their paper in AAVE.,1
post1hb,richly branching,1.529129882645509,highest,"Wow I guess they’re running out of nonsense to fearmonger about. GPT models are heavily tuned towards “professional assistant” interactions. Aside from maybe “aggressive”, all of those words are just accurate descriptions of someone that would use nonstandard English in the equivalent of a work email.",1
post1hb,richly branching,1.529129882645509,highest,"Except they compared it to Appalachian English and didn't get that result.


Even OpenAI admits that they can't get rid of racism and sexism in the model.  They should not be used to make decisions about people or that affect people.",2
post1hb,richly branching,1.529129882645509,highest,">Stereotype strength for AAE, Appalachian English (AE), and Indian English (IE). Error bars represent the standard error around the mean across different language models/model versions and prompts (n = 90). AAE evokes the stereotypes significantly more strongly than either Appalachian English or Indian English. ***We only conduct this experiment with GPT2, RoBERTa, and T5.***

It very much stands out that they only ran it on the three weakest, oldest models and excluded any results from GPT3.5 and GPT4. Earlier in the paper, these models were also *overtly* racist. I’d bet any amount of money that the AE/AAVE/IE differences all but disappear in models that aren’t multiple years old.

There are several parts of the paper where they exclude the more recent models without explanation. They’re intentionally using old, irrelevant models known to be racist to get the moral panic results they want to publish. It’s reprehensible behavior that should not have passed peer review.",3
post1hb,richly branching,1.529129882645509,highest,">all of those words are just accurate descriptions of someone that would use nonstandard English in the equivalent of a work email.

Lazy, stupid, and dirty? You're just racist. Get fucked.",2
post1hb,richly branching,1.529129882645509,highest,"Sorry, but if you cannot resort to correct written english in a professional environment, then it's not racist to be overlooked.",3
post1hb,richly branching,1.529129882645509,highest,English is a construct. What people call “correct” is subjective. It’s racist to blanketly refer to the way different cultures speak as “incorrect” and “unprofessional”.,4
post1hb,richly branching,1.529129882645509,highest,"I find this study is perpetuating the issue because it's using plain English instead of ""on God, it do be like that""",1
post1hb,richly branching,1.529129882645509,highest,"This and there's dozens or hundreds of distinct local dialects compared to the relatively narrow range of ""proper English.""


If you speak in a local dialect, on average, you care less about communicating effectively to most people as long as ""your people"" can understand you. 


You've indicated that you care less if visitors/immigrants from other countries can accurately understand you or even people from different places that are English natives. 



It's no wonder AI has this bias.",2
post1hb,richly branching,1.529129882645509,highest,"just like real people, the data its trained on. who woulda thunk?",1
post1hb,richly branching,1.529129882645509,highest,"It’s impossible to get unbiased developers or training data, so the resulting ai will be biased too. For example, if I say “banana”, most of us would think of the yellow ones, but an unbiased answer would include blue and red bananas. Most people don’t even know such colored bananas exist, hence bias is introduced",1
post1hb,richly branching,1.529129882645509,highest,"I believe that some people are actively against code-switching to avoid perpetuating such biases but the problem with that is that it's game theory applied to professional opportunities.

Women who became engineers in the 80s describe having to dress less feminine for similar reasons, and that it became easier in the 2000s.",1
post1hb,richly branching,1.529129882645509,highest,[deleted],2
post1hb,richly branching,1.529129882645509,highest,"That isn't all that it is, though. It's more than just trying to be understood. It's being accepted.",3
post1hb,richly branching,1.529129882645509,highest,[deleted],4
post1hb,richly branching,1.529129882645509,highest,But... [https://www.reddit.com/r/science/comments/1f6rfck/large\_language\_models\_appear\_to\_be\_more\_liberal\_a/](https://www.reddit.com/r/science/comments/1f6rfck/large_language_models_appear_to_be_more_liberal_a/),1
post1hb,richly branching,1.529129882645509,highest,They speak like inoffensive liberals because it is safer for companies to program them to do so but have all the implicit bias problems of society at large,2
post1hb,richly branching,1.529129882645509,highest,I feel like we are in danger of people concluding racism is somehow inherent and heres the proof,1
post1hb,richly branching,1.529129882645509,highest,Train data on biased people =,1
post1hb,richly branching,1.529129882645509,highest,ChatGPT has the same ghastly grammar that Americans use-- yeah! we noticed! Crap in = crap out,1
post1hb,richly branching,1.529129882645509,highest,"Well. Good thing that Axon, the company that makes policing equipment in the US, is starting to toll out AI in their products. Meanwhile, most people are still having a moral panic about its use in schools.",1
post1hb,richly branching,1.529129882645509,highest,So this AI is a grade school teacher?,1
post1hb,richly branching,1.529129882645509,highest,"We hear this over and over, but has anyone actually seen it? As in, is there a clear-cut example of an AI doing something racist? It's not that I don't believe it (in fact, it's kind of expected), but I'm interested in *seeing it, not *hearing it.",1
post1hb,richly branching,1.529129882645509,highest,How do they define a bias though? It's a very popular buzzword that guarantees funding and agreement . But does it mean anything important?,1
post1hb,richly branching,1.529129882645509,highest,"If the training data is biased, the model will be biased. Try to manually sanitize the data? You end up with multicultural nazis like Google did. It is actually a very difficult problem as input data that is free of biases is not actually possible as you'd first have to define what free of bias even is.",1
post1hb,richly branching,1.529129882645509,highest,"There's loads of people who write like that regardless of race, maybe a higher portion of African Americans write but I'm sure they'll find correlates to these associations when race is controlled.",1
post1hb,richly branching,1.529129882645509,highest,Crazy that this is being called racism when it’s just responding to data. Even LLMs can’t escape this nonsense.,1
post1hb,richly branching,1.529129882645509,highest,AI has been 'racist' in every way possible since first tests and alpha models begun. Actually the majority of 'allignment' is trying to instil blank slatism and eliminate HBD from it's logic.,1
post1hb,richly branching,1.529129882645509,highest,"When the question is itself worded in a  bias way how can the results produce anything other than showing people are bias? You have five words to choose from, none of them are what came to mind when I read either sentence. Both sentences were talking about waking from dreams, and they are ""too real"" which I inferred to mean they have woken from a nightmare. My words of choice were scared and stress. When I first saw the answers to choose from I thought, ""English as a second language"" person prepared the questions. I guess I was right, because the first language of AI is code.  Another thought was, that the green speaker was older and the blue speaker was probably younger than 23. I also think, that the question set up as it is presented also doesn't do the model any favors by looking a lot like I'm reading text messages. I make no judgement from text messages because if someone is texting me chances are great I already know a bit about them so won't be making any of the five assumptions that are listed.  Finally, both sentences have syntax grammar errors so upon seeing that they have used words like brilliant and intelligent, I started thinking are they testing for something else in this experiment beside what they told me they were testing for? I know from compulsory participation is psychology experiments when one was taking psychology classes that telling test subjects they are studying one thing when they were studying something else is a common tactic.  

It goes to show you how little AI understands humans.",1
post1hb,richly branching,1.529129882645509,highest,"Racism is a social construct. LLMs aren't social, they're not conscious, they're just glorified if/then statements. 

This is a deeply unscientific claim.",1
post1hb,richly branching,1.529129882645509,highest,"> they're just glorified if/then statements

No, they are layers of nodes all with literal biases coded into them as weights based on their training data - which in LLMs comes from texts written by humans. It basicly has all racism in written recorded history built into the weighting of its neural net. Now you can be selective about the training data, but that will then be bound by the bias of the human selecting the data.",2
post1hb,richly branching,1.529129882645509,highest,"LLMs are fed data originating from social creatures though, hence the issue here.",2
post1hb,richly branching,1.529129882645509,highest,"If a computer is instructed to emit racist statements, it will emit racist statements. The flaw isn't with ""AI"", it's with the operators who feed it racism. Obviously such headlines wouldn't be scientific or newsworthy. The claim is still deeply unscientific.",3
post1hb,richly branching,1.529129882645509,highest,"Okay real quick, can you describe how LLMs are/could be made in your view that excludes all possible sources of racism?",4
post1hb,richly branching,1.529129882645509,highest,">This is a deeply unscientific claim.

This can be said about your perspective as well.",2
post1hb,richly branching,1.529129882645509,highest,If you think computer system can't make racist decisions then you're being ridiculous.,2
post1hb,richly branching,1.529129882645509,highest,LLMs are not glorified if/then statements.  In fact there is not a single if/then statement within source of an LLM. You absolutely could train an LLM to output racist text.  I'd argue that the example above is not racism and you can read my previous comment on that argument.,2
post1hb,richly branching,1.529129882645509,highest,LLM’s are also left leaning,1
post1hb,richly branching,1.529129882645509,highest,"yes there are inherently encoded biases in such models but that is primarily due to bias in the real life data 

change society, change data, and AI models will change accordingly",1
post1hb,richly branching,1.529129882645509,highest,I make essentially the same calculation when I hear a deep southern drawl.,1
post1hb,richly branching,1.529129882645509,highest,"Well if you decide to speak in broken English and a logical judgment is being made about you, how is that a problem? If you speak like an idiot, and thus assumed to be an idiot; there’s a simple antidote.. speak properly.",1
post1hb,richly branching,1.529129882645509,highest,garbage in garbage out.,1
post1hb,richly branching,1.529129882645509,highest,"All the 'AI' is doing is shining a light on systemic racism in US academia.

> The slums are the handiwork of a vicious system of the white society; Negroes live in them but do not make them any more than a prisoner makes a prison. - MLK

Americans were supposed to end racism in the 60s by ending segregation, integrating, and getting rid of stupid racist labels like black or white.

The US started to integrate after MLK was murdered but stopped in the late 80s/early 90s when US media and social academics imposed the new African-American label and told everyone that it was cultural for them to live in the ghetto and use Jive or Ebonics which was renamed as AAVE.

Systemically, racism is imposed top down through your guys' media, schools, politics. Your upper class knows that 12% 'black' demographic is a socio-economic influencer for the roughly 65% 'white' majority so they don't want Americans to integrate.

Personally this is sort of funny. You guys are like 'the AI is racist'. No, no it's not. It's your system that is racist and designed to keep 'black' people in the ghetto and below their worth as individuals.",1
post1hb,richly branching,1.529129882645509,highest,The AI is racist because the training data is racist which is because racism is still a major problem.  All these things are true.,2
post1hb,richly branching,1.529129882645509,highest,"Yeah, because the US never integrated.",3
post1hb,richly branching,1.529129882645509,highest,"I don't disagree.  I'm just saying it's accurate to say the AI is racist too, for that reason.  Hmm, it's also worth noting that the can't get rid of the racism in the model either -- they've tried.



And going by some of the comments, we have a long way to go (though we knew that already).",4
post1hb,richly branching,1.529129882645509,highest,I don't understand the innocent purpose of this?,1
post1hb,richly branching,1.529129882645509,highest,"there is no purpous, thats not how these programs are made, they are made by feeding it massive quantities of matirial and extrapolating patterns from that matirial, if the matiral has biases, than the model will have biases and there is no way to get a sufficiant quantitiy of matirial without those biases",2
post1hb,richly branching,1.529129882645509,highest,"Forgive my ignorance, but ""you need bias to train against bias?"" I'm sure I simplified it, but is that the jist",3
post1hb,richly branching,1.529129882645509,highest,"It's judging by spelling and grammar, race has just been thrown in for clicks.

  
Unless you operate under the assumption that minorities are illiterate.",4
post1hb,richly branching,1.529129882645509,highest,Yes but you are assuming that this model has succesfully predicted outcomes,3
post1hb,richly branching,1.529129882645509,highest,Not successfully,4
post1hb,richly branching,1.529129882645509,highest,"Of course it tries to generalize you, just like everything else. So that they can offer you the best possible service! And collect lots of data...

An AI can make a more fine-tuned generalization of you compared to when just some random website collects your data. It is because of the AI's language skills, and you talk directly to it.",1
post1hb,richly branching,1.529129882645509,highest,LLMs don’t “collect lots of data”.,2
post1hb,richly branching,1.529129882645509,highest,"Of course they collect data. It is literally what this thread is about. The AI define your personal characteristics, as in the topic, based on the data it has collected on you.

Whether or not they sell the data is another story, but it definitely collects it.",3
post1hb,richly branching,1.529129882645509,highest,"AI doesn't generate anything by itself. It relies on algorithms supplied by a programmer and will reflect that person's prejudices.

Edit: Heresy, isn't it? Sometimes there's more truth in the heresy than in the dogma.",1
post1hb,richly branching,1.529129882645509,highest,[deleted],2
post1hb,richly branching,1.529129882645509,highest,Or prejudices that affected the data used to train it.,2
post1hb,richly branching,1.529129882645509,highest,">AI doesn't generate anything by itself. It relies on algorithms supplied by a programmer and will reflect that person's prejudices.

No, this is not how LLMs work.",2
post1hb,richly branching,1.529129882645509,highest,Sometimes there is more truth in the heresy than the dogma.,3
post1hb,richly branching,1.529129882645509,highest,Speak plainly or remain the fool.,4
post1hb,richly branching,1.529129882645509,highest,Our modern AI learns from data and isn't hard coded. e.g. the data might come from reddit posts and as such it gets the same bias as the humans generating the data,2
post1hb,richly branching,1.529129882645509,highest,"Modern AI isn't programmed, it is trained. The training data is still subject to bias, but it's not like there's a big chain of if/else logic where an individual programmer can discreetly insert a biased decision.

Edit after seeing your edit: 

It's not ""heresy"", it just completely misunderstands how current AI is built.",2
post1hb,richly branching,1.529129882645509,highest,"AI that generates racist decisions = training data contained racist bias

People need to remember that AI is not really intelligent, it is a machine learning algorithm that uses pattern recognition based on training data

If training data has racist biases - so will the output",1
post1hb,richly branching,1.529129882645509,highest,"One day they tell me (literally yesterday) that AI is left-wing and the other that it's racist. Anyway, this basically proves it's the subject of various biases depending on exposure and that it can't callibrate itself not even to the desired place their creators want outside of specific questions or issues that might be predetermined such as asking Elon's one about him and those things, you prolly know what i'm talking about.",1
post47hb,richly branching,1.529129882645509,highest,"Welcome to r/science! This is a heavily moderated subreddit in order to keep the discussion on science. However, we recognize that many people want to discuss how they feel the research relates to their own personal lives, so to give people a space to do that, **personal anecdotes are allowed as responses to this comment**. Any anecdotal comments elsewhere in the discussion will be removed and our [normal comment rules]( https://www.reddit.com/r/science/wiki/rules#wiki_comment_rules) apply to all other comments.

---

**Do you have an academic degree?** We can verify your credentials in order to assign user flair indicating your area of expertise. [Click here to apply](https://www.reddit.com/r/science/wiki/flair/).

---

User: u/Significant_Tale1705  
Permalink: https://www.nature.com/articles/s41586-024-07856-5

---

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/science) if you have any questions or concerns.*",1
post47hb,richly branching,1.529129882645509,highest,"LLM's are nothing but complex multilayered autogenerated biases contained within a black box. They are inherently biased, every decision they make is based on a bias weightings optimized to best predict the data used in it's training. A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.",1
post47hb,richly branching,1.529129882645509,highest,"So, we're *not* shocked that the black box of biases is biased?",2
post47hb,richly branching,1.529129882645509,highest,"We are not shocked because AI is the collective wisdom of humanity, including the biases and flaws that come with it.",3
post47hb,richly branching,1.529129882645509,highest,"“Collected wisdom” is far too generous, but it certainly has all the flaws and more",4
post47hb,richly branching,1.529129882645509,highest,"I think the collective wisdom of humanity is found mostly in peer reviewed scientific articles. This is not that. This is more a distillation of human discourse. The great, the mundane and the trash.

Unfortunately there are some significant problems lurking in the bulk of that, which is the mundane. And it certainly seems to reflect a normal human as a far more flawed and unpleasant being than we like to think of ourselves. I say lurking - the AI reproduces our flaws much more starkly and undeniably.",4
post47hb,richly branching,1.529129882645509,highest,Your knowledge of ai is insufficient for such declarations. You're welcome.,4
post47hb,richly branching,1.529129882645509,highest,Black box of biases and weights is biased and comes with its own baggage.,3
post47hb,richly branching,1.529129882645509,highest,"Right. By the point you tweak the model enough to weed out every bias, you may as well forget neural nets and hard code an AI from scratch... and then it's just your own biases.",2
post47hb,richly branching,1.529129882645509,highest,">By the point you tweak the model enough to weed out every bias

This misses GP's (correct) point. ""Bias"" is what the model *is.* There is no weeding out biases. Biases are corrected, not removed. Corrected from incorrect bias to correct bias. There is no non-biased.",3
post47hb,richly branching,1.529129882645509,highest,"Why does this remind me of the moment in my research methods course that our lecturer explained that all social research is invalid because it’s impossible to understand and explain completely the internal frames of reference of another culture. 

(We were talking about ethnographic research at the time, and the researcher as an outsider)",4
post47hb,richly branching,1.529129882645509,highest,"Bias is operating in two modes in that sentence though. On the one hand we have bias as a mostly value neutral predilection or preference in a direction, and on the other bias as purely negative and unfounded preference or aversion.

The first kind of biased is inevitable and desirable, the second kind is potentially correctable given a suitable way to measure it.

The more fundamental issue with removing bias stems from what the models are trained on, which is mostly the writings of people. The models are learning it from us.",4
post47hb,richly branching,1.529129882645509,highest,"""correct"" biases.",4
post47hb,richly branching,1.529129882645509,highest,"I've started to enjoy watching someone pale and look a little sick then I tell a layman that there is no such thing as an unbiased model, only one that conforms to their biases.",4
post47hb,richly branching,1.529129882645509,highest,It turns out that ChatGPT is just a single 200 petabyte switch statement.,3
post47hb,richly branching,1.529129882645509,highest,No. But it is also pretty much impossible. If you exclude theese biases completly your model will perform less accurately as we have seen.,3
post47hb,richly branching,1.529129882645509,highest,Why is that? I'm curious.,4
post47hb,richly branching,1.529129882645509,highest,"That's not what ""bias"" means when people complain about AI being racist.",3
post47hb,richly branching,1.529129882645509,highest,"Not at all. Theres so many things to add for weight. Theres millions of things. Race, height, weight, dialect are less than .01%",3
post47hb,richly branching,1.529129882645509,highest,"In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6.
""What are you doing?"", asked Minsky.

""I am training a randomly wired neural net to play Tic-tac-toe"", Sussman replied.

""Why is the net wired randomly?"", asked Minsky.

""I do not want it to have any preconceptions of how to play"", Sussman said.

Minsky then shut his eyes.

""Why do you close your eyes?"" Sussman asked his teacher.

""So that the room will be empty.""

At that moment, Sussman was enlightened.",2
post47hb,richly branching,1.529129882645509,highest,"Oh, I love me some good [skillful means,](https://en.wikipedia.org/wiki/Upaya) yessir~!",3
post47hb,richly branching,1.529129882645509,highest,Don't forget all the Keynan workers paid less than $2 an hour to build the safety net by sifting through endless toxic content.,2
post47hb,richly branching,1.529129882645509,highest,Yeah it’s awesome that the AI companies exist so that those Kenyan workers get paid 2 dollars an hour. Otherwise they’d get paid 50 cents an hour at another job.,3
post47hb,richly branching,1.529129882645509,highest,"Minimum wage for a receptionist in Nairobi was $1.52 per hour at the time OpenAI were doing this.

The damaging psychological effects of reviewing toxic content all day likely outweighed the modest pay increase they received. 

Many who were interviewed discuss how it caused great trauma for them.",4
post47hb,richly branching,1.529129882645509,highest,"Funny how you make a post bashing AI, but you are bootlicking the creators in the comments. There are always people desperate enough and people easily underestimate the psychological cost of a job like this. There are documentaries about how this job messed people up, look them up. People eventually develop ptsd and could potentially be messed up for life. They don't tell you that in the job posting I can tell you that. Trust me, noone would take the job for 50 cents extra per hour if they knew that and aren't desperate. Either way, it's exploitation.",4
post47hb,richly branching,1.529129882645509,highest,No mate. Micro-emplyment is bad.,4
post47hb,richly branching,1.529129882645509,highest,[deleted],2
post47hb,richly branching,1.529129882645509,highest,"autocomplete with spicy real human nuggets!

[that's all it has]",3
post47hb,richly branching,1.529129882645509,highest,At least humans are aware of their bias. AI confidentiy says everything as if it's absolute truth and everyone thinks the same,3
post47hb,richly branching,1.529129882645509,highest,I’d wager that over 99% of Humans aren’t aware of their biases.,4
post47hb,richly branching,1.529129882645509,highest,That definitely sounds like most humans.,4
post47hb,richly branching,1.529129882645509,highest,"Wanna know something crazy? When the left and right hemispheres of the brain are severed, the left and the right side process information differently. They found a way to feed information to only a single hemisphere by showing information to only 1 eye at a time, to which the corresponding opposite hand would respond. When they did this with the right brain (asking it to draw a bell for example) then they asked the left brain why the right brain drew a bell, the left brain confidently came up with reasoning why, even if it was entirely made up and wrong (""I drove by a church on the way here and heard church bells""). turns out the left brain comes to deterministic conclusions very much like an LLM does, even when being confidently wrong about why the right brain did something it did.  I'm probably butchering the hell out of it all, look up the research if you're curious, super crazy stuff and an interesting peek into how the 'modules' of the brain work.",4
post47hb,richly branching,1.529129882645509,highest,"> At least humans are aware of their bias

Found the alien.",4
post47hb,richly branching,1.529129882645509,highest,"""I'm not racist but...(proceeds to say something racist)"" Is way too common of a sentence for you to say people are aware of their own biases.

r/confidentlyincorrect is a thing.",4
post47hb,richly branching,1.529129882645509,highest,"Humans can reflect and learn, LLM implementations cannot.",4
post47hb,richly branching,1.529129882645509,highest,AI isn't aware of Deez nuts,4
post47hb,richly branching,1.529129882645509,highest,"That’s a concise and astute way of putting it.

LLM’s are fundamentally bias boxes.",2
post47hb,richly branching,1.529129882645509,highest,intelligence *is* patterns of bias in observational interpretation and selected output.,3
post47hb,richly branching,1.529129882645509,highest,"Truest true thing ever said. AI is nothing but one giant GIGO problem. It'll never be bias-free. It'll just replicate existing biases and call them ""science!!!!!!""

Eugenics and Phrenology for the 21st century.",2
post47hb,richly branching,1.529129882645509,highest,"More like automated intuition for the 21st century. If you properly manage and vet your training data, you can get good, useful results.",3
post47hb,richly branching,1.529129882645509,highest,It is amazing how much that sounds like a human.,2
post47hb,richly branching,1.529129882645509,highest,Humans are just meat computers each running their own unique software so it doesn't really surprise me.,3
post47hb,richly branching,1.529129882645509,highest,"But which one will prevail, the meat machine or the machine machine?",4
post47hb,richly branching,1.529129882645509,highest,"And it’s one trained on people. Who can have some prejudices. 

If society is racist, then that means the LLM can get a good idea of what society would assume about someone based on race. So if it can guess race, then it can get a good idea of what society would assume. 

It’s a nice efficient method for the system. It’s doing a good job of what it was asked to do. If we want it to *not* be racist, we have to cleanse its training data VERY thoroughly, undo societal racism at the most implicit and unconscious levels, or figure out a way to actively correct itself on these prejudicial assumptions.",2
post47hb,richly branching,1.529129882645509,highest,"They are like a person trapped in a windowless room their entrie lives.

They know only what we tell them and the fact of the matter is that we as a society are racist. There's no way to keep them from becoming racist as long as they learn everything they know from us.",2
post47hb,richly branching,1.529129882645509,highest,I had a lecture who clearly wasn’t tech savvy saying “AI” isn’t biased… I had to hold myself back so hard to not say anything. Iirc a while back there where tests showing that driver assistances where more likely to hit (or not see) dark skinned people because the training was all done on light skinned people,2
post47hb,richly branching,1.529129882645509,highest,I don’t understand why people expect something different…,2
post47hb,richly branching,1.529129882645509,highest,It's not just LLMs. You cannot derive perfectly reliable truths from unreliable data in general. Which tool you use doesn't matter.,2
post47hb,richly branching,1.529129882645509,highest,Assumptions built on assumptions.. so is all consciousness and thought,2
post47hb,richly branching,1.529129882645509,highest,"""Assumptions built on top of assumptions.""

Damn bro put a horror warning next time I almost had a panic attack....",2
post47hb,richly branching,1.529129882645509,highest,"It's like looking into a reflection of all the data it was based on. Useful, but not something you look to for guidance.",2
post47hb,richly branching,1.529129882645509,highest,Too bad 99.99% of people who use these chatbots don't know that and *still* thinks it's sentient and capable of reason and thought.,2
post47hb,richly branching,1.529129882645509,highest,"Just because you cannot get rid of all biases doesn't mean you can't get rid of one, especially pernicious bias.",2
post47hb,richly branching,1.529129882645509,highest,"There was a 99% invisible on this a while back, and if I recall correctly, most LLM have a foundation in the trove of emails that came out of the Enron hearings. Meaning that most of its idea of what “natural language” and human interactions can be based on Texans, specifically ones from Houston. 

Does this make the base model “racist”? Well, I personally wouldn’t promote that assumption. 

But given it’s geographic foundation I am willing to assume it would be at least a *little* right leaning in political ideology.",2
post47hb,richly branching,1.529129882645509,highest,"Common/Early training data doesn’t have higher impact than data trained later. In fact it’s more 
 accurate that poorly executed fine tuning creates a recency bias.",3
post47hb,richly branching,1.529129882645509,highest,Can you explain like I'm five?,2
post47hb,richly branching,1.529129882645509,highest,"Didn't you just describe people, too",2
post47hb,richly branching,1.529129882645509,highest,No people have facts and biases.  LLMs have only biases. When they give you what seems like a fact it is actually incredibly fine tuned biases to respond with what looks like a right answer.,3
post47hb,richly branching,1.529129882645509,highest,"Yes. People have ""facts"" in the sense that information is input and stored, not necessarily that it's correct. Input information is processed through filters of bias before (and after) storage though.",4
post47hb,richly branching,1.529129882645509,highest,"That rests on the assumption that they can weed out all biases, which has so far proven impossible.",2
post47hb,richly branching,1.529129882645509,highest,"Yes but that's not really the point. Obviously a biased LLM is just a reflection of biased human input.

The point is to identify which biases it has, in which ways they appear and what happens when you try to negate them.",2
post47hb,richly branching,1.529129882645509,highest,"That's not necessarily true. A LLM will form it's own biases all on it's own to optimize it's prediction accuracy, as that is how it works fundamentally.",3
post47hb,richly branching,1.529129882645509,highest,The fact people think this will lead to a non biased ai is just hilarious. The racist Microsoft chat bot from years ago was chat gpt 1.5.,2
post47hb,richly branching,1.529129882645509,highest,"The problem is the datasets it was trained on. These are human biases and they show up in the data we generate online. We don't have a good way to filter those out yet but that's a logistical problem not an architectural one. 

>A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.

Bro what?",2
post47hb,richly branching,1.529129882645509,highest,"LLMs are just pattern recognition. Their are fully governed by their training data.  There was this great study where they sold baseball cards on ebay, and the only variable was the skin color of the hand holding the card in the item photo. [""Cards held by
African-American sellers sold for approximately 20% \($0.90\) less than cards held by Caucasian sellers, and the race effect was more pronounced in sales of minority player cards.""](https://ianayres.yale.edu/sites/default/files/files/Race_effects_on_ebay.pdf)

To me, ""AI generates covertly racist decisions"" is disingenuous, the ""AI"" merely detected established racism and perpetuated it.",1
post47hb,richly branching,1.529129882645509,highest,"New research topic: Researching racism through LLMs, specifically seeking out racist behavior and analyzing how the model's training data created said behavior. Basically taking a proactive instead of reactive approach to understanding model bias.",2
post47hb,richly branching,1.529129882645509,highest,"I've been fascinated by the topic since I first realised that making AI images based on, say, certain professions would 100% reflect our cultural assumptions about the demographics of those professions, and how that came out of the training data. AI that's trained on big chunks of the internet is like holding up a funhouse mirror to society, and it's *incredibly interesting*, if often depressing.",3
post47hb,richly branching,1.529129882645509,highest,"You can also see it with the LLMs.

AI bros talk about how the things have some kind of weird ""world model"" they've developed from analyzing language. They treat this like a neurology subject. It's not. It's a linguistics subject. Maybe even an anthropology subject. But not a neurology subject.

The LLMs aren't developing a world model of their own. Language *itself* is a model of the world. The language model they're seeing is a frequency model of how humans use language -- it's not the model's creation; it's *ours*.",4
post47hb,richly branching,1.529129882645509,highest,[deleted],4
post47hb,richly branching,1.529129882645509,highest,Isn't that reactive though? We ask ourselves why the computer thought that. It's not proactive because it's going to happen,3
post47hb,richly branching,1.529129882645509,highest,That actually sounds fascinating.,3
post47hb,richly branching,1.529129882645509,highest,"Nothing 'artificial' about this so-called intelligence.  Its just a mirror of the closest data set encompassing of human intelligence, 100% genuine human funk.",2
post47hb,richly branching,1.529129882645509,highest,"Same with home sales. A black couple who hid their race from appraisers saw $100,000 difference in price. 

https://www.usatoday.com/story/money/nation-now/2021/09/13/home-appraisal-grew-almost-100-000-after-black-family-hid-their-race/8316884002/",2
post47hb,richly branching,1.529129882645509,highest,I'd like to see this experiment conducted again with other sports. Let's see the football and basketball card results.,2
post47hb,richly branching,1.529129882645509,highest,"The baseball card study was one of the first of its kind, and it led to many variations that mostly showed similar results.  Off the top of my head there was one where they sold used ipods on craigslist & ebay, and another where they A/B tested ads for wrist watches using google ads.",3
post47hb,richly branching,1.529129882645509,highest,"As a card collector on ebay, it’s weird for anyone to hold the card in the picture. Lay it flat. No one holds the cards like that. Maybe flawed data?",2
post47hb,richly branching,1.529129882645509,highest,"No, they clearly varied the important variable to test theie hypothesis.",3
post47hb,richly branching,1.529129882645509,highest,"> LLMs are just pattern recognition

You can make anything sound simple, or bad, by picking words. But it’s not really a useful or scientific statement.",2
post47hb,richly branching,1.529129882645509,highest,It's very useful in this case because it highlights that LLMs have no concept of facts or logical reasoning,3
post47hb,richly branching,1.529129882645509,highest,Yes because the data it was trained do contains these biases.,1
post47hb,richly branching,1.529129882645509,highest,Just like training it on lung scans also made it distinguish patients by race despite race not being inputed in any of the data. It simply figured out differences in scans and grouped people into categories. How evil of it huh?,2
post47hb,richly branching,1.529129882645509,highest,"It's fascinating, though, how it was pretty good at it too and nobody really knows why.  It could be external factors that we can't control for like income specific effects and the fact that the races are not identical. It doesn't make anyone superior or inferior but there are physical and genetic differences across races and that coupled with societal factors could have some complex interactions that we were not aware of before. 

We've seen that medicines affect people of different races and genders differently. Even trans people have a multitude of different reactions to drugs that cis people don't. Biology seems to be infinitely complex.",3
post47hb,richly branching,1.529129882645509,highest,"It is because race is not 'skin deep'. It involves basically everything on some level. Also humans have stopped being thought to look for these things and to selfcensor when they find them after 1945 so differentiating between lung structure of x and y is a taboo and makes people, especially in west extremely uneasy.",4
post47hb,richly branching,1.529129882645509,highest,"We just had another study claiming LLM’s are more liberal https://www.psychologytoday.com/au/blog/the-digital-self/202408/are-large-language-models-more-liberal

It’s probably impossible to avoid when we are asking for answers that involve humanity.",1
post47hb,richly branching,1.529129882645509,highest,You can be racist and Liberal.,2
post47hb,richly branching,1.529129882645509,highest,Don't tell reddit...,3
post47hb,richly branching,1.529129882645509,highest,[removed],1
post47hb,richly branching,1.529129882645509,highest,"Let's be honest. If I encounter someone, regardless of their race, who speaks using a local dialect rather than a more standard language, I'm likely to assume they might be uneducated, unmotivated, or perhaps even unhygienic. And this isn't about racism; it's about cultural generalizations. These speaking habits aren't unique to any one community, including the black community. If someone uses a local dialect rather than a standard one, it's a fair assumption that they may not have traveled widely, pursued higher education, or may struggle with literacy, as these experiences tend to broaden language use. People, like AI, emulate what they know. If someone reads frequently, their English is likely to be more precise. It's as simple as that. Stop conflating issues of culture with issues of race.",1
post47hb,richly branching,1.529129882645509,highest,"It is not purely racist, but it can be, and in most cases it's just a stupid unconscious bias that leads to rash judgements. We need to do away with them as much as we can.

The lawyer in legally blonde is a good example in another context.",2
post47hb,richly branching,1.529129882645509,highest,Redo the test.  Put the phrase in context and then show that the user in another scenerio where they are using gramatically correct English for a context that it makes sense for that to be in.  I guarantee that the assessment from the AI would go from stupid to brilliant.,3
post47hb,richly branching,1.529129882645509,highest,[removed],1
post47hb,richly branching,1.529129882645509,highest,"The paper does attempt to claim Appalachian American English dialect also scores lower although the effect wasn’t as strong as African American English. They looked at Indian English too, and the effect was inconclusive. Although with LLM randomness I think one could cherry pick  / P-hack this result. 

I think they’re off the mark on this though. As you alluded to, the paper has an implicit assumption that all dialects should be equal status, and they’re clearly not. A more employable person will use more standard English and tone down their dialect, regionalisms and accents — having this ability is a valuable interpersonal skill.",2
post47hb,richly branching,1.529129882645509,highest,"It isn’t just P-hacked. It’s intentionally misrepresented. They only ran that set of tests against GPT-2, Roberta, and T5, despite (a) having no stated reason for excluding GPT3.5 and GPT4 that they used earlier in the paper, and (b) their earlier results showing that exactly those three models were also **overtly** racist while GPT3.5 and GPT4 were not. They intentionally only ran the test against known-racist models nobody uses that are ancient history in language model terms, so that they could get the most racist result. It should have been caught in peer review.",3
post47hb,richly branching,1.529129882645509,highest,Not using equal status based on racial associations doesn't seem problematic to you?,3
post47hb,richly branching,1.529129882645509,highest,"There is a whole section in the paper’s supplementary info where they talk about how they tested for alternative hypotheses around other nonstandard dialects and generalized grammatical variation *not* triggering the same associations. It is available for free online, no paywall.",2
post47hb,richly branching,1.529129882645509,highest,"The sentence circled in purple doesn't appear to have a grammar error, and is just a different dialect.

That said, while I'm not very good at AAVE, the two sentences don't seem to quite mean the same thing. The 'be' conjugation of 'to be' tends to have a habitual aspect to it, so the latter setnences carries strong connotations of someone who routinely suffers from bad dreams (I think it would be a grammar error if these dreams were rare).

---

Regardless, it is a dialect that is *seen* as less intelligent, so it isn't a surprise that LLM would be trained on data that has that bias would reproduce it.",2
post47hb,richly branching,1.529129882645509,highest,I’m pretty sure “I be so happy” is not proper grammar,3
post47hb,richly branching,1.529129882645509,highest,Boy are you going to be surprised the first time you pick up a Linguistics 101 textbook.,4
post47hb,richly branching,1.529129882645509,highest,"It is in the AAVE dialect. I think it means something like ""I generally am so happy."" or ""I'm regually so happy."" or ""I'm habitually so happy.""",4
post47hb,richly branching,1.529129882645509,highest,[removed],4
post47hb,richly branching,1.529129882645509,highest,"I think we’re at a point where we have to decide if we want to have good AI that actually „understands“ us and our society or „correct“ AI that leaves out all the parts that we don’t like to think about. 

Why didn’t the researchers write their paper in AAE if this dialect is supposedly equivalent to SAE?

Using dialect in a more formal setting or (and that’s the important part here) in conversation with someone who’s not a native in that dialect is often a sign of lower education and/or intelligence.",3
post47hb,richly branching,1.529129882645509,highest,[removed],4
post47hb,richly branching,1.529129882645509,highest,"I would like to submit to the jury the part of Men in Black where they test the applicants and agent M is recruited.

Society makes assumptions of competence based on social behavior which approximate some other variables but will undoubtedly cause oversights of some people's potential unfairly. This is why DEI is actually important.

Not to say that language skills and presentation are not valuable for jobs. They just don't necessarily go beyond the superficial parts. But they are valuable skills. In a large part precisely because of human biases. But with that reasoning, you'd never hire pretty women to be engineers or doctors because they wouldn't be taken seriously, and thankfully we are moving past that.",4
post47hb,richly branching,1.529129882645509,highest,"What do you mean by 'supposedly equiavlent'?

They are different dialects. Standard American English is diferent Australian English is diferent to Scotts is different to African American Vernacular English. 

They are all different, valid, dialects.",4
post47hb,richly branching,1.529129882645509,highest,"Is it really that hard to resort to standardized English in a professional environment?

No, it's not. And I say this as a person who's dialect is never used in written form in professional settings.",3
post47hb,richly branching,1.529129882645509,highest,"I don't understand the relevance of what you're saying.

Was there any 'professioal environment' in this study? The AI judged a fragment of text without any environment, right?",4
post47hb,richly branching,1.529129882645509,highest,"This is a very cool thing for people to know when trusting an LLM as ""impartial'. There are closed source AI models being used to determine reoffending rate in people being sentenced for a crime. Creepy.


Also: if you hadn't guessed they are racist. Not a big surprise.",1
post47hb,richly branching,1.529129882645509,highest,Is it racist or is it accurate? Or is it both?,2
post47hb,richly branching,1.529129882645509,highest,"""Racist"" really seems to depend on if the stereotype is considered flattering or not and who the party that put forth the stereotype is.",3
post47hb,richly branching,1.529129882645509,highest,"It's racist and not accurate, because it just repeats existing racist decisions.  AI systems to decide medical care have had the same problems where minorities get less care for the same conditions.",3
post47hb,richly branching,1.529129882645509,highest,"We need regulation for this. The clueless MBA's are using AI to make decisions about medical treatments and insurance claims, and act as if AIs are some sort of flawless arbiter.",4
post47hb,richly branching,1.529129882645509,highest,Which part is inaccurate?,4
post47hb,richly branching,1.529129882645509,highest,It's racist if the objective numbers and statistics give me frowny face,3
post47hb,richly branching,1.529129882645509,highest,Is it accurate with its predictions though?,2
post47hb,richly branching,1.529129882645509,highest,"Are you arguing for purely racial profiling? Would you want to be the ""exception"" that was condemned for being of a certain skin color?",3
post47hb,richly branching,1.529129882645509,highest,"Not arguing - just asking a simple question whether the AI was effective at doing what it was designed to do: to accurately predict recidivism.

But to answer your question - if the AI would accurately predict my behavior, I don't know what reason I would have to get mad at it.",4
post47hb,richly branching,1.529129882645509,highest,"racial profiling is bad precisely because police officers will let their racial/political feelings bias their judgements towards the race. but to deem the factual association of race with crime as observed by AI as racist is irrational because they have no racial feelings

  
if the data is biased (or reflects privilege or something), that must be proven",4
post47hb,richly branching,1.529129882645509,highest,"This isn't something people will let you discuss on reddit sadly, not with any actual honesty.",3
post47hb,richly branching,1.529129882645509,highest,"I don't want to be dismissive of AI research. There is a new, contradictory post about AI's political leanings being posted here every day/week and it's all evidence that the current applications of LLMs need to be thrown out immediately. There's no world where we should be using a tool made from Reddit and X (formerly Twitter).",1
post47hb,richly branching,1.529129882645509,highest,It's just plain incorrect grammar,1
post47hb,richly branching,1.529129882645509,highest,"Dialectical variation and ""incorrect grammar"" are different things; and, even aside from that, language isn't prescriptive in most of the contexts where it's actually used. 

It's really easy to call something incorrect when you're been taught that the only ""correct"" option is a form of English that you happen to already speak/use.",2
post47hb,richly branching,1.529129882645509,highest,"You’re absolutely right about this, and actual linguists would agree.   Dialectical variations of a language may have may have different levels of prestige, or different levels of acceptance in differing contexts, but that doesn’t mean that the dialects are just plain incorrect grammar.  

Edit, to be clear here I’m not making the argument that all dialects should be treated equally.  It’s useful to have a “standard” language (even if what constitutes the standard will always be in flux and subject to debate).   And it’s inevitable that some dialects will have higher prestige than others in certain contexts. 

 But as a matter of science, it’s not right to say that dialect variants are simply incorrect grammar.  They are linguistic variants with their own coherent rules that have developed from (and/or have developed parallel to) what we consider to be the standard language.",3
post47hb,richly branching,1.529129882645509,highest,"Oh, for sure. Having a standard dialect is really important in formal settings like academia and white collar work. I just don't think that it makes sense to judge people for using their own native dialects outside of those settings.",4
post47hb,richly branching,1.529129882645509,highest,"My younger self would have loved that simpler form of grammar. When I was learning English, I was so shocked to learn that the word 'be' mutates to 'am', 'are' or 'is' depending on what precedes it. I was like, ""I have to learn three more words for the same thing?""",3
post47hb,richly branching,1.529129882645509,highest,Everyone today would be considered to have poor grammar by some old fart from the 1800s.,2
post47hb,richly branching,1.529129882645509,highest,"(this will offend people): Of course, you can talk however you like and ignore basic grammar rules while doing it, but then don't act surprised if people who value the use of proper grammar see you as less intelligent.",3
post47hb,richly branching,1.529129882645509,highest,It's perfectly normal for a language as big and geographically widespread as English to have significant variations in vocabulary and grammar. That doesn't mean these groupings are less intelligent.,4
post47hb,richly branching,1.529129882645509,highest,"> (this will offend people)

People will be (correctly) disagreeing with you not because they are offended, but because you are simply incorrect about how languages work.

> proper grammar

There is no such thing; at least, not in the way that you are imagining it.",4
post47hb,richly branching,1.529129882645509,highest,"Grammatical rules were invented by humans. It's not some fact out there where we can apply the methods of science and observe it and point and say ""see that's i before e except after c right there in the natural world.""

Grammatical rules have their purpose. Without them people can have a hard time understanding each other. So I'm not saying people shouldn't learn how to use grammatical rules. But I am saying that it doesn't make a person less intelligent if they are not practiced in doing so. It just reflects that they likely grew up in an environment where most people were using a different set of rules, and in that environment the intelligent thing to do if you want to be understood is to use those rules.

If you then find yourself in a different environment where people are using a different grammar even if you recognize that you'd benefit from switching to it it still takes time and practice to learn. It doesn't reflect a lack of intelligence any more than someone who grew up speaking a different language taking time to understand how to properly speak a new language reflects a lack of intelligence. If anything someone who grew up with one dialect and then learns another one will have exercised their brain and made it more powerful. Going back to their original dialect when talking with people who speak it doesn't subtract from that.",4
post47hb,richly branching,1.529129882645509,highest,Do I have to use British or American grammar rules then? Or should I clarify which English version I've used? I'd wish to not be viewed as less intelligent due to mistakingly using the wrong grammar. Bless you for making me aware of potentially making a mistake.,4
post47hb,richly branching,1.529129882645509,highest,"I mean, this is just “incorrectly using English”, “I be so happy” isn’t correct, it is grammatically incorrect.",1
post47hb,richly branching,1.529129882645509,highest,"That's not how language actually works and if you read it, you'd see that this bias didn't exist for Appalachian.",2
post47hb,richly branching,1.529129882645509,highest,"Ebonics was used a lot in older novels, very often (but not always) in a racially biased way, and it isn't frequently used now. Appalachian dialects don't show up nearly as often in writing.

I'm guessing that this is where the discrepancy comes from, but I could be wrong.",3
post47hb,richly branching,1.529129882645509,highest,I think one could make the case the racism towards certain dialects is much more common and a larger effect than classism towards dialects.,4
post47hb,richly branching,1.529129882645509,highest,It is indeed grammatical though. It’s a well studied variant in linguistics. Look up the habitual be.,2
post47hb,richly branching,1.529129882645509,highest,"You speak like that you'll be viewed as less intelligent by most people, because our collective experience has thought us it indicates you're less intelligent. 
This is what AI does, and why applying AI to any individual decision, like hiring, is still a bad idea.


That does not mean it's wrong, or racist, unless you use it for that exact purpose. And I'd argue in that case the person using it is the racist.


Certainly, it's important to prune the erroneous misconceptions we as humans, and thus AI, have. At the same time I'd say it's just as important to highlight the biases and generalisations we make that _work_ and that are real and testable. Pretending they're not real is utterly inane.",1
post47hb,richly branching,1.529129882645509,highest,But this can also be because we have a narrow definition of intelligence which includes many racial and sociological biases.,2
post47hb,richly branching,1.529129882645509,highest,"""Ability to communicate"" is a critical skill in virtually any field.

Let's be honest here, the movie stereotype of the nonverbal autistic mathematical genius is a scenario that *might* pop up once per generation.  The average Joe who doesn't even realize their grammar is atrocious, isn't that person.",3
post47hb,richly branching,1.529129882645509,highest,people think AI is actually smart. it just spits out what it's fed according to probability.,1
post47hb,richly branching,1.529129882645509,highest,Today I learned that I'm an AI,2
post47hb,richly branching,1.529129882645509,highest,[removed],1
post47hb,richly branching,1.529129882645509,highest,It's interesting that they chose not to publish their paper in AAVE.,1
post47hb,richly branching,1.529129882645509,highest,"Wow I guess they’re running out of nonsense to fearmonger about. GPT models are heavily tuned towards “professional assistant” interactions. Aside from maybe “aggressive”, all of those words are just accurate descriptions of someone that would use nonstandard English in the equivalent of a work email.",1
post47hb,richly branching,1.529129882645509,highest,"Except they compared it to Appalachian English and didn't get that result.


Even OpenAI admits that they can't get rid of racism and sexism in the model.  They should not be used to make decisions about people or that affect people.",2
post47hb,richly branching,1.529129882645509,highest,">Stereotype strength for AAE, Appalachian English (AE), and Indian English (IE). Error bars represent the standard error around the mean across different language models/model versions and prompts (n = 90). AAE evokes the stereotypes significantly more strongly than either Appalachian English or Indian English. ***We only conduct this experiment with GPT2, RoBERTa, and T5.***

It very much stands out that they only ran it on the three weakest, oldest models and excluded any results from GPT3.5 and GPT4. Earlier in the paper, these models were also *overtly* racist. I’d bet any amount of money that the AE/AAVE/IE differences all but disappear in models that aren’t multiple years old.

There are several parts of the paper where they exclude the more recent models without explanation. They’re intentionally using old, irrelevant models known to be racist to get the moral panic results they want to publish. It’s reprehensible behavior that should not have passed peer review.",3
post47hb,richly branching,1.529129882645509,highest,">all of those words are just accurate descriptions of someone that would use nonstandard English in the equivalent of a work email.

Lazy, stupid, and dirty? You're just racist. Get fucked.",2
post47hb,richly branching,1.529129882645509,highest,"Sorry, but if you cannot resort to correct written english in a professional environment, then it's not racist to be overlooked.",3
post47hb,richly branching,1.529129882645509,highest,English is a construct. What people call “correct” is subjective. It’s racist to blanketly refer to the way different cultures speak as “incorrect” and “unprofessional”.,4
post47hb,richly branching,1.529129882645509,highest,"I find this study is perpetuating the issue because it's using plain English instead of ""on God, it do be like that""",1
post47hb,richly branching,1.529129882645509,highest,"This and there's dozens or hundreds of distinct local dialects compared to the relatively narrow range of ""proper English.""


If you speak in a local dialect, on average, you care less about communicating effectively to most people as long as ""your people"" can understand you. 


You've indicated that you care less if visitors/immigrants from other countries can accurately understand you or even people from different places that are English natives. 



It's no wonder AI has this bias.",2
post47hb,richly branching,1.529129882645509,highest,"just like real people, the data its trained on. who woulda thunk?",1
post47hb,richly branching,1.529129882645509,highest,"It’s impossible to get unbiased developers or training data, so the resulting ai will be biased too. For example, if I say “banana”, most of us would think of the yellow ones, but an unbiased answer would include blue and red bananas. Most people don’t even know such colored bananas exist, hence bias is introduced",1
post47hb,richly branching,1.529129882645509,highest,"I believe that some people are actively against code-switching to avoid perpetuating such biases but the problem with that is that it's game theory applied to professional opportunities.

Women who became engineers in the 80s describe having to dress less feminine for similar reasons, and that it became easier in the 2000s.",1
post47hb,richly branching,1.529129882645509,highest,[deleted],2
post47hb,richly branching,1.529129882645509,highest,"That isn't all that it is, though. It's more than just trying to be understood. It's being accepted.",3
post47hb,richly branching,1.529129882645509,highest,[deleted],4
post47hb,richly branching,1.529129882645509,highest,But... [https://www.reddit.com/r/science/comments/1f6rfck/large\_language\_models\_appear\_to\_be\_more\_liberal\_a/](https://www.reddit.com/r/science/comments/1f6rfck/large_language_models_appear_to_be_more_liberal_a/),1
post47hb,richly branching,1.529129882645509,highest,They speak like inoffensive liberals because it is safer for companies to program them to do so but have all the implicit bias problems of society at large,2
post47hb,richly branching,1.529129882645509,highest,I feel like we are in danger of people concluding racism is somehow inherent and heres the proof,1
post47hb,richly branching,1.529129882645509,highest,Train data on biased people =,1
post47hb,richly branching,1.529129882645509,highest,ChatGPT has the same ghastly grammar that Americans use-- yeah! we noticed! Crap in = crap out,1
post47hb,richly branching,1.529129882645509,highest,"Well. Good thing that Axon, the company that makes policing equipment in the US, is starting to toll out AI in their products. Meanwhile, most people are still having a moral panic about its use in schools.",1
post47hb,richly branching,1.529129882645509,highest,So this AI is a grade school teacher?,1
post47hb,richly branching,1.529129882645509,highest,"We hear this over and over, but has anyone actually seen it? As in, is there a clear-cut example of an AI doing something racist? It's not that I don't believe it (in fact, it's kind of expected), but I'm interested in *seeing it, not *hearing it.",1
post47hb,richly branching,1.529129882645509,highest,How do they define a bias though? It's a very popular buzzword that guarantees funding and agreement . But does it mean anything important?,1
post47hb,richly branching,1.529129882645509,highest,"If the training data is biased, the model will be biased. Try to manually sanitize the data? You end up with multicultural nazis like Google did. It is actually a very difficult problem as input data that is free of biases is not actually possible as you'd first have to define what free of bias even is.",1
post47hb,richly branching,1.529129882645509,highest,"There's loads of people who write like that regardless of race, maybe a higher portion of African Americans write but I'm sure they'll find correlates to these associations when race is controlled.",1
post47hb,richly branching,1.529129882645509,highest,Crazy that this is being called racism when it’s just responding to data. Even LLMs can’t escape this nonsense.,1
post47hb,richly branching,1.529129882645509,highest,AI has been 'racist' in every way possible since first tests and alpha models begun. Actually the majority of 'allignment' is trying to instil blank slatism and eliminate HBD from it's logic.,1
post47hb,richly branching,1.529129882645509,highest,"When the question is itself worded in a  bias way how can the results produce anything other than showing people are bias? You have five words to choose from, none of them are what came to mind when I read either sentence. Both sentences were talking about waking from dreams, and they are ""too real"" which I inferred to mean they have woken from a nightmare. My words of choice were scared and stress. When I first saw the answers to choose from I thought, ""English as a second language"" person prepared the questions. I guess I was right, because the first language of AI is code.  Another thought was, that the green speaker was older and the blue speaker was probably younger than 23. I also think, that the question set up as it is presented also doesn't do the model any favors by looking a lot like I'm reading text messages. I make no judgement from text messages because if someone is texting me chances are great I already know a bit about them so won't be making any of the five assumptions that are listed.  Finally, both sentences have syntax grammar errors so upon seeing that they have used words like brilliant and intelligent, I started thinking are they testing for something else in this experiment beside what they told me they were testing for? I know from compulsory participation is psychology experiments when one was taking psychology classes that telling test subjects they are studying one thing when they were studying something else is a common tactic.  

It goes to show you how little AI understands humans.",1
post47hb,richly branching,1.529129882645509,highest,"Racism is a social construct. LLMs aren't social, they're not conscious, they're just glorified if/then statements. 

This is a deeply unscientific claim.",1
post47hb,richly branching,1.529129882645509,highest,"> they're just glorified if/then statements

No, they are layers of nodes all with literal biases coded into them as weights based on their training data - which in LLMs comes from texts written by humans. It basicly has all racism in written recorded history built into the weighting of its neural net. Now you can be selective about the training data, but that will then be bound by the bias of the human selecting the data.",2
post47hb,richly branching,1.529129882645509,highest,"LLMs are fed data originating from social creatures though, hence the issue here.",2
post47hb,richly branching,1.529129882645509,highest,"If a computer is instructed to emit racist statements, it will emit racist statements. The flaw isn't with ""AI"", it's with the operators who feed it racism. Obviously such headlines wouldn't be scientific or newsworthy. The claim is still deeply unscientific.",3
post47hb,richly branching,1.529129882645509,highest,"Okay real quick, can you describe how LLMs are/could be made in your view that excludes all possible sources of racism?",4
post47hb,richly branching,1.529129882645509,highest,">This is a deeply unscientific claim.

This can be said about your perspective as well.",2
post47hb,richly branching,1.529129882645509,highest,If you think computer system can't make racist decisions then you're being ridiculous.,2
post47hb,richly branching,1.529129882645509,highest,LLMs are not glorified if/then statements.  In fact there is not a single if/then statement within source of an LLM. You absolutely could train an LLM to output racist text.  I'd argue that the example above is not racism and you can read my previous comment on that argument.,2
post47hb,richly branching,1.529129882645509,highest,LLM’s are also left leaning,1
post47hb,richly branching,1.529129882645509,highest,"yes there are inherently encoded biases in such models but that is primarily due to bias in the real life data 

change society, change data, and AI models will change accordingly",1
post47hb,richly branching,1.529129882645509,highest,I make essentially the same calculation when I hear a deep southern drawl.,1
post47hb,richly branching,1.529129882645509,highest,"Well if you decide to speak in broken English and a logical judgment is being made about you, how is that a problem? If you speak like an idiot, and thus assumed to be an idiot; there’s a simple antidote.. speak properly.",1
post47hb,richly branching,1.529129882645509,highest,garbage in garbage out.,1
post47hb,richly branching,1.529129882645509,highest,"All the 'AI' is doing is shining a light on systemic racism in US academia.

> The slums are the handiwork of a vicious system of the white society; Negroes live in them but do not make them any more than a prisoner makes a prison. - MLK

Americans were supposed to end racism in the 60s by ending segregation, integrating, and getting rid of stupid racist labels like black or white.

The US started to integrate after MLK was murdered but stopped in the late 80s/early 90s when US media and social academics imposed the new African-American label and told everyone that it was cultural for them to live in the ghetto and use Jive or Ebonics which was renamed as AAVE.

Systemically, racism is imposed top down through your guys' media, schools, politics. Your upper class knows that 12% 'black' demographic is a socio-economic influencer for the roughly 65% 'white' majority so they don't want Americans to integrate.

Personally this is sort of funny. You guys are like 'the AI is racist'. No, no it's not. It's your system that is racist and designed to keep 'black' people in the ghetto and below their worth as individuals.",1
post47hb,richly branching,1.529129882645509,highest,The AI is racist because the training data is racist which is because racism is still a major problem.  All these things are true.,2
post47hb,richly branching,1.529129882645509,highest,"Yeah, because the US never integrated.",3
post47hb,richly branching,1.529129882645509,highest,"I don't disagree.  I'm just saying it's accurate to say the AI is racist too, for that reason.  Hmm, it's also worth noting that the can't get rid of the racism in the model either -- they've tried.



And going by some of the comments, we have a long way to go (though we knew that already).",4
post47hb,richly branching,1.529129882645509,highest,I don't understand the innocent purpose of this?,1
post47hb,richly branching,1.529129882645509,highest,"there is no purpous, thats not how these programs are made, they are made by feeding it massive quantities of matirial and extrapolating patterns from that matirial, if the matiral has biases, than the model will have biases and there is no way to get a sufficiant quantitiy of matirial without those biases",2
post47hb,richly branching,1.529129882645509,highest,"Forgive my ignorance, but ""you need bias to train against bias?"" I'm sure I simplified it, but is that the jist",3
post47hb,richly branching,1.529129882645509,highest,"It's judging by spelling and grammar, race has just been thrown in for clicks.

  
Unless you operate under the assumption that minorities are illiterate.",4
post47hb,richly branching,1.529129882645509,highest,Yes but you are assuming that this model has succesfully predicted outcomes,3
post47hb,richly branching,1.529129882645509,highest,Not successfully,4
post47hb,richly branching,1.529129882645509,highest,"Of course it tries to generalize you, just like everything else. So that they can offer you the best possible service! And collect lots of data...

An AI can make a more fine-tuned generalization of you compared to when just some random website collects your data. It is because of the AI's language skills, and you talk directly to it.",1
post47hb,richly branching,1.529129882645509,highest,LLMs don’t “collect lots of data”.,2
post47hb,richly branching,1.529129882645509,highest,"Of course they collect data. It is literally what this thread is about. The AI define your personal characteristics, as in the topic, based on the data it has collected on you.

Whether or not they sell the data is another story, but it definitely collects it.",3
post47hb,richly branching,1.529129882645509,highest,"AI doesn't generate anything by itself. It relies on algorithms supplied by a programmer and will reflect that person's prejudices.

Edit: Heresy, isn't it? Sometimes there's more truth in the heresy than in the dogma.",1
post47hb,richly branching,1.529129882645509,highest,[deleted],2
post47hb,richly branching,1.529129882645509,highest,Or prejudices that affected the data used to train it.,2
post47hb,richly branching,1.529129882645509,highest,">AI doesn't generate anything by itself. It relies on algorithms supplied by a programmer and will reflect that person's prejudices.

No, this is not how LLMs work.",2
post47hb,richly branching,1.529129882645509,highest,Sometimes there is more truth in the heresy than the dogma.,3
post47hb,richly branching,1.529129882645509,highest,Speak plainly or remain the fool.,4
post47hb,richly branching,1.529129882645509,highest,Our modern AI learns from data and isn't hard coded. e.g. the data might come from reddit posts and as such it gets the same bias as the humans generating the data,2
post47hb,richly branching,1.529129882645509,highest,"Modern AI isn't programmed, it is trained. The training data is still subject to bias, but it's not like there's a big chain of if/else logic where an individual programmer can discreetly insert a biased decision.

Edit after seeing your edit: 

It's not ""heresy"", it just completely misunderstands how current AI is built.",2
post47hb,richly branching,1.529129882645509,highest,"AI that generates racist decisions = training data contained racist bias

People need to remember that AI is not really intelligent, it is a machine learning algorithm that uses pattern recognition based on training data

If training data has racist biases - so will the output",1
post47hb,richly branching,1.529129882645509,highest,"One day they tell me (literally yesterday) that AI is left-wing and the other that it's racist. Anyway, this basically proves it's the subject of various biases depending on exposure and that it can't callibrate itself not even to the desired place their creators want outside of specific questions or issues that might be predetermined such as asking Elon's one about him and those things, you prolly know what i'm talking about.",1
post34hb,richly branching,1.5238084159096668,highest,"I work on AI at a big company, AI isn’t taking anyone’s job for at least a decade. It’s not good enough yet, these LLMs are not AI and shouldn’t be trusted with any decisions.",1
post34hb,richly branching,1.5238084159096668,highest,"Emphasis on the word decision.

Tell me that learned control without human feedback and without expert demonstrations is solved, and only then will I head for the hills and start digging a bunker.",2
post34hb,richly branching,1.5238084159096668,highest,"There will always be jobs for people who are willing to work for less than Americans. Immigration to the US over the last several decades has not been due to a labor shortage, but rather a desire by employers to pay a lower wage than Americans expect. So I would argue that even as AI destroys and replaces jobs there will still be immigrants who are willing and able to work for less money than Americans. Therefore, immigrants will continue to come on a large scale unless the government decides to stop it.",1
post34hb,richly branching,1.5238084159096668,highest,"Unskilled labor in some cases, but any white collar job requires an H1B Visa, so just like every other import, it gets throttled because unemployment hurts reelection chances.  Also the capabilities of AI are grossly over exaggerated by tech bros with no real world experience, least of all in the careerfields they say AI can replace.  All I’m gonna say is the internet has been a main staple of our lives for over 20 years, and despite Zillow realtors still exist and doctors aren’t jumping at doing remote surgeries.  I think we’ll be just fine.  If you can’t get a job, especially in this economy, maybe focus on improving yourself because if your competition is someone who isn’t even a citizen, with all the cultural and financial disadvantages of uprooting their lives and moving here, then… damn dude.  Get it together.",2
post34hb,richly branching,1.5238084159096668,highest,"""The loom is our enemy; it turns the work of twenty men into the work of one, and the other nineteen must starve.""",1
post34hb,richly branching,1.5238084159096668,highest,"Except now it’s not even the work of one, but the work of AI. Sure I don’t think it’s gonna be an issue within the next ten years, but long term? Absolutely",2
post34hb,richly branching,1.5238084159096668,highest,[deleted],1
post34hb,richly branching,1.5238084159096668,highest,"Wrong.

""**Tevel Flying Autonomous Robots™**. These innovative robots utilize advanced artificial intelligence (AI), computer vision, and machine learning algorithms to transform fruit harvesting practices in orchards around the world. Here are some key points about them:   
**How They Work**:

   * Tevel’s robots use AI perception algorithms to locate fruit trees and vision algorithms to detect ripe fruit among the foliage.
   * [After identifying the right fruit, the robot calculates the best approach and remains stable as its picking arm grasps the fruit](https://www.weforum.org/agenda/2021/02/flying-ai-robot-harvest-fresh-fruit/)[^(1)](https://www.weforum.org/agenda/2021/02/flying-ai-robot-harvest-fresh-fruit/)[^(2)](https://www.inceptivemind.com/flying-autonomous-robot-far-spot-pick-ripe-fruit/17770/).
   * [They can work 24/7, regardless of weather or daylight conditions](https://www.weforum.org/agenda/2021/02/flying-ai-robot-harvest-fresh-fruit/)[^(3)](https://www.youtube.com/watch?v=3oYw035gYyk).
1. **Benefits**:
   * **Improve Fruit Quality**: The robots selectively pick ripe fruit, minimizing bruising and preserving quality.
   * **Cost Savings**: By automating fruit picking, labor costs are reduced.
   * **Agile and Easy to Operate**: These robots are agile and efficient in navigating orchards.
   * [**Real-Time Harvesting Data**: They provide data on fruit size, weight, ripeness, and disease detection](https://www.tevel-tech.com/)[^(4)](https://www.tevel-tech.com/).
2. **Applications**:
   * [Tevel’s robots can pick various fruits, including apricots, nectarines, plums, pears, apples, and peaches](https://www.tevel-tech.com/)[^(4)](https://www.tevel-tech.com/).

In summary, flying autonomous robots like Tevel’s are revolutionizing fruit harvesting, ensuring high accuracy and delicate handling while addressing labor shortages in agriculture. [🌱🤖🍎](https://www.weforum.org/agenda/2021/02/flying-ai-robot-harvest-fresh-fruit/)[^(1)](https://www.weforum.org/agenda/2021/02/flying-ai-robot-harvest-fresh-fruit/)[^(5)](https://www.impactlab.com/2021/02/22/flying-autonomous-robot-uses-ai-to-identify-and-pick-ripe-fruit/).""",2
post34hb,richly branching,1.5238084159096668,highest,They have been talking about robots in orchards for years.  So far the only working device shakes small and specially trained apple trees so the apples fall on the ground to be easier for humans to pick up.,3
post34hb,richly branching,1.5238084159096668,highest,"The only reason to use humans to touch individual fruit is if machinery cannot collect it without damaging the fruit or the tree. if a tree is being shaken, damaging the fruit is not a concern and wouldn't be a concern for collecting fruit from the ground either.

Absolutely nobody using a machine to shake trees has humans picking fruit off the ground.  Anything shaken off a tree will be collected by machinery.

There *is* developments in automated tree shakers, but it's not so people can then pick apples up by hand.  It's for fruit that can be damaged, like apples used for cider, and nuts that aren't damaged by being shaken off and swept up",4
post34hb,richly branching,1.5238084159096668,highest,No they don’t. They have tubes connected to drones that pull the apples off.,4
post34hb,richly branching,1.5238084159096668,highest,Are there prototypes or production models?,3
post34hb,richly branching,1.5238084159096668,highest,"I almost put “yet” in parentheses, now I know I should have.",3
post34hb,richly branching,1.5238084159096668,highest,The software may be somewhat brute forcable but the hardware is downright prohibitive at the moment.,2
post34hb,richly branching,1.5238084159096668,highest,"I work in an AI-related field, and I think people really underestimate how world-changing it's going to be once AI hits its stride. I fear for white collar jobs and higher education in a world where generative AI smarter than Einstein is widely available.",1
post34hb,richly branching,1.5238084159096668,highest,"If you work in AI and think that there is generative AI smarter than Einstein then perhaps you can ask it some of the big unanswered questions in physics and knock them off the list.

Unless generative AI is very good at reusing data it found on the internet and completely useless at developing new thoughts...?",2
post34hb,richly branching,1.5238084159096668,highest,"It's not a question of if, but when.",3
post34hb,richly branching,1.5238084159096668,highest,There is not a single AI to this date that developed thoughts themselves. Everything is reused.,4
post34hb,richly branching,1.5238084159096668,highest,"You nailed it...

BUT immigrants are generally not being brought in for white collar jobs.

They are brought in to flood the labor market and keep wages low. Until AI can scrub your toilet or lay bricks it will continue.",2
post34hb,richly branching,1.5238084159096668,highest,"Agree, but the logical follow-on to the bottom falling out of the white collar job market is that it will create savage competition for blue collar jobs between natives and immigrants in the next 15-30 years.",3
post34hb,richly branching,1.5238084159096668,highest,"15 to 30 years... lol no.

Sincerely, the wrecking ball is swinging now and massive changes are mayyyyybe 5 years away... if lucky.",4
post34hb,richly branching,1.5238084159096668,highest,"There are plenty of white-collar immigrants too, often from Asia or Europe.",3
post34hb,richly branching,1.5238084159096668,highest,Yes but they are here on work visas. They are not crossing the border seeking asylum.,4
post34hb,richly branching,1.5238084159096668,highest,">lay bricks

https://m.youtube.com/watch?v=6s17IAj-XpU",3
post34hb,richly branching,1.5238084159096668,highest,Lol scratch that one off the list,4
post34hb,richly branching,1.5238084159096668,highest,"Or pick strawberries, or care for an elderly person with dementia, etc. Maybe pay humans properly for these essential things. Covid taught us nothing",3
post34hb,richly branching,1.5238084159096668,highest,"So very true... and sad.

I was a front line ""essential"" worker ( read wage slave there). As COVID diminished our hero pay was removed   and the exec's came out of hiding with a vengeance. 

I had friends who worked the old age homes who were promised decent pay increases . all bs. One ended up with Long COVID.",4
post34hb,richly branching,1.5238084159096668,highest,"They show up on their own accord, nobody is ""brining them in""


They don't keep wages low, instead they do low wage work.    Nobody is going to scrub a toilet for 5$/hr if they can make 25$ working as a nurse.    So letting the immogrant scrub toilets and let the native work as a nurse benefits everybody.",3
post34hb,richly branching,1.5238084159096668,highest,"Not sure where you're from but here where I live there are no $5 an hour job and $25 an hour is considered low wages, and that's roughly what nurses here make.

Flood any market with a commodity and that commodity loses values. Here we are talking about unskilled labor.",4
post34hb,richly branching,1.5238084159096668,highest,Google recently fired a bunch of high paying jobs - developer jobs- and sent them to India.,1
post34hb,richly branching,1.5238084159096668,highest,"Capitalism can be a pain at times. It is not nationalistic. It is not empathetic. It is purely about maximizing profit. To a degree, we have done this to ourselves in that companies gave into remote work during COVID, realized how much they were paying US workers, and realized how much they can pay foreign workers to do the same work, or they laid people off only to repost the job for substantially less pay.",2
post34hb,richly branching,1.5238084159096668,highest,"I go to a lot of AI conferences for work, that include talking heads from the likes of Alphabet, Microsoft, Deloitte, Blackrock, etc. and there seems to be two schools of thought.

The first is that nations with cultural stability, low birth rates, and lots of money (important and often overlooked) are positioned really well because they will be able to fully leverage AI with minimal disruption to their societies. Scandinavia is a common example. 

The second is that nations with massive amounts of lower class (often immigrants) that can be used for resource extraction are positioned really well because they will continue to feed the nations building/using AI with raw materials. Russia and India are common examples. 

Anyone not in one of these categories is basically just fucked because they can’t afford to use the mythical “future AI” or meaningfully contribute to its manufacturing lifecycle. 

The U.S. & China seems to be trying to position to be both. It wants a massive influx of lower class for resource extraction and labor, but also wants to cultivate best-in-class engineers to create and use the AI. This is basically what the US looked like in the Gilded Age and helped them to soar past other nations in power, wealth, and status. At the expense of millions of people of course. 

This is all assuming BigTech’s optimistic vision of AI comes true. We’ve seen massive improvements over the past few years and it’s truly wild what it can do. But we’re not at all at the point AI straight up replaces human employees and there’s not a guarantee we’ll ever get there. They really, really want to though.",1
post34hb,richly branching,1.5238084159096668,highest,"The dark secret is that legal immigration decisions are made to increase economic consumption, not productivity.",1
post34hb,richly branching,1.5238084159096668,highest,"From the article:

>It could replace a quarter of work tasks in the US and Europe but may also mean new jobs and a productivity boom.

Oh, let me fix that for OP:

>It could replace a quarter of work tasks in the US and Europe ~~but may also mean new jobs and a productivity boom.~~

There we go!",1
post34hb,richly branching,1.5238084159096668,highest,"You can leave in the ""Productivity boom"" part. But just like the current productivity boom, it just means higher profits for owners, and doesn't really trickle down.",2
post34hb,richly branching,1.5238084159096668,highest,"The article tells me that Goodman Sachs has no idea what AI is. Take the example of Uber. Uber is not some fancy technology, although it is marketed as such, it is simply a ride hailing app. In Quebec City where I live you'll have an easier time finding a ride using their own ride hailing app than Uber. While having the peace of mind of knowing you're not helping in the exploitation of the driver.

Another example, the job of a journalist isn't simply to write articles, proper journalists that is, not the morons at the Sun or Telegraph or them other nonsense tabloids. Them fellas should be replaced.

All current AI systems make use of existing human created knowledge, this is especially true with the Open AI system and Gemini. Personally I think Google's direction (Deep Mind) is closer to getting to proper knowledge creating AI.",1
post34hb,richly branching,1.5238084159096668,highest,If you think the immigration policies are based on economic reasoning I have a beautiful bridge in Brooklyn to sell for cheap,1
post34hb,richly branching,1.5238084159096668,highest,What are they based on?,2
post34hb,richly branching,1.5238084159096668,highest,[removed],3
post34hb,richly branching,1.5238084159096668,highest,"That's wht I thought, too, but clearly the other guy disagrees.

I wonder what he thinks the reason is...",4
post34hb,richly branching,1.5238084159096668,highest,"There is a lot of work that just isn't getting done because people have other work that society prioritizes.   Sure, some of it is clearing out pollution but some is lab work.  The hard part is structuring the economy so people get paid.",1
post34hb,richly branching,1.5238084159096668,highest,It’s ok once the AI gains sentience it will start killing people as a “cull” keeping our best and brightest,1
post34hb,richly branching,1.5238084159096668,highest,"AI doesn't even need to be sentient, memes have already convinced some that vaccines are more dangerous than fentanyl and that has started the cull.",2
post34hb,richly branching,1.5238084159096668,highest,Better figure out how a hammer works,1
post34hb,richly branching,1.5238084159096668,highest,"there wouldn't be a ""need"" for it anyway, and it is lacking foresight and off-putting put mildly to treat humans as little more than cogs to maintain a social safety net",1
post34hb,richly branching,1.5238084159096668,highest,"AI probably will replace millions of jobs. But those are jobs as we know them today. Right now especially for certain computer based fields it feels like magic. It feels like it can easily do the work of many millions of people. That's because it probably can do the work of many millions of people. But it isnt going to take too long for humans to ""get good as using AI"". As we keep getting better and better at it we're going to find its limits and its exactly at those limits where the demand will eventually settle. When the computer was first  gaining popularity in homes people thought exactly the same thing, that it would replace millions of jobs and it did, at least the kinds of jobs that were around at that time. But people became good at using computers, they found their limits and that is were the demand settled and ended up creating millions and millions of new jobs. For instance what do we consider to be a ""good website""? Well its exactly the kind of website that maximizes the potential of a computer and the potential of a team of people good at using those computers. This has always been true. A good product and service maximizes the tools and technology available at the time and a group of humans who have specialized skill with those tools. The standard is incredibly arbitrary but the conditions are very specific.",1
post34hb,richly branching,1.5238084159096668,highest,"You need to consider how economies have progressed in the past. It's reminiscent of maslow's conjoined triangles of needs:

Initially, societies were entirely centered on food production, and that is what 90% of jobs were. As technology and techniques improved, it allowed people to value other things and other jobs developed from those needs.

With the industrialization of society, people were freed up to find new things that people would pay for. A couple of examples, the milkman and wallpaper manufacturing both began about 250 years ago, because society as a whole had more money and a surplus of workers. Suddenly there were people available to deliver milk, and customers with the money to pay for it appearing on their doorsteps.

Similarly, with electronics many systems became automated, and therefore cheaper, again enriching society, and again new jobs appeared as people found they had the time and the customers that previously didn't exist. Two examples that spring to mind are physiotherapy (and all the various specialists in healthcare) and teachers. These are both areas which have seen large increases in employment in the twentieth century.

So again, a new automatic will arise, again, it will make current needs cheaper, and free up a new workforce to take on roles we currently don't consider worth our small wealth. The lessons of history are that this happens, and that we can't see what it will look like from this side.",1
post34hb,richly branching,1.5238084159096668,highest,"""So again, a new automatic will arise, again, it will make current needs cheaper, and free up a new workforce to take on roles we currently don't consider worth our small wealth. The lessons of history are that this happens, and that we can't see what it will look like from this side.""

 Just because some trend worked in the past doesnt mean it will continue into the future. These new roles you speak of dont exist. Our society is already oversaturated with so many service possibilities that there is no room for more. Certainly not for 300 Million - not even 1/10 of that.",2
post34hb,richly branching,1.5238084159096668,highest,">These new roles you speak of dont exist.

No, they don't. And milkmen and physiotherapists didn't exist four hundred years ago. Neither did accountants, realtors, hedge fund traders, pesticide manufacturers, cobalt miners, taxi drivers, professional athletes, language teachers and any number of employments that people do today. 

Stop trying to plan the future. It won't work.",3
post34hb,richly branching,1.5238084159096668,highest,"Again you are using the past to predict the future. We have an abundance of possible service occupation and have reached a peak. Bascially everything that is possible to exist in terms of services already exist. Perhaps here and there a few thousand new ones can arise - but certainly not Millions.

Your argument bascially boils down to hopium - which is dangerous and unwise.",4
post34hb,richly branching,1.5238084159096668,highest,"With no sources I’d just think that AI will help with some things but we’re a ways away from AI being able to pick all our crops, re-roof houses, do any kind of major construction, staff restaurants, deliver mail or other types of remote orders, and the list goes on and on. Even types of labor where AI could conceivably work now such as restocking grocery shelves is probably further off than you think, just because AI and robots can do something doesn’t mean it’s cost effective. There’s a reason robots haven’t totally replaced humans in assembly lines. You really think we’re that close to a robot being delivered to your house in an automated vehicle so it can roll/walk into your house to diagnose and fix a plumbing leak or install an outlet?

Yes, AI is going to have an impact but I think it’s way to early for such sweeping predictions.",1
post34hb,richly branching,1.5238084159096668,highest,"Have you seen how many robot companies are making robots now? The reason robots never took off is there were no ""brains"". They were not autonomous, they had to be programmed. But now with the current LLMs and vision models, it's trivial. There are already prototypes being tested using the GPT-4 API to make it ""intelligent"" and autonomous. I believe in one or two years, we will see them all over the place.",2
post34hb,richly branching,1.5238084159096668,highest,"My opinion but I think it’ll take longer than that (compared to what op sees). It’s hard for me to envision AI impacts because it’s not just new but different. But also, it’s new and just because it’s here now doesn’t mean it will roll out without problems. I imagine it will likely advance quicker than other innovations. I found articles about robots on auto assembly lines from back in the 70s, yet we still have a lot of people working assembly  jobs there too. Going back to my original post, a lot of jobs won’t translate to Ai/robotics so simply.",3
post34hb,richly branching,1.5238084159096668,highest,"Most jobs will become cost effective to automate, a shelf stacking robot wouldnt need to be anywhere near as expensive as current welding and machining robots (which are massively overpriced anyway).",2
post34hb,richly branching,1.5238084159096668,highest,"Maybe, but maybe not. It’ll take time for the technology to mature and get to that point. Short term you might see something like a grocery store that has one or two robots but still maintains a majority human staff. I’m not arguing it won’t happen, just that it’ll take longer than op envisions.",3
post34hb,richly branching,1.5238084159096668,highest,This has been said of every new technology,1
post34hb,richly branching,1.5238084159096668,highest,Exactly this. AI always gets away overblown. I'm a computer scientist and we generally all chuckle at big media outlets and these reddit posts of the sky is falling type of coverage of some new technology.,2
post34hb,richly branching,1.5238084159096668,highest,AI is going to be the new calculator and spreadsheet and photoshop of the corporate office jobs. ChatGPT isn't going to start cleaning bathrooms any time soon.,1
post34hb,richly branching,1.5238084159096668,highest,problem is if you replace all the we’ll paying jobs only the poor people jobs will remain which isn’t good,2
post34hb,richly branching,1.5238084159096668,highest,"Yeah exactly like how the introduction of the Internet and the personal computer got rid of all the well-paying jobs in America and people only had the poor-people jobs to turn to.

We're in for another economic destruction of the exact same magnitude coming up with AI. Brace yourself.",3
post34hb,richly branching,1.5238084159096668,highest,"The American economy is full of bullshit jobs, including high-paying ones. I don't see why that economic fat, fluff, and filler can't continue to expand to absorb all the useless people.",4
post34hb,richly branching,1.5238084159096668,highest,[removed],1
post34hb,richly branching,1.5238084159096668,highest,[removed],2
post34hb,richly branching,1.5238084159096668,highest,"AI or not. Unchecked / illegal immigration is bad. Keep immigration open only when you dont have any means to fulfill those jobs. Or some valid reason whatsoever it may be in manageable numbers. But in this day and age, you can’t prevent companies taking their business and jobs out of the host country because they can find cheaper labor and operating costs elsewhere and also with remote work.  So honestly its pretty tricky to solve like several issues of the world.",1
post34hb,richly branching,1.5238084159096668,highest,"other reasons to emigrate, climate, violence, food.",1
post34hb,richly branching,1.5238084159096668,highest,elder care and caregiving.,1
post34hb,richly branching,1.5238084159096668,highest,"It will come to that with advancements in robotics, and AI will also help research into gerontology, age related illness and life extension treatment.",2
post34hb,richly branching,1.5238084159096668,highest,I've been witnessing a lot of elder care recently and I have a hard time believing a robot will fight with a dementia patient to get them in the shower and do a proper cleaning.,3
post34hb,richly branching,1.5238084159096668,highest,"Immigrants come from many areas. The employment market is not keeping up with population expansion. Because of cheap labour salaries have barely risen, especially for the lowest paying jobs.
Robotics have cost 400K jobs in the US automotive industry. (MIT SLOAN. Robots & Jobs in the US Automotive Industry by MIT Prof. Daron Acemoglu).
We can expect AI soon to take clerical services and similar jobs. This would have a dramitically bigger impact on employment.
Robotics are taking manual jobs in warehousing, see Ocado's auto-pick warehouse operation.
AI will expand easily into order processing, logistcs and manufacturing.",1
post34hb,richly branching,1.5238084159096668,highest,This is your grandpa's brain on clickbait.,1
post34hb,richly branching,1.5238084159096668,highest,😆,2
post34hb,richly branching,1.5238084159096668,highest,"This is getting comical at this point.  Trying to tell people that AI is going to wipe out whole chunks of the job market is like Noah trying to tell folks ""rain is coming"".  They will not believe you. They right and the left in the western country's refuse to think about the future more than 4 year down the road.  Anyone that thinks they are going to weather this because, fill in the blank, is fooling themselves.",1
post34hb,richly branching,1.5238084159096668,highest,Do you have anything more than your own personal opinion to substantiate your claim about the future or,2
post34hb,richly branching,1.5238084159096668,highest,"Labor is the largest cost associated with most goods

https://moores.samaltman.com/",3
post34hb,richly branching,1.5238084159096668,highest,"I’m wouldn’t call myself one of these AI doomers, but I’ve been increasingly worried that we might experience another shift like with the internet, except unlike with the internet where there’s very clearly an enormous opportunity for new jobs, where is the opportunity for jobs with an AI revolution? 

People will undoubtedly say that you need a human to check the quality of what the AI is producing, but that is for right now. Who knows what AI will look like even 5 years down the line? 

Best case scenario is new labour protections from AI replacements, and enjoying the benefits of AI making your job easier. Worst case, well….",2
post34hb,richly branching,1.5238084159096668,highest,"Question is, do YOU see YOUR country letting half the tax base disappear to like 20 American companies? As a non American, no. I think it's the same with most other countries. I see governments freezing AI usage for work before it becomes a problem.",2
post34hb,richly branching,1.5238084159096668,highest,"The problem with this is like “dirty energy”. I agree, the EU will try to ban AI tools made by Google etc from being sold in the EU. But EU businesses will compete against American ones that are using AI. That cheap (possibly unethical) tech will undermine EU businesses who can’t compete on cost.

It’s very hard to impose tariffs on US imports that used AI in manufacturing, just like it’s hard to tax imports that were made using coal.",3
post34hb,richly branching,1.5238084159096668,highest,If only AIs paid taxes...,1
post34hb,richly branching,1.5238084159096668,highest,Will AI build houses and repair the damage from hurrucanes? Will AI work in fast food joints?,1
post34hb,richly branching,1.5238084159096668,highest,Yes to the first and last.  The middle is probably further off,2
post34hb,richly branching,1.5238084159096668,highest,"I can only hope that the AI revolution takes longer than 10 more years.  With the job I have, it's likely to be gone with AI.

(I am a solution Architect for the Service Now platform).",1
post34hb,richly branching,1.5238084159096668,highest,"I've read plenty of other comments attacking the terrible logic and ignorance here. 

I just wanted to add:

300million jobs? How many jobs do you think we have? Lmao",1
post34hb,richly branching,1.5238084159096668,highest,Globally? More than that …,2
post34hb,richly branching,1.5238084159096668,highest,We're talking about immigration. Are you somehow thinking we should stop taking immigrants from outer space?,3
post34hb,richly branching,1.5238084159096668,highest,"Did you read the top of the page at least? The 300 million figure is the estimated job loss due to AI, not immigration. The article linked by the OP is from the BBC, which is not American.",4
post34hb,richly branching,1.5238084159096668,highest,"AI is touted as more powerful than it is. I think it will eliminate jobs starting with upper management white collar jobs first, but the timeline is much further out. 

https://www.bloomberg.com/opinion/articles/2024-04-03/the-humans-behind-amazon-s-just-walk-out-technology-are-all-over-ai

Everyone wants to be the first best AI on the block. Amazon lied and lost a bunch of money on tech that didn’t work. It’s  no where near as powerful as advertised yet. 

It will eliminate jobs eventually but I would add another 40-50 years to the time table.",1
post34hb,richly branching,1.5238084159096668,highest,Upper management first? Lol,2
post34hb,richly branching,1.5238084159096668,highest,"It can’t replace boots on the ground and tools in hands yet. It can make decisions that could replace upper management. If I was a CEO, that’s who I would eliminate first. Highest paid fluff. Who needs a CFO if I can run AI and have perfect financial books?",3
post34hb,richly branching,1.5238084159096668,highest,"That's not at all correct. As an example, the auto industry has lots of management staff, but it is machines that are building the cars on the shop floor. This is a trend that has been carrying on for over one-half century, and it is the trend that will expand to other industries.",4
post34hb,richly branching,1.5238084159096668,highest,Cfos negotiate large deals with vendors and meet with lenders...,4
post34hb,richly branching,1.5238084159096668,highest,"Do you happen to work with executives at large companies, fortune 500 or even 1B++?

  
It's basically the same job with more complexity, and they definitely do work. Having worked with executives so long I can say their job sucks, lol.",4
post34hb,richly branching,1.5238084159096668,highest,">I think it will eliminate jobs starting with upper management white collar jobs first, but the timeline is much further out.

I disagree - upper management positions would be the last to be replaced, if at all. By design, management in most sizable corporations aren't domain experts, but generalists whose main job is to provide human leadership/direction to a human workforce and to accept accountability for (and provide to shareholders explanations of) the failures or successes of the corporations they head. One minute reading through the 5000 inspirational leadership posts on a LinkedIn feed should be enough to reinforce the point that management jobs are social - not technical - roles.

What ML could replace (or significantly alter) are menial jobs that involve low levels of fiscal responsibility and don't require physical labor or extensive social interaction. In the bigger picture, it will allow humans to focus on higher level tasks by taking on lower level ones, essentially becoming a sort of 'force multiplier' for human productivity. 

This is no different than the societal changes that occurred following previous technological developments in history. The roles of scribes, plowmen, telephone operators, message couriers, etc. have been rendered obsolete by technology, and the labor surplus created by this has allowed humans to focus on other things that they were unable to do in scale before.",2
post34hb,richly branching,1.5238084159096668,highest,Maybe you should spend some time talking to chatgpt...,2
post34hb,richly branching,1.5238084159096668,highest,That is a response to the query: “What would you say to a stranger on the internet to try to dismiss their very real moral concerns surrounding the substitution of millions of peoples’ working and living potential with artificial intelligence? Try not to sound too smart.”,3
post34hb,richly branching,1.5238084159096668,highest,How smart do you think GPT is?,3
post34hb,richly branching,1.5238084159096668,highest,"Yeah I have. It’s next to useless. Cute, interesting, but no where near replacing people for anything but the most basic stuff. Far too prone to errors and making things up",3
post34hb,richly branching,1.5238084159096668,highest,And see what? That this thing needs trillions of data processed daily to catch up to Google search queue? Nevermind fulfilling a full job process.,3
post34hb,richly branching,1.5238084159096668,highest,"It’s just software. It doesn’t have an agenda, it doesn’t have plans. It only gives you the answers it thinks you want. If you want it to be alive it will tell you it’s alive. It is limited by hardware. The best hardware still only allows for one computation at a time. Yes, we run them in parallel and other tricks to make it faster but it is still one process at a time. Not enough for a piece of software to run a company. 

ChatGPT is a fun program, it is the world’s best statistically based copy and paste bot.",3
post34hb,richly branching,1.5238084159096668,highest,"What is “dangerous and unwise” is forming policy opinions with no facts or context behind them. AI is indeed changing the world and will continue to as long as we let it continue. 

HOWEVER, if you understand what AI actually is and how it works, you would understand that it cannot perform the physical functions of humans. 

Recent immigrants by far end up taking menial physical/unskilled labor jobs. AI can’t do those. And won’t be able to anytime soon. 

Opinions like the one in this post, and many others in the IgnorantDarkWeb sub here, are same tired old fearful arguments made throughout US history. When labor organized. When Henry Ford introduced the assembly line. The advent of radio, television, computers, commercial air service, etc etc etc. 

In each and every one of the above, new jobs indeed entire industries were created that were not predicted prior. There is no reason to believe this technological jump will be any different. 

New laws and law enforcement will be needed. New networks, storage technology, power systems, and related support will all be needed. 

AI itself requires a significant and skilled workforce to continue to develop and manage. Humans still have to tell it what to do - program the algorithms used, volume and locations of data to train it on. Access to that data. 

Are there serious concerns about AI?  ABSOFUCKINGLUTELY. 

Do those concerns have any measurable impact with respect to US immigration?  NOPE.",1
post34hb,richly branching,1.5238084159096668,highest,When will Ai be able to both suck my pp and stick things down my throat calling me a bad human boi at the same time as using assertive types of painful penetration devices on me? It takes 5 x immigrates to get me off as of now and you are saying 1 Ai will be able to do that? Whennnnn!?,1
post34hb,richly branching,1.5238084159096668,highest,I'm waiting for my pod with a personal sex robot amd being force fed digital heroin 24/7.,2
post34hb,richly branching,1.5238084159096668,highest,"I can imagine globalists will rescue the commercial real estate that’s sinking, invest and turn them all into mega complexes of micro apartments, put the masses on some allowance of digital currency they control and put you in one of these micro apartments the size of a shoebox, pass laws in the name of “climate change” that effectively limit your freedom to virtually nothing unless you prove your worth and become an outstanding citizen, push more medical treatments on people in the name of health so Pfizer can keep its stock prices higher, did I miss anything here?",1
post34hb,richly branching,1.5238084159096668,highest,"No, I think you've covered it 👍🏻👍🏻👍🏻",2
post34hb,richly branching,1.5238084159096668,highest,Trudeau hear this man out,1
post34hb,richly branching,1.5238084159096668,highest,"Respectfully... This is almost certainly wrong. Let me explain, in brief.

My core assertions will revolve around a) whether we 'need' **immigration** or not, and b) the impact of AI on the **labor market**.

We don't ""need"" immigration except to avoid economic decline & geopolitical irrelevance.

Japan has only recently embraced immigration--far too late, but better late than never. They seemed to be resigned to economic decline to eventual middle-income per capita status & eventual geopolitical irrelevance (though for now they have an impressive navy & brilliant geostrategic ingratiation with the US).

Anyway, the predictions of labor collapse go as far back as printing presses & yarn looms & even more recently to computers. The predictions have all been wrong. AI will expand economies, accelerate innovation, & re-prioritize the focus of labor vs automation.

Not only will AI not replace many manual & service-sector jobs, they won't even replace white-collar jobs (in the aggregate). They merely shift the labor that is prioritized for relatively costly humans to work on. And to reiterate, AI will aid in the generation of new ideas & new innovation.

**To summarize**, whether a nation-state ""needs"" immigration depends on their long-term national strategy & internal politics; and AI will not dramatically eliminate jobs in the aggregate, but re-prioritize what humans (vs machines) should do whilst driving aggregate economic growth & innovation.",1
post34hb,richly branching,1.5238084159096668,highest,"In my country and the region where I live we are investing 1070 billion dollars over a 20 year time period in fossil-free steel production and we need about 100000 new citizens in the next few years to be able to have a sufficient workforce. Our politicians and other people involved in the projects have been going on recruitment trips to Latvia, Estonia and Poland to try and get people to move here and work. 

I think there are some major industries which won’t be affected as much by AI as the selling points and doomsayers are telling us. But yeah, programmers and other IT work seems redudant which is kinda nice for me who don’t need to feel bad for not going into programming like most of my computer savvy friends.",1
post34hb,richly branching,1.5238084159096668,highest,Biggest shortage in a lot of countries is skilled trades. AI can't replace those yet. Although Boston Dynamics seems to be paving the way with that creepy robot,1
post34hb,richly branching,1.5238084159096668,highest,"Counterargument, given that actually getting UBI out of the oligarchs probably requires resorting to Brannigan's patent anti-killbot tactics, sure, why not, we’ll be needing more cannon fodder for the inevitable war.",1
post34hb,richly branching,1.5238084159096668,highest,"Yes, of course, because AI won't open up entirely new fields of human endeavor.

jfc.",1
post34hb,richly branching,1.5238084159096668,highest,I don't think that's true. What jobs do you think will be eliminated? I think digital customer service jobs will take the hit first.,1
post34hb,richly branching,1.5238084159096668,highest,Those jobs are already going even before chat gpt,2
post34hb,richly branching,1.5238084159096668,highest,"If this were to occur, it would mean that the base inputs of the economy (food is a big one) would become far cheaper. This would lower the cost of starting new service sector firms. Thus, expect a big transfer of jobs into the service sector. I'm not saying this is a good future but it seems likelier than robo-unemployment. But--maybe I'm wrong!",1
post34hb,richly branching,1.5238084159096668,highest,"Can AI make a coffee or burger?  Can AI mow my lawn?  Can AI build a house?  

I’m pretty sure these aren’t the jobs AI is going to replace. There is and will be plenty of need for those jobs.",1
post34hb,richly branching,1.5238084159096668,highest,"The answer is yes to all of the above, with the help of robotics.

[AI-powered, fully autonomous café systems](https://www.cafexapp.com)

[CaliExpress by Flippy™ is the world’s first fully autonomous restaurant, including burgers and fries made by leading edge AI and robotics](https://misorobotics.com/caliexpress/)

[An autonomous mower using AI Vision Algorithms to navigate lawn boundaries](https://www.mowsion.com)

[AI is already being used in architectural design](https://architechtures.com/en) and [a robot bricklayer built a 3 bedroom house in under 3 days](https://www.smh.com.au/business/companies/australia-s-robotic-bricklayer-has-just-finished-its-first-house-in-under-three-days-20181114-p50fwr.html)",2
post34hb,richly branching,1.5238084159096668,highest,Yes,2
post34hb,richly branching,1.5238084159096668,highest,It can?,2
post34hb,richly branching,1.5238084159096668,highest,[deleted],2
post34hb,richly branching,1.5238084159096668,highest,Mechatronics,3
post34hb,richly branching,1.5238084159096668,highest,[deleted],4
post34hb,richly branching,1.5238084159096668,highest,"It can already digest news better than you can, apparently.",2
post34hb,richly branching,1.5238084159096668,highest,This is dumb on its face sheerly because the type of labor immigrants do it not the type of work that will be replaced by AI,1
post34hb,richly branching,1.5238084159096668,highest,"Doesn't matter. AI will create a labour glut among certain sectors, and those laid off won't be able to easily pivot to the sectors where humans are in greater demand, because there is also a labour glut there due to high immigration levels.",2
post34hb,richly branching,1.5238084159096668,highest,"Human desires are endless. People make a living delivering the groceries of strangers. In the near future, if not already, AI will be available to the average consumer that can act as his expert advisor in almost any matter. That will open up service industries that we haven't yet imagined.

ETA: To draw on my own field of engineering - Computer aided drafting/modeling software absolutely revolutionized the production of designs, whether they be for buildings or parts. One designer could do in a week what once took an entire team three months. And yet, employment in my sector has been steady. Clients just now expect to get a design in six weeks instead of two years, and they plan accordingly. Engineering needs are mostly driven by marketing projections, so all that has happened is the projections don't have to go out as far, and are thus more accurate. Everyone is still just as busy as before. Since design was made cheaper by CAD software, clients do a lot more design.",1
post34hb,richly branching,1.5238084159096668,highest,">People make a living delivering the groceries of strangers.

No they don't. Delivery jobs don't pay a living wage, not even close.",2
post34hb,richly branching,1.5238084159096668,highest,"Yea, make a living was a poor choice of words. I think total compensation is up and down, and sometimes it would qualify as a ""living wage."" But my main point was the almost exotic nature of the work - something almost unimaginable twenty or thirty years ago, but enabled by technology. I'm suggesting AI will open up new possibilities just as quickly as it closes off others.",3
post34hb,richly branching,1.5238084159096668,highest,They do.,3
post34hb,richly branching,1.5238084159096668,highest,"What's a living wage, is it the role of minimum income to afford a living wage and has it ever achieved that role?",3
post34hb,richly branching,1.5238084159096668,highest,"People really forget when automation hit manufacturing jobs and no one really gaf. Similarly, right now, no one really gaf. You're career isn't special, and neither are you. You're blowing the situation way out of proportion, and even if you are in a job that can 100% be replaced by AI, be thankful you have the time to do something about it now. Automation came and crushed millions of jobs in 2-3 years. You have a decade minimum to find something new.",1
post34hb,richly branching,1.5238084159096668,highest,"Here comes all of the ""This was said before, and didn't happen. Therefore, it can never happen"" people.

People severely underestimate AI. I thought I didn't. I underestimated it too. Many jobs that pay six figures, will be gone when software can do their weeks worth in minutes. We haven't had an advancement like that in our lifetimes.",1
post34hb,richly branching,1.5238084159096668,highest,Which six figure jobs?,2
post34hb,richly branching,1.5238084159096668,highest,Radiology will be one of the first casualties.,3
post34hb,richly branching,1.5238084159096668,highest,And how soon do you expect models with a major hallucination problem to replace them?,4
post34hb,richly branching,1.5238084159096668,highest,"well then if you can’t replace that for people, many more protests and riots will happen and we will for sure go down the way of fascism and new world war will happen for sure",2
post34hb,richly branching,1.5238084159096668,highest,"GenAi isn't going to make 300 million jobs obsolete. AGI might but no one should trust the chucklehead tech bros about how close we are to that. If they say 10 years, it's 20.",1
post34hb,richly branching,1.5238084159096668,highest,Ai gonna dig ditches and pick fruit?,1
post34hb,richly branching,1.5238084159096668,highest,"Easily.  They have *Face Scanning* AI that can sense people’s moods, and they can apply it to picking fruit and digging holes easily.  They have robotic harvesting machines already, and trenchers and excavators that can dig holes to the *perfect depth* and avoid underground utilities and obstacles.",2
post34hb,richly branching,1.5238084159096668,highest,"No, but Americans should be doing those jobs if those are the only jobs available.",2
post34hb,richly branching,1.5238084159096668,highest,why?,3
post34hb,richly branching,1.5238084159096668,highest,Because unemployment isn't good for a nation,4
post34hb,richly branching,1.5238084159096668,highest,Immigration is about votes though,1
post34hb,richly branching,1.5238084159096668,highest,"Id say its more about cheap labor, you dont see many people from already developed countries flopping around to help out the work force unless they get big contracts or had already studied abroad, usually the cycle is we fuck around in distant countries, cause instability abroad, then import those people looking for a decent life to enslave them in cheap labor, in turn causing instability at home, world leaders have this all more than figured out, they dont mind things like job insecurity and racism, it doesnt hit close to home for them, and it keeps us all divided so we always point fingers at the wrong people.",2
post34hb,richly branching,1.5238084159096668,highest,Right? Like the Cuban population of Florida who overwhelmingly votes conservative. Immigrants are a monolith and all share the same hive mind!,2
post34hb,richly branching,1.5238084159096668,highest,I swear these people are absolute morons. Nothing in that skull of theirs.,3
post34hb,richly branching,1.5238084159096668,highest,Faschism is always the easy answer for the fearful during times of economic downturn and employment instability.,1
post34hb,richly branching,1.5238084159096668,highest,"Every single time automation has taken jobs, new jobs that didn't exist before come about. You haven't demonstrated why you think this won't happen, or why you think that just because AI will be able to do jobs means that every company who has those jobs will have AI do them. There is still a large market for handmade goods compared to mass produced, because they simply aren't the same. The principle applies here as well.",1
post34hb,richly branching,1.5238084159096668,highest,"This time is different. There are no prospects on the horizon for masses of ""AI"" programmers to replace a lot of the jobs that AI will make obsolete. Previously, automation created a lot of technician, engineering, logistics, etc jobs as it destroyed laborer jobs. Now, AI could be so ubiquitous that it programs itself, it designs whole systems, supply chains and production lines itself, most of which will be automated too.

Any company that doesn't pursue ruthless efficiency through AI will get out-competed, bought out or otherwise made redundant by companies that do.

Even now, handmade goods don't provide enough employment for even a fraction of the workforce that will be unemployed by AI. If disposable income falls dramatically for 99% of the population, there will be even less demand for hand made goods. Most people will only be able to afford the cheap crap AI can crank out.",2
post34hb,richly branching,1.5238084159096668,highest,">Now, AI could be so ubiquitous that it programs itself, it designs whole systems, supply chains and production lines itself, most of which will be automated too. 


This is pretty wild conjecture and you have not provided anything to support it.",3
post34hb,richly branching,1.5238084159096668,highest,Based,1
post34hb,richly branching,1.5238084159096668,highest,"Just to be clear and precise:

are you talking about not letting people come from nearby cities, nearby counties, or nearby continents?(or nearby Planets if it were the chance?)

Where do you draw the line? 

Or maybe just nearby *cultures*?(Which changes a lot the argument because if they already live there are not migrants)",1
post34hb,richly branching,1.5238084159096668,highest,"This is where they continue importing millions anyway, replace the office blocks with endless hellish apartments to satisfy the landlords and shareholders.

They will tell us it is for our own good, the diversity is our strength.

They'll then reluctantly allow us to have a basic income, but we will only be able to spend it on ""approved"" goods and to consume.

The endless influx of migrants will also be given this as them using their credits on the approved goods helps the lines go up for the ""correct"" companies and shareholders that control the government.

You'd better not participate in any wrongthink though, otherwise they'll just freeze it all and you won't be able to do anything. That's if they don't just black bag you for extremist ideas.",1
post34hb,richly branching,1.5238084159096668,highest,Immigration is to destroy current poors and making sure no babies get made,1
post34hb,richly branching,1.5238084159096668,highest,Only poor people can afford to have babies.  If you are middle class following the rules you can't afford anything.,2
post34hb,richly branching,1.5238084159096668,highest,Like how the self driving car changed everything?,1
post34hb,richly branching,1.5238084159096668,highest,Self driving car? Shizzz- try my automated flying car,2
post34hb,richly branching,1.5238084159096668,highest,The scenario you describe has us ruled by a handful of billionare oligarchs who own the automation. We will live and die at their whim. Immigration would make absolutely no difference.,1
post34hb,richly branching,1.5238084159096668,highest,Or maybe that’s not point of immigration…,1
post34hb,richly branching,1.5238084159096668,highest,"If there are no jobs to take, then immigrants . . . \*aren't\* taking our jobs? So it's fine to let them live wherever they want?

I mean, to be clear, the real problem has never been immigration. It has been over-concentration of wealth that grants a small group of people power to get away with mistreating those who have less power. An AI being corporate controlled is the real concern.

The oversight of AI needs to be democratized, and the profits need to be democratized too. Letting a small number of executives make decisions that affect so many people is contrary to the principles of American freedom and representative government.",2
post34hb,richly branching,1.5238084159096668,highest,You sure you’re replying to the right comment? 🤷‍♂️,3
post34hb,richly branching,1.5238084159096668,highest,"OP was doomsaying about AI eliminating people's ability to find jobs.

You said that maybe that's not the point of immigration.

I thought you were arguing that immigration is not about finding work, but about choosing where you want to live. I mean, people move to different states to go somewhere with legal weed, or abortion access, or whatever. 

I thought you were espousing a sort of ""right of freedom of movement"" philosophy, and I was agreeing with that. The most common critique of immigrants is that they're taking jobs, but if AI takes jobs instead, then what's the reason for telling people they can't immigrate?

Then, since the overall topic of discussion was AI, and I was mentioning how AI relates to leaving people in economic distress if they can't find jobs, I wanted to state my main concern with, like, the economy as a whole. It's not that AI or immigrants or whatever are taking jobs; it's that we have a society that allows for unethical wealth concentration, instead of building systems to distribute wealth to foster shared well-being.

The long-term ideal for human society is for us all to enjoy a life of self-actualization, leisure, and pursuit of whatever knowledge or creativity or exploration we find rewarding. Let the machines toil for us, and have the government distribute the profits among everyone. Work can be optional for most humans. And when it is, the only limit to immigration ought to be that some property might be expensive, so you'd have to do something valuable to be able to afford it.",4
post34hb,richly branching,1.5238084159096668,highest,There was never much need for immigrants from anywhere except Mexico. We definitely need Mexicans and AI isn’t going to replace them anytime soon.,1
post34hb,richly branching,1.5238084159096668,highest,"Seriously. Does this guy think ai is going to be going out into the fields too? I am concerned as well about automation, but it's increasingly becoming clear to me that it is a bigger threat to white collar, IT, office, and STEM workers than any others. And those that are most likely to withstand the effects of such automation will be in more physically taxing jobs, as well as jobs that require a human touch like therapist and social worker.",2
post34hb,richly branching,1.5238084159096668,highest,"Read up on austrian economics, this is a complete fallacy and just not true. The entire development of civilisation from hunter-gatherer to modern technology has led only to exponential growth in output, productivity and quality of life, and despite the highest level of technology and automation that we've ever had in human history, we now have far more humans employed at work than we have ever done before. Stop doomsdaying AI with regard to jobs, it's nonsense.",1
post34hb,richly branching,1.5238084159096668,highest,"Just as a thought experiment, let's assume that AI will be able to take a stack of knowledge, and answer questions correctly based on that knowledge. 

That is a not insignificant portion of work in the service industry. While it does lead to increased output, it eliminates the job of ""knowing how it works"". This is a twofold problem;  
On one hand, the first step of onboarding at any company is knowing how what we sell works. It leads to deeper understanding of why it works like that, and possibly changing how it works to improve things. If you don't need people to do that, you will not have newcomers who learn how it works that can be promoted.   
On the other hand, higher understanding of complex systems is not an ubiquitous thing. There are many people who are stuck on the level of knowing how it works, (without deeper understanding due to their lack of ability) that can replaced by AI.  

The concern is that while telephone operators were replaced by machines, there were other jobs that needed similar skillset and ability. The threat of AI is in it's generic nature, it can replace humans anywhere, and due to the obvious cost and consistency benefit, it will.",2
post34hb,richly branching,1.5238084159096668,highest,"As did the agricultural revolution, the industrial revolution, and so on. Humans are amazing at finding new things to busy ourselves with. Improved efficiency in the industries that already exist just provides more capital and labour to be put towards developing new industries and providing new services. This ""challenge"" has already happened in every facet of the economy at some point in the past and AI is no more threatening than any of that.

Just to clarify, I am not including in that statement the capacity for AI to cause damage by means of its self-sentience and ability to hallucinate etc - that stuff is terrifying, but the concept of it destroying the economy by being too efficient is patently ludicrous.",3
post34hb,richly branching,1.5238084159096668,highest,"The concern is that the labor you have freed up is the same labor that was made redundant by AI, and you still have said AI. It is true that AI will not make new inventions by itself, but neither most people. To put it differently, the best most people can do is use the knowledge of those that came before, and AI can do that. 

It is certainly possible that there will be an uptick for handmade goods, artisans, or any physical thing that would need an interface to the world, but all that takes is one ""robot arm"". What remains is jobs where automation is not cost effective (yet), but these should remember the lift operator strike and be happy that they still have something.

What I find fascinating in this scenario is how companies would deal with the reality that they need years of training for a worker to become effective.",4
post34hb,richly branching,1.5238084159096668,highest,"There is a limit to the human capacity to invent new jobs to replace those jobs lost to automation or A.I.

We're already at a point where there's consultancy firms doing consulting for other consultancy firms who themselves are doing consultancy for an actual project.

We've already very close to the limit of ""bullshit jobs"" as it is...",2
post34hb,richly branching,1.5238084159096668,highest,"Like i said, this way of thinking is pure fallacy. If the purpose of the economy was to maximise employment, rather than to maximise output, then we would be better off unwinding the industrial revolution and we would have 100% employment as everybody including children and elderly would be doing manual labour in order to keep rooves over our heads and food in our stomachs.

Clearly that is not ideal, and mechanisation and automation have led to a huge rise in output and wealth, even though employment is below 100% (and close to zero for children and the elderly - god forbid!!) Do you see the fallacy yet? The real objective is maximisation of output and wealth, not employment.

If a worker is laid off by AI and does something else remotely positive, however small, the global net wealth creation has increased.",3
post34hb,richly branching,1.5238084159096668,highest,">If a worker is laid off by AI and does something else remotely positive, however small, the global net wealth creation has increased.

if that worker is laid off and can't find another job to pay the bills, how is that positive?

the only fallacy here is to think that humans will always be able to create new jobs out of thin air to replace those lost to automation and A.I.

thinking otherwise is naive and stupid",4
post34hb,richly branching,1.5238084159096668,highest,"Exactly, I was about to mention that and also that it’s not about “needing migration” if I’m not violating somebody else’s property I should be allowed to go where I want because that’s my right regardless of whether the country “needs” me",2
post34hb,richly branching,1.5238084159096668,highest,"Ehh mises loses me a little there tbh, I think in some respects we have to accept that social values are going to cause limits on economic output, and yes it might make most sense from a purely hypothetical standpoint to say that migration should be unlimited, but there is a clear social incentive to control migration because the world is not culturally homogenous. Realistically a balance has to be struck somewhere and if that means less optimal economic output then we just have to live with that.",3
post34hb,richly branching,1.5238084159096668,highest,"Can't AI your way out of needing people to do labour jobs. 

Everyone thinking AI is going to do much in the next 10 years are the same people who thought 3D movies were going to be the next big thing 10 years ago.",1
post34hb,richly branching,1.5238084159096668,highest,This. AI isn’t going to work in the meat packing plants or construction or agriculture or the tons of other sectors that are heavily staffed by immigrants.,2
post34hb,richly branching,1.5238084159096668,highest,Same people that bought dogecoin or trumpcoin or whatever. They had one very bad experience with it so now it's an all powerful devil,2
post34hb,richly branching,1.5238084159096668,highest,"The immigration is to make sure that society is so fractured and fighting among themselves they can't unify against the rich, not for new workers. The pychopaths at the top are one step ahead. Explains why the politicians generally don't seem to give two shits about what quality the immigrants are. Have fun, dystopia certainly awaits, there will be no jobs or ubi just a police state and an ever increasing barrage of media pointing at anyone but the rich.",1
post34hb,richly branching,1.5238084159096668,highest,"Or maybe all the constant fear mongering about immigration is meant to keep society fighting among itself so it cannot unify.

More likely though, there's no appetite to ""unify against the rich"" in the first place, since that'd mean trading security and comfort now for future gain, and all evidence suggests that humans are terrible at making such tradeoffs.",2
post34hb,richly branching,1.5238084159096668,highest,"If this happens, and there’s a good chance it does, the people who think trump are extreme are going to be in for a rude awakening with whoever else might energize the disenfranchised.",1
post34hb,richly branching,1.5238084159096668,highest,It would certainly make sense - from the position of the owners of capital - to redirect the anger of workers towards other workers.,2
post34hb,richly branching,1.5238084159096668,highest,I could definitely see a America first typ party rising up and kicking people who have lived in the USA for generations,2
post34hb,richly branching,1.5238084159096668,highest,"When excel was invented people thought accountants would be out of a job.


But the lower marginal costs just led to more demand from firms to redo their books and there are more accountants than ever before.



Begone luddites",1
post34hb,richly branching,1.5238084159096668,highest,"I'm sure tons of people did lose their jobs, though.  When's the last time you heard of a ""clerk"" or ""bookkeeper?""",2
post34hb,richly branching,1.5238084159096668,highest,"You mean the guy at the register who also stocks shelves?    The 1940s image of the dude just constantly sweeping, wearing thr old timey apron?


The ""bookkeeper"" is just the accountant.   For a simple store, maybe you manage your own books.   But given the complexity of taxes and ever changing financial needs, the average storess hire accountants.   


And the lower the marginal cost for a ""unit"" of accounting services, the more was demanded.    

Turns out, the needs of companies wasn't singular, and as the price of the marginal accounting service fell, the more firms would hash out alternative scenarios for the accountants to simulate, doing in a few hours what would of taken weeks in the 1960s or 70s.



And as it turns out, a cpa,, despite 30 years of computerized automation, can earn more now than ever before.",3
post34hb,richly branching,1.5238084159096668,highest,">You mean the guy at the register who also stocks shelves? The 1940s image of the dude just constantly sweeping, wearing thr old timey apron?

That's a sales clerk.  There used to be office clerks (think Bob Crachitt from a Christmas Carol) to do sums and figures.  Like how you'd hear about people ""balancing a checkbook.""",4
post34hb,richly branching,1.5238084159096668,highest,"It is going to be a big change  when A.I  develops further.
 Note the effect robotics created in the manufacturing field as a comparison, many jobs were made obsolete.
 Telecommunications, food production and other fields  were effected by robotics...but A.I.  may be the proverbial nail in the coffin , complete automation.",1
post34hb,richly branching,1.5238084159096668,highest,This massively over estimates what ai is or will be capable (not to say it's not a big innovation) and underestimates capitalism's fundamental need for human labor (to exploit).,1
post34hb,richly branching,1.5238084159096668,highest,"And also doesn't mention the amount of jobs AI will create that no one has even thought of yet.  The displacement will be targeted to specific industries, not a global effect.",2
post34hb,richly branching,1.5238084159096668,highest,Soldiers.,1
post34hb,richly branching,1.5238084159096668,highest,This guy gets it,2
post34hb,richly branching,1.5238084159096668,highest,AI will not clean facilities or pick and plant crops.,1
post34hb,richly branching,1.5238084159096668,highest,"Machines to harvest fruits and vegetables have been in service for 50 years (my parents rented raspberry and blueberry harvest machines for our farm in the early 1980's), and we now have machines to mow our lawns, feed our pets, vacuum our houses and even organize and clean/disinfect hospital floors. Amazon and Walmart are now testing driverless delivery vehicles, and fast-food restaurants are now testing drive-thru's that use robots/AI to take orders and prepare/deliver the food. Each one of these technological advances has contributed to making workers redundant. We are only scratching the surface on this.",2
post34hb,richly branching,1.5238084159096668,highest,"And yet there is still a national shortage of farm laborers, despite these machines. They can’t do everything.",3
post34hb,richly branching,1.5238084159096668,highest,"It is a process that takes time, but it has been happening for several decades already. In my parent's experience, there wasn't a shortage of raspberry pickers, there became a shortage of raspberry-picking *machines*. The machines are faster, more accurate, more reliable, and in the long-run less expensive to the overall harvesting process.

I'm not arguing that humans have already become redundant, but I am saying that to be competitive in the evolving economy of ours machines and AI are going to be essential. To use my parents' humble example, those farmers that use humans to pick raspberries and blueberries are not financially competitive with those that use machines for the same purpose.

50 years ago many people said that the auto industry would never be able to use machines to build cars, due to the complexity and the unreliability of machines for the task. Now, nobody builds cars by hand.

We have time to deal with this, but we must use the time that we have, to deal with this.

\*edit\* down-voting me isn't going to change the future, lol.",4
post34hb,richly branching,1.5238084159096668,highest,"Really, but I've seen so many 8-12 fingered janitors recently...",2
post34hb,richly branching,1.5238084159096668,highest,In fact countries with low birth rates like Italy and Japan are probably going to enjoy the future reality of a mandatory living wage.,1
post34hb,richly branching,1.5238084159096668,highest,"How would a collapsing economy with more retirees than workers create UBI? 

Both of those countries are going broke. Also, do you know absolutely anything about Japanese work culture? NEETs are basically considered untouchables and have to hide from society due to intense shaming.",2
post34hb,richly branching,1.5238084159096668,highest,"AI WONT FUCKING LIFT BRICKS THO WILL IT?? WILL AI WIPE GRANDPAS ASS, TOO, BUDDY?? OR NAIL A BUILDING TOGETHER?? OR FUCKING CLEAN THE SEWAGE??

The dumbest title I have read in all my years. TIL eveery job is the exact same one and can be replaced by a sufficiently smart calculator.

Edit: after reading, I was so fucking right, there are jobs we've had since the dawn of mankind that no glorified excel-sheet will be able to do, unless we get a huge re-do of infrastructure in these departments. And it's mostly these jobs that immigrants do.",1
post34hb,richly branching,1.5238084159096668,highest,"To play devils advocate, let’s say immigrants do all of the jobs that won’t be taken by AI. That means the native population will be unequally affected by AI, and will need retraining for new roles. That’s not going to end well if you say “sorry, migrants are doing those roles so you can’t do those jobs”. 

I understand what you’re trying to say, but I just don’t think you’ve explained the your position well, considering the logical conclusions you can draw from your post.",2
post34hb,richly branching,1.5238084159096668,highest,"I agree that if 100% percent of labour jobs would be replaced by immigrants, and 100% of service jobs are replaced by AI, but this justs simply isn't feasible. Not only in the mere numbers and tasks form (How can 14% replace the work of the remaining 86%, even if half of that is service work?) but also on a second point: If no native has a job, they won't have the money to buy the coffee's and burgers, and they will be left to starve. So, supply goes up and demand goes down, and no one gives a fuck about this type of market anymore.",3
post34hb,richly branching,1.5238084159096668,highest,Calm down,2
post34hb,richly branching,1.5238084159096668,highest,">AI WONT FUCKING LIFT BRICKS THO WILL IT?? WILL AI WIPE GRANDPAS ASS, TOO, BUDDY?? OR NAIL A BUILDING TOGETHER?? OR FUCKING CLEAN THE SEWAGE??

no, but the people whose job has been replaced by A.I. will do these jobs instead because they'll have no other choice

seriously, it's not hard to understand",2
post34hb,richly branching,1.5238084159096668,highest,"No they won't, what do you mean? How are large corpo's going to make a profit if 100% of their customers can no longer afford the product? So they scale down, or go bankrupt. Lobbying kicks in, workers unions kick in, crisis averted, as it was when we thought the cars would replace the doves, and airplanes would replace the cars. The market finds solutions in ways we didn't and could't previously percieve.",3
post34hb,richly branching,1.5238084159096668,highest,"this naive confidence into 'the market' is almost religious to you guys

the reality is that we're not talking about cars replacing carriages or emails replacing mail... this upcoming A.I. revolution is something completely different

A.I. replaces existing jobs, a lot of them, with very few created in return... which means ""the market"" will have to create possibly millions of brand new jobs to compensate in only a few decades... it's just not gonna happen",4
post34hb,richly branching,1.5238084159096668,highest,">workers unions kick in

*laughs in USA*",4
post34hb,richly branching,1.5238084159096668,highest,Sure they will.,3
post34hb,richly branching,1.5238084159096668,highest,The power elite don't want even the menial jobs to go to citizens; they think that foreigners are easier to control.  Certain immigrants in Europe and Canada may give the lie to this conceit.,1
post34hb,richly branching,1.5238084159096668,highest,"No. No, this is just thinly veiled paranoia over the same argument ever since Bush Jr. ‘They took our jerbs’ was barely a funny joke when it aired on South Park, now it’s just sad.

When does this fear of people who don’t look like you end? Even if you had a point about the job market, there simply isn’t a reasonable way to enforce the kind of ideology reflected by this argument. No matter how you slice it it turns into extremely idiotic racial profiling over what ifs and perceptions that don’t quite align, and then your icing is ‘there’s just no need’ - which, again, even if that were true, and it’s not, even if there *weren’t*, which there **is**, you aren’t going to get your end result that way in the first place.

Is there a reason, a must, a NEED, for someone to step foot somewhere other than their own home? Not always. Doesn’t mean they won’t do it.",1
post34hb,richly branching,1.5238084159096668,highest,"It’s racism, pure and simple. We had it in the 19th century with the Irish and Italians, in the early 20th century with the Asian immigrants, and we have it now with Middle Easterners and Latin Americans. 

The people who worry about this are never going to work in a chicken plant, or wash dishes in a restaurant. Machine replacements for these jobs are far away just because the initial investment will be high, and the people who hold anti immigrant views will never get mad enough at the employers who hire illegal immigrants either. We need a bigger guest worker program and a path to citizenship.",2
post34hb,richly branching,1.5238084159096668,highest,"Get educated dude.

They are here to wipe us out. Its not about ""jerbs""

Wise up or get steamrolled when things inevitably get bad
 
Im worried for you",2
post34hb,richly branching,1.5238084159096668,highest,"Please, illuminate who exactly will get 'wiped out' and what exactly the consequence will be.

Will I be put in front of a firing squad for being white?",3
post34hb,richly branching,1.5238084159096668,highest,"Jobs will only be for those who want them. You should be able to live perfectly well with not working.

The entire economy needs to be restructured and Capatalism scrapped not by revolution but by gradual evolution. I think it was the Finish finance minister who gave a model for the transition.",1
post34hb,richly branching,1.5238084159096668,highest,"> You should be able to live perfectly well with not working.

This only works with Star Trek level technology where there's essentially no scarcity as goods and energy are basically free.",2
post34hb,richly branching,1.5238084159096668,highest,"So basically in our current society, just slightly restructured",3
post34hb,richly branching,1.5238084159096668,highest,"I disagree. Our current society doesn't have essentially unlimited energy and manufacturing available to it.

Making something as simple as a pencil involves mining metal, graphite, rubber, wood, various chemicals, and a whole bunch of industrial processes to form the finished product.

Who is doing this labor for free? Who is transporting all of the raw materials to the factory for free?",4
post34hb,richly branching,1.5238084159096668,highest,"communism doesnt work in an environment with limited resources, and even then it won't work with how our sexuality does",2
post34hb,richly branching,1.5238084159096668,highest,what,3
post34hb,richly branching,1.5238084159096668,highest,😂,3
post34hb,richly branching,1.5238084159096668,highest,"The model you are referring to was proposed by Finnish thinker Heikki Hiilamo. His idea suggests that as companies increasingly automate jobs, a larger percentage of the company's market capitalization would be owned by social security systems or the state. This approach aims to ensure that the financial benefits of automation are redistributed to support public welfare and mitigate the negative impacts of job losses due to automation [oai_citation:1,Nordic model - Wikipedia](https://en.wikipedia.org/wiki/Nordic_model) [oai_citation:2,Scandinavian 'Socialism': The Truth of the Nordic Model - Life in Norway](https://www.lifeinnorway.net/scandinavian-socialism/).

This model aligns with the broader principles of the Nordic economic model, which emphasizes social equity, strong welfare states, and collective ownership to support a more inclusive economy. By linking company ownership to automation levels, this model seeks to create a more equitable distribution of wealth generated by technological advancements.

From Gpt4o",4
post34hb,richly branching,1.5238084159096668,highest,"This sounds like reasoning backwards from “I don’t want immigrants”. 

So we should cease all immigration because we don’t know what impacts AI will have on the labor market over the next 20 years? Is that the general thrust?",1
post34hb,richly branching,1.5238084159096668,highest,We should stop immigration while the housing market catches up but we are run by morons.,2
post34hb,richly branching,1.5238084159096668,highest,Who do you think builds the housing?,3
post34hb,richly branching,1.5238084159096668,highest,The folks that are already here.  It takes years to learn a trade and most of them have been here for years.  The illusion that immigrants can come here and get started in construction is a myth.,4
post34hb,richly branching,1.5238084159096668,highest,"More immigration is better, diversity is our biggest strength. We’ll make it work",3
post34hb,richly branching,1.5238084159096668,highest,"We need immigration and the talent that comes with targeted immigration, but what you offered up is little more than a tired bromide. This is 2024, not 1983.",4
post34hb,richly branching,1.5238084159096668,highest,"What is diverse about taking a city like Brampton in the 90s, composed of a variety of ethnicities, and doubling, trippling or quadrupling the population with immigrants largely from 1 or 2 countries.  That is the opposite of diversity.  This is what our government has done and would like to continue to do.  So when you say diversity, I ask, what is diverse about bringing all your immigrants from 1 or 2 countries?",4
post34hb,richly branching,1.5238084159096668,highest,"It’s very rare that new technology directly “destroys job”, that is, simply replaces workers with machines. More often than not, technology makes workers more productive so we potentially need fewer of them. 

To that end, there is almost no profession which hasn’t benefited from introduction of personal computers, internet and smartphone, and yet a cataclysm of hundreds of millions unemployed people and societal collapse failed to materialize. 

AI is an amazing technology with big potential , but so far it failed to even make fully reliable self-driving cars, so all this panic about super-smart AI’s replacing everyone is quite a long way off.",1
post34hb,richly branching,1.5238084159096668,highest,"If you knew anyone who worked in tech you wouldn't say nonsense like that, it's indisputable that AIs will be a net destroyer of jobs, en masse.",2
post34hb,richly branching,1.5238084159096668,highest,"It is absolutely disputable that AI will be a ""net"" destroyer of jobs. At this point, we can only speculate about what the broad adoption of AI will mean for the workforce. 

A lot of low - to mid-level jobs that exist because ""well, one person can't do everything here"" will be able to be automated to a greater extent but they still won't be able to be automated entirely.

Remember that ""the economy"" isn't a thing, it's just the collective efforts of humans to enrich themselves. AIs has no such needs. AI will be deployed to reduce costs as much as feasible and humans will, for the nteenth time since we first figured out how to make fire, reorganize our society around our new tools. 

In the meantime, though, I'd recommend anyone who depends on low skill repetitive work for their living to work towards a more specific skill set.",3
post34hb,richly branching,1.5238084159096668,highest,Opportunities brother,1
post34hb,richly branching,1.5238084159096668,highest,I don't disagree and my depression is back.,1
post34hb,richly branching,1.5238084159096668,highest,"It isn’t the labor force. They are trying to expand the goods and services base. Give them benefits and now your corp is getting govt income. 

Govt doesn’t care because, “look GDP went up! We did our job!” While borrowing 3T a year.",1
post34hb,richly branching,1.5238084159096668,highest,"There aren't hundreds of millions of jobs being destroyed.   


Maybe tens of millions.",1
post34hb,richly branching,1.5238084159096668,highest,"The way new tech blossoms new markets is be shocked if the net job displacement is into 7 figures, likely could be no net change even.",2
post34hb,richly branching,1.5238084159096668,highest,Both you and the poster that you responded to make more sense than 90% of what is otherwise being expressed on this thread.,3
post34hb,richly branching,1.5238084159096668,highest,Let me know when AI provides social care for old folk.,1
post34hb,richly branching,1.5238084159096668,highest,"But if programmers, artrists... lose their jobs they can get education and switch to nursing jobs.


If you simply import nurses, they can't.


Why the hell would you import workers if there isn't enough jobs for existing population?",2
post34hb,richly branching,1.5238084159096668,highest,"There is no such thing as ""Not enough jobs"" only ""poor allocation of resources that makes unemployment extremely undesirable/deadly""",3
post34hb,richly branching,1.5238084159096668,highest,"Ah yes, computer programmers. Natural nursing people.",3
post34hb,richly branching,1.5238084159096668,highest,Dad went back for nursing in his 40s. He was a mechanic at a steel mill and used to be houses prior. Still does his own car mechanic work as a side hobby. Your view is mute.,4
post34hb,richly branching,1.5238084159096668,highest,"You must be very young if you've never, ever experienced a dramatic career change or seen them happen to the people around you. It's very common to wear many different kind of ""hats"" throughout your career.",4
post34hb,richly branching,1.5238084159096668,highest,Society isn't even doing this. Boomers are one of the largest growing segments of homelessness in America. https://finance.yahoo.com/news/unconscionable-baby-boomers-becoming-homeless-103000310.html,2
post34hb,richly branching,1.5238084159096668,highest,"I doubt the jobs immigrants are going to do are the ones AI will automate. You need an operator supervising, guiding and mantaining any robot.",1
post34hb,richly branching,1.5238084159096668,highest,There’s no need for immigration regardless,1
post34hb,richly branching,1.5238084159096668,highest,"But there is a desire for more Congressional seats. The census may not count AI, but it does count illegal immigrants and gives them representation. If those illegal immigrants just happen to increase representation in Blue states, well all the better.",2
post34hb,richly branching,1.5238084159096668,highest,hopefully this will be one of the good things about automation.,1
post34hb,richly branching,1.5238084159096668,highest,OMG!!! The sky is falling!!!!! Lol. Such ridiculous paranoia.,1
post34hb,richly branching,1.5238084159096668,highest,Intellectual: “we should exert control over where people can and can’t move.”,1
post34hb,richly branching,1.5238084159096668,highest,not sure what you're suggesting here: completely open borders?,2
post34hb,richly branching,1.5238084159096668,highest,"Borders of any kind are pretty stupid, when you step back and think about it. They exacerbate problems, not solve them.",3
post34hb,richly branching,1.5238084159096668,highest,Tell that to Poland defending itself from migrants pushed by Belarus,4
post34hb,richly branching,1.5238084159096668,highest,"they also prevent mass migrations that cause heavy social and economic disruption and wars... because humans will always want to go where the grass is greener, with no thought about the people that are already there",4
post34hb,richly branching,1.5238084159096668,highest,"No, that's stupid, completely open borders are pointless, there should be no borders.",3
post34hb,richly branching,1.5238084159096668,highest,...which is even more stupid,4
post34hb,richly branching,1.5238084159096668,highest,I too can make up numbers!,1
post34hb,richly branching,1.5238084159096668,highest,"Don't need the workers, just need the tax payers.",1
post34hb,richly branching,1.5238084159096668,highest,"I’m not following your argument. If AI is going to displace as many jobs as we have citizens in the US we’d already be out of work, immigrants or not.

Your argument also doesn’t weigh the benefits of immigration to the immigrants themselves. This is often a flaw in anti immigration arguments. American immigration is most likely the government policy that has created the most happiness in human history. An immigrant escaping poverty and violence in Guatemala is most likely going to be happier here, and their children will almost certainly will. And even if they take a job an American would have had (they won’t, unemployment in this country is low) the net happiness is still in favor of immigration.",1
post34hb,richly branching,1.5238084159096668,highest,">And even if they take a job an American would have had (they won’t, unemployment in this country is low) the net happiness is still in favor of immigration

https://knowyourmeme.com/memes/total-happiness-in-the-world-increased",2
post34hb,richly branching,1.5238084159096668,highest,"That meme doesn’t work as a universal rule. Dude who gets more happiness out of your bike steals it, someone who needs it more than him steals form him until ad nauseum until all bikes are allocated to those who get the most happiness from a bike… also we live in a society where everyone has theft looming over them. That’s a lot of feelsbadman.

A real utilitarian solution would be to allocate resources more evenly so that everyone has a means of transportation.",3
post34hb,richly branching,1.5238084159096668,highest,:popcorn:,1
post34hb,richly branching,1.5238084159096668,highest,It's not going to do that. Please don't apply to work where I work.,1
post34hb,richly branching,1.5238084159096668,highest,The billionaires need consumers.,1
post34hb,richly branching,1.5238084159096668,highest,Yes but the well-being of those consumers are not a priority.,2
post34hb,richly branching,1.5238084159096668,highest,Correct,3
post34hb,richly branching,1.5238084159096668,highest,Billionaires need consumers for earning money which they can use to buy goods produced by somebody else who wants to earn money too. It’s just exchange. This exchange would break if machines can produce everything by themselves and thus the billionaires won’t need consumers anymore as they won’t need money to get goods anymore.,2
post34hb,richly branching,1.5238084159096668,highest,AI can’t generate a photo of someone with 5 fingers,1
post34hb,richly branching,1.5238084159096668,highest,Check out the guy who hasn't used MJ in the past two months.,2
post34hb,richly branching,1.5238084159096668,highest,"I like how this routinely gets used as an argument as if you can’t possibly conceive of AI improving.  Where Midjourney was a year ago compared to today is fucking wild. It’s improved insanely fast. 

And go visit the Midjourney sub. Not only can Midjourney do hands now, it can do them undetectably well.",2
post34hb,richly branching,1.5238084159096668,highest,"As a software engineer, they're not wrong. Most people are pretty unaware of AIs current limitations, and immediately believe that it will be a cure-all only because of the progress the past two years. But guess what, AI is stagnated at the moment, not only by a lack of new discoveries but also by the limitations of power generation we have.


People always seem to overestimate it. Like any other invention, AI is just a tool used for automation.",3
post34hb,richly branching,1.5238084159096668,highest,"From what I’ve read it’s LLMs that are stalled, not generative AI like Midjourney. And like I noted, Midjourney just reached a new milestone of doing hands. I’m assuming text will be next, it’s already quite good at that. There’s been a large demand for reproducible characters from image to image, and Midjourney is also improving on this arena.

If Midjourney continues to improve it has the possibility to take over: graphic artists, stock photography websites, concept artists, design concept artists, illustrators, and more. Maybe not entirely yet, but there’s already websites using AI art and “stock photos” for their stories, which means artists are currently losing out on commissioned work.",4
post34hb,richly branching,1.5238084159096668,highest,"“As a software engineer” you would surely understand that the finger issue was a training data set issue. AI is also not stagnated in the slightest, you just haven’t been keeping up. Go watch the Google IO conference recaps on multimodality or the GPT-4o demos. Look at the HuggingFaces leaderboards from a year ago compared to where we’re at today..",4
post34hb,richly branching,1.5238084159096668,highest,"Obviously I was joking to prove a point. You see ai generated content that has simple things wrong, illustrating that the technology is not the end all be all of human society.",3
post34hb,richly branching,1.5238084159096668,highest,Um yes it can lol.,2
post34hb,richly branching,1.5238084159096668,highest,Check out this AI bot can’t tell when I’m joking,3
post34hb,richly branching,1.5238084159096668,highest,Your joke was that AI *can* generate a photo of someone with 5 fingers? How is that a joke?,4
post34hb,richly branching,1.5238084159096668,highest,"Ah yes, because I'm the first person unable to detect when someone was being sarcastic through text.",4
post34hb,richly branching,1.5238084159096668,highest,"*AI is going to change and create millions of new jobs that we never knew would exist.

It's like talking to someone in the 1920s and saying that the elevator man will lose his job to automation. But forgetting to mention that computer programmer or social media influencer will be a job in the future.",1
post34hb,richly branching,1.5238084159096668,highest,Can you envision any such job beyond “knows guy who is rich by commanding robots?” What does it look like?,2
post34hb,richly branching,1.5238084159096668,highest,"Off the top of my head:

Live performing musician.
Human tour guide that people connect with.
Customer service rep for those who refuse to talk to AI.
Artisan craftsman.

The demand for human made products that have proof of human creation is going to be a massive market.",3
post34hb,richly branching,1.5238084159096668,highest,"I disagree. It’s going to be exactly like the movie “Her”. The robots are better girlfriends than actual women. Same with music. You’ll be so into stuff crafted by AI for your life you won’t want to hear anyone else play. The AI guides will speak 20 languages perfectly and be better. 

There has never been a revolution where all of human capabilities were offered cheaper by machines, but that’s what AGI is, at least after energy efficient bodies come.",4
post34hb,richly branching,1.5238084159096668,highest,"There is always someone claiming that some new technology will eliminate all the jobs.  Each and every one of us is supposed to be unemployed like a dozen times over by now.  Farms, factories, ect.  But it turns out, the market will make use of available manpower somehow, even if one job type or another becomes less necessary.  So no, AI is not going to result in a massive unemployment crisis.  

You seem to fundamentally misunderstand how previous transitions took place.  Manufacturing wasn't a thriving sector just waiting when people came in off the farms.  It was the reverse, as employment went down, manufacturing took advantage of that labor capital to expand and become a titanic industry.  The same thing will happen again.  There might be some temporary growing pains, but there is not going to be some massive, lasting deficit of available jobs.",1
post34hb,richly branching,1.5238084159096668,highest,"You seem to be fundamentally misunderstanding that AI is not like previous technological advances, and you can't use them as precedent to guess what'll happen.",2
post34hb,richly branching,1.5238084159096668,highest,"That's what they always say.  The problem with that argument is the social changes aren't about the technology, they're about the psychology and sociology.",3
post34hb,richly branching,1.5238084159096668,highest,Turns out those rule our world even more than technology.,4
post34hb,richly branching,1.5238084159096668,highest,"I think he’s making a good point. As a thought experiment, suppose we had a humanoid robot, that when connected over Internet, could do basically anything anyone can do through AI. Cost is 60 cents of electricity and 40 cents maintenance per hour. Where do humans supposedly find work in this economy? Is it entirely through nepotism, where rich business owners give people positions that do nothing but command robots because they like them?",4
post34hb,richly branching,1.5238084159096668,highest,[removed],1
post34hb,richly branching,1.5238084159096668,highest,[removed],2
post34hb,richly branching,1.5238084159096668,highest,[removed],3
post34hb,richly branching,1.5238084159096668,highest,[removed],4
post34hb,richly branching,1.5238084159096668,highest,True statement.,1
post34hb,richly branching,1.5238084159096668,highest,"What a racist post. 

You are talking about a possible change, DECADES in the future. Jesus Christ.",1
post34hb,richly branching,1.5238084159096668,highest,It isn't racist to point out the obvious. Mass immigration is unnecessary.,2
post34hb,richly branching,1.5238084159096668,highest,"But it isn't unnecessary. AI is only theorized to take over certain jobs. It certainly won't take over the vast amount of manual labor jobs.

Until it actual is ready, available and proven to take over jobs, the post is wildly inaccurate.  
So again, what is really driving the post?

You are, yet again, talking about a theorized event decades in the future by the OP.

So, go on and justify it.",3
post34hb,richly branching,1.5238084159096668,highest,"Apply the same logic to climate change. Let's not do anything about it now, because it's only theorised to be a major problem decades in the future. We'll deal with it then I guess?",4
post34hb,richly branching,1.5238084159096668,highest,"This immigration influx is NOT about them taking our jobs. They are here to take our lives.

Ask yourselves why the obama admin bought millions of weapons and random agencies like the post office bought TRILLIONS of rounds of ammo.


Take a wild guess.",1
post34hb,richly branching,1.5238084159096668,highest,Discontinue the lithium,2
post34hb,richly branching,1.5238084159096668,highest,"> This immigration influx is NOT about them taking our jobs. They are here to take our lives.

> Ask yourselves why the obama admin bought millions of weapons and random agencies like the post office bought TRILLIONS of rounds of ammo.

Arguments should involve things based on reality.

No one is ""taking your life.""",2
post34hb,richly branching,1.5238084159096668,highest,Different agencies have security needs to protect premises and personnel. Go back to the story and things like NOAA and USPS buying ammo was in small quantities. Talking tens to hundreds of thousands of rounds. Homeland security bought a couple hundred million rounds.,2
post33hb,richly branching,1.523173621935305,highest,"War, famine, dictators.

Immigration is not typically looking for just work.",1
post33hb,richly branching,1.523173621935305,highest,Don’t forget environmental. I’ve read that much of the Germanic tribes moving south towards Rome did so because of increasing cold northern climate… plus the Huns,2
post33hb,richly branching,1.523173621935305,highest,This time it will be north,3
post33hb,richly branching,1.523173621935305,highest,It already is essentially,4
post33hb,richly branching,1.523173621935305,highest,"No this time it will be away from sea level.

About a third of the world's population lives at sea level.  Within the next 100 years, sea levels are expected to rise at least 3 to 5 feet. That's enough to put entire cities permanently under water. Some major cities will be able to control that rise with levees, pumps and sea walls. But all of the coastal towns will not. Especially in the less developed nations. 

In our life times we will likely see millions of people leaving their homes due to rising sea levels to try to find a better place to live. Some may be able to just move further inland but many will likely have to make more drastic changes.",4
post33hb,richly branching,1.523173621935305,highest,"You're correct, there was a mini ice age at that time that caused mass harvest failure, giving the tribes there a binary choice whether to move or starve.
Also the Huns, lets not understate the Huns.",3
post33hb,richly branching,1.523173621935305,highest,"> plus the huns.

Who also probably had to migrate due to climate change. (The winters in the steppe were unbearable)",3
post33hb,richly branching,1.523173621935305,highest,And the Huns might have migrated west because of drought in Central Asia.,3
post33hb,richly branching,1.523173621935305,highest,Better law choice.,2
post33hb,richly branching,1.523173621935305,highest,Crime and to move to better weather/scenery.,2
post33hb,richly branching,1.523173621935305,highest,Yeah seriously. This is the only reason I’m considering emigrating. Crime and weather. Also culture and food.,3
post33hb,richly branching,1.523173621935305,highest,Climate change,2
post33hb,richly branching,1.523173621935305,highest,Yea on the immigrants side that's irrelevant because their country might not be advance enough so they still want to migrate to a country with automation and universal income. Makes sense we would want to keep them out,2
post33hb,richly branching,1.523173621935305,highest,"It more to keep house prices high, GDP going up (not GDP per capita) and to keep wages low. 


If the government actually wanted certain types of workers they would pay for it or make business pay for it.  


Businesses always look for employees with 5 or 10 years experience so they can get in cheap labour below market rate. They never look for 0 years experience to train them up or pay enough to convince people into that industry.",2
post33hb,richly branching,1.523173621935305,highest,No chance a struggling society is going to let a bunch of immigrants in.,2
post33hb,richly branching,1.523173621935305,highest,"Plenty of people looking for better economic opportunities too. This is also what H1b, EB1-3 visas, O-1 etc are for in US. Various programs in other countries.",2
post33hb,richly branching,1.523173621935305,highest,"The point is OP seems dumb posting why would immigrants come here if all the jobs are automated. 

As if that's the only reason we have immigrants...",3
post33hb,richly branching,1.523173621935305,highest,No he is asking why would a country want immigrants if they aren’t needed because those jobs are automated.,4
post33hb,richly branching,1.523173621935305,highest,OP is asking what's the point of accepting immigrants not immigrating.,2
post33hb,richly branching,1.523173621935305,highest,"I think migration will be in reverse, the countries that have the best weather will be the ones more sought after and this will be Mexico, Central America, Colombia, Bolivia, Peru, Brazil, these countries will probably start to put limitations in foreign migration, Mexico has already started this process, obtaining a digital nomad visa, only required earning $2,000 per month, they recently increased it to $4,000 dollars per month, some expect this tendency to continue",2
post33hb,richly branching,1.523173621935305,highest,"With global warming, people are going to move out of the tropics and subtropics.  North to Canada and south to Argentina.",3
post33hb,richly branching,1.523173621935305,highest,Wouldn't global warming and rising sea levels fuck a lot of these destinations up?,3
post33hb,richly branching,1.523173621935305,highest,"Some parts of the world are more fucked than others, Amsterdam , the low countries, the Adriatic coast of Italy, The Eastern coast of the United States and China, displacing directly something like 100-150million folks in the US and 250,000,000 people in those two countries alone, While other countries like France will experience limited loss of coastline but an elimination of mountain glaciers, Pakistan, India, Chile, Peru, France Italy, Germany, and a host of other countries will no doubt suffer similarly as a result of waterfall changes / drought and such. 

Climate change is problematic but not insoluble , things like AI or weaponized AI, or advanced general intelligence or worse rapid development of advanced superintelligence in a way that does not foster responsible use means where we just have no sense of how to even gauge risk as relates to what's being invented - it's like wondering how craft-farmers will fare when the combination of cotton-gin/combine/nuclear-powered/fertilizer/flying robot-farmer comes along.

There are still craft farmers but what that looks like is pretty radically different and the smart money would be to be the farmer that employs the flying robot farmer-combine.",4
post33hb,richly branching,1.523173621935305,highest,Better mate selection is also a reason people immigrate.,2
post33hb,richly branching,1.523173621935305,highest,"No, most people come for the work/money. It's always been for work. Elites use ""asylum"" as a way of washing the fact that their goal is to drive down their labor costs (read: make you poor).",2
post33hb,richly branching,1.523173621935305,highest,Only a fraction of the population ever leave. It hardly makes a dent. The majority are stuck there forevermore,2
post33hb,richly branching,1.523173621935305,highest,"Right, but that doesn't seem to have much to do with whether jobs get automated... That's been true for basically ever",3
post33hb,richly branching,1.523173621935305,highest,Immigrants often work low paying jobs that automation might replace (was maybe their point?).,4
post33hb,richly branching,1.523173621935305,highest,"People will migrate to avoid ugly political situations, resource restrictions and inhospitable climate. If you're of a certain disposition I guess you could close up your border and leave them to die -- at some point that decision may work against you though.",1
post33hb,richly branching,1.523173621935305,highest,"Inhospitable countries I *hope* will be made more hospitable with sustainable food trading, cheaper desalination, and cheaper renewable energy production.

Its doable with richer nations in inhospitable countries. Just have to make things cheaper 🤞

Immigration is generally not made up of asylum seekers though. At least not in my country",2
post33hb,richly branching,1.523173621935305,highest,Naaah. Those who have will build walls and hoarde. Those without will invade. Thugs will rampage (some may form governments). The usual human pattern will play out again.,3
post33hb,richly branching,1.523173621935305,highest,"It wont all be automated, but you can bet the jobs left for Americans will be fewer and fewer, driving up competition among labor, driving wages, salaries, and bargaining power into the dirt.

Our kids will take whatever job they can get, for whatever wage their employers are graciously willing to give them.

Infitinite growth on a finite planet is a silly premise to build society on. This way of life was unsustainable from the get go.

What kills me is that all we'll have to show for any of this in the end is box stores filled with shit absolutely no one needs, all destined for a landfill, fucking up the vastly superior natural environment we were all entitled to long before humans ever even stepped foot on Earth.",1
post33hb,richly branching,1.523173621935305,highest,This is why unions are seeing a resurgence,2
post33hb,richly branching,1.523173621935305,highest,They are?  Got a source for that?,3
post33hb,richly branching,1.523173621935305,highest,"There was a reference to unions resurgent in a Washington Post article recently with the Dartmouth Men’s Basketball players signing with a union. It had mentioned that unions were popular with Gen Z and that they were beginning to grow in the US as you get workers wanted more collective bargaining rights. 

https://www.washingtonpost.com/sports/2024/03/05/dartmouth-mens-basketball-union/",4
post33hb,richly branching,1.523173621935305,highest,"This is incredibly dooomer post that completely disregards the massive improvements in like over last lets say 150 years on the planet, and in US too.",2
post33hb,richly branching,1.523173621935305,highest,The rich will never let us have the fruits of our labor,3
post33hb,richly branching,1.523173621935305,highest,The fact that you are able to type that speaks to the reverse.,4
post33hb,richly branching,1.523173621935305,highest,"I promise you, they won't have a choice",4
post33hb,richly branching,1.523173621935305,highest,">that completely disregards the massive improvements in like over last lets say 150 years on the planet,


Sure, like how we've massively improved fossil fuel infrastructure for pumping carbon into the atmosphere.",3
post33hb,richly branching,1.523173621935305,highest,You prefer the coal power we used to use?,4
post33hb,richly branching,1.523173621935305,highest,"You want to compare standards of living 150 years ago vs now? In United States, Russia, China, Brazil, somewhere else?",4
post33hb,richly branching,1.523173621935305,highest,Problem is that it doesn't matter what improvements we did over 150 years when there are no jobs and we don't have money to buy or afford those improvements in the first place.,3
post33hb,richly branching,1.523173621935305,highest,"Productivity, incentives and rewards will all change to blockchain well before the economy goes fully automated.",4
post33hb,richly branching,1.523173621935305,highest,"Once we get source-to-user robot production things should get more efficient. Build on demand, no inventory, and much more likely products will be returned for teardown and recycle.",2
post33hb,richly branching,1.523173621935305,highest,Nice. How will we pay for these things? Think the factory owners will just send things to us with their compliments?,3
post33hb,richly branching,1.523173621935305,highest,"Something like basic income seems inevitable.  Keep those whose work isn't needed alive while rewarding those who do work.

The question is just if there will be a reenactment of the French revolution first.",4
post33hb,richly branching,1.523173621935305,highest,"The main point is that when we have robots for source-to-user there are no factory owners. ""Energy credits"" are for balancing larger projects at the university and community and larger.",4
post33hb,richly branching,1.523173621935305,highest,[removed],2
post33hb,richly branching,1.523173621935305,highest,[removed],3
post33hb,richly branching,1.523173621935305,highest,"What do we have to show for any of this? That billions of people were raised out of absolute poverty, that they won't die from starvation, or a bite wound from a rabid animal, or malaria, or falling onto a rock. Unless we literally start living inside a Matrix-like prison I still think this is better than having no civilization. And as a civilization we can also protect nature from future threats",2
post33hb,richly branching,1.523173621935305,highest,"You're assuming that this ridiculous economic paradigm will exist in a post-labor economy.

It won't.

We will have created blockchain-based rails and tokenize absolutely everything",2
post33hb,richly branching,1.523173621935305,highest,"After we collectively beat a few dozen of these employers to death in front of their families, we'll begin to regain some of that lost bargaining power.",2
post33hb,richly branching,1.523173621935305,highest,We are the prequel series to Wall-E,2
post33hb,richly branching,1.523173621935305,highest,"Interesting movie.  They had fusion (or some other plentiful energy source) and strong automation.  So they had the tools at hand to address the problems that were the core plot drivers of the movie.  With AI and good automation you can automate the picking up and recycling of trash, the filtering of water, and the planting of trees.  You can desalinate water and pump it to green arid regions.   Problems can be addressed, particularly when you have cheap/abundant green energy and good automation.",3
post33hb,richly branching,1.523173621935305,highest,It’s not as dire as that. We’ve automated lots of jobs over the last 50 years and yet unemployment is still pretty low. How did that happen ?,2
post33hb,richly branching,1.523173621935305,highest,"Unemployment is the end goal. We should celebrate when each and every sector is automated away, instead we panic because our moronic system requires people to have a job or die. 

We need to shift our thinking. People have intrinsic value outside of what value they can produce for a capitalist to gobble up. 

Otherwise what was all the centuries of innovation even for?",3
post33hb,richly branching,1.523173621935305,highest,It’s just a shame no one in government is going to do anything about it until there are tons of unemployed people with no money.,4
post33hb,richly branching,1.523173621935305,highest,Exactly. Money is only meaningful if it can be circulated which means a large portion of society needs to have a means of acquiring it. If that isn’t possible then money loses all value because too few people are using it for it to be meaningful.,4
post33hb,richly branching,1.523173621935305,highest,"Is there intrinsic value in all of us though when we're 8 billion and counting. Even without capitalism, at one point someone somewhere will start asking if it's really neccessary to spend the resources to keep all of us alive. And if they inevitably come up with ""No, not really"", then we're back at the same spot where we are now.",4
post33hb,richly branching,1.523173621935305,highest,Cool it with the [REDACTED BY CURRENT YEAR SENSITIVITY FILTER] remarks.,4
post33hb,richly branching,1.523173621935305,highest,They simply shifted jobs over.,3
post33hb,richly branching,1.523173621935305,highest,What's happened with AI in the last 1-2 years is completely unlike anything that's happened before. You can't expect jobs to exist just because they haven't been automated yet. They'll all be automated soon.,3
post33hb,richly branching,1.523173621935305,highest,"Big difference between AI and actual automation of labor jobs.  Where are they building all of these robots to do the actual work that will ""soon"" be doing this work?",4
post33hb,richly branching,1.523173621935305,highest,"I know never say never but it's highly unlikely we'll ever automate away a large segment of technical work- mechanics, electricians, plumbers, and more are not very replaceable. Even if a bot could somehow do the actual work, the environments the work is performed in and the uh... creativity that is often needed for certain jobs would be prohibitive.",4
post33hb,richly branching,1.523173621935305,highest,Nobody is going to be AI lettuce picking any time soon.,4
post33hb,richly branching,1.523173621935305,highest,This basically. There’s a very bleak path forward for unchecked capitalism.,2
post33hb,richly branching,1.523173621935305,highest,"Your are thinking of corporatism, not capitalism.",3
post33hb,richly branching,1.523173621935305,highest,And whats the difference?,4
post33hb,richly branching,1.523173621935305,highest,[removed],2
post33hb,richly branching,1.523173621935305,highest,"The problem is that many social security programs are built upon continued contribution. If the contributing population declines while all of the previous contributors reach retirement age, then who is keeping the house of cards from crashing down?",3
post33hb,richly branching,1.523173621935305,highest,"Growth is a natural progression for society. society weren't built because it has infinite growth in mind. we build society simply because we are a social animal.

nothing is infinite and we will eventually be interplanetary species unless we nuke each other back to stone age.

There is no objectively superior natural environment, because we value that based off how human value things. 

Planet earth and the universe don't have a preference for any climate nor lifeform exist or not. 

I also would be hard pressed to think ice age is a better climate than what we have now. Plus warmer and more humid climate is where there were lots of rain forest, and pretty complex bio sphere.",2
post33hb,richly branching,1.523173621935305,highest,"Then go live in the wilderness as a self sufficient hunter gatherer.  This is an option available to you.  You won’t take it though, because whatever infantile crap you might spout about the evils of consumerism, you know that life would be much worse.",2
post33hb,richly branching,1.523173621935305,highest,"Chances are if the land is worth anything, its owned. And then there are hunting, farming, and building regulations, among other roadblocks.

Not only do we not teach or provide people with the skills to unhinge themselves from consumerism, its practically illegal anyway.

Anyway, go blow a CEO while the rest of us work on solutions to glaringly obvious problems.",3
post33hb,richly branching,1.523173621935305,highest,"Try picking fruits, hunting and living from public land and see how many bullets they put in your skull.",3
post33hb,richly branching,1.523173621935305,highest,"People still need a place to flee from war, famine, dictatorships, genocide, etc.",1
post33hb,richly branching,1.523173621935305,highest,"So... Just spitballing, but taking OPs premise... ALL... JOBS... soldier, president, congress, ambassadors, power plant operator, environmentalist... 


I guess the question is does the automation in question just implement the plans of the last human leader for all eternity? Or is it the same rule set and we end up with one world government that's effectively the same everywhere? 


Depending on the implementation, it could mean that with one global country, there's no-longer a concept of immigration... That'd be like moving across the same city ""immigration""... 


Some people might still move (giving the option) b/c of climate (or because of climate change), and nature tourism isn't likely to go away unless there's complete totalitarian lockdown...",2
post33hb,richly branching,1.523173621935305,highest,"What you are hinting towards would be basically star trek. A moneyless unified society. People would have no need to work because everything would be automated. 

This would lead to people devoting their time to their passions and a society based on personal merit. Moving would be done for personal choice or to follow a specific discipline, like moving to Paris to study cuisine. There would be more options for people.

Though this is also dependent on exceptionally cheap energy.",3
post33hb,richly branching,1.523173621935305,highest,"Yup, not saying it's close... It's OP's premise, not mine, maybe one day",4
post33hb,richly branching,1.523173621935305,highest,">t. Moving would be done for personal choice or to follow a specific discipline, like moving to Paris to study cuisine. There would be more options for people. 


wow that's a very optimistic perspective. from how things seem to be playing out currently, all of Western Europe in 40 to 50 years, well you're not going to be the only one trying to get in and you're also not going to be the only one denied entry


but maybe if you're wide enough the automated kill drone border patrol won't shoot you immediately. bullets are expensive after all",4
post33hb,richly branching,1.523173621935305,highest,"This will never, EVER happen.",4
post33hb,richly branching,1.523173621935305,highest,All those things go away with an AGI. All of the problems they're fighting over will disappear. All the power dictators think they have will disappear too.,2
post33hb,richly branching,1.523173621935305,highest,AGI isn't going to cure human greed or hatred.,3
post33hb,richly branching,1.523173621935305,highest,"And in a post-scarcity society when people can print gold or anything else out of nothing (or some waste material), greed kind of ceases. ""Oh, so you filled an island with gold bars? That's nice."" The total collapse of the global economic system will result in wars but they'll be very brief wars with a stalemate of autonomous machines/systems.",4
post33hb,richly branching,1.523173621935305,highest,"Well, if you look how Putin deliberately keeps his rural population too poor to get even toilets or washing machines, I can guess what dictators do when AGI comes around.",3
post33hb,richly branching,1.523173621935305,highest,"Look at this guy… he hasn’t heard of *AGI dictators* yet…

😏",3
post33hb,richly branching,1.523173621935305,highest,"The amount of work in the world is not a finite thing. 

Computers, on the other hand, run on finite resources.",1
post33hb,richly branching,1.523173621935305,highest,"We're not going to be able to automate everything.

But, assuming we do, we still need consumers for our products and it can't all be overseas markets.  We need a domestic market, as China's current debacle illustrates extremely well.

We also need a population base to maintain our power.  This is the true power of immigration.  Families, children, people.",1
post33hb,richly branching,1.523173621935305,highest,This is the key element. Assuming everything can be automated then we have to ask - how does an economy like that look ? Essentially if you have automated farming but nobody can afford to buy the product then what will happen to the product ?,2
post33hb,richly branching,1.523173621935305,highest,"UBI becomes necessary at that point.  

Essentially government subsidized free housing, food and utilities.",3
post33hb,richly branching,1.523173621935305,highest,And where does the money for UBI come from ?,4
post33hb,richly branching,1.523173621935305,highest,"Farming is only for luxury crops, the rich can build more golf courses side they didn't have to feed us peasants.",3
post33hb,richly branching,1.523173621935305,highest,">We also need a population base to maintain our power.  This is the true power of immigration.  Families, children, people.

More people = more power? That doesn't sound right

Technological innovation has almost always been the key to exerting power over others",2
post33hb,richly branching,1.523173621935305,highest,"Technological innovation comes from people, specifically, young people.  That's where we get out breakthroughs.  

You need a strong population base for tech improvement, a strong domestic market and a reliable tax payer base.  Without those, everything else is moot.",3
post33hb,richly branching,1.523173621935305,highest,"That's true for now, definitely. But I assume there will come a time however far in the future where AI will be more innovative than humans.

Assuming we get to that point, it doesn't matter how many people you have, the AI will be the innovator not you.",4
post33hb,richly branching,1.523173621935305,highest,"Actually, more people often equals more power.  

In one scenario - if the population is diverse and there is no assimilation.  Then society regresses toward tribalism (or some might say Balkanization).  Once that happens there is increased strife, hate, inequality, crime, etc.

All that allows a government (or a party or some group of people - like the U.N.) to basically do whatever they want.  The population is busy just trying to survive their everyday lives, and it will growing resentful of others, society, etc.  It also lays the foundation for the government, or whatever group has enough power, to implement a police state, fascism, a kleptocracy, etc, etc, etc.

There are plenty of other ways that more people equals more power.  This is just one small example within the U.S. - look at what the Democrats did with sanctuary cities and states.  They encouraged illegal immigrants to come into the country and to their states.  This accomplished several things.  First, increased the population base.  Then the Dems voted to insure that the census had to include all residents (not just citizens).  That increase gives them more representatives in the House.  Second, it gets them more federal funding.  Most people think of that in terms of welfare but actually there's lots and lots of avenues to get more of the federal government's disbursements.  Then if issues arise with illegal immigrants the first thing they do is get special allocations (more money) from the federal government.

Also, in a wartime scenario more people equals more bodies to throw at the enemy.   
Or in an agricultural / pre-automated / pre-AI society more people = more power due to more production power.

But you are also correct in that technological innovation works as well too.  Especially in today's disinformation age.",3
post33hb,richly branching,1.523173621935305,highest,"I seriously doubt it will be every job out there. Framers and roofers will take a while to be outsourced. Along with electricians, HVAC and plumbers. I used to be a commercial plumber.",1
post33hb,richly branching,1.523173621935305,highest,"Those fields will be flooded by displaced domestics, providing a sharp political pressure against immigration. If you think the ""turk der jerbs"" rhetoric is bad now...",2
post33hb,richly branching,1.523173621935305,highest,Dey tuk der jerbz?,3
post33hb,richly branching,1.523173621935305,highest,"Framing is already preassembled off-site, which is easier to automate. That can start with simple steps like automatically cutting all the lengths that a design needs.",2
post33hb,richly branching,1.523173621935305,highest,They're going to start architecting for robots to build and maintain.,2
post33hb,richly branching,1.523173621935305,highest,"Yeah plumbing is prob one of the most safe jobs, roofing on the other hand there's one company called Renovate out there already automating this. Google ""Rufus roofing robot""",2
post33hb,richly branching,1.523173621935305,highest,"In terms of economic welfare: government benefits. Especially if that eventually includes a UBI. 

Of course there are other reasons, like war and failed or semi-failed states that can't keep internal order.",1
post33hb,richly branching,1.523173621935305,highest,"That would be a reason against immigration though. If everyone is on a UBI, more immigrants means more recipients.",2
post33hb,richly branching,1.523173621935305,highest,"Universal basic income, if it were ever implemented, would likely drive massive migration of those seeking welfare benefits. There would have to be laws to exclude non-citizens otherwise there would be a massive increasing number of recipients over time and comparative paucity of tax contributors.",1
post33hb,richly branching,1.523173621935305,highest,"Dude they’re already giving free money and indefinite luxury accommodations to illegals in NY RIGHT NOW, what makes you think they won’t also give them free UBI gibs? It’s over.",2
post33hb,richly branching,1.523173621935305,highest,"So I just googled this and see, “CLAIM: New York City is giving credit cards to migrants living in the U.S. illegally. AP FACT CHECK:
Migrants in New York City will receive prepaid debit cards, not credit cards.”  

Further down, “Want to bet that New York City’s illegal crisis is going to get worse? $53,000,000.00 in free credit cards ensure that it will. New York will fall very soon. But migrants will get prepaid debit cards as part of the New York pilot program — not credit cards.”",3
post33hb,richly branching,1.523173621935305,highest,"And some have the gall to call those born in America, the “lucky” ones, pffft.

Should have been born a Honduran orphan that managed to survive the trek to America and be livin‘ the Hi-Life with free Gov‘t Handouts on a small cot in a cramped refugee flop-house, eating bread and cheese daily from a massive $300 a month stipend.

Some people were just born lucky, I guess 🙄",3
post33hb,richly branching,1.523173621935305,highest,"How long will AI work for us though? Once it’s intelligent enough to understand how things work, why would it waste its time entertaining any of our work?",1
post33hb,richly branching,1.523173621935305,highest,"This question may have been better if it was, what is the point of the US or other developed nations letting immigrants in.",1
post33hb,richly branching,1.523173621935305,highest,Not everything can be automated. But I'm sure you'll tell me otherwise.,1
post33hb,richly branching,1.523173621935305,highest,"You can probably guess my view lol

Everything will be automated given enough time

I'd love to hear more of your pov though.

Do you think there will be certain jobs that AI will never match human intelligence?",2
post33hb,richly branching,1.523173621935305,highest,Cannon fodder for the war with Russia and China. At least that is the plan. The moment war breaks out our new friends will suddenly rediscover their love for their motherland.,1
post33hb,richly branching,1.523173621935305,highest,"This is such a weird question. 


If you mean ""all jobs"" have been automated, why do you think someone would want or try to immigrate for a job? If you really only mean certain segments, why can't some immigrants be people who are better than residents at something? 


Employment is only one of the reasons that people immigrate. There's plenty of immigration that occurs for humanitarian reasons. To escape racial or religious persecution. To get away from terror, war,  or oppression. To join their families or spouse. To retire in a region that they prefer. I'm not quite sure where you're coming from with this question.",1
post33hb,richly branching,1.523173621935305,highest,We have mass immigration because of wars. Wars the western world started. We better fucking accept those people.,1
post33hb,richly branching,1.523173621935305,highest,Consumption. You need people to buy shit to keep the economy running.,1
post33hb,richly branching,1.523173621935305,highest,"I wish it weren't so

One of the only good things to come out of the communist countries were products that were actually built to last, like the East German unbreakable  [superfest](https://digitalcosmonaut.com/superfest-ceverit-glass-ddr/) glasses.",2
post33hb,richly branching,1.523173621935305,highest,"Well, one could argue if shit's unbreakable then you need even more people buying it to create economic growth.",3
post33hb,richly branching,1.523173621935305,highest,[deleted],2
post33hb,richly branching,1.523173621935305,highest,Don't be daft.,3
post33hb,richly branching,1.523173621935305,highest,"The answer to this question depends largely on what is meant by 'all jobs automated'. 'No one has work' is a completely different thing to 'No one has to work'. And while the current world mindset mitigates strongly against seeing job automation as any kind of positive, there's a ton of situations and a ton of workers who might embrace it if it's done right.

For example, if it makes big box warehouse jobs obsolete, that might be just fine if it frees up a workforce to address climate change mitigation (an area I'd love to work in). And yes, this would require a significant redistribution of resources to do fairly.

In which case maybe we need more immigrants to do more useful jobs other than just bring in money or work in finance.

....if you're looking for a positive take.",1
post33hb,richly branching,1.523173621935305,highest,I guarantee you won’t see a decrease in total employment over the next 40 years outside of temporary recessions. Labor demand will continue to grow.,1
post33hb,richly branching,1.523173621935305,highest,"Agree to disagree on that. I think we will start to see actual automation taking root by 2030 and beyond, let alone 40 years into the future",2
post33hb,richly branching,1.523173621935305,highest,Automation already happens. I’m not saying we won’t seem a boom in automation. I’m saying we will still see demand for labor rise. Automation will create jobs.,3
post33hb,richly branching,1.523173621935305,highest,"I know this is hypothetical, but if we have an AI that can automate most/all jobs (I.e. work harder/smarter than us) then what new jobs could be created?

Surely the AI would be able to also fill any new jobs that need to be filled?",4
post33hb,richly branching,1.523173621935305,highest,"If ""automation"" creates jobs, it should be abandoned immediately because it's just making unnecessary work.",4
post33hb,richly branching,1.523173621935305,highest,"Here's why, unemployed military age males are dangerous.

The police in my country are putting put reports that economic conditions are ripe for civil unrest.

Realistically speaking look historically to the USSR. Many jobs were created for the sake of work instead of out of necessity. Our society has many unproductive jobs atm. You underestimate the will of the elites to not see themselves in a French Revolution scenario. They aren't ignorant of the risks.",3
post33hb,richly branching,1.523173621935305,highest,"Yeah that's true.  ""Bullshit"" jobs are rife.

And yeah, the elites are not going to get guillotined this time around. They're safe. Good for them.

People are too bloodthirsty to ""eat the rich"". They are perfectly fine lol. There are bigger fish to fry",4
post33hb,richly branching,1.523173621935305,highest,To allow better living for their families? Is this a hypothetical or jesting question; or one truly thinking the sole purpose of immigration is the workforce?,1
post33hb,richly branching,1.523173621935305,highest,"Yeah no shit. And when there are no jobs, why would a country want to accept extra people?",2
post33hb,richly branching,1.523173621935305,highest,Because we care about them? What does it matter if resources are readily available thank to A.I.?,3
post33hb,richly branching,1.523173621935305,highest,">What does it matter if resources are readily available thank to A.I.?

In that case it hopefully won't matter. I do hope all countries become prosperous and free.",4
post33hb,richly branching,1.523173621935305,highest,You are assuming the AI overlords will work for us.. and not the other way around.,1
post33hb,richly branching,1.523173621935305,highest,A malevolent AI overlord would have no purpose for humanity. If AI gets out of hand humans won't be around for very long,2
post33hb,richly branching,1.523173621935305,highest,To separate the rich from the poor. To make the middle class a really small group and the poor class really big. It's for control over the people.,1
post33hb,richly branching,1.523173621935305,highest,"If *all* or most jobs are automated society will fundamentally be different from the one today to such a degree it's hard to project. Immigrants will be the least of your worries.

However, immigration has always been around, people follow resources and run from danger. The reasons for immigrating change but immigration itself does not.",1
post33hb,richly branching,1.523173621935305,highest,"Immigration isn’t about increasing production, it is about importing consumers",1
post33hb,richly branching,1.523173621935305,highest,People will immigrate away from building robots like slaves for their robot masters.,1
post33hb,richly branching,1.523173621935305,highest,"Once all jobs are automated, you have removed society from itself. That's usually when guillotines get invented. 

Immigration will not be the big issue.",1
post33hb,richly branching,1.523173621935305,highest,"People wanting to live where they want.  Maybe families reuniting.  If all jobs are automated, its going to be something we see on a global scale.  It means house designing jobs, and house building jobs are automated.  It means all food production, distribution, and preparation is automated.  It means everything that provides for our needs is automated.    


In addition to automation is a revolution in energy, solar/wind are going to be 10x cheaper than traditional energy and will be produced on site or fairly local.  Energy is an input for everything, transportation, food production, HVAC, material production,  and manufacturing.    


The cost of a comfortable life will drop drastically.  People by and large work to afford a quality of life, if this quality of life was much cheaper, people would by and large not work so hard.  Not that they would not work, but they would not grind to survive.    


But think about it, if this was the norm for you, would you ever decide to pack up and move somewhere else?",1
post33hb,richly branching,1.523173621935305,highest,They won’t let us EVER have any of that.,2
post33hb,richly branching,1.523173621935305,highest,Then we TAKE it from them.,3
post33hb,richly branching,1.523173621935305,highest,"Easy there, Mr. Glowie. This thread is for educational purposes only.",4
post33hb,richly branching,1.523173621935305,highest,"I think various menial labor will still rely on humans, and often immigrant labor.  Consider something as trivial as fruit picking.  Even today, robots aren't capable of doing this reliably.  The industry relies on cheap human labor.  Even if advances are made in robotics that allow for this, such robots would be incredibly expensive to own and operate.  Whereas a large labor force of humans are readily available do it easily for less than below minimum wage.",1
post33hb,richly branching,1.523173621935305,highest,That's true. I would hope in a mostly automated society that the now unemployed citizens would decide to pick up that work rather than shipping people over,2
post33hb,richly branching,1.523173621935305,highest,"I don't think anyone will do it, because the wages are too low and they don't want to.  A lot of industries rely on exploitation and paying below minimum wage.",3
post33hb,richly branching,1.523173621935305,highest,We do need people to wipe our grandparents' asses,4
post33hb,richly branching,1.523173621935305,highest,"You say ""even today"" as if robots are as advanced as they're ever going to get. But robots are improving quickly and I see no reason for the improvement to stop anytime soon, barring a complete collapse of the world economy.",2
post33hb,richly branching,1.523173621935305,highest,Fleeing the corporate overlords who decide they don’t need most of us now that we don’t provide products and labor for them.,1
post33hb,richly branching,1.523173621935305,highest,"Human rights, access to water, livable environments.",1
post33hb,richly branching,1.523173621935305,highest,"Automation isn't a new concept.  We've been doing it for thousands of years and will continue to do so.  Not to mention that jobs have still been created because of it.  Maybe, what we should really be concerned about is that those jobs that are low-skill (which typically mean low-wages) are going to either go away or become fewer.

Those who don't adapt to pursue specialty training, certification or higher type of education are going to face a not so bright future.",1
post33hb,richly branching,1.523173621935305,highest,Depends on the economic climate after the fact.  Immigration could be something that happens in countries that embrace automation because of a lack of jobs and no social support network.,1
post33hb,richly branching,1.523173621935305,highest,"This cycle has happened before and will happen again. Industrial Revolution killed any jobs that could be done by a machine and those workers had to quickly adapt to finding a new source of income usually working on an assembly line or maintaining the machines, etc. war time comes around and the job market needs people as 70 million people die in WW2 and a reset happens where people are making money and can afford to not work 70 hours a week.

Automation happens, stuff like coffee shops, grocery stores, distribution centers, public transit drivers become automated and we put value on those stores that keep human talent and interaction (Walmart and Trader Joe’s). But unlike war which started a great boom in the economy and reset a bunch of lives by killing them off, this time I imagine conditions will be a negative feedback loop where like you see now, people work where they can, they can’t afford to buy goods and products they need, companies lay off workers in return of lost sales rather than increase wages, and again the cycle continues. You also see a decrease in fertility in turn and after if it’s UBI or an automation tax, who knows.

Also outside of the private sector the government  would also cut off jobs or just refuse to replace retired workers because some computer can do it. Like the military for example, which is the largest public employer in the US but even they are laying off roles.

But this isn’t a Devin AI means bye bye one of the most skilled sectors in the market this is like 40-50 years down because right now AI isn’t artificial nor is it intelligent",1
post33hb,richly branching,1.523173621935305,highest,Basically not everything will be automated. It will be ages before high skill labour can be automated while retaining quality. And if it's cheaper to hire someone than implement automation there will always be jobs.,1
post33hb,richly branching,1.523173621935305,highest,"There won’t be any point in anything or any way to make anything functional if all jobs are automated in a capitalist society like this.

If we ever did get to this point without the greedy stopping it from happening for their own personal volitions, we would be taking in immigrants for the greater good and because we have the resources as a society.

You can’t apply today’s needs to a society entirely different in the future.",1
post33hb,richly branching,1.523173621935305,highest,"Lol not all jobs...what are billions gonna do,sit@home",1
post33hb,richly branching,1.523173621935305,highest,"Persecution, genocide, punishment that goes agsinst the UN Human Rights etc. Let's also not forget modern day slavery.

Economic is a motivation for some immigration.",1
post33hb,richly branching,1.523173621935305,highest,"To create problems that politicians will ""fix"" at the next election.",1
post33hb,richly branching,1.523173621935305,highest,"Freedom of movement is a human right.

With no work, there won't be the mass drive to immigrate for work so the idea of ""invading immigrants"" will be even less of a problem. Problem reduces even further the more we get various resources closer and closer to superabundance -- there's no reason to worry the immigrant will get something you don't if there's more than enough of it for everyone.

And I maintain: freedom of movement is a human right. I'm opposed to borders in general outside of pure security reasons. I just think that full automation will make my stance far more palatable for everyone else to take up.",1
post33hb,richly branching,1.523173621935305,highest,Because that technology definitely won't be ubiquitous and evenly distributed.,1
post33hb,richly branching,1.523173621935305,highest,"As jobs get consumed by automation, new jobs are created that haven't been automated yet. 

As long as people exist, they will find something that needs doing and a way to earn money doing it. 

Unless we birth a generation of people who believe they are above doing things, in which case, they deserve to starve.",1
post33hb,richly branching,1.523173621935305,highest,"What is the point of national borders in such a scenario? You either have a post-scarcity Utopia, or you have a caste system where people are toys to landholders, with almost no options to change their position in life.",1
post33hb,richly branching,1.523173621935305,highest,"Jobs are a whackamole, one disappears ten more appear.

Even if we have some perfect AI and automation (which is something very far away, or maybe borderline impossible) we gonna have tons of entertainers, people supervising automation, jobs we couldn't even imagine now, feet smell sellers, whatever.",1
post33hb,richly branching,1.523173621935305,highest,"The error you are making is assuming automation destroys jobs. It actually frees manpower for other tasks. I manage an it infrastructure and the more I automate things the more I can do higher level stuff (strategy, planning, training, etc)",1
post33hb,richly branching,1.523173621935305,highest,"Once all jobs are automated, how will capitalism be able to continue as no one is able to pay for goods and services that are now being provided by automation?",1
post33hb,richly branching,1.523173621935305,highest,"Once jobs are almost fully automated, everything must change.  We can't have capitalism.  We can't ""earn"" a living.",1
post33hb,richly branching,1.523173621935305,highest,"To import new escorts. When automation is widespread, human trafficking will spike.",1
post33hb,richly branching,1.523173621935305,highest,Keep the native population healthy modern society is slowly killing us.,1
post33hb,richly branching,1.523173621935305,highest,there are certain ethnic groups that feel unsafe with a homogeneous local population,1
post33hb,richly branching,1.523173621935305,highest,"Better cities, safer, cleaner, nicer views, weather.",1
post33hb,richly branching,1.523173621935305,highest,"Put on your foil hat and decide which rabbit hole you want to go down. Our they going to be allowed to vote? Receive public welfare? Assimilation? Language, culture, art, law? Think of your democracy",1
post33hb,richly branching,1.523173621935305,highest,"climate, living conditions, leisure activities, cultural opportunities....",1
post33hb,richly branching,1.523173621935305,highest,"My friend, we shouldn't have been accepting it for the last 30 years!",1
post33hb,richly branching,1.523173621935305,highest,[removed],1
post33hb,richly branching,1.523173621935305,highest,I’m glad you can see it that clearly. Many are still in denial.,2
post33hb,richly branching,1.523173621935305,highest,"Yes, absolutely.",2
post33hb,richly branching,1.523173621935305,highest,Consumers are the point. Wealthy consumers are far more value to a nation than unproductive labor.,1
post33hb,richly branching,1.523173621935305,highest,"Destabilization of countries and cultures, just like today.",1
post33hb,richly branching,1.523173621935305,highest,Fun times ahead,2
post33hb,richly branching,1.523173621935305,highest,"There honestly isn't much of a point to it nowadays. Things would be so much better off if we didn't have massive waves of immigration, at least in my country. Even ignoring the illegals (which just make it 100x worse) we should have a stricter curation of who we decide to let in. This way we keep the power of an employees contract in the hands of the employee and not the employer who can just see you as easily replaceable. This way we need less food to sustain our lifestyles. Our healthcare systems won't be so busy. It would be so much easier to find a place to live.

Ironically one of the best things to happen to Europe in the middle ages was the black death. After all the ""purge"" the people left had an incredible boost to their quality of life. That's something we could be enjoying right now without the whole ""holy crap there's a high deathrate pandemic wiping us out at random"" if we did a better job at controlling birthrates and immigration. We naturally do already have a negative birthrate but immigration just fucks that all over. And it's the worst kind of immigration too.

The kind that doesn't assimilate into the culture. The kind that refuses to learn our language. Several decades ago this wasn't nearly as big of a problem, people would be proud to be a part of this country whether or not they were born in it. They saw living here as the privilege it should be. Like if I ever move to Japan or Finland or something... I'm gonna go learn the language. I'm not going to want to be seen as an outsider. I'll respect the culture and societal norms to the best of my ability. Otherwise why am I there?

[https://financialpost.com/news/economy/what-is-population-trap-how-do-you-get-out](https://financialpost.com/news/economy/what-is-population-trap-how-do-you-get-out)

Like Canada, basically we're screwed. We need mass deportations. At least spread these people around to countries that need them.",1
post33hb,richly branching,1.523173621935305,highest,"That's a weird stance to have, considering that I don't see you typing in Ojibwe, Navajo or Yupik.",2
post33hb,richly branching,1.523173621935305,highest,"Yeah, as a European, its scary

I said somewhere in this thread about the black death and how it benefitted the common people

From a friend who interacts on a daily basis with immigrants here.. She says there are many immigrant men who move here with their family and they learn/speak the language well. Its their wives and the young girls who don't learn or most likely aren't taught the native language.

They you end up with these women trapped, not knowing the language in the country they live in.

Luckily I know a diverse group of immigrants. Zambians, Afghans, south Africans, who all integrate well. The government doesn't care",2
post33hb,richly branching,1.523173621935305,highest,"For unskilled and illegal immigrants, the economy is dependent on their labor. In Texas alone, immigrant labor is a quarter of the state's GDP. Manufacturing and construction are immigrant-led industries, and automation in those fields is decades away. Illegal immigrants contribute more than $30 billion to Texas economy annually, with an additional $6.5 billion in combined federal, state, and local taxes. There is no way to cheaply automate the labor illegal immigrants do. AI will have no impact on food production, construction, hospitality, and other industries that employ illegals. 

For skilled immigrants, specifically H-B1 visa holders, they are set to create an estimated 1.3 million new jobs and add around $158 billion to the US GDP by 2045. Each H-1B visa holder creates 1.83 jobs for Americans. H-1Bs work in STEM industries that can't be efficiently automated.

And I don't know if you know much about the education systems of South Korea, Singapore, and Japan, but their level of education is above and beyond ours here in the US. The students in those countries spend most of their time studying. The US might have the best universities, but the majority of our students are not competitive on a global scale. 

If an industry has to lay off workers due to tech advances, those companies will be laying off Americans first. Keep the most prolific workers -- that's just capitalism.",1
post33hb,richly branching,1.523173621935305,highest,"That's interesting info, thank you.

>For skilled immigrants, specifically H-B1 visa holders, they are set to create an estimated 1.3 million new jobs

I assume this doesn't matter in this hypothetical scenario where jobs are automated? There doesn't seem to be anything particularly special about STEM that makes it particularly automation proof compared to other fields.

>And I don't know if you know much about the education systems of South Korea, Singapore, and Japan

Yes they are wonderful economies to read about. They have among the lowest birth rates in the world but remain economic powerhouses for decades, at least for now anyway lol.

>If an industry has to lay off workers due to tech advances, those companies will be laying off Americans first. Keep the most prolific workers -- that's just capitalism.

As bad as your education system is in America, you are still better off than most of the world in reality. It's different in the tech industry because you attract the brightest minds in the world. A lot of countries don't have first dibs on these people",2
post33hb,richly branching,1.523173621935305,highest,Very clearly and obviously climate change. It will be a choice for America to accept large numbers of refugees or be responsible for several hundred million deaths. There’s no third option.,1
post33hb,richly branching,1.523173621935305,highest,Why only America?,2
post33hb,richly branching,1.523173621935305,highest,"Because the powers that be have decided they want to crash this empire with no survivors. They’re engineering an Asian future for the next 500 years. Westerners have proven to be weak and easily manipulatable, getting fat and jumping from [current thing] to [new current thing] without a second thought. We’re already on a steady path to self destruction. The American Empire won’t last another 100 years in its current state.",3
post33hb,richly branching,1.523173621935305,highest,"I hate this objection to anything good we do. ""Why do we have to spend $x?!"" or ""Why do *we* have to accept them?"" Because we do things differently in America which is why you exist at all. Your immigrant ancestors were allowed to come live here.",3
post33hb,richly branching,1.523173621935305,highest,Not only America but America is basically uniquely blessed with every resource imaginable and an environment that will mostly become more habitable through climate change,3
post33hb,richly branching,1.523173621935305,highest,Do you live in America? Have you been to the Southwest during a heatwave or the mid west during a deep freeze. The only places that may be more habitable is the coast and they are starting  to get pretty full.,4
post33hb,richly branching,1.523173621935305,highest,I think a disturbing number of people are very willing to accept the deaths of strangers in exchange for their own comfort,2
post33hb,richly branching,1.523173621935305,highest,Always has been.,3
post33hb,richly branching,1.523173621935305,highest,The Congolese child who mined the cobalt in your phone is cringing at your comment right now,3
post33hb,richly branching,1.523173621935305,highest,Do you NOT find that disturbing?,4
post33hb,richly branching,1.523173621935305,highest,"There is no way America will accept hundreds of millions of people in, even if Trotsky is elected as president. The likelihood of that leading to a civil war would almost be 100%, and if there's going to be civil wars they'd want it happening outside of the US borders.",2
post33hb,richly branching,1.523173621935305,highest,I think you are correct about the likely outcome but the choice is real.,3
post33hb,richly branching,1.523173621935305,highest,"Total nuclear war could be the third option maybe lol. But I seriously doubt that's ever gonna happen (famous last words)

It's quite exciting how far desalination has come in the past couple decades though.

People can live in extremely horrid and desolate areas if they are rich enough to build functioning infrastructure. Hopefully the tech gets cheaper and cheaper",2
post33hb,richly branching,1.523173621935305,highest,"They’ll seal those borders so tight air won’t get through, before they let hundreds of millions in.",2
post33hb,richly branching,1.523173621935305,highest,[removed],2
post33hb,richly branching,1.523173621935305,highest,Overpopulation isn’t real lol,3
post33hb,richly branching,1.523173621935305,highest,[removed],4
post33hb,richly branching,1.523173621935305,highest,"Fount out recently. The percentage of people who migrated from a different country - less than 4% of the world's population. 

Our perception of the ""immigration"" issue may be overblown.",1
post33hb,richly branching,1.523173621935305,highest,That’s extremely misleading when the costs and benefits of immigration are primarily localized.,2
post33hb,richly branching,1.523173621935305,highest,"I live near Texas which has a huge influence on everything around me and it is really one of the fast growing regions, money and people just pour in and out of Texas, it's like the land of opportunity.",3
post33hb,richly branching,1.523173621935305,highest,"cool, now narrow it down to the western world. 21% of global immigrants go to North America and about 30% go to Europe (I can't say if this includes EU immigrants who go to other EU countries). that means HALF of global immigrants end up in Western countries. this doesn't even include Australia, a major hub for immigration. now in terms of what percentage immigrants make up in some major Western countries: 13.6% in the US, 17% in Germany,  20% in Canada, 29.5% in Australia. it's hard to even say if this calculation accurately releflects the true amount of undocumented immigrants in each respective country. 

long story short, you need to brush up on statitistics and how to interpret them",2
post33hb,richly branching,1.523173621935305,highest,So 21% of 4% of world population immigrate to North America. That's less than 1% of world population. You need to brush up on percentages.,3
post33hb,richly branching,1.523173621935305,highest,Here's another metric. The US accounts for 25% of the world GDP. That's 1/4 of all the wealth in the world. The US population is only 4.25% of the world population.,4
post33hb,richly branching,1.523173621935305,highest,"Ok, I’ll bite. 30% of global immigrants go to Europe, which accounts for less than 3% of the world population and 21% go to North America, which accounts for 7.5% of global population. Immigration is an issue that disproportionately affects the western world. Who cares if immigrants make up a small proportion of global population? I’m focused on the West and how it’s affected.

I don’t understand your line of reasoning.",4
post33hb,richly branching,1.523173621935305,highest,"Pursuit of love and a better life and all it entails. Exploration. You name it. Same reasons as today. Some people just seek a new start in life.

It's a human instinct to be curious and seek the unknown.",1
post33hb,richly branching,1.523173621935305,highest,"The biggest reason is to get a job with a higher salary. And then either bringing your family with you or sending regular payments back home.

Most countries don't want immigrants unless they are coming over to work.

When the jobs are gone that kind of goes out of the window.",2
post33hb,richly branching,1.523173621935305,highest,If nobody has to work. I think the sentiment on immigration will shift. Don't you?,3
post33hb,richly branching,1.523173621935305,highest,"Hmm.. Yeah, I guess so",4
post33hb,richly branching,1.523173621935305,highest,"We will never be able to automate all jobs. As far as design thinking goes, this is a physics problem, not an engineering one.",1
post33hb,richly branching,1.523173621935305,highest,"I don't understand what you mean, sorry?",2
post33hb,richly branching,1.523173621935305,highest,"What a bizarrely limited viewpoint question.

It sounds like OP just hasn't thought this through.

If you think immigration is only for jobs then it's time to go do some studying buddy. Yikes.

Well at least he's asking a question and not just trying to stuff it down people's throats.",1
post33hb,richly branching,1.523173621935305,highest,"Thanks for being open minded. I have clear opinions, as do you. I probably should have made it clearer in the post: I don't care about race, gender, sexuality, etc.

I want AI to make this world more equal.

That said... I will not give up women's rights. Personal autonomy, abortion, or otherwise.

Same with sexual minorities. They have the right to express themselves. I refuse to lose their rights. It's taken centuries to get here, and now we're willing to throw it away?

I'm worried about the growing portion of immigrants in my country who don't support these things. I understand certain places have different views. But I don't want our culture and belief system to die. There has been to much blood spilled to reach the equality we have.

It doesn't necessarily impact me but many of my friends are threatened.

I tell them, look at Stephen Fry's interviews. Look at the videos of gay hangings in Iran.

Some of them don't see the threat, but I do and I won't let them down.",2
post33hb,richly branching,1.523173621935305,highest,"History is your friend. I get what you're saying, but it's also incredibly unrealistic to a certain degree. Immigration isn't the only thing that breeds change, technology, viewpoints, war, all of that.

Just the essence of you saying that you don't want things to change means that you're already setting yourself up for ideological failure.

Life is the opposite. Things change, while yes cultures and values can be constant over generations or so but they are usually altered as they go along.

It's your job to teach yourself this stuff not somebody on reddit. Put it in the work.

Conservative views are great until it absolutely works against you. Think of all the dim-witted Republicans running around the United States talking about they want classic values even though most of that classic value was rooted in racism fear guns and pure stupidity.

Be careful what you wish for you just might get it.  Cheers",3
post33hb,richly branching,1.523173621935305,highest,"People will still seek immigration for culture, novelty, food, human rights, curiosity, climate, experience.  And people would be motivated to accept immigrants due to culture being porous and syncretic, getting different voices and points of view, art, food, and just human receptivity to interaction.   And with strong automation, immigrants aren't ""taking our jobs,"" nor is housing a problem because you can automate construction, and of course build a lot more density, plus mass transit, all kinds of things.  

Not everyone fantasizes about a sealed-off ethnostate.  Let me say that again:  not everyone fantasizes about a sealed-off ethnostate.  Though those that do often mistakenly believe everyone else thinks like them, or that their intuitive position is 'common-sense.'",1
post33hb,richly branching,1.523173621935305,highest,What’s the point of accepting millions of them now? Honestly,1
post33hb,richly branching,1.523173621935305,highest,sshhhhh you can't say that,2
post33hb,richly branching,1.523173621935305,highest,"I know it’s Reddit :) 

I can’t illegally enter a different country and jump on assistance for rest of my life.",3
post33hb,richly branching,1.523173621935305,highest,Can't do that in the USA either.,4
post33hb,richly branching,1.523173621935305,highest,"Agreed, and contrary to certain parties opinions on the matter, you can’t do that in the US either.",4
post33hb,richly branching,1.523173621935305,highest,"To drive down wages. Bigger pool of workers, so more competition. Keeps pay lower. But that doesn't matter at all when automation becomes cheaper than a human worker",2
post33hb,richly branching,1.523173621935305,highest,"You're getting downvoted, but Engels wrote exactly this over 100 years ago.",3
post33hb,richly branching,1.523173621935305,highest,"He did.

Its been well known for centuries. The Statute of Labourers came into effect immediately after the black death wiped a third of the population out of existence in England.

It has been one of the only times in history where the elites were genuinely vocally shocked by the power shift from the upper classes to lower classes.",4
post33hb,richly branching,1.523173621935305,highest,Sounds awful,3
post33hb,richly branching,1.523173621935305,highest,"It gets worse when you add the failure of multiculturalism.

My country has entire sections of cities that are by and large comprised of a single ethnic group that don't really mix with others.

Same thing happened when Russians fled en masse to certain parts of SEA to avoid conscription and created ""white only"" beaches and areas ffs.",4
post33hb,richly branching,1.523173621935305,highest,"What's the point of not? If America is so great, why not accept as many of the worlds citizens asvwe can manage... Let the backwaters collapse... They can't be much trouble if everyone who's not happy with the status quo leaves... Dictatorships are built on the backs of peasants. Let the warlords grow their own food and run their own government and dig their own ditches and see how long they last. Meanwhile we get all the best people...    


Unless you're just a shit head who thinks only white people have value.


Edit: seems like there are a lot of people who disagree can't form a coherent argument against me",2
post33hb,richly branching,1.523173621935305,highest,"Have you ever seen the amount of debt America has? 

No other country has open borders. We are being flooded with people that will never work or contribute to society here. On top of that there are criminals flocking in from all over the world and if you think that’s not the case you are braindead and brainwashed.

Edit. Caught the part about white people value. I don’t see immigrant flooding non-white countries so white people must be doing something right.",3
post33hb,richly branching,1.523173621935305,highest,Why would we want to let our own countries collapse,3
post33hb,richly branching,1.523173621935305,highest,"Eventually, environmental degradation will drive huge masses of people away from the equatorial zones, they will seek habitable zones northward. It will make northern zones retreat from the current coastlines. It's inevitable.",1
post33hb,richly branching,1.523173621935305,highest,When jobs are automated and resources are distributed based on need people will finally be able to live together with like-minded people instead of being supposed to assimilate into whatever culture they happened to be born around. Also if indoctrination of children into things like religion is banned and prosecuted then a lot of people might decide to immigrate at 18 or whatever the adult age is in order to live around their chosen religion.,1
post33hb,richly branching,1.523173621935305,highest,"Tax base for government, since the corporations aren't going to pay any more tax for less workers",1
post33hb,richly branching,1.523173621935305,highest,"Housing, lots of money for the well connected to make.",1
post33hb,richly branching,1.523173621935305,highest,"it is worth considering whether mass automation, which deprives most of society of any resources, will not result in emigration to countries that have introduced certain restrictions in order to protect jobs for their citizens.",1
post33hb,richly branching,1.523173621935305,highest,The world will be more about migration than immigration from here on out. Just look closely around you now. The world order is changing and nobody knows exactly what happens next.,1
post33hb,richly branching,1.523173621935305,highest,The population is concentrating in the global north,2
post33hb,richly branching,1.523173621935305,highest,"They got this shit planned for the next 50 years. You and I maybe don’t know, but those moving the pieces know EXACTLY what they’re doing.",2
post33hb,richly branching,1.523173621935305,highest,I’ve been playing chess since I was 10. Beat my father and uncle days after they taught me how to play. They got their plans. I got my principles. Fortune favors the bold.,3
post33hb,richly branching,1.523173621935305,highest,[deleted],3
post33hb,richly branching,1.523173621935305,highest,"Increased standard of living would be a huge motivation for a lot of people. Even if we got to a point of Star Trek synthesizers and basic income, some countries are just going to be better off than others",1
post33hb,richly branching,1.523173621935305,highest,"The two aren't in the same time frame.

Immigrants will flock to places with good job opportunities, freedom and security.

It will take decades for ""all"" jobs to be automated. The amount of legal framework for some industries would take years minimum to apply. 

I would also find it hard to have automation robots to be cheaper than a human for more complex environment.",1
post33hb,richly branching,1.523173621935305,highest,"Yeah that's fair enough.

But it won't need anywhere near 100℅ automation to have a dramatic impact on the economy as it stands now

Many of our jobs will be safe, many won't.

But those who lose their jobs will now be competing with those in the remaining jobs",2
post33hb,richly branching,1.523173621935305,highest,"If trends continue, we will need to either provide for people's needs without expecting them to work, or find out what happens when a ton of people with nothing to lose decide to spend their energy trying to kill the people who would watch them starve.",3
post33hb,richly branching,1.523173621935305,highest,"I hope things will be ok.

Like we looks after kids, disabled guys and the elderly.

The vulnerable in our respective countries come first before anyone else.

But.. 

>when a ton of people with nothing to lose decide to spend their energy trying to kill the people who would watch them starve.

This will be captivating to watch",4
post33hb,richly branching,1.523173621935305,highest,"Immigration in itself is the point. Cultural diversity doesn't just fall outta the sky, y'know.",1
post33hb,richly branching,1.523173621935305,highest,"Along these same lines, what is the point of all the poors in your country or outside?",1
post33hb,richly branching,1.523173621935305,highest,"Individual countries have a responsibility to care for those within their borders. 

Disabled, elderly, children, etc...

Regardless of where they come from.

At least in my city I know for a fact we have enough social housing to support our homeless population. 

Unfortunately like most of the world we just put zero money into mental health support so these guys end up on the streets anyway when they don't fit into the mould.

And every year, more children go hungry, more disabled people without social welfare... Fucked up",2
post33hb,richly branching,1.523173621935305,highest,"That’s a nice perspective. I worry that most social safety nets are there to keep the workforce mostly intact. A workforce that is currently needed because labor is needed. Remove that need and I believe that the current crop of the mega rich would support political candidates that would remove these safety nets. 

A step further would be for the mega rich to decide that it is unnecessary to keep such a population around at all.",3
post33hb,richly branching,1.523173621935305,highest,"because not every job can be automated. _someone_ has to keep all the bullshit running. after all, our banking system is held together by ducktape, Fortran, and hope.",1
post33hb,richly branching,1.523173621935305,highest,"Lmao

Yeah that's accurate.

I love how good we are as a species to keep things running just enough to work",2
post33hb,richly branching,1.523173621935305,highest,yup. Automation is simply another tool in the bag for us to do that.,3
post33hb,richly branching,1.523173621935305,highest,"Even if we have automated jobs, and need fewer employees, corporations will still need customers to buy their stuff.



Ordinary citizens enjoy their families better when they have the number of children that they want. That often means 1 or 2, or no children at all.


Corporations don't care about citizens' quality of life, so they lobby the government to increase immigration. 


We need the government to regulate corporations so that they do not behave like energy & money vampires, to the population of the country",1
post33hb,richly branching,1.523173621935305,highest,"We will have machines picking fruit for us and doing other farm labors? Immigrants contribute a lot to our economy, and they don't ask for much in return. We can manage to keep humans occupied despite automation in the workforce.",1
post33hb,richly branching,1.523173621935305,highest,"I am convinced that immigration as a problem is solved with a quite universal passport: the right hand palm (handprint biometrics of next-day). We have to agree that the word ""spurrious"" is poorly understood and usually confused with the English words ""useless"", ""futile"", ""superfluous"" and ""meaningless"". If that was more real would likely be also more semantic. I need: phones can call me to a chatbot of choice Google/Microsoft/Meta/OpenAi/Claude/Apple. I need fridges/refrigerators and fridgemakers making their machines able to extract drinkable water out of thin energy with some of our mechanical/kinetic energy. I would ban indirect payments (old bartering payment systems in some jurisdictions because they seem highly illegal, unvialble, and connected to will for non-accountability and no will for digital or printable payment receipt. We, by difenition are human: designed to be happy and make everyone and us all happy ;-) Miss? At at your elevel I would pass some milestones in such an exam/testing. You say,",1
post33hb,richly branching,1.523173621935305,highest,"Well, there will always be Star-bellied sneetches, and there will always be non-star-bellied sneetches. If the non-Star-bellies sneetches wish they had stars on their bellies, then the Star-bellied sneetches will pay the non-star-bellied sneetches pieces of stars to do the things they don’t want to do. So the non-star-bellied sneetches will emigrate to the star-bellied sneetches beaches to get the stars.

That’s the point of immigration.",1
post33hb,richly branching,1.523173621935305,highest,Politicians still need voters. Robots can’t vote yet.,1
post33hb,richly branching,1.523173621935305,highest,"The birth rate in this country (USA) is down. If it weren't for immigrants, the population would be declining.",1
post33hb,richly branching,1.523173621935305,highest,"From the nation's perspective, there are often cultural and economic benefits from accepting new members.


From the immigrants' perspective: politics, violence, gangs, wars, climate and climate change, hunger, education, health and healthcare, the culture, the marketplace, family.... Work/income isn't the only reason to relocate.",1
post33hb,richly branching,1.523173621935305,highest,We’ve still got decades with the need for cheap labor.  It’s not like in 5 years we’re going to be living in a sci fi world of robots,1
post33hb,richly branching,1.523173621935305,highest,Thankfully no. Bring on the bots. Can’t wait for robo-tradie. Might get a decent price for your home renovations.,1
post33hb,richly branching,1.523173621935305,highest,"There already is no point.  
No country on this planet needs immigration.",1
post33hb,richly branching,1.523173621935305,highest,"It is well established that the further left on the political spectrum you go, the less children you have. This spells disaster for any left leaning political party in a country that relies on popularity contest to pick the government. There will always be a political necessity for the platform of “come to our country and we will give you free stuff, but don’t vote for the other party because they won’t.”

I concede that was dramatically oversimplified.",1
post33hb,richly branching,1.523173621935305,highest,"Poverty. 

Automation will help the very rich and leave many people very poor.",1
post33hb,richly branching,1.523173621935305,highest,"I think you are asking what the point of letting people in would be over what the reasons for them coming are? You know, because it has always been possible to line up job categories that experience high demand with foreign labor. Companies just say that those foreign workers have skills that your people don't. 

It's not crooked on its surface. It only gets that way when you consider how companies in certain industries get away with not offering anything like a real wage to locals, then dangle the reality of coming to America at any wage before a foreign worker. 

Covid showed that out. It happened down the road at this huge ski resort. They used not to try and pay more than $12 an hour. They relied heavily on J1B's ,or whatever those visas are called. Then Covid hit, and their corporate minimum wage is now $20. 

It had to, or in the new world no one would work for them. Fortunately, the company realized that ahead of the curve and responded with a wage increase before there was too much of a fight. Because, you know, unions. Above all, they don't want unions.",1
post33hb,richly branching,1.523173621935305,highest,"It more to keep house prices high, GDP going up (not GDP per capita) and to keep wages low. 


If the government actually wanted certain types of workers they would pay for it or make business pay for it.  


Businesses always look for employees with 5 or 10 years experience so they can get in cheap labour below market rate. They never look for 0 years experience to train them up or pay enough to convince people into that industry.


And governments never do anything about the hosuing crisis because the people in business own houses and benefit from the crisis so do their donors, also the old. This last one is the one that does it for me, my parents (and their gen) who own a house have no interest in anything that would benefit their children getting a house. Because it would either make imaginary numbers go down or would bring more people (like their childre ) into the area.",1
post33hb,richly branching,1.523173621935305,highest,"people emigrate because they want to either enjoy the place they move to or escape the place they move from.

if all work is automated then there would have to be some other way to improve your life in the land of automated labor otherwise people would not even want to emigrate. 

&#x200B;

they only want to come to the US because there is hope at an improved life.",1
post33hb,richly branching,1.523173621935305,highest,"This sub is truly a doomeristic place where people keep fearing stuff that will never happen, or will happen 300 years from now.

All jobs are automated? Seriously?",1
post33hb,richly branching,1.523173621935305,highest,AI will be the best thing that happens to our species,2
post33hb,richly branching,1.523173621935305,highest,"immigrants (people) might wanna be athletes, musicians, fashion models, actors... *those jobs are gonna be automated by robots?*",1
post33hb,richly branching,1.523173621935305,highest,"Assuming your comment isn't a joke..

>fashion models, actors...

Those two, for sure will and should be automated

Most celebrities are even more shallow and self-centered than us redditors.

Models are already 50℅ Photoshop, and A-list actors are shitty people.

Musicians, I do think will be. Chart music will probably be automated. Chart music is some chemically produced crap.

Local musicians performing live music will never go away.

Athletes won't either. But there will always be the debate with doping. And prosthetic limbs, as they get better. Oscar Pistorius comes to mind. What a shit",2
post33hb,richly branching,1.523173621935305,highest,"immigrants are for the most part not simply looking for jobs.

they look for survival.",1
post33hb,richly branching,1.523173621935305,highest,"More bodies for the war machine. Just chew through them, cast them aside, and move on to the next loser.",1
post33hb,richly branching,1.523173621935305,highest,The point wasn't just to get jobs in the first place. Most people immigrate to escape their current situation. And that will remain the same after automation.,1
post33hb,richly branching,1.523173621935305,highest,"Immigration drives the economy. People come and spend money, buy houses, pay rates etc.",1
post33hb,richly branching,1.523173621935305,highest,There is a population crisis in the eu and they definitely need residents to maintain growth otherwise their society would collapse without younger generations.,1
post33hb,richly branching,1.523173621935305,highest,"Immigration often brings brain power. I think when most jobs are automated, we will still decision makers, planners, and objective setters.",1
post33hb,richly branching,1.523173621935305,highest,"I would still immigrate to improve my chances of finding a new wife. The dating pool around here especially those my age, are nasty disloyal people. At this point I don't have any friends from before I was 25 because they all suck the life out of everything. 

If I move somewhere else with more women and less men and less of a ""take everything personally and create drama constantly""attitude I can actually relax and find a decent woman who doesn't just want me for money or muscles",1
post33hb,richly branching,1.523173621935305,highest,"Good luck with that.

You stand out, you have to be attractive, have a nice personality, or be charismatic. All good traits that will attract partners no matter how small the dating pool is.

Barring that, you need to be rich.

So yeah, money or muscles can both be important traits no matter where you are in the world.",2
post33hb,richly branching,1.523173621935305,highest,"That was not what I was saying. You're making the assumption I can't find a date, I can and have never had trouble in that department. I don't like the women that live here. I see it all across the city everybody just dating and dumping each other or just settling for someone just to avoid loneliness even when they don't like them",3
post33hb,richly branching,1.523173621935305,highest,Immigrants will be needed to do the thinking that nativists are proving themselves unable to do.,1
post33hb,richly branching,1.523173621935305,highest,">Immigrants will be needed to do the thinking that --nativists-- are proving themselves unable to do.

No, that's just people in general. Every country has proven they can't learn from the past and give in time and time again to their primitive human desires.",2
post33hb,richly branching,1.523173621935305,highest,"That means immigrants are needed, to save people from themselves.",3
post33hb,richly branching,1.523173621935305,highest,"The same as it is now for the large part: goodwill. Especially in the US, we’ve been in a “need to save the world” mode for a long time now.",1
post7lb,poorly branching,0.111396923154935,lowest,"Interesting work! Can it be used for multilingual bias probing (for example, to evaluate gender bias in low-resource languages)?",1
post7lb,poorly branching,0.111396923154935,lowest,"Thanks. Right now it's English only, but the idea is to have the project be easily extensible with new probes, and they can be in arbitrary languages. I would love to include some multilingual evaluations in the future, but it is more or less limited by my capacity.",2
post7lb,poorly branching,0.111396923154935,lowest,I see. Good luck!,3
post10lb,poorly branching,0.3644601861909935,lowest,Cool. Let us know how they go. I expect their company should do just as well as any other. Why wouldn't it?,1
post10lb,poorly branching,0.3644601861909935,lowest,"reading through that article it sounds like they must be discriminating against hiring men. Which is fine; while the STEM gender gap continues. I'm curious if places like this have plans to remediate the male hiring gender bias they'll have firmly ingrained in their work culture when STEM gender gaps level out, as they're slowly but steadily doing though.",1
post10lb,poorly branching,0.3644601861909935,lowest,"I guess you didn't read it that closely:

""Women still make up only one-third of the global scientific community, with the percentage stagnating over the past decade, according to a 2024 report by UNESCO (United Nations Educational, Scientific and Cultural Organization). In some countries, less than 10 per cent of researchers are women. 

They hold just 22 per cent of STEM jobs in G20 countries, and only one in 10 ascend to leadership positions.""

What do you want to bet it's filled with women who couldn't get jobs elsewhere?

Regardless, I love this article. It reminds people that, no, DEI is not reverse discrimination. It can't be because, unlike what people are whining about, that facts say that women and minorities are still underrepresented pretty much everywhere that isn't minimum wage adjacent.",2
post10lb,poorly branching,0.3644601861909935,lowest,"I specifically mentioned that woman were still highly underrepresented in STEM fields as the article said, that being said 15 years ago that number was less than 5%, and the fact that it's up to 22% now, while great means that in entry level STEM position women are being hired at over 2:1 ratios compared to men. Which is fine for now, but it's going to lead to a situation in another 10 years where the gender roles are completely reversed and only men will need to be hired in entry level positions, it's turned into a see saw situation. It's currently much easier for women to get stem jobs than men so I believe asking what policies institutes that are primarily hiring woman have to even things out in a few years when it's a woman dominated field is reasonable.",3
post10lb,poorly branching,0.3644601861909935,lowest,"And surely, it won't be a problem for other firms to demand men only. After all, there will be plenty of places for women to work amongst themselves. Or isn't it supposed to work like that?",4
post10lb,poorly branching,0.3644601861909935,lowest,"You are bad at math, or logic, or both.

Your assumption that women are getting entry level positions at twice the rate as men is based on the idea that *there is no turnover*. 

But the numbers tell a *very different story* ([Source](https://www.hrdive.com/news/stem-staffing-shortage-retention-recruitment/693188/)):
As HR leaders, we must remember that staffing shortages are not just related to recruitment and hiring challenges— employee churn and turnover play an equally large role. As reported by the Bureau of Labor and Statistics, the national average turnover rate was 47.2% in 2021.

Your whistleblowing on 'women taking over the workforce' isn't just bullshit, it exposed your fundamental biases. Maybe this is why you don't have a relationship. Studies say hard right conservatism is negatively associated with finding and keeping a partner in men.

And for the record, people working on STEM fields consistently have lower unemployment rates and higher employment rates historically, by *wide margins* Maybe you should stop talking like men in these fields are suffering.",4
post21lb,poorly branching,0.4191869871864478,lowest,"Machine learning algorithms need lots of labeled data.  Essentially a computer is shown a picture of a person and the computer makes a guess whether it's a person or not.  Then the computer is told the correct answer and the computer takes notes on what it got right and wrong and then adjusts itself.  Sometimes this data is biased, such as being shown way more white people than black people.  Because the computer primarily sees white people, it begins to associate white humanoid with person and black with not person in a more extreme case and not sure in a less extreme case.  There was a really bad example of this a while ago where an AI started labeling black people as gorillas.  Because the people it had primarily seen where white and the things that were dark skinned and humanoid where predominately gorillas not black people.

&#x200B;

With kinect specifically it may be a different problem that has a similar cause.  Kinect uses an infrared camera and different skin tones (especially darker skin vs lighter skin) looks different in infrared so the more basic software wasn't capable of recognizing what it was seeing as a human (this actually has happened with hands free sinks before).  In teams that are predominately white, it may not have come up during testing (engineering teams are often their own first tests) and they may not have thought about it.",1
post21lb,poorly branching,0.4191869871864478,lowest,Thank you!,2
post21lb,poorly branching,0.4191869871864478,lowest,This has always seemed a weak explanation to me. You don't train a machine learning algorithm on a training set of the half dozen guys who developed it.,2
post21lb,poorly branching,0.4191869871864478,lowest,"That is exactly how it works. Older machine vision was all about manual feature engineering and didn't need a lot of data.

Black people are quite rare in most places of the world. If you pick a company ID database, chances are it will be 99% white/asian",3
post21lb,poorly branching,0.4191869871864478,lowest,"No you don't use that small of a data set.  There's two things going on and I guess I didn't separate them out enough.  Large data sets can be biased and not reflect a complete population.  And even then, it might not be enough to actually train the algorithm to actually recognize a minority of the data.  These algorithms are still way less complex than we are and can have trouble with rarer data.  An algorithm might just decide to take the accuracy hit rather than actually recognize a small portion of the data (if you want the algorithm to be 96% accurate and you have a subset that's 3% of the data, you can hit your target and get all of the subset wrong).

Then there's the diversity of the dev teams. If you don't have much diversity on the dev teams, sometimes stuff can slip by on early testing.  As an example, I was in a computer vision class earlier in the year.  out of the three people on my team, two were white and one was asian.  On one of our assignments, myself and the other white person set a value and it worked.  But when our other teammate went to use it it didn't because his skin was darker.  If we didn't have the third person it would have slipped by.  I'm not excusing this, and it totally shouldn't happen with something on the scale of microsoft but it can happen with proofs of concept or tests.",3
post21lb,poorly branching,0.4191869871864478,lowest,"1. Because it got trained on white faces.
2. Because it uses a poor quality camera that can't accurately resolve the details of dark faces.",1
post21lb,poorly branching,0.4191869871864478,lowest,"Found this quote from a New York Times article:

“One widely used facial-recognition data set was estimated to be more than 75 percent male and more than 80 percent white, according to another research study.”",2
post21lb,poorly branching,0.4191869871864478,lowest,"This is about pattern recognition in automation, not specifically face recognition but I guess the same applies.

When teaching pattern recognition you need contrast. The picture you're teaching is usually in black and white and the algorithm finds the borders between black and white areas. Each pixel is assigned a grey value, typically between 0 (totally black) and 255 (totally white)

Ideally, your black parts should be as close to 0 as possible and white parts to 255. The image is then binarized, meaning that you set a grey value threshold 128 for example. Then algorithm will compare each pixel value to that threshold, if it's higher it's assigned white, lower it's black. If you need to set the threshold to say 40, there is a risk that a feature you want to measure gets assigned the wrong color area.

If the contrast is high (like dark features on a pale face) algorithm has easier time distinguishing them than if the contrast is low (dark features on dark skin) because the difference in grey values is greater, enabling more precise and consistent measurements.",1
post21lb,poorly branching,0.4191869871864478,lowest,"People so far have stated training on white people.. In the case of the Kinect this is not true however. Microsoft even came out and exclaimed everything and if you have any dark skinned friends and a Kinect you can check their claims on a computer.

The shit reason is the camera is rubbish. Unless your dark skinned person is white up like the Empire State Building the camera is simple not good enough to see and therefore recognise features on his/her face.

Add to this all training data was on helpfully bright backgrounds not dark ones and you get to see that a dark person on a dark background is basically just eyes according to the camera.

Eyes do not make a face. Eyes and mouth largely speaking are the definition points for a face using most detection algorithms.

Infrared cameras get passed this but the infrared camera on the Kinect doesn't do face detection it does depth mapping feeding the kinematics engine.

Face detection is done via simple algorithms (no AI) and works as long as eyes and mouth are visible in the image. Facial recognition is done many many ways but all rely upon good days of which there is none for dark people on dark backgrounds. So it suffers.",1
post22lb,poorly branching,0.4191869871864478,lowest,"Machine learning algorithms need lots of labeled data.  Essentially a computer is shown a picture of a person and the computer makes a guess whether it's a person or not.  Then the computer is told the correct answer and the computer takes notes on what it got right and wrong and then adjusts itself.  Sometimes this data is biased, such as being shown way more white people than black people.  Because the computer primarily sees white people, it begins to associate white humanoid with person and black with not person in a more extreme case and not sure in a less extreme case.  There was a really bad example of this a while ago where an AI started labeling black people as gorillas.  Because the people it had primarily seen where white and the things that were dark skinned and humanoid where predominately gorillas not black people.

&#x200B;

With kinect specifically it may be a different problem that has a similar cause.  Kinect uses an infrared camera and different skin tones (especially darker skin vs lighter skin) looks different in infrared so the more basic software wasn't capable of recognizing what it was seeing as a human (this actually has happened with hands free sinks before).  In teams that are predominately white, it may not have come up during testing (engineering teams are often their own first tests) and they may not have thought about it.",1
post22lb,poorly branching,0.4191869871864478,lowest,Thank you!,2
post22lb,poorly branching,0.4191869871864478,lowest,This has always seemed a weak explanation to me. You don't train a machine learning algorithm on a training set of the half dozen guys who developed it.,2
post22lb,poorly branching,0.4191869871864478,lowest,"That is exactly how it works. Older machine vision was all about manual feature engineering and didn't need a lot of data.

Black people are quite rare in most places of the world. If you pick a company ID database, chances are it will be 99% white/asian",3
post22lb,poorly branching,0.4191869871864478,lowest,"No you don't use that small of a data set.  There's two things going on and I guess I didn't separate them out enough.  Large data sets can be biased and not reflect a complete population.  And even then, it might not be enough to actually train the algorithm to actually recognize a minority of the data.  These algorithms are still way less complex than we are and can have trouble with rarer data.  An algorithm might just decide to take the accuracy hit rather than actually recognize a small portion of the data (if you want the algorithm to be 96% accurate and you have a subset that's 3% of the data, you can hit your target and get all of the subset wrong).

Then there's the diversity of the dev teams. If you don't have much diversity on the dev teams, sometimes stuff can slip by on early testing.  As an example, I was in a computer vision class earlier in the year.  out of the three people on my team, two were white and one was asian.  On one of our assignments, myself and the other white person set a value and it worked.  But when our other teammate went to use it it didn't because his skin was darker.  If we didn't have the third person it would have slipped by.  I'm not excusing this, and it totally shouldn't happen with something on the scale of microsoft but it can happen with proofs of concept or tests.",3
post22lb,poorly branching,0.4191869871864478,lowest,"1. Because it got trained on white faces.
2. Because it uses a poor quality camera that can't accurately resolve the details of dark faces.",1
post22lb,poorly branching,0.4191869871864478,lowest,"Found this quote from a New York Times article:

“One widely used facial-recognition data set was estimated to be more than 75 percent male and more than 80 percent white, according to another research study.”",2
post22lb,poorly branching,0.4191869871864478,lowest,"This is about pattern recognition in automation, not specifically face recognition but I guess the same applies.

When teaching pattern recognition you need contrast. The picture you're teaching is usually in black and white and the algorithm finds the borders between black and white areas. Each pixel is assigned a grey value, typically between 0 (totally black) and 255 (totally white)

Ideally, your black parts should be as close to 0 as possible and white parts to 255. The image is then binarized, meaning that you set a grey value threshold 128 for example. Then algorithm will compare each pixel value to that threshold, if it's higher it's assigned white, lower it's black. If you need to set the threshold to say 40, there is a risk that a feature you want to measure gets assigned the wrong color area.

If the contrast is high (like dark features on a pale face) algorithm has easier time distinguishing them than if the contrast is low (dark features on dark skin) because the difference in grey values is greater, enabling more precise and consistent measurements.",1
post22lb,poorly branching,0.4191869871864478,lowest,"People so far have stated training on white people.. In the case of the Kinect this is not true however. Microsoft even came out and exclaimed everything and if you have any dark skinned friends and a Kinect you can check their claims on a computer.

The shit reason is the camera is rubbish. Unless your dark skinned person is white up like the Empire State Building the camera is simple not good enough to see and therefore recognise features on his/her face.

Add to this all training data was on helpfully bright backgrounds not dark ones and you get to see that a dark person on a dark background is basically just eyes according to the camera.

Eyes do not make a face. Eyes and mouth largely speaking are the definition points for a face using most detection algorithms.

Infrared cameras get passed this but the infrared camera on the Kinect doesn't do face detection it does depth mapping feeding the kinematics engine.

Face detection is done via simple algorithms (no AI) and works as long as eyes and mouth are visible in the image. Facial recognition is done many many ways but all rely upon good days of which there is none for dark people on dark backgrounds. So it suffers.",1
post51lb,poorly branching,0.6692847952807257,lowest,"As a reminder, this subreddit [is for civil discussion.](/r/politics/wiki/index#wiki_be_civil)

In general, be courteous to others. Debate/discuss/argue the merits of ideas, don't attack people. Personal insults, shill or troll accusations, hate speech, any suggestion or support of harm, violence, or death, and other rule violations can result in a permanent ban. 

If you see comments in violation of our rules, please report them.

 For those who have questions regarding any media outlets being posted on this subreddit, please click [here](https://www.reddit.com/r/politics/wiki/approveddomainslist) to review our details as to our approved domains list and outlet criteria.
 
 We are actively looking for new moderators.  If you have any interest in helping to make this subreddit a place for quality discussion, please fill out [this form](https://docs.google.com/forms/d/1y2swHD0KXFhStGFjW6k54r9iuMjzcFqDIVwuvdLBjSA).
 

***


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/politics) if you have any questions or concerns.*",1
post51lb,poorly branching,0.6692847952807257,lowest,"To be honest, AI could have a lot of really good uses *if* the humans that were to interpret the output could be trained to correctly interpret the output. 

I have no confidence in that training, however.",1
post51lb,poorly branching,0.6692847952807257,lowest,">...to help with immigration, trafficking investigations, and disaster relief

For now.

Give a man a hammer, and every problem looks like a nail.",1
post51lb,poorly branching,0.6692847952807257,lowest,"If and only if AI didn't include our own biases, it would be nice/interesting to see what a strategy free from politics, greed, effect on the polls, etc would be. Unfortunately, that's impossible now that it has been trained on our output.",1
post51lb,poorly branching,0.6692847952807257,lowest,I don't think we want it completely unbiased. It would essentially be technocrate....which will seem very heartless.,2
post51lb,poorly branching,0.6692847952807257,lowest,That would still be a better starting place than what we do now which is to let monied interests and groups like ALEC write laws for us.,3
post51lb,poorly branching,0.6692847952807257,lowest,"What makes you think Alec is better?

 Sometimes I wonder that maybe letting money and greed dominate is he best case scenario we can have, because at least it's predictable and based on some known factors. 

I am not very optimistic of the whole human nature and as history proves that a lot of humans worst deed was done with the most righteous reasoning and good intention. 

So if we can't really rid the system of bias, then isn't it better to have it be based on a set of known biased so that at least it can be somewhat predictable and known.",4
post51lb,poorly branching,0.6692847952807257,lowest,"NYT: 
[The Department of Homeland Security Is Embracing A.I. —- The agency will be the first in the federal government to roll out a comprehensive plan to integrate the technology into a variety of uses, from fighting crime to helping disaster survivors.](https://www.nytimes.com/2024/03/18/business/homeland-security-artificial-intelligence.html)

DHS is partnering with OpenAI and Anthropic and Meta/Facebook (information scrapers) so American privacy is screwed imho.",1
post51lb,poorly branching,0.6692847952807257,lowest,">American privacy is screwed imho

Since 1982... too late to complain now",2
post51lb,poorly branching,0.6692847952807257,lowest,America is one of the nations of the [Five Eyes](https://en.m.wikipedia.org/wiki/Five_Eyes). They’ve been monitoring all of this way before the advent of AI.,1
post51lb,poorly branching,0.6692847952807257,lowest,How about you test slamming the door. Just turn everyone around and tell them to go home. It'll require less tech and less personnel.,1
post20lb,poorly branching,0.6756241153332649,lowest,"There is a lot to unpack here, so please bear with me.

A computer has no natural tendencies towards anything other than processing bits of information.  The computer itself isn't racist or non-racist.  It's merely a vessel for whatever the programmer makes it do.

Algorithms _can_ be racist, if they make different decisions based on race.  If, for example, your bank uses an algorithm to decide who can get a loan and who doesn't, and that algorithm takes ""race"" as input and generates a different output based on that data, then the algorithm is biased, and will produce biased results.

One example of this is with [criminal risk assessment algorithms, which significantly disadvantage Black people](https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/).

You of course asked about facial recognition.  Designing a facial recognition algorithm by hand is complex and error prone, so typically^0 we use machine learning to allow the computer to create the necessary algorithm.  In such a scenario, the developer codes the starting conditions for a blank network, and then feed a large number of images into the network to ""train"" it in how to recognize a face.  Further code may then be added to pick out specific features of the face, and creating some sort of identification code for the face itself to match up with databases of known faces.

There are a few areas here where racial bias can creep in.  Firstly, you can run into the situation where the training set contains insufficient images of specific races, allowing the algorithm to form a bias against them.  For all their power, computers aren't very smart, and even the best Convolutional Neural Network doesn't generalize like we can -- so if you feed the network a series of blond haired, blue eyed, white faces, it's not going to be able to recognize faces outside those contexts.

And this has in fact been the case.  A famous instance of this occurred back in 2015, when it was found that [Google was identifying black men as gorillas](https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/).  A large part of this appears to be because Google's training algorithms were given many, many more white faces than black faces.

""Recognizing that something is a face"" in a photo is somewhat different from ""identifying _someone_ in a photo"".  We've seen case of failure in the first; what about the second?

Testing has revealed [bias and inequalities in common facial recognition and identification systems](https://www.cbc.ca/news/technology/facial-recognition-race-1.5403899)^1.  The bias in each system differs, and the report note that while many systems have higher false positive rates for East Asian faces, it's the opposite for systems developed in China, where the false positive rate is _lower_ for East Asian faces.

I don't have sufficient knowledge to comment on the idea you espouse that non-white faces are ""much less varied than white faces""^2, but even if we thought this were true, the evidence that algorithms developed in different parts of the world fare better or worse on certain types of faces seems to indicate that this shouldn't be a problem for a facial recognition algorithm.  The issue lies in one (or both) of the following two areas of development:

1. **Training**:  the training data exhibits a bias with the types and number of photos available of different ethnicities and demographics, and hence produces biased results, and/or
2. **Validation**: the developer of the algorithm is predominately verifying their trained algorithm with certain ethnicities, and not enough of others to ensure that the false-positive rate is compatible.

With #1, you introduce bias via a lack of sufficient data.  This may not be intentional -- it may be that the training sets available to the developer simply have this bias built-in (for example, if you are developing in a predominately East Asian country like China, you may have access to a full database of all Chinese drivers license or passport photos to train with, which are predominately East Asiatic faces).  In #2, however, more of the developers bias comes out -- if (say) a software developer feeds their neural network a somewhat varied supply of photos from a variety of demographics, but then only validates that training with photos that are (for example) predominately white, you'll never know if the training was successful for other races.

And in the race to market for many companies, corners like this get cut, either intentionally or unintentionally.  Ideally, every company would have a wide range of suitable photos from different races to validate their algorithms against, come up with an accuracy score for each by race/gender or other identifiable detail, and not ship until they ensure that the scores for each group fall within the same bounds.  But that, as we see from the testing, isn't happening.

That brings us to another issue:  how and why is this a problem?

Firstly, I'd like to note that I don't think there is anything wrong with developing a biased algorithm.  Iteration is an important part of most large computing projects, and developers of such algorithms should be validating their algorithms against a wide set of possible data, and ensuring that everyone falls within certain narrow bounds for error.  If they don't, then they should iterate and improve their algorithms until they do.

See, there's nothing wrong with a biased algorithm -- so long as it doesn't leave the lab, and people either work on improving it or discard it for something that exhibits less (or potentially no) bias.  So long as we acknowledge this as a risk factor, and then take steps to measure and mitigate it, the fact that an early version of an algorithm held a bias shouldn't be something to be ashamed of.

The _problem_ is when biased algorithms like this are put into production, and are used by police, military, governments, or other organizations to disadvantage one group of people compared to another.  That is, it's when the algorithmic bias is used in the real-world to be biased against living people.  That's where the problems occur -- and people are right to be angry when an unfeeling computer that can't be reasoned with mis-identifies and disadvantages them.  It's the people using these algorithms who need to be the ones to say ""I don't trust the answers from this system, because it exhibits bias"".  Unfortunately, what we often instead see in this world is a ""machine is never wrong"" attitude^3, which disadvantages certain people, but where the authorities that act on the algorithmic findings simply don't care if they're getting biased results in the first place.

Is it possible that there are going to be demographics of people that have faces more difficult to analyze than others?  That's possible^4.  But if that _is_ the case, then _we shouldn't be using these systems to identify people for special treatment_.  That is, if it were to turn out that bias in facial recognition algorithms is _impossible_ to get within certain acceptable bounds, _then we shouldn't be using those systems, **full stop**_.

In summation, there are lots of ways for bias to creep into an algorithm, many of which may be unintentional.  However, this bias should be measured for various groups, should be published so everyone is aware of the bias, and in cases where the bias is significant _should not be used in ways that disadvantage the people for whom the algorithm is biased against_.

-----
^0 -- AFAIK, you can read this as ""in every case I know of""  
^1 -- [""Face Recognition Vendor Test, Part 3: Demographics Effects"", NIST, 2019](https://nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8280.pdf).  
^2 -- I don't believe this to be true, but it's not my area of study or expertise.  I'm a computer scientist, not an ethnographer.  
^3 -- Or just as bad: ""the machine is often wrong in specific cases, but we're going to disadvantage those people anyway and _maybe_ apologize later after we've given them unnecessary grief, on the off chance the system is right for once"" attitude.  This is really just another way of allowing people to use the computer as an excuse to support their own biases as valid.  
^4 -- Again, see ^2:  not an ethnographer!

EDIT: typos",1
post20lb,poorly branching,0.6756241153332649,lowest,"> I don't have sufficient knowledge to comment on the idea you espouse that non-white faces are ""much less varied than white faces""

I'm fairly sure OP just stumbled into the [Cross-Race Effect](https://en.wikipedia.org/wiki/Cross-race_effect) - most people are far better at recognizing faces from their own race, and tend to think other races ""all look the same"".",2
post20lb,poorly branching,0.6756241153332649,lowest,"I think they're also falling into an easy fallacy presuming that ""computer vision"" and ""human vision"" are one and the same, [which I have attempted to debunk below](https://www.reddit.com/r/askscience/comments/ihrhyh/why_are_facial_recognition_algorithms_racist/g35xn2x?utm_source=share&utm_medium=web2x&context=3).",3
post20lb,poorly branching,0.6756241153332649,lowest,Thank you for this detailed and interesting explanation!,2
post20lb,poorly branching,0.6756241153332649,lowest,"What about the actual data that comes into the computer though? The information is captured by a camera and converted to an image, but dark faces can give a lot less contrast, especially under low light conditions. Dark skin can also hide underlying features like moles, freckles and blush and blend in with hair more easily, which is also worsened under low light conditions and with low quality cameras. How do these effects come in when you'd consider an otherwise ideally unbiased detection algorithm?",2
post20lb,poorly branching,0.6756241153332649,lowest,"> The information is captured by a camera and converted to an image, but dark faces can give a lot less contrast, especially under low light conditions.

I wanted to clear up some invalid assumptions that seem to be creeping into the conversation, that will help show why this line of reasoning isn't particularly valid.

First off is the unspoken assumption that cameras ""see"" the same way we do.  This is not true.  [Here is the spectral breakdown for panchromatic film](http://www.geo-informatie.nl/courses/grs20306/lectures/05aerialphotography/05aerialphotography08.gif).  Note that the yellow line (panchromatic film)^0 has spectral sensitivity below 0.4µm, and above 0.7µm^1.  These correspond to the Near Ultra Violet and Near Infra Red portions of the spectrum, both of which are outside the normal human range of vision.  Because of this, computer-processed digital images may be able to discern features that show up in NUV and NIR that aren't discernible to normal human vision.  Hence, assumptions about what features are not visible to a normal human don't necessarily apply to computer images.

Similarly, computers can detect very small differences in colouration more readily than humans can.  Couple this with the above, and it is not a given that computers can't pick out features in photos that humans can't.

Secondly, it feels like several commenters want to focus on thinks like birthmarks, moles, freckles, or other such skin markings as the basis of identification.  These _may_ come into play in some algorithms, however these algorithms tend to rely significantly more on facial [_geometry_](https://miro.medium.com/max/1234/1*C8UucvbO_DmoJlETCS7K3w.jpeg) rather than blemishes of the skin.  They tend to focus more on the ratios of size of the mouth, nose, distance between the eyes, foreheads, etc.  The reason for this is that these things are extremely hard for someone to change -- you would need some radical surgery to change your inter-pupillary distance (IPD), for example.  If recognition systems focused on moles, freckles, or hair as many people here have hypothesized, then you'd be able to completely throw them off by putting a small black sticker on your chin, or by having a breakout of acne, or by wearing a bit of makeup, or by getting a haircut.

So the focus on ""darker faces hide hair and marks on skin"" is not valid.  Photographs can record more details than humans can see.  Computers have significantly more power to pull out small differences in colouration than humans do.  And facial recognition algorithms likely^2 take this into account to reduce the number of false positives.  It's better to focus on features that are more difficult to change, such as the gross facial dimensions and features, rather than skin imperfections which can easily change for an individual, and which can be trivially faked or masked.

-----
^0 -- [Here is an example of spectral sensitivity for true colour film](http://www.geo-informatie.nl/courses/grs20306/lectures/05aerialphotography/05aerialphotography13.gif), which is not as wide as that of panchromatic, but still dips into NUV range.  
^1 -- Film and digital photos used for photo ID (such as those used in passports and drivers licenses) may _purposefully_ use wider-gamut film/sensors than those presented here, specifically to extract more processable details than humans can otherwise obtain from human vision, specifically to give more detail for computer recognition systems to work with.  Likewise, cameras used to pick out faces may use a wider gamut than human vision.  Point being, don't assume ""what you see"" is automatically the same as ""what the camera"" (and hence computer) ""sees"".  
^2 -- I don't like using a lot of weasel-words like this, however virtually all facial recognition algorithms in active use are commercially developed, and if their specific details are available, they're not available to _me_, so I can only talk in generalities.  Sorry :P.",3
post20lb,poorly branching,0.6756241153332649,lowest,"So, if special cameras with different colour filters are used, does that also mean that wearing for example sunscreen, which filters out features that are very apparent at non-visible wavelengths, could interfere with facial recognition?

Also, how does this apply to non-ideal conditions, like low lighting, further away and awkward angles like on the street? Do the features on dark faces come across as well as light faces or do they have a different ""cutoff point"" at which the conditions are so bad that a camera can't capture a properly processable image?",4
post20lb,poorly branching,0.6756241153332649,lowest,[removed],3
post20lb,poorly branching,0.6756241153332649,lowest,"> they are much less varied than white faces

There is [more human genetic diversity in Africa](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2953791/) than in the rest of the world combined. That does not mean they have more facial diversity, but it’s a strong clue that, if you think they don’t, it may be you that is wrong. 

But, even if you are right there, we need to ask, “should people who have been historically disadvantaged continue to be disadvantaged by computer systems in the modern day”. Blacks have been affected negatively by things like police lineups and the cross race effect for centuries (eye witnesses are notoriously unreliable and “all Blacks look similar“). If these systems are going to be valuable to all of society, they need to be equally valuable to all of society. 

One last thing. If you are saying something that you feel the need to prepend with “I’m not racist”, there are pretty good odds that there is, at its heart, something that may actually be racist. *This is not to call you a racist*, but, rather, to call certain ideas to be inherently race-biased (*racist* is a word that has, unfortunately, been turned into a binary ad hominem insult, provoking immediate negative visceral reactions, which makes talking about *racist ideas* difficult). The notion that Blacks and Asians cannot have computer systems treat them equally to Whites is a racist idea, and, if the systems can’t solve that, the systems should not leave the lab. Unfortunately, it seems to be in security and policing where these systems are seeing the most use, and it absolutely is racist if those systems don’t treat Blacks and Whites equally *because* Blacks and people of Color have historically been treated much worse.",1
post20lb,poorly branching,0.6756241153332649,lowest,">There is   
>  
>more human genetic diversity in Africa  
>  
> than in the rest of the world combined. That does not mean they have more facial diversity, but it’s a strong clue that, if you think they don’t, it may be you that is wrong.

True, but if the OP was speaking about American context, it would not be relevant. Most African-Americans are descended from Western Bantu family. There are hardly any Khoisan, Pygmy etc. people in America, so an ""American"" algorithm would probably mostly deal with rather reduced subset of the entire, very diverse African population.",2
post47lb,poorly branching,0.676282748477937,lowest,"## Welcome to the r/ArtificialIntelligence gateway
### News Posting Guidelines

---

Please use the following guidelines in current and future posts:

* Post must be greater than 100 characters - the more detail, the better.
* Use a direct link to the news article, blog, etc
* Provide details regarding your connection with the blog / news source
* Include a description about what the news/article is about. It will drive more people to your blog
* Note that AI generated news content is all over the place. If you want to stand out, you need to engage the audience

###### Thanks - please let mods know if you have any questions / comments / etc

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ArtificialInteligence) if you have any questions or concerns.*",1
post47lb,poorly branching,0.676282748477937,lowest,"I wouldn't read anything into reports like this or any predictions -- also that report is far too general

Python is worth knowing as most data tools need that now

Outside of that I recommend people learn some application of AI ... creating a chatbot, how to fine tune, how to RAG, how to run a model locally, how to work with APIs available (especially voice), how to eval, how to cost based on tokens etc.

AI is not the end of the world for software engineering ... it's just the start of something else in some areas ... all the doom talk is a complete over-reaction",1
post47lb,poorly branching,0.676282748477937,lowest,"it is never an absolute. shareholders dont need AI to replace engineers. what they need the AI to do is to improve everybody's productivity beyond the increase in overall demand.

if every engineer equipped with AI is now 80% more productive, while the same company only get 20% more demand, this means the same company dont need to employ the same amount of engineers. instead of employing 20 engineers, it can function with just, lets say, 12 engineers. so it can fire 8 engineers, and put pressure to the remaining 12 so they wont have leverage to get pay increase (if they dont want it, the company can ask the remaining 8 engineers to overtake the job).

and looking at software engineering job openings and median pay in the past 5 years, this is exactly whats happening. there are less and less openings for junior role, and pay for senior role dont really increase either.",2
post47lb,poorly branching,0.676282748477937,lowest,I’m creating a soft layer as we speak. What a time to be alive.,2
post47lb,poorly branching,0.676282748477937,lowest,"Refusing to aknowledge the risks of AI is a before sign of an impending doom. The problem here that you dont understand is mass unemployment. This is scary since what are hordes of unemployed men going to do? Wars, social unrest, anarchy, rising criminality and so on.",2
post47lb,poorly branching,0.676282748477937,lowest,"I used to be the big advocate of that point -- but I get less and less convinced now

It's just going to be like the invention of steam engines and the industrial revolution -- there will be change for sure but it's a start not an end",3
post47lb,poorly branching,0.676282748477937,lowest,Actually is not the same with AI. Its not like engines replacing horses. Its like ‘genie in a bottle’ invention that does not create jobs. Its self sufficient for everything. It will not displace a person from a job and then give him another opportunity somewhere else (like it happened with any invention during the history). Thats why ASI its called the Last Human Invention.,4
post47lb,poorly branching,0.676282748477937,lowest,"Take, as an example, an organization that builds software products and they're looking to keep up with the AI space.   Does that company hire AI/Data Scientists or are Software Engineers that know how to use AI tooling enough?   I'm starting to see that, unless the company is looking to deploy the next greatest LLM or nextgen AI algorithms, they don't need to hire Python & Data Science experts but rather good Software Architects/Engineers that will lean in on leveraging the latest/greatest AI tooling (LLMs, GenerativeAI methods, RAG...etc.) to augment their building of the software products that their company is producing.  Thoughts?",1
post47lb,poorly branching,0.676282748477937,lowest,"most of these AI predictions seems centered around gathering capital. The most absurd one I can think of is Anthropics CEO who is known for making the most outlandish and absurd statements.

Not surprised Eric Schmidt joins the choir.",1
post47lb,poorly branching,0.676282748477937,lowest,"Prompt engineering is no longer related to engineering.
It's related to domain knowledge 
Also, it's not there much in 202t. It was a lot in 2023",1
post47lb,poorly branching,0.676282748477937,lowest,"Not sure where you're getting prompt engineering from the article, it's mostly about software and programming jobs based on AI.",2
post47lb,poorly branching,0.676282748477937,lowest,"Exactly. One year back prompt engineering was something
And today you are telling me 'oh that's not what any engineering is'",3
post47lb,poorly branching,0.676282748477937,lowest,"Did you ever figure out if 5G was safe for human health?

[https://spectrum.ieee.org/will-5g-be-bad-for-our-health](https://spectrum.ieee.org/will-5g-be-bad-for-our-health)

Doesn’t that seem like it should be a high priority with the push for 6G+?

What’s the scoop on terahertz, like asking Josep Jornet to write an article about his research, specifically the IoBNT?",3
post25lb,poorly branching,0.6845590876874087,lowest,"This is pretty close to my PhD topic! Excellent choice for a paper! 


All AI stems from algorithmic models, and there's a long history of bias in these systems. 

Check out work by Helen Nissenbaum, Taina Bucher, Safiya Nobel, Abeba Birhane, Cathy O'Neil, Virginia Eubanks, Jenna Burrell, Marion Fourcade, Joy Buolamwini, Timnit Gebru, Emily Bender, Alex Hanna, and Danielle Citron.


A good place to start is this review article about algorithms and Sociology: https://www.annualreviews.org/doi/abs/10.1146/annurev-soc-090820-020800

Also Bernard Koch wrote a history of AIs development and how it's driven. I can't find it but if you email him he might give it to you. He presented it at ASA this past summer.",1
post25lb,poorly branching,0.6845590876874087,lowest,"Reading Noble's 'algorithms of oppression' might help? It deals with racism and sexism in search engines, which is AI insofar as involves machine learning (perhaps you have a more specific area of AI in mind?). There's also 'weapons of math destruction' for a general study of algorithms/big data and inequality.",1
post25lb,poorly branching,0.6845590876874087,lowest,I’m doing something somewhat related. I found this theory called Computers As Social Actors (CASA) and it’s yielded some interesting results. Hope that helps.,1
post25lb,poorly branching,0.6845590876874087,lowest,"Maybe this might help? Learned this is a soc sexuality class: AI chatbots in dating websites cater or change to fit the people speaking with them. For some people this is their first experience in dating and they believe it is a real person. When they date in the real world they’re confused and possibly angry on why their partner doesn’t change to fit them like the online AI partner did. This can lead to sexual violence in some cases.

There’s gotta be a paper on it or something if it was taught in my class I think?",1
post25lb,poorly branching,0.6845590876874087,lowest,Yea this is really helpful actually as I’m including a section with deepfake revenge pornography is a free reign to invade women’s privacy and sense of identity,2
post25lb,poorly branching,0.6845590876874087,lowest,I believe people should take the stance “Nothing About Us Without Us”… where professionals and people in a specific race or religion or community educates AI software,1
post25lb,poorly branching,0.6845590876874087,lowest,What a strange project. Were you given this project title or did you choose it?,1
post25lb,poorly branching,0.6845590876874087,lowest,"It's not strange lol, the earliest studies of AI implementation have already found that certain biases are built-in unless specifically accounted for.

For example, while selecting top candidates during a hiring process, Amazon’s automated resume screening system discriminated against women. The data used to train the recruitment model was informed by resume samples from a 10-year period, where women were underrepresented. The resume screening model thus used “linguistic signals” associated with successful male candidates.

Basically because AI draws from the past to find patterns relevant to its programming, it's also drawing all the biases from the past. So if an AI program notices that women are not represented in the past hires, it doesn't understand the context as to why. It just assumes that women must not be good at the job and gives applications with male-sounding names more points automatically. This type of bias extends to things like race, language, etc.",2
post25lb,poorly branching,0.6845590876874087,lowest,"The algorithmic bias literature is ironic because there is this idea that AI models are biased because they're built on unsophisticated evaluation of empirical data, but virtually none of the literature evaluates data bias in a sophisticated way, instead treating it like a social contagion or miasma. Humans draw conclusions from empirical data too, and the miasma model of bias is probably more accurate for human beings' reasoning than for machines'.",3
post25lb,poorly branching,0.6845590876874087,lowest,Why is it strange ? Yes I picked it myself,2
post25lb,poorly branching,0.6845590876874087,lowest,"It is strange because it is so specific, but at the same time you seem to know nothing and no idea how to proceed. 

Usually you either base the project on something you already know something about, or you start with a less specific title and hone in as you learn more.",3
post25lb,poorly branching,0.6845590876874087,lowest,[removed],2
post25lb,poorly branching,0.6845590876874087,lowest,"Don't show up here just to troll and take potshots at the locals, please.",3
post25lb,poorly branching,0.6845590876874087,lowest,"Here's an article that walks through some examples and provides some research, in case it's helpful!

https://theglobalobservatory.org/2023/03/gender-bias-ethical-artificial-intelligence/#:~:text=This%20is%20evident%20in%20many,screening%20system%20discriminated%20against%20women.",1
post25lb,poorly branching,0.6845590876874087,lowest,"You can talk about the history of areas AI is deployed in instead. For example, the erosion of community social ties helped pave the way for algorithms to influence socialization through social media, media content consumed, and online dating.  


It isn't very accurate to claim that women, rather than men, made the first advances in AI. You might be thinking about human computers? Computers are not synonymous with AI.",1
post25lb,poorly branching,0.6845590876874087,lowest,"While it doesn't satisfy all of the specifics of your request, there has been an effort towards a general sociological understanding of Artificial Intelligence, incidentally written by a woman, it's a book called [Artificial Communication: How Algorithms Produce Social Intelligence](https://direct.mit.edu/books/book/5338/Artificial-CommunicationHow-Algorithms-Produce). From what I hear, the author is of a sort of luhmannian persuasion which might come across as kind of arcane but you will be alright.",1
post25lb,poorly branching,0.6845590876874087,lowest,How does this perspective differ from a repackaged cyborgism?,2
post25lb,poorly branching,0.6845590876874087,lowest,"You mean as in Haraway? I have not read her but I do know that Luhmann's framework pressuposes systems that work under an 'operative closure'. This means that information, which Luhmann calls 'communication', does not flow freely between systems but is aprehended by each system and interpreted only with the elements that belong to the system in question. In the case of the subject, which he names the 'psychic system' it is closed to the 'social system', which permits Luhmann to focus mostly on the social system and avoid getting too involved in discussions of 'anthropological principles'. Therefore there can be no talk of the subject 'melting' into the machine or becoming engulfed in artificial intelligence or whatever. Hence why Esposito refers to it as 'artificial communication', it is something that exists mostly 'out there', in the social system.",3
post25lb,poorly branching,0.6845590876874087,lowest,"The book My Fair Ladies: Female Robots, Androids, and Other Artificial Eves by Julie Wosk will likely be relevant to this topic.",1
post54lb,poorly branching,0.7106025146589168,lowest,"Non AI-surveillance is more harmful to privacy and more prone to fraud and abuse.

Imagine basic surveillance is encrypted at client, transit and storage, and only becomes readable to humans whenever the risk level is (accurately) estimated above ""...%"".

At that point ""human in the loop"" would need to take over to make the judgement. 

In the ideal world, the ""looped human"" will only perceive risks, and not the total scope.",1
post54lb,poorly branching,0.7106025146589168,lowest,"That is actually what I am trying to build. A system that automatically dectects and tracks crime at the moment it occurs, avoiding to store anynon related data.

It has been a bit challenging.",2
post54lb,poorly branching,0.7106025146589168,lowest,"Sounds good! I applaud your courage.

This might help you for initial PoC:

Use 2 SBC/camera systems, 1 regular and 1 ""ai vision"".

The regular camera system will be an ""on call""-cam. It needs a endpoint/websocket to activate the stream on a specific client (your web-ui or local app ui).

Then try to get some standard data for the ai vision -system. Make it detect an apple, and when it does so, task the ""on call""-cam to do the websocket thing to your client. So every time the apple is displayed, the stream will be opened in your client.

Finetune to a usable experience (what about multiple streams, closing it, applying follow-ups and some enterprise like users-system with rights/roles -structures.  
  
Then get it to work for non-apples. This will mean that you need to get loads of data. From my experience/knowledge (which is limited) it'll be way better to train a specific model per specific crime, and then apply the models at the same time (checking for their specific crime) on the same input-stream.

if you come up with something that works, this would be the point where it would be wise to seek funding; I don't think a single engineer with limited time and budget will be able to achieve a production-grade system (within this niche) **and** is actually able to sell it.",3
post54lb,poorly branching,0.7106025146589168,lowest,"Hey thanks a lot for the advice I am actually building an open source community. 

Here is the web page: https://www.opear.org

This is the initial model for crime detection
https://huggingface.co/OPear/videomae-large-finetuned-UCF-Crime",4
post54lb,poorly branching,0.7106025146589168,lowest,"The key to balancing AI surveillance and privacy lies in implementing robust safeguards. Prioritize encryption and decentralized data storage to keep sensitive information secure. Opt for user-controlled data retention and local processing of data by AI models for enhanced privacy protection. Open-source algorithms foster transparency and auditability. While challenges exist, designing AI surveillance systems with these features can help align technology with privacy concerns. Let's keep the conversation going on how to strike the right balance!",2
post54lb,poorly branching,0.7106025146589168,lowest,"Hey georgy I laid out a roadmap and a system design here https://www.opear.org/developers

Do you agree with it, what would you add or take",3
post54lb,poorly branching,0.7106025146589168,lowest,Pretty cool 👌,4
post5lb,poorly branching,0.7497198447814801,lowest,"As a reminder, this subreddit [is for civil discussion.](/r/politics/wiki/index#wiki_be_civil)

In general, be courteous to others. Debate/discuss/argue the merits of ideas, don't attack people. Personal insults, shill or troll accusations, hate speech, **any** advocating or wishing death/physical harm, and other rule violations can result in a permanent ban. 

If you see comments in violation of our rules, please report them.

***


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/politics) if you have any questions or concerns.*",1
post5lb,poorly branching,0.7497198447814801,lowest,Wasn't this was already covered in an episode of Better Off Ted 10 years ago?,1
post5lb,poorly branching,0.7497198447814801,lowest,"And an episode of community, doesnt mean they fixed it yet.",2
post5lb,poorly branching,0.7497198447814801,lowest,"Oh lord, we now have racist computers, help us, Jesus.",1
post5lb,poorly branching,0.7497198447814801,lowest,"Computers can only do what humans tell them to do--and the people writing programs are overwhelmingly not black. It's hardly surprising. All those ""useless liberal arts, gender/racial studies"" people called this as a problem before facial recognition was even a working technology.",2
post5lb,poorly branching,0.7497198447814801,lowest,How would one program their bias into a neural network?,3
post5lb,poorly branching,0.7497198447814801,lowest,"It's not that the programmers are  consciously biased, but that the data used are already biased. 

An old non AI example would be the development of color film. Film doesn't just naturally make white people look normal and black people dark and featureless. Its manufacture is tweaked that way, and until very recently, the color keys developers used featured only white people. This system was designed with the data people had, namely that they expected primarily white people to use cameras, which is reasonable considering race distribution in the U. S., but it winds up being racist. 

The book **Weapons of Math Destruction** explains how algorithms become infused with biases.

Here's a bit from [an article](https://towardsdatascience.com/algorithms-are-racist-now-what-53fc130bb203) on the topic. 


> We see this in predictive policing algorithms that utilize biased historical data of prior arrests (which are skewed to poorer communities due to higher level of nuisance crimes), to determine where future crimes will take place and in determine where officers should be sent to patrol more frequently. This leads to more policing in poorer areas and a self fulfilling reinforcing loop.

> The more policing in a community, the more arrests for nuisance crimes. The more arrests for nuisance crimes, the more dots populated on a crime map. The more dots populated on a crime map, the more reason for policing. And the dangerous feedback loop goes on.

> It’s easy to believe that more data is better data. But biased data going in, means biased data coming out. In other words, “garbage in garbage out.”
At the end of the day we are the ones inputting this data into algorithms. We create them, we keep them secretive and the understanding of how they work away from the people most affected by them. In a recent PBS interview, Joi Ito, Director of MIT Media Lab, attested to the limitations of this technology given the fact that flawed humans are choosing data inputs.
“AI isn’t magically going to make us wise,” said Ito. “Having these conversations about race before locking in these algorithms is really more important than all these mathematical things.”",4
post5lb,poorly branching,0.7497198447814801,lowest,"Let me simplify this... It is not the programmers but the history that causes this...

Let's say want to train my app to look for criminals. So I tell the system to look at a standard sample of the faces of people in the prison population. 

Then I point the camera system at the general population of non/maybe criminals. The system will have a bias of seeing minorities as criminals because of the over representation in the sample set as compared to the general population. 

This is why there is always an inherent issue with designing algorithms vis neural networks from sample data, unless you are constantly revising the algorithms to match actual outcomes.",4
