fract_dimension_type,keyword,score
highest,ai_can_predict_peoples_race_from_xray_images_and,0.6717
highest,ai_can_predict_peoples_race_from_xray_images_and i9q2rwj,0.6103
highest,ai_can_predict_peoples_race_from_xray_images_and i9o4ui0,0.5964
highest,uvxpli ai_can_predict_peoples_race_from_xray_images_and,0.5677
highest,ai racists,0.5533
highest,racists ai,0.5477
highest,racist ais,0.5473
highest,ai racial,0.5426
highest,ai racist,0.5322
highest,ai discriminate,0.5279
highest,racist ai,0.5257
highest,ai racism,0.5167
highest,diagnosis racial,0.5146
highest,ai identifies,0.5119
highest,mexicans ai,0.5118
highest,diagnosing racial,0.5108
highest,minorities ai,0.5086
highest,ais racist,0.5076
highest,diagnoses racist,0.5066
highest,diagnosis racially,0.5005
highest,discriminating race,0.498
highest,ai misdiagnosed,0.4943
highest,ai misdiagnosing,0.494
highest,ais racial,0.4924
highest,racism ai,0.4909
highest,diagnosis races,0.4907
highest,investigating racial,0.4898
highest,xrays ai,0.4854
highest,discern racial,0.483
highest,racially discriminating,0.4819
lowest,hiring gender,0.6221
lowest,stem gender,0.5999
lowest,gender bias,0.5984
lowest,multilingual bias,0.5767
lowest,gender gaps,0.537
lowest,gender gap,0.5013
lowest,gender roles,0.496
lowest,evaluate gender,0.4898
lowest,gender racial,0.4766
lowest,male hiring,0.4747
lowest,bias literature,0.4738
lowest,women stem,0.4683
lowest,women hired,0.4565
lowest,hiring men,0.4507
lowest,race gender,0.4462
lowest,sexism search,0.4374
lowest,potentially bias,0.4257
lowest,multilingual evaluations,0.4207
lowest,discriminated women,0.4207
lowest,bias probing,0.4194
lowest,hiring woman,0.4183
lowest,women minorities,0.4182
lowest,stem jobs,0.4161
lowest,racial bias,0.4133
lowest,discriminating hiring,0.4118
lowest,hiring challenges,0.3971
lowest,bias unfortunately,0.3912
lowest,women data,0.3898
lowest,racism sexism,0.3893
lowest,position women,0.3877
