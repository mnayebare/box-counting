{
    "post_title": "AI researchers improve method for removing gender bias in machines built to understand and respond to text or voice data",
    "post_timestamp": "2022-09-08 23:14:43",
    "last_comment_timestamp": "2022-09-13 06:03:20",
    "time_difference": "4 days, 6:48:37",
    "comments": [
        {
            "author": "FuturologyBot",
            "body": "The following submission statement was provided by /u/WallStreetDoesntBet:\n\n---\n\nAs it becomes refined, the methodology could offer a flexible framework other researchers could apply to their own word embeddings. \n\nWhile a computer itself is an unbiased machine, much of the data and programming that flows through computers is generated by humans. \n\nThis can be a problem when conscious or unconscious human biases end up being reflected in the text samples AI models use to analyze and \"understand\" language.\n\n---\n\n Please reply to OP's comment here: https://old.reddit.com/r/Futurology/comments/x9k8p4/ai_researchers_improve_method_for_removing_gender/inoihd4/",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-09-08 23:48:37",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "From the research paper, the old strategy was to train the AI and then brainwash it afterwards. Now it's possible to brainwash the AI during the training phase.",
            "score": 11,
            "depth": 0,
            "timestamp": "2022-09-09 00:36:55",
            "replies": [
                {
                    "author": "Deleted",
                    "body": "What do you mean by brainwashing?",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-09-10 10:21:00",
                    "replies": [
                        {
                            "author": "Deleted",
                            "body": "They're finding new and creative ways to correct the AI's \"perceptions\" until it matches our collective prejudices.",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2022-09-10 13:35:55",
                            "replies": [
                                {
                                    "author": "Deleted",
                                    "body": "\"For example, when considering a word like 'nurse,' researchers want the system to remove any gender information associated with that term while still retaining information that links it with related words such as doctor, hospital and medicine.\" (Citation)\n\n&#x200B;\n\nI see. We think about a woman, with the word nurse. But the system has not to know this.",
                                    "score": 2,
                                    "depth": 3,
                                    "timestamp": "2022-09-10 13:53:26",
                                    "replies": []
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "doubleistyle",
            "body": "Future AI's gonna be like: \"believe all women!\" \"Down with the patriarchy!\" \"Women don't lie!\" \"There is no physical strength differences between men and women!\"",
            "score": 19,
            "depth": 0,
            "timestamp": "2022-09-09 06:06:35",
            "replies": [
                {
                    "author": "Unpleasantend",
                    "body": "I know you joke, but this stuff genuinely concerns me. The hope that we may one day build AI that does a better job of managing us and understanding us well enough to help build some kind of utopian society for us is definitely going to be dashed if we force it accept modern ideals not based on reality.\n\nAs unpalatable as it might be to \"woke\" types the majority of stereotypes and biases exist for a reason, and are often heavily influenced by fairly unchangeable biology. Trying to force some egalitarian fantasy onto that isn't actually going to help, accepting reality and trying to find the best solutions for everyone's well-being seems more plausible.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-09-13 06:03:20",
                    "replies": []
                },
                {
                    "author": "whybepurple",
                    "body": "Well you would know wouldn't you, bot? :P",
                    "score": -9,
                    "depth": 1,
                    "timestamp": "2022-09-09 07:19:09",
                    "replies": [
                        {
                            "author": "doubleistyle",
                            "body": "\"I don't like this comment, therefore it must have been made by a bot\"",
                            "score": 10,
                            "depth": 2,
                            "timestamp": "2022-09-09 08:23:58",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "dberis",
            "body": "The real question here is what is wrong with gender bias? Is bias inherently negative? Why?",
            "score": 11,
            "depth": 0,
            "timestamp": "2022-09-09 02:47:02",
            "replies": [
                {
                    "author": "federico_alastair",
                    "body": "Not inherently no. And in a research environment, bias can give info about human thinking and ai learning  patterns .\n\nBut AIs arent just in research. They are more and more implemented in everyday human interaction. Since these AIs are trained majorly in informal internet conversations, they have a tendency to get quite bigoted. And no just word filters won't work since like humans, ais will find ways around it.\n\nImagine you're a woman and ask Alexa something remotely masculine and she replies you with \" Bitch, go play with candles or something\" . And vice versa for men. We already get targeted ads based on our sex. This will be the next level.\n\n\nRemember that AI that immediately turned Nazi after few hours on the internet. Yes we need to protect AIs from humans.",
                    "score": 2,
                    "depth": 1,
                    "timestamp": "2022-09-09 04:26:43",
                    "replies": []
                },
                {
                    "author": "Venaliator",
                    "body": "We must all be the same with no discernible traits. It makes it easier for the rulers of our world.",
                    "score": 2,
                    "depth": 1,
                    "timestamp": "2022-09-10 04:07:28",
                    "replies": []
                },
                {
                    "author": "Deleted",
                    "body": "[removed]",
                    "score": -2,
                    "depth": 1,
                    "timestamp": "2022-09-09 04:38:29",
                    "replies": [
                        {
                            "author": "MightyDickTwist",
                            "body": ">So in the real world rife with these biases, it's important to remove the biases if you want your model to be accurate. Of course it depends on the goal of the model but biases are generally undesirable. \n\nI do not know what happened to this comment section, but you are correct. And you're being downvoted for it.\n\nBiases don't just show up in gender bias/racial bias. Should we just ignore those too?\n\nSay, just allow a facial recognition app to not detect a face just because the subject is wearing a hat (which is a rare thing in comparison)? Allow a system to learn how to differentiate a Wolf from a Husky based on how, generally speaking, wolves appear with snowy backgrounds?\n\nThese are all real problems there were only fixed once we figured out how to deal with bias.\n\nSeriously, a wolf/husky classifier that classifies any picture with a white background as a wolf is not useful! We can't simply \"let things be\", it'd fucking ruin our systems.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-09-09 18:21:00",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "LSeww",
            "body": ">With wide applications of NLP systems to real life, biased word embeddings have the potential to aggravate and possi- bly cause serious social problems. For example, translating \u2018He is a nurse\u2019 to Hungarian and back to English results in  \n\u2018She is a nurse\u2019\n\nOh no the horror! I remember this method can give you such a gibberish, that you could only laugh at it.",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-09-09 11:14:47",
            "replies": []
        },
        {
            "author": "gaslightranch",
            "body": "God these comments give me some glimmer of hope for this site. I usually avoid 99.9% of this Leftist cancerhole like a plague but I can'st stay away from this sub. Thank God not everyone is a brainwashed intersectionalist.",
            "score": 3,
            "depth": 0,
            "timestamp": "2022-09-10 10:47:37",
            "replies": []
        },
        {
            "author": "WallStreetDoesntBet",
            "body": "As it becomes refined, the methodology could offer a flexible framework other researchers could apply to their own word embeddings. \n\nWhile a computer itself is an unbiased machine, much of the data and programming that flows through computers is generated by humans. \n\nThis can be a problem when conscious or unconscious human biases end up being reflected in the text samples AI models use to analyze and \"understand\" language.",
            "score": 0,
            "depth": 0,
            "timestamp": "2022-09-08 23:15:12",
            "replies": []
        }
    ]
}