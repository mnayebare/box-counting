{
    "post_title": "Why are facial recognition algorithms \"racist\"?",
    "post_timestamp": "2020-08-27 15:18:22",
    "last_comment_timestamp": "2020-09-05 09:33:24",
    "time_difference": "8 days, 18:15:02",
    "comments": [
        {
            "author": "YaztromoX",
            "body": "There is a lot to unpack here, so please bear with me.\n\nA computer has no natural tendencies towards anything other than processing bits of information.  The computer itself isn't racist or non-racist.  It's merely a vessel for whatever the programmer makes it do.\n\nAlgorithms _can_ be racist, if they make different decisions based on race.  If, for example, your bank uses an algorithm to decide who can get a loan and who doesn't, and that algorithm takes \"race\" as input and generates a different output based on that data, then the algorithm is biased, and will produce biased results.\n\nOne example of this is with [criminal risk assessment algorithms, which significantly disadvantage Black people](https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/).\n\nYou of course asked about facial recognition.  Designing a facial recognition algorithm by hand is complex and error prone, so typically^0 we use machine learning to allow the computer to create the necessary algorithm.  In such a scenario, the developer codes the starting conditions for a blank network, and then feed a large number of images into the network to \"train\" it in how to recognize a face.  Further code may then be added to pick out specific features of the face, and creating some sort of identification code for the face itself to match up with databases of known faces.\n\nThere are a few areas here where racial bias can creep in.  Firstly, you can run into the situation where the training set contains insufficient images of specific races, allowing the algorithm to form a bias against them.  For all their power, computers aren't very smart, and even the best Convolutional Neural Network doesn't generalize like we can -- so if you feed the network a series of blond haired, blue eyed, white faces, it's not going to be able to recognize faces outside those contexts.\n\nAnd this has in fact been the case.  A famous instance of this occurred back in 2015, when it was found that [Google was identifying black men as gorillas](https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/).  A large part of this appears to be because Google's training algorithms were given many, many more white faces than black faces.\n\n\"Recognizing that something is a face\" in a photo is somewhat different from \"identifying _someone_ in a photo\".  We've seen case of failure in the first; what about the second?\n\nTesting has revealed [bias and inequalities in common facial recognition and identification systems](https://www.cbc.ca/news/technology/facial-recognition-race-1.5403899)^1.  The bias in each system differs, and the report note that while many systems have higher false positive rates for East Asian faces, it's the opposite for systems developed in China, where the false positive rate is _lower_ for East Asian faces.\n\nI don't have sufficient knowledge to comment on the idea you espouse that non-white faces are \"much less varied than white faces\"^2, but even if we thought this were true, the evidence that algorithms developed in different parts of the world fare better or worse on certain types of faces seems to indicate that this shouldn't be a problem for a facial recognition algorithm.  The issue lies in one (or both) of the following two areas of development:\n\n1. **Training**:  the training data exhibits a bias with the types and number of photos available of different ethnicities and demographics, and hence produces biased results, and/or\n2. **Validation**: the developer of the algorithm is predominately verifying their trained algorithm with certain ethnicities, and not enough of others to ensure that the false-positive rate is compatible.\n\nWith #1, you introduce bias via a lack of sufficient data.  This may not be intentional -- it may be that the training sets available to the developer simply have this bias built-in (for example, if you are developing in a predominately East Asian country like China, you may have access to a full database of all Chinese drivers license or passport photos to train with, which are predominately East Asiatic faces).  In #2, however, more of the developers bias comes out -- if (say) a software developer feeds their neural network a somewhat varied supply of photos from a variety of demographics, but then only validates that training with photos that are (for example) predominately white, you'll never know if the training was successful for other races.\n\nAnd in the race to market for many companies, corners like this get cut, either intentionally or unintentionally.  Ideally, every company would have a wide range of suitable photos from different races to validate their algorithms against, come up with an accuracy score for each by race/gender or other identifiable detail, and not ship until they ensure that the scores for each group fall within the same bounds.  But that, as we see from the testing, isn't happening.\n\nThat brings us to another issue:  how and why is this a problem?\n\nFirstly, I'd like to note that I don't think there is anything wrong with developing a biased algorithm.  Iteration is an important part of most large computing projects, and developers of such algorithms should be validating their algorithms against a wide set of possible data, and ensuring that everyone falls within certain narrow bounds for error.  If they don't, then they should iterate and improve their algorithms until they do.\n\nSee, there's nothing wrong with a biased algorithm -- so long as it doesn't leave the lab, and people either work on improving it or discard it for something that exhibits less (or potentially no) bias.  So long as we acknowledge this as a risk factor, and then take steps to measure and mitigate it, the fact that an early version of an algorithm held a bias shouldn't be something to be ashamed of.\n\nThe _problem_ is when biased algorithms like this are put into production, and are used by police, military, governments, or other organizations to disadvantage one group of people compared to another.  That is, it's when the algorithmic bias is used in the real-world to be biased against living people.  That's where the problems occur -- and people are right to be angry when an unfeeling computer that can't be reasoned with mis-identifies and disadvantages them.  It's the people using these algorithms who need to be the ones to say \"I don't trust the answers from this system, because it exhibits bias\".  Unfortunately, what we often instead see in this world is a \"machine is never wrong\" attitude^3, which disadvantages certain people, but where the authorities that act on the algorithmic findings simply don't care if they're getting biased results in the first place.\n\nIs it possible that there are going to be demographics of people that have faces more difficult to analyze than others?  That's possible^4.  But if that _is_ the case, then _we shouldn't be using these systems to identify people for special treatment_.  That is, if it were to turn out that bias in facial recognition algorithms is _impossible_ to get within certain acceptable bounds, _then we shouldn't be using those systems, **full stop**_.\n\nIn summation, there are lots of ways for bias to creep into an algorithm, many of which may be unintentional.  However, this bias should be measured for various groups, should be published so everyone is aware of the bias, and in cases where the bias is significant _should not be used in ways that disadvantage the people for whom the algorithm is biased against_.\n\n-----\n^0 -- AFAIK, you can read this as \"in every case I know of\"  \n^1 -- [\"Face Recognition Vendor Test, Part 3: Demographics Effects\", NIST, 2019](https://nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8280.pdf).  \n^2 -- I don't believe this to be true, but it's not my area of study or expertise.  I'm a computer scientist, not an ethnographer.  \n^3 -- Or just as bad: \"the machine is often wrong in specific cases, but we're going to disadvantage those people anyway and _maybe_ apologize later after we've given them unnecessary grief, on the off chance the system is right for once\" attitude.  This is really just another way of allowing people to use the computer as an excuse to support their own biases as valid.  \n^4 -- Again, see ^2:  not an ethnographer!\n\nEDIT: typos",
            "score": 72,
            "depth": 0,
            "timestamp": "2020-08-27 18:35:43",
            "replies": [
                {
                    "author": "Astromike23",
                    "body": "> I don't have sufficient knowledge to comment on the idea you espouse that non-white faces are \"much less varied than white faces\"\n\nI'm fairly sure OP just stumbled into the [Cross-Race Effect](https://en.wikipedia.org/wiki/Cross-race_effect) - most people are far better at recognizing faces from their own race, and tend to think other races \"all look the same\".",
                    "score": 34,
                    "depth": 1,
                    "timestamp": "2020-08-27 20:17:20",
                    "replies": [
                        {
                            "author": "YaztromoX",
                            "body": "I think they're also falling into an easy fallacy presuming that \"computer vision\" and \"human vision\" are one and the same, [which I have attempted to debunk below](https://www.reddit.com/r/askscience/comments/ihrhyh/why_are_facial_recognition_algorithms_racist/g35xn2x?utm_source=share&utm_medium=web2x&context=3).",
                            "score": 11,
                            "depth": 2,
                            "timestamp": "2020-08-28 15:16:57",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "Spreelicious",
                    "body": "Thank you for this detailed and interesting explanation!",
                    "score": 3,
                    "depth": 1,
                    "timestamp": "2020-08-28 04:08:56",
                    "replies": []
                },
                {
                    "author": "LunaLucia2",
                    "body": "What about the actual data that comes into the computer though? The information is captured by a camera and converted to an image, but dark faces can give a lot less contrast, especially under low light conditions. Dark skin can also hide underlying features like moles, freckles and blush and blend in with hair more easily, which is also worsened under low light conditions and with low quality cameras. How do these effects come in when you'd consider an otherwise ideally unbiased detection algorithm?",
                    "score": 2,
                    "depth": 1,
                    "timestamp": "2020-08-28 05:35:51",
                    "replies": [
                        {
                            "author": "YaztromoX",
                            "body": "> The information is captured by a camera and converted to an image, but dark faces can give a lot less contrast, especially under low light conditions.\n\nI wanted to clear up some invalid assumptions that seem to be creeping into the conversation, that will help show why this line of reasoning isn't particularly valid.\n\nFirst off is the unspoken assumption that cameras \"see\" the same way we do.  This is not true.  [Here is the spectral breakdown for panchromatic film](http://www.geo-informatie.nl/courses/grs20306/lectures/05aerialphotography/05aerialphotography08.gif).  Note that the yellow line (panchromatic film)^0 has spectral sensitivity below 0.4\u00b5m, and above 0.7\u00b5m^1.  These correspond to the Near Ultra Violet and Near Infra Red portions of the spectrum, both of which are outside the normal human range of vision.  Because of this, computer-processed digital images may be able to discern features that show up in NUV and NIR that aren't discernible to normal human vision.  Hence, assumptions about what features are not visible to a normal human don't necessarily apply to computer images.\n\nSimilarly, computers can detect very small differences in colouration more readily than humans can.  Couple this with the above, and it is not a given that computers can't pick out features in photos that humans can't.\n\nSecondly, it feels like several commenters want to focus on thinks like birthmarks, moles, freckles, or other such skin markings as the basis of identification.  These _may_ come into play in some algorithms, however these algorithms tend to rely significantly more on facial [_geometry_](https://miro.medium.com/max/1234/1*C8UucvbO_DmoJlETCS7K3w.jpeg) rather than blemishes of the skin.  They tend to focus more on the ratios of size of the mouth, nose, distance between the eyes, foreheads, etc.  The reason for this is that these things are extremely hard for someone to change -- you would need some radical surgery to change your inter-pupillary distance (IPD), for example.  If recognition systems focused on moles, freckles, or hair as many people here have hypothesized, then you'd be able to completely throw them off by putting a small black sticker on your chin, or by having a breakout of acne, or by wearing a bit of makeup, or by getting a haircut.\n\nSo the focus on \"darker faces hide hair and marks on skin\" is not valid.  Photographs can record more details than humans can see.  Computers have significantly more power to pull out small differences in colouration than humans do.  And facial recognition algorithms likely^2 take this into account to reduce the number of false positives.  It's better to focus on features that are more difficult to change, such as the gross facial dimensions and features, rather than skin imperfections which can easily change for an individual, and which can be trivially faked or masked.\n\n-----\n^0 -- [Here is an example of spectral sensitivity for true colour film](http://www.geo-informatie.nl/courses/grs20306/lectures/05aerialphotography/05aerialphotography13.gif), which is not as wide as that of panchromatic, but still dips into NUV range.  \n^1 -- Film and digital photos used for photo ID (such as those used in passports and drivers licenses) may _purposefully_ use wider-gamut film/sensors than those presented here, specifically to extract more processable details than humans can otherwise obtain from human vision, specifically to give more detail for computer recognition systems to work with.  Likewise, cameras used to pick out faces may use a wider gamut than human vision.  Point being, don't assume \"what you see\" is automatically the same as \"what the camera\" (and hence computer) \"sees\".  \n^2 -- I don't like using a lot of weasel-words like this, however virtually all facial recognition algorithms in active use are commercially developed, and if their specific details are available, they're not available to _me_, so I can only talk in generalities.  Sorry :P.",
                            "score": 16,
                            "depth": 2,
                            "timestamp": "2020-08-28 15:09:08",
                            "replies": [
                                {
                                    "author": "LunaLucia2",
                                    "body": "So, if special cameras with different colour filters are used, does that also mean that wearing for example sunscreen, which filters out features that are very apparent at non-visible wavelengths, could interfere with facial recognition?\n\nAlso, how does this apply to non-ideal conditions, like low lighting, further away and awkward angles like on the street? Do the features on dark faces come across as well as light faces or do they have a different \"cutoff point\" at which the conditions are so bad that a camera can't capture a properly processable image?",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2020-08-28 17:29:24",
                                    "replies": [
                                        {
                                            "author": "YaztromoX",
                                            "body": "It's worth noting that any facial recognition system designed to _identify_ someone has two significant phases:\n\n1. Detect the face, and\n2. Match the face against a database of known faces.\n\nYou can effectively throw off the entire process simply by ensuring your face isn't detected as a face in the first place, or by changing your face so it isn't matchable against any known photos of you.\n\n[It is possible to use makeup and other costume accessories to trick facial recognition systems](https://dl.acm.org/doi/pdf/10.1145/3038924).\n\nThere are in fact [makeup tutorials online designed to foil specific types of facial recognition algorithms](https://cvdazzle.com).\n\nAnd you may find it a bit instructive to see how OpenCV, a Computer Vision library, [detects faces](https://vimeo.com/12774628).\n\nTricking facial recognition systems is an active area of research, with the intent being to hopefully improve facial recognition systems further (usually in reference to #1 above -- detecting that something is a face.  Even humans can be tricked into not recognizing someone with suitable makeup and prosthetics, and I suspect computers will likely never overcome this hurdle either).",
                                            "score": 2,
                                            "depth": 4,
                                            "timestamp": "2020-08-28 20:52:39",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "Deleted",
                            "body": "[removed]",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2020-08-28 07:57:19",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "SoftwareMaven",
            "body": "> they are much less varied than white faces\n\nThere is [more human genetic diversity in Africa](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2953791/) than in the rest of the world combined. That does not mean they have more facial diversity, but it\u2019s a strong clue that, if you think they don\u2019t, it may be you that is wrong. \n\nBut, even if you are right there, we need to ask, \u201cshould people who have been historically disadvantaged continue to be disadvantaged by computer systems in the modern day\u201d. Blacks have been affected negatively by things like police lineups and the cross race effect for centuries (eye witnesses are notoriously unreliable and \u201call Blacks look similar\u201c). If these systems are going to be valuable to all of society, they need to be equally valuable to all of society. \n\nOne last thing. If you are saying something that you feel the need to prepend with \u201cI\u2019m not racist\u201d, there are pretty good odds that there is, at its heart, something that may actually be racist. *This is not to call you a racist*, but, rather, to call certain ideas to be inherently race-biased (*racist* is a word that has, unfortunately, been turned into a binary ad hominem insult, provoking immediate negative visceral reactions, which makes talking about *racist ideas* difficult). The notion that Blacks and Asians cannot have computer systems treat them equally to Whites is a racist idea, and, if the systems can\u2019t solve that, the systems should not leave the lab. Unfortunately, it seems to be in security and policing where these systems are seeing the most use, and it absolutely is racist if those systems don\u2019t treat Blacks and Whites equally *because* Blacks and people of Color have historically been treated much worse.",
            "score": 3,
            "depth": 0,
            "timestamp": "2020-08-31 11:03:50",
            "replies": [
                {
                    "author": "DefenestrationPraha",
                    "body": ">There is   \n>  \n>more human genetic diversity in Africa  \n>  \n> than in the rest of the world combined. That does not mean they have more facial diversity, but it\u2019s a strong clue that, if you think they don\u2019t, it may be you that is wrong.\n\nTrue, but if the OP was speaking about American context, it would not be relevant. Most African-Americans are descended from Western Bantu family. There are hardly any Khoisan, Pygmy etc. people in America, so an \"American\" algorithm would probably mostly deal with rather reduced subset of the entire, very diverse African population.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2020-09-05 09:33:24",
                    "replies": []
                }
            ]
        }
    ]
}