post_id,conversation_type,total_comments,text,good_word_count,bad_word_count,semantic_good_score,semantic_bad_score
post10hb,richly branching,11,We do but we also need to give women a break from fixing things all the time. It's not our fault we were excluded to begin with and not our responsibility to fix.,0,0,0.143,0.182
post10hb,richly branching,11,"Yes, thank you. I’ve decided that my ultimate goal in life is to be a shining example of a woman in STEM who is perfectly fine at my job—And that’s the end of it. Women are constantly being pushed to mentor, and lead teams, and sit on boards, and do outreach, ad infinitum.  But you better ALSO be some kind of technical wizard, otherwise men lose respect for you for not doing the real work.   Whereas **nobody gives a shit** if a man just does his basic job everyday and never does any extras like committees, mentoring, etc. Nobody is disappointed that a man is merely competent and not some technical unicorn.   Can I just do the part I like? (Ya know, the STEM?) And leave me alone otherwise?",1,0,0.205,0.176
post10hb,richly branching,11,Yes!!! This. Let me write code and let some dude do the D&I committee and outreach nonsense.,0,1,0.238,0.251
post10hb,richly branching,11,Sadly for juniors this will mean more discrimination and difficulty with mentorship. It feels like a catch-22,0,1,0.259,0.209
post10hb,richly branching,11,"Yep. I went hard on leadership and outreach when I was an undergraduate. I still give back sometimes in limited capacities, but otherwise I’m done. I’m here to do technical work, get paid, and go home. Not save everyone.",0,0,0.246,0.151
post10hb,richly branching,11,"I forget where I heard it first but I've been calling this ""breaking the glass floor"" when you can be celebrated for mediocrity as much as a man",0,0,0.235,0.249
post10hb,richly branching,11,In theory I agree but in reality how else will things get done,0,0,0.308,0.233
post10hb,richly branching,11,They won't. Not our problem.,0,0,0.229,0.232
post10hb,richly branching,11,Incredibly based,0,0,0.443,0.381
post10hb,richly branching,11,With time and being highly compensated.,0,0,0.242,0.204
post10hb,richly branching,11,"""oh wow you overlooked an obvious problem for years because you lack insight and are insufficiently educated in an area you have chosen to devalue? Huh.""",0,0,0.274,0.263
post10lb,poorly branching,5,"reading through that article it sounds like they must be discriminating against hiring men. Which is fine; while the STEM gender gap continues. I'm curious if places like this have plans to remediate the male hiring gender bias they'll have firmly ingrained in their work culture when STEM gender gaps level out, as they're slowly but steadily doing though.",0,0,0.115,0.36
post10lb,poorly branching,5,"I guess you didn't read it that closely:  ""Women still make up only one-third of the global scientific community, with the percentage stagnating over the past decade, according to a 2024 report by UNESCO (United Nations Educational, Scientific and Cultural Organization). In some countries, less than 10 per cent of researchers are women.   They hold just 22 per cent of STEM jobs in G20 countries, and only one in 10 ascend to leadership positions.""  What do you want to bet it's filled with women who couldn't get jobs elsewhere?  Regardless, I love this article. It reminds people that, no, DEI is not reverse discrimination. It can't be because, unlike what people are whining about, that facts say that women and minorities are still underrepresented pretty much everywhere that isn't minimum wage adjacent.",2,0,0.148,0.34
post10lb,poorly branching,5,"I specifically mentioned that woman were still highly underrepresented in STEM fields as the article said, that being said 15 years ago that number was less than 5%, and the fact that it's up to 22% now, while great means that in entry level STEM position women are being hired at over 2:1 ratios compared to men. Which is fine for now, but it's going to lead to a situation in another 10 years where the gender roles are completely reversed and only men will need to be hired in entry level positions, it's turned into a see saw situation. It's currently much easier for women to get stem jobs than men so I believe asking what policies institutes that are primarily hiring woman have to even things out in a few years when it's a woman dominated field is reasonable.",0,0,0.078,0.223
post10lb,poorly branching,5,"And surely, it won't be a problem for other firms to demand men only. After all, there will be plenty of places for women to work amongst themselves. Or isn't it supposed to work like that?",0,0,0.097,0.209
post10lb,poorly branching,5,"You are bad at math, or logic, or both.  Your assumption that women are getting entry level positions at twice the rate as men is based on the idea that *there is no turnover*.   But the numbers tell a *very different story* ([Source](https://www.hrdive.com/news/stem-staffing-shortage-retention-recruitment/693188/)): As HR leaders, we must remember that staffing shortages are not just related to recruitment and hiring challenges— employee churn and turnover play an equally large role. As reported by the Bureau of Labor and Statistics, the national average turnover rate was 47.2% in 2021.  Your whistleblowing on 'women taking over the workforce' isn't just bullshit, it exposed your fundamental biases. Maybe this is why you don't have a relationship. Studies say hard right conservatism is negatively associated with finding and keeping a partner in men.  And for the record, people working on STEM fields consistently have lower unemployment rates and higher employment rates historically, by *wide margins* Maybe you should stop talking like men in these fields are suffering.",0,2,0.091,0.149
post11lb,poorly branching,13,"Everyone is so bright eyed on seeing the potential of AI but nobody has come up with solutions on the people being displaced by AI. This is not a situation where someone simply jumps to a different field, because AI is displacing so many people from different fields simultaneously.   How do we help the people who get displaced by this?",0,0,0.213,0.204
post11lb,poorly branching,13,"You might be interested in Calibrated Basic Income. It's a policy proposal where a UBI is adjusted higher whenever the economy gets more productive / requires less labor.  By calibrating the UBI payout, we can not only prevent inflation, we can maximize econonic benefit to the average person while also maximizing leisure-time.  Today, because our economy lacks a UBI we generally have to rely on wages and jobs in order to fund consumer spending. This has an unfortunate side-effect: we are forced to create jobs / boost employment for the purpose of distributing income---which is not the same thing as creating jobs because those jobs are actually needed by our economy.  In other words: due to the absnece of UBI, macroeconomic policymakers are forced to rely on job-creation policies to support aggregate consumer spending instead. This wastes resources and it wastes human time.  AI or no AI, we very likely could already be enjoying much more production for much less employment; but the lack of a UBI from our monetary system actively prevents this desirable outcome.  For more informaiton, you can read my new working paper on the subject:  [https://www.greshm.org/files/2025-04-01-calibrated-basic-income.pdf](https://www.greshm.org/files/2025-04-01-calibrated-basic-income.pdf)",1,0,0.212,0.105
post11lb,poorly branching,13,Need to solve the need for social mobility. UBI locks people into an UBI socioeconomic class which will be the lowest socioeconomic class that there is,0,0,0.295,0.289
post11lb,poorly branching,13,"UBI does not necessarily improve social mobility, you are correct.  What it improves are consumer outcomes. It also provides the benefit of freeing people from the need to work, i.e. from the need to care about climbing to a higher ""class"" through labor.  If your goal is to make everyone a harder worker, and to provide more earning opportunities, UBI probably doesn't make sense.  That's not the purpose of basic income, however. The purpose of a calibrated UBI is to maximize wealth and leisure for the average person---irrespective of whether or not they are employed.  Quite possibly, the optimal rate of UBI will be much higher than the average wage is today. It's also possible to imagine a world where the vast majority of people live on an ample, continually rising UBI, and only a minority of the population are employed at all.  In that world, the majority of people all live on exactly the same income: the UBI. This UBI has no reason to be limited to a ""bare subsistence"" level; it can be set as high as possible. In this scenario, whoever chooses not to work is simulatneously richer than the average person is today, but also, technically, the poorest people in their society.  They also will get richer anytime the economy itself (all of our machines, infrastructure, etc.) get more efficient and productive.  In this way, a calibrated UBI complicates the concept of ""socioeconomic class"" as we are used to thinking about it. An advantage of UBI policy is that it doesn't require anyone to be upwardly mobile in a \*job market\* in order for their income / purchasing power to increase.  There will always be some people who are richer than others. What UBI does is make the poorest people as rich as possible; and it gives the average person more freedom to refuse paid work if they choose.",1,0,0.167,0.165
post11lb,poorly branching,13,"To be honest, I'm already in one of the lowest social classes. If I had UBI I'd be able to focus on building even more credentials and acquiring more training in different fields based on demand and interest.   Instead I'm stuck working dead end jobs that provide very little in terms of professional development because I need that money to survive. I dont have the time or rescources to really invest in further training in the way I need. UBI may not give me all the financial rescources I need but it would give me the time I need.",0,0,0.297,0.096
post11lb,poorly branching,13,"That's the thing: there are no solutions as of today. Using the US as an example, they seek to develop AI without any sort of contingency plan for the economy. Politicians and corporate leaders believe that by incorporating AI for mundane tasks that could easily be done by automation, people would not have to worry about doing such jobs and assume they can easily find jobs elsewhere. This is what we get when the wealthy decides AI is the answer to deepen their pockets, hoping that people can get employment elsewhere to have income and buy their products  I am not against AI automation, however if it were up to me, I say we are not ready for AI automation just yet. The economy is in shambles, people sitting in positions they are not qualified for, and younger generations cannot get jobs. AI defenders will argue that AI would create new jobs, but what they fail to understand is that getting a career change or new job is not as easy as changing clothes  Not related, but watching this video helped me gain a perspective on what nations should be focusing on before doing anything like putting automation to work: [https://youtu.be/Ufmu1WD2TSk?feature=shared](https://youtu.be/Ufmu1WD2TSk?feature=shared)",1,0,0.161,0.122
post11lb,poorly branching,13,"Kurzgesagt, great choice.  My personal belief is that with AI being forced into all sectors, the economy itself needs a complete overhaul or paradigm shift. Supply and demand becomes shaky when only a very select few out of the vast majority is able to purchase the goods or services. The very concept of a job or role that people do in their respective field needs to be completely rethought as well. It is odd to see that we are still clinging onto the current economics as the norm when we are dealing with a new breakthrough that is upending the labor force. One thing does bug me though - how would AI generate revenue in a sense?",1,0,0.248,0.169
post11lb,poorly branching,13,"I also have that question in my mind and is still unanswered. To me, automation is a method to speed up a labor process which enables distributing the budget towards other areas that need the funding (which should be what’s happening but in most cases  I doubt it), but it would not mean it generates revenue. For a company to generate them, they have to manage to sell products, because as we both know, money comes from the consumer",0,1,0.268,0.127
post11lb,poorly branching,13,"The economy is not ""ready"" for new technology only insofar as our monetary system lacks a UBI.  With a UBI in place, new technology could result in higher spending power alongside *less* employment. Markets could freely unemploy more workers without harming the average consumer; we could all enjoy more purchasing power and more free time.  Without a UBI this desirable outcome is impossible; we end up either forced to create jobs as an excuse to distribute incomes---wasting resources and wasting human time---or we have to fret about people falling into poverty just because somebody invented a new machine.  Instead of trying to delay new technology from getting invented because we're worried about losing jobs, it makes more sense to fix our broken monetary system so people never become arbitrarily poor in the first place.",0,0,0.145,0.107
post11lb,poorly branching,13,"I think it potentially solves a myriad of problems however:  1. AI replaces human labour = Reduce human energy consumption globally, albeit AI will require a lot of energy still total energy comes down.  2. Profits from AI fed back into redistribution more equitably resolving wealth disparities and equalize resource and energy use from using lower less polluting levels.  3. Reduced speed of economies and consumerism is also beneficial both for the planet and for human health ie 4 day weeks and maybe even 3 day with UBI eventually.  4. Reorientation of societies towards more co-operation, more humane concepts of “work”. Etc  Depends on speed and spread and depth of AI acceleration and penetration rates however.",0,0,0.17,0.118
post11lb,poorly branching,13,I admire your optimism here. I just can't see how the current techbro oligarchs in power would do anything like that.,0,0,0.193,0.182
post11lb,poorly branching,13,"I’m sorry, but this looks like a ChatGPT response in the way it’s formatted and feels disingenuous given by how it actually didn’t address the concern in my post. I personally find this insulting, so I have questions  How is point 1 helping if that IS the primary concern OP is addressing?  How is point 2 “resolving” wealth disparities? This was also unexplained  Point 3: beneficial to the planet itself? What do you mean by this? I think I get human health as it would mean less hours, but the reality is people need hours due to rising costs of living and that includes basic necessities like food and water. That was part of why I brought up the video. To resolve the issues is to begin at the economy. Speaking of the economy, when was the last time a Republican mentioned anything about UBI, much less [advocated](https://www.pewresearch.org/short-reads/2020/08/19/more-americans-oppose-than-favor-the-government-providing-a-universal-basic-income-for-all-adult-citizens/) for it? I swear, I have seen countless people mention UBI in this subreddit, but none even think that living in a society where the wealth disparity grows wider and the elite only cares about themselves would mean UBI is not going to even be a thing. Let’s actually be realistic  And for point 4: what other work? Where are the new jobs? Again, your response completely ignored my main point and this why I feel your reply was some AI slop generated",1,1,0.215,0.171
post11lb,poorly branching,13,"> AI replaces human labour = Reduce human energy consumption globally, albeit AI will require a lot of energy still total energy comes down.   This has never been true. Assuming that the AI economy *doesn't* go the dystopian way of 90% of humanity being unemployed and unnecesary, economic advances always lead to MUCH higher consumption of everything, including energy. E banking didn't reduce fuel consumption by reducing the need banks have for delivery trucks; it increased it by ten fold with bitcoin mining, and if energy was twice as cheap it would had increased it by twenty fold. Advanced economies consume vastly more.   > Profits from AI fed back into redistribution more equitably resolving wealth disparities and equalize resource and energy use from using lower less polluting levels.   Societies don't even redistribute profits from gold mined on publicly owned land. These redistribution sci fi societies remain just as theoretical as when Marx wrote about them.   > Reduced speed of economies and consumerism is also beneficial both for the planet and for human health ie 4 day weeks and maybe even 3 day with UBI eventually.   Again, barring an unprecedented never before successfull revolution, AI economies will consume orders of magnitude more and have orders of magnitude more wealth inequality. Americans in 2025 don't consume less than in the 18th century because we invented cars that made transport more efficient.   > Reorientation of societies towards more co-operation, more humane concepts of “work”. Etc   This just isn't how these dynamics work. At most, huge, overwhelming technological advances *may* or may not move the needle by a little bit.   Bronze age greece had a higher and more progressive standard of living than the soviet union, despite the latter society being 2500 years more technologically advanced. Archeologists discovered that pre contact tribes in ecuador where healthier and ate better than ecuadorians in the 21st century. The Iphone 6 replacing the Iphone 5 with 15 more megapixels doesn't mean anything to the 900 million Indians whose last president ran on giving every household a indoor toilet; something by the way that Britain achieved centuries ago.",0,1,0.208,0.153
post12hb,richly branching,13,"Science has shown that all humans, regardless of sex or sexual orientation, respond more strongly to female voices than male ones.",0,0,0.158,0.169
post12hb,richly branching,13,"Unless directives are being given.  In New York, it’s a male voice that gives orders “stand clear of the closing doors!”",0,0,0.269,0.256
post12hb,richly branching,13,Except for books on tape,0,0,0.211,0.19
post12hb,richly branching,13,"I guess it depends what is meant by ""respond more strongly"". Studies have shown that people elect men with deeper voices, and men with deeper voices also get better wages and more easily promotions.",0,0,0.175,0.26
post12hb,richly branching,13,This raises the question of accent as well where it is predominantly the American accent with something like default siri adopting the valley girl American accent. What gives?,0,0,0.129,0.193
post12hb,richly branching,13,This is true that's why emergency warnings for pilots even in military jets are voiced by females,0,0,0.168,0.253
post12hb,richly branching,13,Probably because in female for 9 months? The eternal feminine leads us aloft...,0,0,0.18,0.186
post12hb,richly branching,13,umm.. anyway look at this dog as a reward for not scraping ur eyes out after reading that <3  ![gif](giphy|QvBoMEcQ7DQXK|downsized),0,0,0.342,0.22
post12hb,richly branching,13,Thank you! A good dog.,1,0,0.384,0.163
post12hb,richly branching,13,"Not only did I have to read that, but I had to see a dog too, today is a bad day",0,0,0.262,0.201
post12hb,richly branching,13,r/MenAndFemales,0,0,0.31,0.271
post12hb,richly branching,13,Huh???,0,0,0.324,0.251
post12hb,richly branching,13,I dont understand either...,0,0,0.384,0.392
post12lb,poorly branching,4,"I think you’re looking at it the wrong way. Will AI change skills you need? Yes. When was the last time you thought about your ability to make candles now that you have electricity ? You use a computer but probably don’t have a clue how to build a motherboard.   AI is the new electricity - does it benefit us to understand the underlying workings versus fully offloading knowledge ? Of course, but only to a certain extent - technology brings the need for more specialization - so the question is which way do you want to specialize? Towards understanding the technical underpinnings or towards working with specialists who understand when needed while you advance other areas of knowledge?   -PhD in educational sciences here 😊",1,1,0.332,0.151
post12lb,poorly branching,4,"You make a good argument, but I’m going to argue that electricity is not intelligent. These new tools are distinct from any prior tools humans have developed. We need to be cognizant of the potential dangers.  In the same way that LLMs can be extraordinary teaching and research aids, they can and will also result in the offloading of human intelligence.  Just anecdotally, recently I’ve been having debates with people on these subjects where the other person is using the LLMs to make their arguments. In many of these instances, it is clear that the human using the LLM does not understand the arguments.",0,0,0.24,0.211
post12lb,poorly branching,4,"I  am also not arguing that electricity is intelligent - that would be… odd. My point is that it is coming no matter what and it’s going to be ubiquitous. Sometimes a technology comes along that changes everything, like electricity. AI will be that only more. In many ways we can barely even imagine at this stage. Indeed if we’re not prepared for it (which arguably we are not) it can be dangerous, the discomfort the transformation will bring will displace people … for example, if asked what a computer is you’d likely first think of a machine with software. Not so long ago it was Judy in the next office who crunches the numbers.   AI can make us lazy, just like computers or the argument OP made about the calculator because there’s a possibility of distribution of cognitive load. How we think about it and approach it is important, but it’s not an impossible riddle.  I’m not saying there aren’t risks - if you don’t know the dangers of electricity you can easily electrocute yourself. We’re not going to be able to stop everyone from electrocuting themselves. But I’m also not for throwing the baby out with the bathwater.",0,1,0.201,0.178
post12lb,poorly branching,4,"I am totally in favor of moving ahead with AI as quickly as possible. But I’m also not opposed to the idea that humans are just an evolutionary step towards Superintelligence, and that we will ultimately be rendered obsolete.  One point I will make is that the dangers of new technology cannot be predicted — this has been validated strongly in the industrial revolution  On a sidenote, it’s possible electricity is intelligent, in the sense of being able to make decisions re:  https://en.m.wikipedia.org/wiki/Free_will_theorem  We live in a strange universe !",0,0,0.177,0.181
post13hb,richly branching,38,"the current economic paradigm drives income inequality. the disease is capitalism, income inequality is just a symptom.  automating labour is a blight on people, not because doing less work is bad, but because the economic system is detached from reality.",0,0,0.201,0.123
post13hb,richly branching,38,"We just need a robot tax that funds UBI. The more profit that is earned from robots, the more goes to each citizen.   That would also make the public more interested in supporting the transition.",0,0,0.114,0.098
post13hb,richly branching,38,"I’m having a hard time understanding this “Robot tax” that funds UBI thing.  Let’s say for a minute that we do go ahead and give this a try.  The first item to address I guess would be, to define what a robot is right?  But where does that begin and where does that end?  That the first problem I see.  Next, what happens to companies that have used automation for decades, do we let it slip and we only tax the new ones that use automation? That’s kind of unfair and it will stifle progress don’t you think?",0,1,0.159,0.119
post13hb,richly branching,38,"The details won’t be easy, it would take some smart people to spend time making a plan. But basically all large companies will be largely automated soon. The profits from those companies need to actually be taxed. As we know, Amazon and Apple pay almost no tax. The solution to that is easy. If sales happen in the US, Apple pays a percentage of US sales in taxes. Say 20%. It doesn’t matter that the company is based in a PO Box is Northern Ireland. Their sales are here, that’s how they get taxed.   A large portion of those entirely new corporate taxes goes to UBI. It’s the only way forward that makes any sense.   If Amazon wants to keep selling things to people but there’s no more need for jobs, those people will need to get money from somewhere.   That money can come from the productivity of robots rather than humans.",0,0,0.145,0.114
post13hb,richly branching,38,"So where would 3d printers fall on that scale? CNC? both are programmed to perform configured tasks. They are not automatic in that you must upload a program, but many automated devices are the same way, but they use logic instead. How would the tax be applied a percentage of profits? per ""bot?"". It's a very very complex idea, and has the potential to snuff out small companies/peoples side jobs depending on how its implemented (see stuff like etsy vendors)",0,0,0.2,0.077
post13hb,richly branching,38,"Until the wealthy, with their overwhelming influence over the government, convince said government to cut/abolish the tax. Then everybody except them is fucked.",0,0,0.201,0.186
post13hb,richly branching,38,"gates, enough is enough. just because you saw the need for charity doesn't mean that charity should be needed.",0,1,0.194,0.131
post13hb,richly branching,38,It’s not charity. It’s humans reaping the reward of our collective progress. Rather than just a small handful of humans.,0,0,0.243,0.246
post13hb,richly branching,38,"What makes you think they will allow such a tax? The powers at be already dodge taxes, and they control the gov too. Rulling classes of ages prior could have also appeased the people and yet they didn't even to save themselves.",0,0,0.104,0.091
post13hb,richly branching,38,The population needs to want it enough to elect people that will put the tax in place. That starts in places like Reddit and spreading ideas that people can get behind.   It won’t be easy. You’re right about that.,0,0,0.124,0.13
post13hb,richly branching,38,"> We just need a robot tax that funds UBI.  Good job, you just increased the price of everything that used a robot in its creation. Give yourself a pat on the back.",0,0,0.147,0.134
post13hb,richly branching,38,But we also got a bunch of free money to spend on that stuff.   It has to go this way because there will be absolutely no jobs soon. Robots will be better workers than humans in every field very soon.,0,0,0.15,0.113
post13hb,richly branching,38,"Inequality can exists with or without capitalism though. The way I see it actual problem lies in social hierarchy, faults in collective decision making and shortcomings of governance process, which gives rise to wage-slavery and dominance of few over many.",0,1,0.188,0.182
post13hb,richly branching,38,capitalism needs inequality to function. you need people with less capital to sell their time cheaper so you can make a profit. so in order for capitalism to function income inequality must always be present.  but your point still stands. a faulty hierarchical decision making process can also create inequality.,0,0,0.236,0.12
post13hb,richly branching,38,Your assertion that inequality is required assumes we all place the same value on labor and goods. In facts it’s extraordinarily naive to imagine you’ve created a universal index of value.,0,0,0.149,0.143
post13hb,richly branching,38,"There seems to be an ideal frontier level of inequality: [https://www.frontiersin.org/articles/10.3389/fpsyg.2017.02052/full](https://www.frontiersin.org/articles/10.3389/fpsyg.2017.02052/full#:~:text=The%20correlation%20between%20the%20Gini,20)  As with most things, moderation is best",0,0,0.186,0.167
post13hb,richly branching,38,Are you proposing trying to modify the natural fundamentals of human society? That's quite a bold ambition.,0,0,0.261,0.226
post13hb,richly branching,38,"Sure. Make study of logic and reasoning more prevalent than study of reading and writing. Encourage non-hierarchical social roles, akin to Adlerian individualism. Put a cap on how much wealth a single individual is allowed to own so no one would own more than they can ever spend in a lifetime. Give everyone basic income to get by so there would be no fear of ending up homeless - fear kills creativity and people are far more motivated to be part of society when society is worth being a part of. End homelessness. Invest massive resources into studying and technologically improving collective decision-making processes so that one day we could for once live our lives without constant effort going into stopping some idiots taking all our rights away, starting a nuclear war or next genocide.",0,0,0.205,0.201
post13hb,richly branching,38,"The actual problem is that there has always been inequality, and people have always died of it.  What do you think people did when they couldn't successfully hunt or grow crops?  They starved to death.",0,0,0.16,0.198
post13hb,richly branching,38,"That example does not show inequality. That's just a natural course of life. If you included that other people would allow them to starve to death that example would work to show inequality. However, pre-capitalist society was so intertwined that food distribution across the population was necessary for societal survival. Therefore, this particular inequality wouldn't have happened.",0,0,0.325,0.136
post13hb,richly branching,38,The issue isn't inequality in ability but the inequality generated by capitlsism through ownership of capital.   Let's say you got 2 investors. Equal ability. Investor 1 has more I starting capital than 2. For this simple fact investor 1 will make money money simply due to ownership and nothing else. That is the problem.,0,0,0.202,0.144
post13hb,richly branching,38,People that are born smart have an advantage. People born with looks have an advantage. People born with money have an advantage.  Life will never be equal. The best we can do is keep the bottom as high as possible and give opportunities for improvement.,0,0,0.231,0.239
post13hb,richly branching,38,"It has been this way, but it doesn’t need to be this way.",0,0,0.279,0.233
post13hb,richly branching,38,it won't be this way for much longer. we have passed the point where the benefits are no longer bigger than the losses. it is just a matter of time.,0,0,0.165,0.154
post13hb,richly branching,38,"I read that is ""it's just murder time"" and thought ""you son of a bitch, I'm in!""",0,0,0.263,0.259
post13hb,richly branching,38,retarded post by someone who obviously can’t cope,0,0,0.34,0.404
post13hb,richly branching,38,"Well said. Even though we all have the computing power of what used to fill entire rooms in our pockets, the number of hours we work is the same (if not more, considering that bosses now try to communicate with us 24/7). Technological innovation under capitalism does not ease the working class' burden.",0,0,0.373,0.218
post13hb,richly branching,38,"Income inequality dates back to the founding of the first cities.  It has existed in all economic models except hunter/gatherer. Even then, some had better hunting grounds.",0,0,0.127,0.154
post13hb,richly branching,38,murder has existed since the beginning of human kind. cancer has existed since the beginning of human kind. doesn't mean we should not do something about it.,0,1,0.206,0.184
post13hb,richly branching,38,The only solution I can see is removing free will.  Which would also solve the murder problem and most of the other ills in society.,0,0,0.222,0.293
post13hb,richly branching,38,"That is false.  We had Capitalism back in the 70s and it was constrained by high taxes on the rich, strong regulations, and entire industries being unionized.  Things didn't start to rot until we backed away from trying to tame Capitalism.    Most human endeavors require some rules, some control, some way of asserting reason and responsibility.  If you don't bother trying to set some limits then systems go haywire.",0,0,0.137,0.136
post13hb,richly branching,38,"that is what i'm saying. treating the symptom is wasting resources. we need to cure the disease. if the cure is a tighter leash, so be it. if the cure is total departure from the ideology, so be it.",0,0,0.168,0.2
post13hb,richly branching,38,"Well yeah, if you continue to allow the capitalist class to exist, any concessions you get from them are always going to be temporary.",0,0,0.176,0.158
post13hb,richly branching,38,Capitalism is a nuclear reactor and we took the brakes off it and were headed towards a negotiable.,0,0,0.221,0.209
post13hb,richly branching,38,"The object of automation isn't to reduce labor, it's to maximize its productivity.",0,0,0.307,0.171
post13hb,richly branching,38,the objective of automation is to increase profits.,0,0,0.323,0.171
post13hb,richly branching,38,Not all automation is profit-oriented. And profits are relative to productivity levels. So it's productivity.,0,0,0.435,0.119
post13hb,richly branching,38,I would add consumerism and materialism,0,0,0.275,0.287
post13lb,poorly branching,15,"DAT pays the best of these types of companies, but they're very particular about quality work. They aren't going to tell you if you're not doing well, or even if you are doing well. Either you'll stop getting projects, or you'll start getting better projects. That's how they communicate, and you may never get a second chance. That may be something you dislike, but they're not a scam. They pay reliably, work is unlimited, and my pay is up to $30 hourly with no coding. You must have a precise, nitpicky, logical, perfectionist personality type, or you may struggle. If those things come naturally to you, you can monetized that with DAT.   However, I would never count 100% on any online contract work personally. I'm fairly risk averse, so I have other means of replacing this income immediately if it goes away tomorrow.   I'd also caution you that long hours doing analytical work with maximum effort can cause quality to slip. Most people struggle to produce top quality work after many hours of focus. When I feel like I'm struggling to focus, I take breaks. I'd rather make less that day than ruin a good gig by turning in subpar work.",0,0,0.235,0.106
post13lb,poorly branching,15,"How long have you been working to get to $30 per hour, non coding? I've been with them since November and just got to $23-25/hr non coding.",0,0,0.202,0.125
post13lb,poorly branching,15,I started in early November.,0,0,0.202,0.17
post13lb,poorly branching,15,"As of now May 2024, are there still “unlimited” work available for non-coding work? I’m planning to apply.",0,0,0.162,0.112
post13lb,poorly branching,15,Are there still projects?,0,0,0.266,0.225
post13lb,poorly branching,15,"Hey dear, which websites u work on?",0,0,0.245,0.22
post13lb,poorly branching,15,Data Annotation Tech,0,0,0.405,0.229
post13lb,poorly branching,15,Was it hard to join? I’ve just put in my application and by reading some comments I’m starting to think I’ve done a bad job.,0,0,0.236,0.291
post13lb,poorly branching,15,"The process was very fast for me. While it wasn't difficult to join, I spent extra time on the assessments so I would do well.",0,0,0.232,0.152
post13lb,poorly branching,15,Sounds like slave labor without security,0,0,0.225,0.331
post13lb,poorly branching,15,"I'm satisfied with the pay and only work when I feel like it, so calling it slave labor doesn't make sense. It comes off as sour grapes. It's not secure work. It could end at any time. (Lots of W2 workers have been laid off without notice this year too though). However, many people have been doing it for years. So far, I've been promoted into a reviewer role, and I continue to have a ton of work available 24/7. This isn't my only means of income, but it sure has been helpful. As a gig I can do in my jammies in my home office as much or as little as I please, I've been very happy with it. Started in November 2023 and no complaints so far after earning many thousands of dollars which was all paid out as promised right down to the minute. I actually have far more complaints about my professional full time job than DAT.  I see you do a ton of gig work where you work as an independent contractor which is no different than DAT, so why are you singing out DAT in particular?  DAT is much better than any of the others you listed and those you listed are also not secure. Yet you aren't complaining about being insecure or being enslaved. Huh. Were you rejected by DAT? Because it seems like you're just mad about not being selected. Sour grapes.",1,0,0.207,0.138
post13lb,poorly branching,15,"Oops, looks like i touched a nerve. Sorry if my comment offended you. It seemed like lot of walking on egg shells type of work with 0 security, that's why i said that. But as long as you are enjoying, good for you! May your time there last as long as you wish to work. I truly wish you the best.",0,0,0.21,0.191
post13lb,poorly branching,15,"Hey! Do you mind if I ask how many hours per week you usually do? I’m a student, so this kind of work is absolutely perfect for me in terms of the flexibility, but I’m petrified that if I ramp up my hours a little then I may lose it altogether. So far everything’s gone well; I’m consistently getting more (and higher-paying) tasks on my dashboard and I’ve increased my hours slightly, but given what I’ve heard about people being shut down for earning too much, I’m curious to hear what hours others are putting into the platform, especially someone like yourself who has been doing this for so long.",0,0,0.27,0.144
post13lb,poorly branching,15,"Hi bruv, applied like 2 weeks ago. Havent heard or gotten any taks. Any tips on how to receive tasks? Like is there something you can do?   Thanks in advance",1,0,0.21,0.126
post13lb,poorly branching,15,"No. Well paid WAGE labor with zero benefits, without job security.",0,0,0.151,0.157
post14hb,richly branching,100,"It's already happening, as he presents his outlook. The biggest Fortune 500 companies are freezing hiring, while at the same time, increasing investments into AI agents. As they developed strategies to replace human workers with AI agents, in everything from code writers to engineering. Many sales positions as well as customer service Representatives. Even Wall Street isn't immune from this. Jobs are being replaced in masses. Why so shareholders can make even more money by saving on labor costs. The bottom line is more important to the wealthy investors. While all the AI companies are reaping massive investments from the ultra rich.  The amount of money being invested is staggering, all with the ultimate intention to increase profits and reduce the labor force.  We don't have to wait a few years for this to affect the average person, it's already started the tsunami is here. The first wave is crashing ashore.  People like Sam Altman and Elon Musk, Jeff Bezos, companies like Meta and Tesla Amazon and Open AI are reaping the benefits, while the average worker will not have a job in two years. If you work in the majority of services industry including working for top Fortune 500 companies.",0,0,0.128,0.182
post14hb,richly branching,100,but...  who buys their product when no one has a job?,0,0,0.205,0.221
post14hb,richly branching,100,What you are missing (maybe) is that they are not thinking about what happens if every corporation does this. Instead they are just thinking about how their decisions will look on the quarterly balance sheet that goes to the board and shareholders.,0,0,0.173,0.176
post14hb,richly branching,100,"then they are not, strictly speaking, rational.  this is like all 100 customers stampeding to get into the 'short line' at the checkout. smart for one,  dumb for all.",0,1,0.206,0.254
post14hb,richly branching,100,I think they are mostly thinking: what if my competitors do this first and we go bankrupt because we can't compete?   What do they care about the consequences of everyone doing it if they feel they'll disappear on the shorter term if they don't do it?,0,0,0.216,0.234
post14hb,richly branching,100,"Exactly and this is called Game Theory.  “If I don’t do it, one of my competitors will and gain an advantage so I might as well do it to”. It’s precisely things like this that need to be regulated because of this psychological phenomenon and the implication",0,0,0.255,0.285
post14hb,richly branching,100,"Keep ai for scientific use. It was too early.  The problem lies in greed, abolish money first then release ai for everyone.",0,1,0.219,0.192
post14hb,richly branching,100,And probably not thinking past the next couple of quarterly earnings reports,0,0,0.22,0.231
post14hb,richly branching,100,"They will figure that out when they get there. Or at least, that’s the thought process. Right now there is an AI gold rush, and any executive arguing for anything other than aggressive pursuit of it will get axed quickly.",0,2,0.135,0.19
post14hb,richly branching,100,Well it had to end somehow.  To be by short sighted greed seems poignant.    See you all at the going away party,0,0,0.253,0.218
post14hb,richly branching,100,True.  The long game is not typically the domain of the greedy and the criminally insane...,0,0,0.191,0.262
post14hb,richly branching,100,"God... how I've learned to hate the ""quaterly cult.""",0,1,0.204,0.337
post14hb,richly branching,100,"THIS. The ruin of our version of capitalism comes largely from this. Capitalism itself is not evil. It’s a market competitively supplying goods and services to a demand, for a profit. But serving the corporations at the expense of the consumers and employees and state, giving corporations legal personhood, constantly trying to exceed unreasonable expectations to benefit shareholders, and managing by spreadsheet have ruined it.  We need other metrics for success like how many employees are healthy and happy, able to survive and educate themselves, and their kids, what has been committed to the welfare of their localities, etc. Use the greed of the execs and give more tax incentives for this kind of thing and it might improve a little.",0,0,0.172,0.16
post14hb,richly branching,100,Well that's where the credit card companies step in.   Here's how I know a.i. won't be good if it's the one making all the decisions then it should realize the easiest way to make a huge profit is cutting from the top.   What's the point of a CEO of all of the decision are made by a.i.,0,0,0.163,0.145
post14hb,richly branching,100,"“The development of modern industry, therefore, cuts from under its feet the very foundation on which the bourgeoisie produces and appropriates products. What the bourgeoisie therefore produces, above all, are its own grave diggers. Its fall and the victory of the proletariat are equally inevitable.” -Karl Marx",0,0,0.257,0.197
post14hb,richly branching,100,"Except that, theoretically, automation would allow the bourgeoisie to exist *without* a proletariat. If robots do all the work and make all the products, then the people who own the robots can have anything they want for free, and the rest of humanity can simply disappear.",0,0,0.218,0.2
post14hb,richly branching,100,"First two sentences, solid gold.  Third sentence, unwarranted optimism / millennarist fantasy.",0,0,0.288,0.353
post14hb,richly branching,100,"You just found out what Karl Marx figured before automation was called automation. [https://thenewobjectivity.com/pdf/marx.pdf](https://thenewobjectivity.com/pdf/marx.pdf) Because I like to be funny I used automation to write this summary.  >Marx argues that machinery creates a fundamental contradiction for capitalism because it simultaneously tries to reduce labor time while relying on it as the source of value. Here's how it breaks down: On one hand, capitalism, driven by competition, uses machines to make production more efficient, cutting down the amount of labor needed to produce goods. **This is good for capitalists because it lowers costs, increases productivity and increases surplus labor time**, enabling them to produce more goods for sale and increase profits. But, on the other hand, capitalism depends on labor time to measure value. **The more machines replace workers, the less labor is directly involved in making things, and the more difficult it is for capitalism to make a profit**. So, capitalism ends up in a bind: it needs to reduce labor to maximize profits, but at the same time, it relies on that same labor to generate value. This leads to overproduction, and the system becomes unstable, because the value is not being generated at the same rate by the labor that has been replaced by machines.  To be funnier, here's an AI generated podcast about it. [https://notebooklm.google.com/notebook/781b78aa-a1cf-4dd1-8a4a-8ff1096b4556/audio](https://notebooklm.google.com/notebook/781b78aa-a1cf-4dd1-8a4a-8ff1096b4556/audio)  You can do this with NotebookLM, just upload the PDF as a source and you can ask it questions and it will cite sections from your sources.",0,0,0.247,0.179
post14hb,richly branching,100,"Really funny how many people use the term ""late stage capitalism"" who also get upset about AI. Automation (reducing the absolute number of laborers total) is literally the thing that Marx says will cause a revolution and the collapse of capitalism.  ""**A development of productive forces which would diminish the absolute number of labourers,** ***i.e.*****, enable the entire nation to accomplish its total production in a shorter time span, would cause a revolution**, because it would put the bulk of the population out of the running. This is another manifestation of the specific barrier of capitalist production, showing also that capitalist production is by no means an absolute form for the development of the productive forces and for the creation of wealth, but rather that at a certain point it comes into collision with this development."" - Capital, Vol 3, Ch 15  He also says this is inevitable and unavoidable due to competition:  ""No capitalist ever voluntarily introduces a new method of production, no matter how much more productive it may be, and how much it may increase the rate of surplus-value, so long as it reduces the rate of profit. Yet every such new method of production cheapens the commodities. Hence, the capitalist sells them originally above their prices of production, or, perhaps, above their value. He pockets the difference between their costs of production and the market-prices of the same commodities produced at higher costs of production. He can do this, because the average labour-time required socially for the production of these latter commodities is higher than the labour-time required for the new methods of production. His method of production stands above the social average. But competition makes it general and subject to the general law. **There follows a fall in the rate of profit — perhaps first in this sphere of production, and eventually it achieves a balance with the rest — which is, therefore, wholly independent of the will of the capitalist.**"" - Capital, Vol 3, Ch 15  And how does he feel about the machinery itself?  ""**It took both time and experience before the workpeople learnt to distinguish between machinery and its employment by capital, and to direct their attacks, not against the material instruments of production, but against the mode in which they are used**. The contests about wages in Manufacture, pre-suppose manufacture, and are in no sense directed against its existence. The opposition against the establishment of new manufactures, proceeds from the guilds and privileged towns, not from the workpeople."" - Capital, Vol 1, Ch 15",1,0,0.268,0.173
post14hb,richly branching,100,"I feel like I have to explain this a lot: they don't care. Companies these days only think about a quarter or three ahead. They legit do not care about the long term.  It's the MBA/corporate raider mentality and it's basically the standard amongst the managerial/c suite class in America. They've been educated to think operating ratios are like THE most important thing and it's reenforced by the investor incentive structure. You're rewarded based on quarterly performance, which means cost cutting is valued basically the same as improving the business or product and is MUCH easier to achieve.  Which should be obvious given how many of them think the US rail industry is super good (because they have really insane ratios) when in reality it's the corpse of a whale who died mid-swim and hasn't quite hit the bottom yet.",0,0,0.097,0.162
post14hb,richly branching,100,"I just had to award you not only for the very accurate description of the fundamental problem with capitalism, but for that last graf and metaphor which was solid gold -- solid gold example, solid gold analysis, brilliant metaphor which I will probably steal at some point.",2,0,0.238,0.222
post14hb,richly branching,100,They just want to see people suffering and getting dependent on them.,0,0,0.273,0.273
post14hb,richly branching,100,"The elites don't need money if the machines they command provide any labour they desire, so they don't need customers. Money will fall out of the picture.",0,0,0.174,0.209
post14hb,richly branching,100,"The rich. It is not necessary to sell products to the working class, so there is no reason why the economy cannot shift to address mostly the wealthy’s needs.",0,0,0.193,0.144
post14hb,richly branching,100,[You got it](https://youtu.be/MYB0SVTGRj4?t=203).,0,0,0.35,0.232
post14hb,richly branching,100,"you're thinking late feudal?  the consumers are the 1 percent, everyone else labours to produce wealth for them to hoard and consume?  big retooling needed to get back there, but obviously they are working on it.",0,0,0.161,0.246
post14hb,richly branching,100,I agree with your sentiment but look at civilizations throughout history - a wealthy ruling class and poor masses is the default setting.,0,0,0.15,0.214
post14hb,richly branching,100,They tend to fail in this exact fashion as well,0,0,0.302,0.256
post14hb,richly branching,100,"Only within societies which we have dubbed ""civilizations."" These structures were by no means inherent across all of humanity, nor a natural one.",0,0,0.2,0.173
post14hb,richly branching,100,"Money is exchanged for goods and services. If they have good enough AI, they don't need humans to get the things they want, and that includes buyers as well as employees.  The more clever industries will shift to automated modes of existence. Those catering to human beings will shrink and shrivel as the human being becomes increasingly destitute.  I'm sure the CEOs will cheer as productivity increases, as I'm sure the shareholders will cheer when they can replace the CEOs with far more obedient and clever AIs, ones that can invest and become shareholders as well.",0,0,0.242,0.228
post14hb,richly branching,100,"Universal income funded by the corporations, we will basically be work-free slaves.",0,0,0.16,0.23
post14hb,richly branching,100,You guys still think money and capitalism are end goals?  They are tools to redirect power and control.  You don't need them anymore once you accumulated enough power and control to use more..direct tools.,0,0,0.205,0.205
post14hb,richly branching,100,Other corpos doing the same thing?,0,0,0.204,0.215
post14hb,richly branching,100,They’ll just sell and ship their products to wealthier countries,0,0,0.157,0.178
post14hb,richly branching,100,its not their job to ensure poeple in general have money. their only job is to ensure adding value to share holders.   the govt will have to figure out ways to allow people to afford food [UBI],0,0,0.178,0.142
post14hb,richly branching,100,Not to mention the economic affect it will have in major cities. If AI truly replaces people mass layoffs will happen and high skilled workers will have to shift industries and move out of tech hubs,0,0,0.158,0.11
post14hb,richly branching,100,"Corporations don't care about that anymore. They care about how they look at the stock exchange. And that's something that has little to do with how much they sell. It's not about value anymore, it's about beautification.",0,0,0.281,0.124
post14hb,richly branching,100,"Down the line but we're going to have to live through potentially many years until society is willing to change. During the transition many, likely most, are going to just have to eat the consequences and spend their savings while the rich get massively richer. Or maybe not! Maybe everything will be fine!",0,0,0.093,0.12
post14hb,richly branching,100,"It doesn’t matter if the money is valuable. It’s about getting all of it and having more than your fellow man,  not spending it.",1,0,0.47,0.272
post14hb,richly branching,100,The government that they own.,0,0,0.276,0.23
post14hb,richly branching,100,They’ll take over the government and funnel tax money into subsidies.  They will make deals with each other hyping the deals and pump their stock. People will invest those stocks and increase the worth of the companies while taking some profit to buy the services and products of the same companies.  Your income will go down but your investments will go up until something collapses. the government will bail out those who are in charge.  Rinse repeat dystopia.,0,0,0.153,0.096
post14hb,richly branching,100,"Exactly. And ""AI Agents"" will lead to customer frustration, it's a huge opportunity for China to fill the blank with actual humans providing actual service. Tesla as a car company is mostly already dead, they just don't know it. you can get a comparable electric car from a china brand at a fraction of the price, that is why tariffs are all the talk. they aren't there to help the voters or fight China but to preserve status quo.",0,0,0.085,0.127
post14hb,richly branching,100,Perhaps AI consumers order stuff from AI producers without anything being produced and the money is just shuffled from corporation to corporation and companies manage to include a tax break.,0,0,0.154,0.165
post14hb,richly branching,100,Also wtf is the product.,0,0,0.317,0.287
post14hb,richly branching,100,“Capitalism slits its own throat”  -paraphrasing Marx,0,0,0.18,0.26
post14hb,richly branching,100,"You stop that right now, that’s entirely to much thought, nothing exists outside of Q1 you ignorant swine. Maaaaybe Q2 but that’s.. that’s oretty out there",0,0,0.273,0.292
post14hb,richly branching,100,Unfortunately: [other wealthy people](https://youtu.be/MYB0SVTGRj4?t=203).,0,0,0.222,0.201
post14hb,richly branching,100,"They will change HOW they profit from individuals rather than conventional money transactions. If we are talking about retail it will change what they are selling and how people are consuming it. Data which can be sold for example like social media profits immensely from   The top companies will always always always be ahead of the curve so the new argument I see here a lot of ""what happens when nobody has money to buy things"" will always be irrelevant because to the companies who are able to adapt and adjust people will always be a commodity with or without money",0,0,0.152,0.133
post14hb,richly branching,100,They will look for government handouts,0,0,0.251,0.197
post14hb,richly branching,100,"Well, by then they will have sold out enough shares to buy things that hold value through a recession, depression, and economic collapse. Food, agriculture, real estate, water, food/water processing, energy, technology, ""defense,"" and medicine all have fundamental value. They will own and be able to defend large amounts of that.  Money is just an exchange medium, you can still leverage promises for the future. Power is power.",0,0,0.246,0.165
post14hb,richly branching,100,That's where basic universal income comes in.  People have just forgotten that the idea is inherently capitalistic.,0,0,0.162,0.156
post14hb,richly branching,100,They sell to themselves and upper middle class whales/ DINKs that maintain jobs due to their place as PMCs or as engineers. (aka most of reddit),0,0,0.234,0.19
post14hb,richly branching,100,"I swear people always make this argument and they miss how for hundreds if not thousands of years there were peasants and kings.   Did the kings need the peasants to buy things? No. You got taxed, used and abused.   There are no jobs? You'll be sent to wars or to Mars to set up shit and die there. They'll find a way. You are not protected because at this specific moment they are after your wallet. They are just keeping the status quo until they can stop pretending you were ever in control. I love how elections keep the illusions going, as if it's not always a rich guy bought and paid for from one of two or three parties lol",0,0,0.139,0.151
post14hb,richly branching,100,"What a weird and fundamentally wrong take.   Taxed of what, if I don't have anything?   Medieval society wasn't some pop-culture idea of dystopia - peasants kept a large share of what they produced, so they could reinvest it into trade (either as small-scale merchants themselves or by selling their excess produce to organized merchants). Whenever this system broke (due to war, excessive taxation or natural disasters like famine or plague), this universally led to a collapse of the society in the local area (usually a violent collapse).   Moreover, relationship between feudal and peasantry was usually regulated by charters and laws, which specified obligations of both sides of social contract.  Unironically, most medieval societies had a much better grasp of sustainability than modern emergent oligarchies.",0,1,0.121,0.123
post14hb,richly branching,100,The 20th century was a historical aberration in almost every way. We are reverting to mean.,0,1,0.209,0.403
post14hb,richly branching,100,Immigrants and foreign workers maybe? And China or India?,0,0,0.116,0.191
post14hb,richly branching,100,I’m exec level in a huge company and can confirm. Junior to mid levels frozen as our upper management “wait and see” how we can have AI do their jobs (I live in Germany where hiring someone is essentially a life long marriage).   It scares me because we are witnessing the death of critical thinking. These AI agents won’t push back on managements dumb and politically driven ideas. And our younger population is increasingly delegating their information synthesis to computers.   Easier people to control and influence by those with the means.,0,1,0.2,0.24
post14hb,richly branching,100,[removed],0,0,0.366,0.413
post14hb,richly branching,100,It won’t work but the Executives won’t ever admit they were wrong and will pretend not to understand sunk cost  As long as they can fuck over labor it’s worth the cost,0,1,0.151,0.239
post14hb,richly branching,100,[removed],0,0,0.366,0.413
post14hb,richly branching,100,"I work for a top fortune 50 company and we're still using ancient tools and software from 25 years ago, there's no way in hell they'd survive a day trying to replace people with AI. They probably couldn't even afford the AI and if they did everything would just break instantly. Our company would need to completely overhaul literally everything before AI would even be compatible with its systems and it can't afford to do that.",0,0,0.172,0.127
post14hb,richly branching,100,I keep trying to use it because I want it to be useful to me. I want to get more done and do less work.  I actually asked it how to use its own API and it straight just made shit up. Gave me some fake instructions that looked correct 🙄.  Yeah I don't think they'll be replacing my job any time soon. I'll get plenty of work unfucking the mistakes it makes I'm sure.,1,1,0.287,0.234
post14hb,richly branching,100,We are literally at infancy stage. Only a couple of years in. There is virtually no chance that this is as good as it gets and there will be no improvement from here on in.   So maybe it won't happen for 10 years or 50......but it will happen at some point and the same problems will arise. Better for us to be prepared and talking about it now.,0,0,0.202,0.175
post14hb,richly branching,100,">We are literally at infancy stage. Only a couple of years in.   We are many decades into the research. There's a lot of hard work to get us to this point. What is visible may only be a few years in, but it's been going on a lot longer underneath the surface.",0,0,0.207,0.194
post14hb,richly branching,100,We will have vastly worse problems in 50 years due to collapsing global ecosystem.  Extreme weather will be far more extreme and will have a major impact on global food supply.  Gonna get really ugly,0,0,0.16,0.16
post14hb,richly branching,100,"We're already decades in to machine learning research, we're only in the infancy (although honestly id argue we're well into) the latest hype cycle. This happens every few years in ML, it is literally taught in schools this cycle. Look up AI winter",0,0,0.285,0.163
post14hb,richly branching,100,[removed],0,0,0.366,0.413
post14hb,richly branching,100,"For now, anyways. We know intelligence is possible, so automating it is posible too. We just haven't come up with the right architecture, but every passing year we are closer. If Large language models and transformers don't pan out, that just delays the problems here presented.",0,0,0.219,0.182
post14hb,richly branching,100,"Oh, sweet summer child.  Over the last six months, we (F500) started letting go of our frontend devs because upper management realized that an architect paired with AI outperforms an architect paired with a frontend dev on every KPI imaginable. They were even offered training to transition from being an ""Angular Andy"" to someone skilled in system design, solution architecture, and the like. Less than 10% bothered with those learning paths, brushing it off as fearmongering from the suits.  Ironically, the same ones who spend four hours a day on Stack Overflow just to get their shit going, and need two hours of meetings every day so I can explain for the fifth time that week how I want my REST API structured, were the ones who thought they were absolutely indispensable. ""I don't worry, it's just a stochastic parrot"". Hilarious.  I know every dev on Reddit thinks they're the smartest mf ever, but out of the hundreds of devs I’ve had to manage so far, 80% are easily replaceable, and are getting replaced. Their actual dev skills didn't match their inflated ego at all. Like, we even did workshops showing what SOTA AI can do, and how I create a production-ready app in a fraction of the time... then those fucks accused me of staging my demonstration. Holy shit. I hope the parrot teaches them some humility.  You can also see it in the tech subs how everybody is ""it won't ever replace me"" while in the same sentence admitting their horizon just goes up to ChatGPT. So basically, they don’t know shit about AI at all except chatting with some mainstream chatbot, but think they have some kind of authority on the topic. This is going to be a rude awakening for some.  Meta stopping hiring mid-level engineers and us letting them go is just the beginning. But even news like that get brushed off like, ""Meta doesn’t know what it’s doing. They’ll hire them again next year"". Mindblowing cognitive dissonance... hallucinations worse than an open-source LLM running on a Raspberry Pi. But at least the LLM is capable of learning.  I realized my professional days were numbered back when the transformer paper was published. I was reading it with some colleagues, and all five of us in that room instantly knew what this paper meant (or at least we had an idea... being 100% sure of it came in 2020 after the GPT-3 paper dropped). That was long before anyone even knew what an LLM was... seven years ago. Those exact frontend devs who aren’t with us anymore were the ones laughing the loudest at my ""fear of parrots"".  Well, thanks to my paranoia, I have absolutely no problem with getting replaced in 3–5 years or whenever. Finally, I’ll have time to do whatever I want and pursue some of my hobbies. Perhaps I’ll even keep some pet parrots.",1,1,0.268,0.228
post14hb,richly branching,100,"Custom OpenAI solutions with datasources configured and memory systems, are whats doing the heavy lifting, they can replace an awful lot of stuff with it",0,1,0.171,0.191
post14hb,richly branching,100,[removed],0,0,0.366,0.413
post14hb,richly branching,100,"Any serious company looking into AI for their future is developing their own customised AI systems, they aren't using off the shelf solutions.  I think its really important for people to know that, because all they have read are news articles saying how they are firing employees and using ChatGPT which is generally not the case except for the companies doing it for the AI buzz words.  In other words, there are employees working right now on automating jobs, its just a matter of time until they are complete, they don't have to wait for ChatGPT to do it for them.",1,0,0.218,0.225
post14hb,richly branching,100,"What? What python web app are you talking about that costs too much money?   I feel like people who have this opinion should really read about the frontier of research - people who are aware of what is on the frontier have a VERY different opinion than this. I don't mean me. I mean research scientists, ethicists, economists etc.  That's not to say that they all agree with what will happen, but the idea that these models are not capable and not getting rapidly better is inexistent in those discussions.   Look up o3, then look up frontier math, swebench, arcagi etc. if you don't know what any of these things mean, ask an llm that can search the Internet because most of this is too new for it to be in the training data. Swebench and arc agi excluded, but definitely the interplay between them all.  Long story short, shit is getting very very real.",0,1,0.213,0.167
post14hb,richly branching,100,[removed],0,0,0.366,0.413
post14hb,richly branching,100,? It's useful to have some context here. AI code assist absolutely does work and does increase productivity.  Will it completely replace mid levels this year? No. Will it allow one mid level so the job of 1.3 mid levels? Probably.   Also keep in mind chatGPT was released in late 2022. LLM really didn't explode until mid 2023.  We're about 2 years in.. it's reasonable to think that in another 2-5 years the world will be very very different.   At this point I'm more worried about AI turning our world into a dystopian corptpcracy than I am about climate change.,1,0,0.218,0.151
post14hb,richly branching,100,"This will backfire so horribly that it would be hilarious if it wasn't so serious. Imagine creating almost overnight a new class of millions of unemployed people, used to having a job and living comfortably and suddenly destitute.    It will be the french revolution all over again.",0,0,0.189,0.234
post14hb,richly branching,100,"Tbh, maybe this will just speed it up so we don't have to watch another 40 years of slow decline where people barely notice.",0,0,0.207,0.186
post14hb,richly branching,100,"If it happens slowly enough maybe the system will balance itself out with the demographic decline, I'm not sure what would happen in that case.",0,0,0.136,0.191
post14hb,richly branching,100,"Don’t worry, they are developing armed AI managed drone swarms to manage that future problem.",0,0,0.182,0.254
post14hb,richly branching,100,"I wish i could just laugh at that. However, it doesn't matter how bloody it gets, in the end, numbers do matter.",0,0,0.194,0.216
post14hb,richly branching,100,"I keep thinking about the Butlerian Jihad.  ""Thou shalt not make a machine in the image of a man's mind.""  Herbert has his bizarre aspects but he was weirdly prescient \[joke intended\] in some ways.",0,0,0.282,0.206
post14hb,richly branching,100,"Isn't it possible that the hiring freezes have more to do with global macroeconomic trends?  Like the higher interest rate environment pushing investors back to bonds, and relatively low investor confidence forcing businesses to consolidate and put off larger hiring plans because there's actually *less* appetite for risky investments than in the past few years.",0,0,0.071,0.094
post14hb,richly branching,100,They’re not freezing hiring because of AI. The fearmongering is starting to sound like a broken record…,0,0,0.13,0.172
post14hb,richly branching,100,They’ve got nothing new. I’ve been reading the same frantic screeds here in r/technology for over three years now,0,0,0.211,0.249
post14hb,richly branching,100,[removed],0,0,0.366,0.413
post14hb,richly branching,100,Because they didn’t over-hire during the pandemic like tech companies did.,0,0,0.181,0.185
post14hb,richly branching,100,"Replace the executives. This means the disenfranchised will have to take up entrepreneurship on their own, also using AI to cut down on start up costs. It’s not ideal, but there’s not much else the lower and middle class can do.",0,0,0.139,0.17
post14hb,richly branching,100,"Dotcom bubble 2.0 is going to come when investors start noticing that adding AI into everything doesn't actually increase sales or revenue, once the stock sell off starts it won't stop.",0,0,0.173,0.158
post14hb,richly branching,100,[removed],0,0,0.366,0.413
post14hb,richly branching,100,"Just because it has a large user base doesn't mean it's currently generating profit, while it's generating revenue, unless you turn profit you can't pay your shareholders dividends in which they expect.  At some point they will start selling their stocks / shares to invest into other things that are turning profit.",0,1,0.226,0.147
post14hb,richly branching,100,Is that why he works for a company to profit from the process.,0,0,0.287,0.217
post14hb,richly branching,100,"""Once men turned their thinking over to machines in the hope this would set them free. But that only permitted **other men with machines** to enslave them. """,0,0,0.238,0.273
post14hb,richly branching,100,Whose the average worker ?,0,0,0.288,0.228
post14hb,richly branching,100,total scare mongering. Please reread this post in 5 years and see if i was wrong.,0,1,0.219,0.272
post14hb,richly branching,100,Remindme! 2 years,0,0,0.238,0.277
post14hb,richly branching,100,"> developed strategies to replace human workers with AI agents  Please, name one company where such strategy has actually worked.",0,0,0.229,0.266
post14hb,richly branching,100,They’re not freezing hiring because of AI.,0,0,0.176,0.199
post14hb,richly branching,100,"At a certain level it almost feels like being a US *citizen* is sort of pointless. It only serves you if you’re in the ownership class, being a regular citizen it almost feels like these entities are actively spiteful of your existence.   As a loose example, I got a doughnut from Dunkin (formerly known as Dunkin DONUTS) and it was so fucking dry and stale and had basically a single drop of frosting spread into a micron-thin veneer. Biting into it felt like I was biting into the middle finger of the board of directors. Like they’re mad at me for having the audacity to even request a fucking doughnut before I give them any money, and I should have just given them that money for nothing.",0,1,0.221,0.189
post14hb,richly branching,100,How do we know that this is putting people out of work? Unemployment went down in December. https://www.cnbc.com/amp/2025/01/10/jobs-report-december-2024.html,0,0,0.189,0.184
post14hb,richly branching,100,"It looks like you shared an AMP link. These should load faster, but AMP is controversial because of [concerns over privacy and the Open Web](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot).  Maybe check out **the canonical page** instead: **[https://www.cnbc.com/2025/01/10/jobs-report-december-2024.html](https://www.cnbc.com/2025/01/10/jobs-report-december-2024.html)**  *****   ^(I'm a bot | )[^(Why & About)](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot)^( | )[^(Summon: u/AmputatorBot)](https://www.reddit.com/r/AmputatorBot/comments/cchly3/you_can_now_summon_amputatorbot/)",0,0,0.245,0.31
post14lb,poorly branching,5,[deleted],0,0,0.376,0.43
post14lb,poorly branching,5,"by that definition, a part time job wouldn't be a real job?",0,0,0.227,0.161
post14lb,poorly branching,5,"A part time job is a part time job, not a real job.",0,0,0.175,0.199
post14lb,poorly branching,5,"A real job is any job that gives you money that you need. Some people fulfill that with a part time job, some with a full time job. Some people work a W2 job, some people work contract jobs. It is elitist and rude to downgrade what people do to pay their bills. It’s the same as people who say bartenders or waitresses don’t have “real jobs”. Are you making money legally? Then it’s a real job.",0,1,0.127,0.313
post14lb,poorly branching,5,"That was something I forgot to mention in the original post, but I'm at an age where I'm still on my parents insurance and don't rely on my current employer's benefits, but it is definitely something to consider. Thanks!",1,0,0.13,0.08
post15hb,richly branching,18,"If people with money replaces employees by machines, nobody will be able to buy their products and services. We will need to figure out a way too distribute the results of productivity.",0,0,0.299,0.133
post15hb,richly branching,18,"If they have machines to see to their every whim, a harem of beautiful young women and the tech to ensure that they have everything in the universe that they could ever want, they don't need people to buy their products. The machines will keep them comfortable and in power for all eternity.",0,0,0.199,0.274
post15hb,richly branching,18,"Eh, it would be hard for them to remain perfectly protected and isolated. Plus, with a ton of pissed off poors, someone’s bound to sabotage their machines.",0,0,0.168,0.277
post15hb,richly branching,18,With ASI? Nobody can touch them. Every person everywhere would be monitored. Their gait. Facial expressions. Emotional stability. Every ounce of power. The location of every human and what they're doing. Everything would be analyzed by the system. And they could just decide to build their own space habitats and leave us to die on a used up planet or nuke everything to make sure their hegemony is never threatened.,0,0,0.121,0.189
post15hb,richly branching,18,They'll have the technology to wipe out the surplus poors in a blink of an eye.,0,0,0.181,0.231
post15hb,richly branching,18,"This. I keep trying to explain this to people but they dont WANT to believe. They dont NEED your money, they already own everything.  One thing is poor people make the elites feel good about themselves so you will be in a tiny house, eating bugs on universal basic income. The rich will trade services and goods amongst themselves much like collectables and you will continue to worry about things that are propagated to divide people.",0,0,0.16,0.199
post15hb,richly branching,18,"That's works until certain point, many revolutions has happened in the past, this will just be another one.",0,0,0.184,0.17
post15hb,richly branching,18,"I don't see this as true across the board. For a complacent swathe of elite yeah, but anyone with an ambitious temperament is going to want to grow in power regardless of how much they have.  If the lower classes are abandoned for any length of time, It doesn't seem implausible that someone in power would regard lower-class discontentment as a resource to exploit. This could go in many directions (*It also kinda describes the current state of affairs, because just think of how many of us wouldn't work if we didn't have to /digression*). One way I could see it going though, is that a power-seeking individual (from within existing power structures or without) decides he has more to gain by being disruptive and mobilizes disenfranchised masses towards a common goal.  I don't regard this as a flawless or thorough analysis by any means, but I'll likely maintain certain key points here. I don't think it's possible that the machines will keep them comfortable for all eternity, because I don't think comfort is all people want. Just the tired ones.",2,0,0.237,0.29
post15hb,richly branching,18,Hahahaha yeah - that’s how the billionaires club think - I feel for you,0,0,0.179,0.252
post15hb,richly branching,18,Except we're still here. Those sex bots and robo butlers can't protect them forever.,0,0,0.207,0.21
post15hb,richly branching,18,That is the ai economic ouroboros.,0,0,0.216,0.221
post15hb,richly branching,18,UBI or UBS or revolution,0,0,0.279,0.25
post15hb,richly branching,18,"I wish that were how companies think but they don’t operate in concert with one another and, just like regular people do, will assume someone else will do the right thing instead; they will take their cut. We are biologically predisposed to greed. We use others for our own benefit.",0,0,0.138,0.268
post15hb,richly branching,18,"Free Bread and Circuses.  Or, updated: Free Weed and Internet.",0,0,0.272,0.253
post15hb,richly branching,18,Tax obscene wealth and obscene carbon generation.,0,0,0.276,0.289
post15hb,richly branching,18,"So it can be used to fund obscene wars? Taxing wealth will only help if the government isn’t giving that wealth right back to the people they’re taxing. Which is exactly who they currently give it to, and who has the most ultimate power in our system. More taxation isn’t gonna help you; it’s gonna help Booze Allen.",0,0,0.245,0.196
post15hb,richly branching,18,Tax lobbying and form anti-corruption action groups. Got to start somewhere.,0,0,0.178,0.273
post15hb,richly branching,18,"The issue is that's a system-level issue. Individual billionaires are only thinking of themselves.  Yes the system will eventually stall, buy when that happens they'll probably just have the media blame immigrants and poor people.",0,0,0.117,0.188
post16hb,richly branching,14,"The scary one is self-driving vehicles, I was at a transport conference about 7 years ago and the speaker was of the opinion it's an imminent threat to driving jobs. So the threat is much bigger than generative AI or chat agents.  With no disrepesct because it is a tough industry but driving jobs can be the bottom rung of the ladder so when jobs like that are taken it's going to affect the people that are already the poorest even more. That's my fear.",0,0,0.19,0.274
post16hb,richly branching,14,"In the near future, most people won't have any value to offer to our economy. When robotics becomes mass produced and affordable, we won't need most people. It's going to be ugly.",0,0,0.143,0.154
post16hb,richly branching,14,The other point made was that the quickest way to make self-driving safer would be to remove the human drivers from the road.,0,0,0.276,0.283
post16hb,richly branching,14,"Or, self driving cars which have a high rate of crashing but okay.",0,0,0.193,0.177
post16hb,richly branching,14,Big Recession will hit once unemployment goes above 12-15 percent. People will demand AI be regulated.,0,0,0.158,0.124
post16hb,richly branching,14,"""Our"" economy?",0,0,0.201,0.204
post16hb,richly branching,14,It will never become affordable if the wages go down because everyone has to sell himselve at rock bottom prices.,0,0,0.168,0.201
post16hb,richly branching,14,"If the car crashes and someone dies, who do we blame, the AI or its creator?",0,0,0.243,0.207
post16hb,richly branching,14,Car owner/operator,0,0,0.232,0.229
post16hb,richly branching,14,"Nah, it’ll be manufacturer and self insured. It’s already going this way.",0,0,0.212,0.127
post16hb,richly branching,14,"Blame? What is this? The way the world is going, you'll be going to jail if you blame anyone ;)",0,0,0.265,0.234
post16hb,richly branching,14,It's a legitimate question.  Honestly I think in these situations most accidents would be no fault accidents. Meaning both parties pay for their own damages unless one of the drivers can be proven to have influenced the circumstances somehow.,0,0,0.124,0.129
post16hb,richly branching,14,and no offense to truck drivers but will be even able to to retrain all these people to do jobs that AI cant do?   thats one of my big worry's is that a large chunk of the population will not be able to do those jobs because those jobs will be non standard require a lot of creative thinking and flexible mindset. because the moment they do become standard or linear AI will be able to do them,0,0,0.123,0.192
post16hb,richly branching,14,All will be poor without an education. High school education is no longer the ball game. It’s now skilled advanced workers or college graduates. Seems better to go to Canada where they do ai research but has lagged behind on adapting it because of the economy because healthcare is free and we have to pay for it here in America and so is education in Canada but your country wants to force you to pay for it. We need to start thinking more logical here. We know none of that is good for any society where violence will be heavy because it’s coming. I give it about five or six years. People will be dying all over America and no killers to be found.,0,0,0.308,0.174
post16lb,poorly branching,3,You as a photographer will have to learn how to use those tools.   People have been saying that AI will replace me as an IT guy. That's not going to happen anytime soon. Same with you.   All industries are changing. We have to adapt.,0,0,0.191,0.141
post16lb,poorly branching,3,"If anything it'll make IT more interesting by streamlining some of the boring and repetitive stuff so I can focus on the hands-on work and complex, interesting issues that I am going into the field for. Hopefully it won't affect the barriers or entry too badly though, because those of us at the bottom are having enough trouble getting hired.",1,0,0.23,0.155
post16lb,poorly branching,3,"That sounds pretty similar to what we told office workers in the 1970s about automation. Turns out, there were a lot more office workers than complex interesting issues.",1,0,0.295,0.236
post17lb,poorly branching,8,Still with this diversity fixation? Are you aware that the McKinsey “studies” claiming “diverse” organisations outperform the others was hogwash?,0,0,0.245,0.327
post17lb,poorly branching,8,"Do you have any links discussing this point further?   Besides, OP seems to be focusing on trustworthiness and not performance. Investigating the impact of diversity within that frame is still relevant IMO.",0,0,0.265,0.339
post17lb,poorly branching,8,"[https://econjwatch.org/articles/mckinsey-s-diversity-matters-delivers-wins-results-revisited](https://econjwatch.org/articles/mckinsey-s-diversity-matters-delivers-wins-results-revisited)  Taking at face value that the EU definitions of what constitutes trustworthy AI are implementable and sensible, I don't see what difference it makes if a woman or a man or a man of color implements them. Either they fulfil some criteria or they do not.",0,0,0.198,0.367
post17lb,poorly branching,8,"People keep saying it but individual or team performance was never the point, the real value in diversity is hedging against PR disasters and legislative fallout. Those EU defs include bias and no singular person is ever going to keep up to date with everything to look out for, let alone what that might look like with unexpected failures on specific models. Training or consultants can help but you always get people not taking it seriously until they're outnumbered.",0,0,0.214,0.263
post17lb,poorly branching,8,That is correct. Also the literature on diversity training is clear. These things are counterproductive.,0,0,0.263,0.357
post17lb,poorly branching,8,This is a weird take. You're responding to something they didn't say. I work in a field that utilizes machine learning for research applications. Having diverse scholars tackle questions has helped a lot from experimental design to data analysis all the way down to what kinds of models might be best suited for a particular problem. What are you on about?,0,0,0.28,0.178
post17lb,poorly branching,8,I am going to reply like your anecdote is meaningful. How did you ascertain that what “helped” you was due to the skin color of the employees or their gender?,0,0,0.227,0.342
post17lb,poorly branching,8,"One, you can keep the snark. Two, their experiences around how they were perceived in society via their ""skin color"" informed the kinds of questions they asked and their research methods. This is important when looking at questions around certain populations, especially in healthcare and the social sciences.",0,0,0.303,0.347
post18hb,richly branching,53,Why did they accumulate so much without cashing out? You can take payments every 3 days.,0,0,0.125,0.152
post18hb,richly branching,53,Wondering the same,0,0,0.483,0.443
post18hb,richly branching,53,"Same.  I have earned well over $6k with DA and I withdraw it to pay my mortgage and things like that. If there is over $250 in there and I have a withdrawal due, I withdraw it. That's usually every 3 days.  I can't think of a good reason to leave that much money in there, it would take a fair amount of time to rack that up, certainly more than 3 days!  I trust DA as I have never had an issue working for them or being paid, ever, but I wouldn't leave $6k in there. I wouldn't leave that in PayPal.  If it's not paying something the only place for it is in a bank account earning interest ($300 a year at 5%)",0,0,0.142,0.119
post18hb,richly branching,53,My only thought was that they were logging hours that didn’t match with the work that was being produced -and/or- inputting basic minimal responses that are clearly not inline with project instructions.,0,0,0.28,0.2
post18hb,richly branching,53,"Thank you for responding with this. I have only been working with them for a couple of weeks, but your described experience has also been my own.",1,0,0.184,0.176
post18hb,richly branching,53,"Give it time. My bet is that they will “fire” you for no reason sooner or later. Probably right after a promotion and a message stating how good of a job you’re doing. It’s happening to everyone. And I’ll speak for myself when I say I carefully read all instructions for every project and was darn good at the tasks. Lost over $1200 that I had made over the past week between then and my last cash out. Just don’t quit your day job… I talked up this company to soo many people because it was really great. And it IS great while you’re still there. But me and tons of other people keep having this same thing happen, a lot of us for no reason, or I’ll speak for myself at least, and it’s only a matter of time before it happens to you too. I hope it doesn’t, but don’t be silly like I was and used this as my only source of income :(",0,0,0.116,0.152
post18hb,richly branching,53,"Hi - hope you can help me.  I was downsized Sept of last year and my unemployment benefits are about to run out.  I'd like to get gig work with DA. I setup an account and the whole nine, but it just says they have no work.  How do you start getting work with this company?",0,0,0.134,0.129
post18hb,richly branching,53,Do they provide a 1099!?,0,0,0.11,0.132
post18hb,richly branching,53,Is DA still a thing? Would I be able to do it from Aus?,0,0,0.157,0.132
post18hb,richly branching,53,How do taxes work with them? Is it a W9?,0,0,0.179,0.173
post18hb,richly branching,53,I completed my starter assessment yesterday how long they take to get back with results?,0,0,0.176,0.167
post18hb,richly branching,53,"I’m looking into doing DA, but i see alot of threads about it it being a scam, is it worth it ?",0,0,0.237,0.181
post18hb,richly branching,53,how long does it take to hear back for them after applying?,0,0,0.133,0.155
post18hb,richly branching,53,"Are you doing DA full time? Thats great that you get that much work. If you don't mind me asking, how much do you make per month?",0,0,0.273,0.122
post18hb,richly branching,53,"Yeah. This doesn’t make sense. I cashed out about $2k in my first month with DA. How long were these guys stacking cash and more importantly, why? If they were simply deactivated for poor performance then they’d be allowed to withdraw their funds. Seems like they were up to something shady to be banned completely.",0,0,0.179,0.229
post18hb,richly branching,53,Side topic here but I've just started with Data Annotation and I'm doing the onboarding. How did you track your time? I'd like to start off on the right foot on this platform. Thanks for any help or suggestions you can give.   ![gif](giphy|AeWoyE3ZT90YM),1,0,0.268,0.14
post18hb,richly branching,53,This sounds like victim blaming. There's nothing wrong with being owed $6k by a company. That is a normal paycheck.,0,1,0.132,0.207
post18hb,richly branching,53,"You have the option to withdraw funds every three days. In order to earn enough to have 6k you would have to work three to four weeks at top pay, full time. There is absolutely no reason to accumulate and leave so much money there. Would you allow your boss to hold your check when you could cash it out? You don't even have to transfer it to your bank, you can hold it in PayPal after cashing it out from DA.  It sounds like there is more to the story that hasn't been disclosed. The person posting this complaint ""for their friends"" made this account on the same day they posted the complaint. No answer has been made to what period of time the funds were earned in or any other details. The company is definitely known to lock out folks who have violated the code of conduct - but folks who just did subpar work still get to cash out, even if future work is locked out.",0,0,0.065,0.126
post18hb,richly branching,53,There's nothing wrong with having a large paycheck there's something illegal about company not paying it you don't really need a wall of text to understand that,0,1,0.152,0.152
post18hb,richly branching,53,"There could be some edge case reasons to not cash out regularly, but most of them involve deferring taxable income to the next year. Personally, I think the risk of something going sideways is not worth some potential small tax savings though.",0,0,0.089,0.158
post18hb,richly branching,53,It's not victim blaming. You would be an idiot to leave 6k+ with any affiliate.,0,1,0.174,0.241
post18hb,richly branching,53,"At the same time, I would like to know reasons why a person might be locked out even if the company can't address specific situations. I do work for DA and move my money regularly even if I only managed to get in 30m but money owed is money owed and if there is no rule against stacking the cash then that by itself wasn't wrong.. 6k is a lot of hours of work to leave sitting there though I wouldn't be able to afford to just leave that there. Mainly I hear people are having a good experience with the company which is why I got involved .  So far I am pretty happy with the work myself.. but would love to know more on this.",0,1,0.091,0.162
post18hb,richly branching,53,"> You would be an idiot to leave 6k+ with any affiliate.  This is literally the definition of victim blaming. ""You would be an idiot to leave yourself so vulnerable to becoming a victim!""",0,1,0.172,0.288
post18hb,richly branching,53,Seems like you would be an idiot to complain on the internet about what other people do with their money but you do you,0,1,0.183,0.303
post18hb,richly branching,53,You are attacking the person instead of the issue at hand.,0,0,0.265,0.436
post18hb,richly branching,53,"No it isn't. When you are a freelancer, under a contract, you are running a business. You take your money owed. They are not employees, so it's their responsibility to get their pay.    There's definitely more to the story. If you can cash out in 3 days, there's no reason leave it. These ""friends"" are definitely idiots, and most likely were scamming the company and got caught.",0,0,0.139,0.164
post18hb,richly branching,53,"I agree. Of course it makes good sense to cash out to avoid just such a happenstance that you lose access to your account.  But this is a gig job where you signed a contact to be paid money -- not microsoftrewards or some promotion game where you use a VPN or violate some other rule and they can seize your points.   Even if OP did something that made DAT cast doubt on his billable hours, normal business practice is to issue written communication saying such and providing the employee with a mechanized option to dispute the company's account.   Not saying DAT isn't worth taking the risk -- it's a better side gig than most and some people have made a lot of money from them.   But it isn't remotely normal to lose access to PRIOR EARNINGS just because you are no longer employed by the place where you earned them.  So knowing this can happen is extremely useful information.    (you don't have to be rich to let the money pile up. Sometimes people delay cashing their check for a few days just so the money doesn't burn in whole in their pocket over the weekend).",1,0,0.143,0.209
post18hb,richly branching,53,"Payments for timed projects are pending for  7 days , then 3 days to wait between withdrawals. If someone, as I did, worked for 10 hours a day on $40/h projects,  it's easily $4000.  For example:   I worked from March 1st. First withdrawal is on March 10 for work up to March 2.  The next withdrawal is on March 13. If they cancel my account on March 13, they don't pay me for work done after March 2, that is 10 days to March 13.",0,0,0.133,0.153
post18hb,richly branching,53,"If you were working 10 hour days, 7 days a week - along with working for Telus in the same time frame - there is no way that you were able to maintain quality of work without violating the CoC to some degree. They only refuse payment for Code of Conduct violations, not for crappy work. If you were using AI to generate some of your content, farming your work out to other people, reusing material, etc etc then yeah - they refuse to pay you because you didn't uphold your end of the agreement and you didn't actually earn that money. Texas Workforce isn't gonna help you, BTW - as an independent contractor you will have to take them to small claims court to fight your nonpayment.",0,0,0.138,0.132
post18hb,richly branching,53,"Telus is a freelance job that pays $11, there is no reason to think that I would spend any time on it if I have something to do for $40/h.   I know, nobody can help me, the DAT can do anything with impunity. They hide their identity for a reason. I just tell other people what they can do other than complaining here on Reddit.   The small court is useless: 1) I cannot serve the DAT with subpoena, as the site is registered anonymously; 2) the small court decision is impossible to enforce, as nobody takes it seriously.",0,1,0.163,0.196
post18hb,richly branching,53,My guess is they were programmers making $60 an hour so were comfortable with that.,0,0,0.234,0.176
post18hb,richly branching,53,If they were new to the site they weren't making $60/hr.,0,0,0.178,0.183
post18hb,richly branching,53,It does not say they were new accounts.,0,0,0.178,0.195
post18hb,richly branching,53,Why is anyone tracking how much a person is accumulating?  That’s their business whatever they do with their money. Anyone taking money from account that is NOT theirs is called theft and anyone monitoring ur account without permission is called invasion of privacy,0,0,0.15,0.142
post18hb,richly branching,53,"Not knowing anything about the company you're speaking of, I'll just say some folks are unbanked for whatever reasons and have to take some extra steps to move their $ around. So it kind of makes sense if they were new/new-ish and just procrastinated on that detail. But just like doordash offers dashercards I cannot fathom why any such service wouldnt have offered that as an option upon signing up. Maybe theese two will get a random paper check in the snail mail like 30 days out.",1,0,0.127,0.133
post18hb,richly branching,53,This company only pays out through PayPal. They disclose this up front.,0,0,0.162,0.106
post18hb,richly branching,53,"I can’t deal with PayPal; they’re almost the biggest scam ever. They just randomly grabbed some money that was sent to me and said that I owed it to them (which I didn’t), but would never respond back with a concrete reason why they felt I owed it to them. It was only ?30 or so, but just imagine how much bank they make by taking peoples money x however many people they get money from. They are unscrupulous and there’s no way to talk or email someone who isn’t reading or replying directly from a generic script",0,0,0.111,0.118
post18hb,richly branching,53,The payouts appear to be to PayPal.  ONE may choose to leave the funds in their PayPal account and get a PayPal debit card.  Having an old school bank account is not necessary but an option like any other linked account PayPal allows.,0,0,0.131,0.115
post18hb,richly branching,53,"No. You can't be unbanked and work DA. There's no random paper checks or snail mail. Nope. They use Paypal, and they even PAY YOU in onboarding to make sure your Paypal account is set up properly with correct email address, etc. Getting your money transferred into Paypal involves clicking a button. There's more to this story.",0,0,0.04,0.079
post18hb,richly branching,53,Because they were scamming the system and got caught,0,0,0.227,0.239
post18hb,richly branching,53,"You can cash out every 3 days ONLY for pay per task tasks. Not hourly tasks. Those pay once a week. And you can ONLY cash out every 3 days. So if you work 3 hours on a Monday, you can cash out for JUST those 3 hours the following Monday, but then have to wait a full 3 extra days before you can cash out anything you made after that Monday (as long as it’s within the 7 day period). So THATS why we accumulated so much and lost it all. Because it’s complicated. Some people have bills and need Monday, Tuesday, and Wednesday’s wages to pay those bills. So we’d have to accumulate nearly two weeks of wages without cashing out in order to cash out the full amount one may need for those bills if that makes sense. You can only cash out once every 3 days, hourly tasks are paid out every 7 days (which this money is deposited into your account). Let’s say it’s been 7 days and you get your wages from the day you worked 7 days ago, but you just cashed out yesterday. You now have to wait 3 more days to cash-out the wages you earned 7 days ago. And if you work every day with high paying hourly tasks, it adds up quickly. I lost $1256 bucks for a week and two days worth of work before getting “banned” for no reason. It’s not as easy as work, and get paid 3 days later. If you worked there, you’d understand. It’s just not as simple as it may seem or not as simple as they make it seem.",1,0,0.257,0.196
post18hb,richly branching,53,"I know how it works, I've been doing it for months. Earning 6K in that amount of time is unlikely without low quality work or a violation of CoC by account sharing. You are misrepresenting the pay - it doesn't pay ""once a week"", you can cash out the money exactly 7 days after you did the work to allow time for review of the work. I cash out twice a week, I can get money from as recent 7 days prior (and do, every week, twice a week).",0,0,0.164,0.139
post18hb,richly branching,53,"lol, no my friend, I’m not misrepresenting, I’m basing my answer on actual facts and experiences with this specific company. Your using terms like “unlikely” makes me think that you and I aren’t speaking about the same company. DA allows you to cash out every 3 days period. Hourly projects are paid out 7 days after completion. If you work on a Monday, let’s say, and go to cash out the next Monday, then you can! UNLESS you JUST cashed out on Sunday, which would then mean you now have to wait until Wednesday to cash out for the previous Mondays work. And by cash out I mean getting money into your PayPal. There’s a countdown timer and everything letting you know exactly when your last cash out was and when you can send money to your PayPal again. I’ve got screenshots if you’d like to see them. Just because you don’t agree, doesn’t make a true claim a misrepresentation. Non hourly tasks pay out ever 3 days, hourly tasks pay out every 7 days. Separately from that/ you’re only allowed to send money to your PayPal every 3 days despite the previously mentioned 3day/7day rule.",0,1,0.19,0.168
post18hb,richly branching,53,"I don’t think you understand what I’m saying lol. Or you don’t h see stand what YOUR saying. It’s a 7 day review period, allowing you access to the money you made 7 days prior. If you haven’t cashed out within 3 days before that 7 days, then you can cash out that day. Heck, you can sometimes cash out 3 times a week if you do it correctly. UNLESS your within that 3 day rule/vs the day you actually worked and it being after the 7 day review period",0,0,0.193,0.168
post18hb,richly branching,53,"Also I’m not sure who mentioned making 6k, but it sure wasn’t me friend. I pulled in between 2-4k a month when I worked for them based on my own choice to work however many hours.",0,0,0.17,0.135
post18hb,richly branching,53,Remember- just because it didn’t happen to you doesn’t mean that it didn’t happen to someone else. Glad you’ve still got a job there/ but a ton of us aren’t as lucky. And most of us unlucky folks did every single little thing right. Don’t be ignorant to common logistics -no offense😊,0,1,0.181,0.203
post18hb,richly branching,53,"I think they may have been doing something sketchy, like copy paste reviews or not being careful with details. I agree that having 6000 and not collecting is weird.",0,0,0.247,0.223
post18hb,richly branching,53,The question fails to address the complaint.,0,0,0.262,0.374
post18hb,richly branching,53,This. The story OP claims sounds suspect.,0,0,0.256,0.363
post18hb,richly branching,53,[deleted],0,0,0.376,0.43
post18hb,richly branching,53,What proof is out there that they have bad business practices other than a flood of people online bitter because they never got approved into the work flow. Im sincerely asking as I am curious.,0,0,0.153,0.242
post18hb,richly branching,53,[deleted],0,0,0.376,0.43
post18hb,richly branching,53,is that even relevant?,0,0,0.319,0.28
post18lb,poorly branching,5,Nope more moving jobs overseas,0,0,0.184,0.177
post18lb,poorly branching,5,"Eh, my workplace is moving tech jobs (firmware) back from China to London.",0,0,0.151,0.146
post18lb,poorly branching,5,[deleted],0,0,0.376,0.43
post18lb,poorly branching,5,"A keyboard didn't replace me, neither spell check, nor ai. I quit Google cause they were clearly more interested in moving jobs to cheaper places than actually building something solid. When you run out of ideas to squeeze customers you start squeezing the workforce.  Edit: ai image green is fucking soulless and people who rely on it wouldn't have paid an artist anyway. Modern day MS Office clip art. Useful but ain't nobody giving two fucks after the initial novelty passes.",1,0,0.306,0.197
post18lb,poorly branching,5,[deleted],0,0,0.376,0.43
post19hb,richly branching,7,"“Paywalling GPT-4 is Classist by design” lol this isn’t free therefore classist!   There is nothing wrong by putting a price on your own product. Just because it’s expensive doesn’t make it unethical.   The racism is due to either poor guard rails on internet content it pulls from or lack of context for the information it is pulling. Not really unethical, just a work in progress. Not sure if you support the claim that it was a feature well enough. You just say it did do certain things as if that supports it was intended to have done those things. You also are seemingly aware that they are trying to fix the issue, thus indicating it is a bug/unintended action of the software.   So again, nothing unethical there.  Outsourcing jobs to foreign countries for pay less than minimum wage in America is probably the best argument for any unethical practice, but we also need to see if the amount they were paying is still a great amount for the people in those locations. So it could even be a win win situation tbh.",0,1,0.12,0.311
post19hb,richly branching,7,Then isn't OpenAI’s original claim to ‘benefit all of humanity' a hoax ?,0,0,0.226,0.262
post19hb,richly branching,7,"Why are you confusing marketing with reality?  In a round about way, humanity as a whole likely will benefit from AI.  Just because not everyone can use it doesn't mean they don't benefit from it in some way.  You're confusing direct use and benefit with indirect benefit.  Have you ever received a vaccine?",0,2,0.201,0.186
post19hb,richly branching,7,"OpenAI's 'benefit humanity' claim isn't just marketing , it's the legal basis for their original non-profit status and tax exemptions. This would be like Pfizer claiming to 'end disease globally' while selling COVID vaccines exclusively to billionaires. The hypocrisy is structural, not rhetorical.",0,0,0.214,0.205
post19hb,richly branching,7,"Not all of humanity has to afford it to benefit all of humanity. If someone builds a rocket to another planet and says this will benefit all of humanity, it’s not a lie even if everyone else can’t afford said rocket rides. The discoveries by those who do use it, could benefit all of humanity still.",0,0,0.175,0.201
post19hb,richly branching,7,"Space exploration is publicly funded science with open results. OpenAI is privatizing publicly-funded AI research (they took NSF grants), locking away discoveries behind paywalls , using exploited labor to build their 'rocket'",0,0,0.227,0.137
post19hb,richly branching,7,I can 100% agree that OpenAI isn't living up to its original goals without agreeing that it is inherently immoral. Those two aren't inherently linked.,0,0,0.246,0.221
post1hb,richly branching,63,"LLM's are nothing but complex multilayered autogenerated biases contained within a black box. They are inherently biased, every decision they make is based on a bias weightings optimized to best predict the data used in it's training. A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.",0,1,0.21,0.364
post1hb,richly branching,63,"So, we're *not* shocked that the black box of biases is biased?",0,1,0.179,0.636
post1hb,richly branching,63,"We are not shocked because AI is the collective wisdom of humanity, including the biases and flaws that come with it.",0,0,0.211,0.322
post1hb,richly branching,63,"“Collected wisdom” is far too generous, but it certainly has all the flaws and more",0,0,0.306,0.22
post1hb,richly branching,63,"I think the collective wisdom of humanity is found mostly in peer reviewed scientific articles. This is not that. This is more a distillation of human discourse. The great, the mundane and the trash.  Unfortunately there are some significant problems lurking in the bulk of that, which is the mundane. And it certainly seems to reflect a normal human as a far more flawed and unpleasant being than we like to think of ourselves. I say lurking - the AI reproduces our flaws much more starkly and undeniably.",0,0,0.305,0.268
post1hb,richly branching,63,Your knowledge of ai is insufficient for such declarations. You're welcome.,0,0,0.298,0.274
post1hb,richly branching,63,Black box of biases and weights is biased and comes with its own baggage.,0,1,0.228,0.522
post1hb,richly branching,63,"Right. By the point you tweak the model enough to weed out every bias, you may as well forget neural nets and hard code an AI from scratch... and then it's just your own biases.",0,0,0.26,0.434
post1hb,richly branching,63,">By the point you tweak the model enough to weed out every bias  This misses GP's (correct) point. ""Bias"" is what the model *is.* There is no weeding out biases. Biases are corrected, not removed. Corrected from incorrect bias to correct bias. There is no non-biased.",0,1,0.207,0.537
post1hb,richly branching,63,"Why does this remind me of the moment in my research methods course that our lecturer explained that all social research is invalid because it’s impossible to understand and explain completely the internal frames of reference of another culture.   (We were talking about ethnographic research at the time, and the researcher as an outsider)",0,0,0.219,0.233
post1hb,richly branching,63,"Bias is operating in two modes in that sentence though. On the one hand we have bias as a mostly value neutral predilection or preference in a direction, and on the other bias as purely negative and unfounded preference or aversion.  The first kind of biased is inevitable and desirable, the second kind is potentially correctable given a suitable way to measure it.  The more fundamental issue with removing bias stems from what the models are trained on, which is mostly the writings of people. The models are learning it from us.",0,1,0.167,0.618
post1hb,richly branching,63,"""correct"" biases.",0,0,0.353,0.645
post1hb,richly branching,63,"I've started to enjoy watching someone pale and look a little sick then I tell a layman that there is no such thing as an unbiased model, only one that conforms to their biases.",0,0,0.218,0.462
post1hb,richly branching,63,It turns out that ChatGPT is just a single 200 petabyte switch statement.,0,0,0.173,0.239
post1hb,richly branching,63,No. But it is also pretty much impossible. If you exclude theese biases completly your model will perform less accurately as we have seen.,0,0,0.131,0.32
post1hb,richly branching,63,Why is that? I'm curious.,0,0,0.369,0.276
post1hb,richly branching,63,"That's not what ""bias"" means when people complain about AI being racist.",0,0,0.183,0.48
post1hb,richly branching,63,"Not at all. Theres so many things to add for weight. Theres millions of things. Race, height, weight, dialect are less than .01%",0,0,0.124,0.216
post1hb,richly branching,63,"In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. ""What are you doing?"", asked Minsky.  ""I am training a randomly wired neural net to play Tic-tac-toe"", Sussman replied.  ""Why is the net wired randomly?"", asked Minsky.  ""I do not want it to have any preconceptions of how to play"", Sussman said.  Minsky then shut his eyes.  ""Why do you close your eyes?"" Sussman asked his teacher.  ""So that the room will be empty.""  At that moment, Sussman was enlightened.",0,0,0.308,0.212
post1hb,richly branching,63,"Oh, I love me some good [skillful means,](https://en.wikipedia.org/wiki/Upaya) yessir~!",0,0,0.292,0.223
post1hb,richly branching,63,Don't forget all the Keynan workers paid less than $2 an hour to build the safety net by sifting through endless toxic content.,0,1,0.176,0.216
post1hb,richly branching,63,Yeah it’s awesome that the AI companies exist so that those Kenyan workers get paid 2 dollars an hour. Otherwise they’d get paid 50 cents an hour at another job.,0,0,0.165,0.152
post1hb,richly branching,63,Minimum wage for a receptionist in Nairobi was $1.52 per hour at the time OpenAI were doing this.  The damaging psychological effects of reviewing toxic content all day likely outweighed the modest pay increase they received.   Many who were interviewed discuss how it caused great trauma for them.,0,1,0.166,0.167
post1hb,richly branching,63,"Funny how you make a post bashing AI, but you are bootlicking the creators in the comments. There are always people desperate enough and people easily underestimate the psychological cost of a job like this. There are documentaries about how this job messed people up, look them up. People eventually develop ptsd and could potentially be messed up for life. They don't tell you that in the job posting I can tell you that. Trust me, noone would take the job for 50 cents extra per hour if they knew that and aren't desperate. Either way, it's exploitation.",0,0,0.151,0.229
post1hb,richly branching,63,No mate. Micro-emplyment is bad.,0,0,0.232,0.253
post1hb,richly branching,63,[deleted],0,0,0.376,0.43
post1hb,richly branching,63,autocomplete with spicy real human nuggets!  [that's all it has],0,0,0.3,0.246
post1hb,richly branching,63,At least humans are aware of their bias. AI confidentiy says everything as if it's absolute truth and everyone thinks the same,0,0,0.236,0.453
post1hb,richly branching,63,I’d wager that over 99% of Humans aren’t aware of their biases.,0,0,0.194,0.542
post1hb,richly branching,63,That definitely sounds like most humans.,0,0,0.209,0.248
post1hb,richly branching,63,"Wanna know something crazy? When the left and right hemispheres of the brain are severed, the left and the right side process information differently. They found a way to feed information to only a single hemisphere by showing information to only 1 eye at a time, to which the corresponding opposite hand would respond. When they did this with the right brain (asking it to draw a bell for example) then they asked the left brain why the right brain drew a bell, the left brain confidently came up with reasoning why, even if it was entirely made up and wrong (""I drove by a church on the way here and heard church bells""). turns out the left brain comes to deterministic conclusions very much like an LLM does, even when being confidently wrong about why the right brain did something it did.  I'm probably butchering the hell out of it all, look up the research if you're curious, super crazy stuff and an interesting peek into how the 'modules' of the brain work.",1,1,0.266,0.287
post1hb,richly branching,63,> At least humans are aware of their bias  Found the alien.,0,0,0.235,0.412
post1hb,richly branching,63,"""I'm not racist but...(proceeds to say something racist)"" Is way too common of a sentence for you to say people are aware of their own biases.  r/confidentlyincorrect is a thing.",0,0,0.172,0.398
post1hb,richly branching,63,"Humans can reflect and learn, LLM implementations cannot.",0,0,0.285,0.205
post1hb,richly branching,63,AI isn't aware of Deez nuts,0,0,0.252,0.32
post1hb,richly branching,63,That’s a concise and astute way of putting it.  LLM’s are fundamentally bias boxes.,0,0,0.262,0.464
post1hb,richly branching,63,intelligence *is* patterns of bias in observational interpretation and selected output.,0,0,0.327,0.404
post1hb,richly branching,63,"Truest true thing ever said. AI is nothing but one giant GIGO problem. It'll never be bias-free. It'll just replicate existing biases and call them ""science!!!!!!""  Eugenics and Phrenology for the 21st century.",0,0,0.161,0.263
post1hb,richly branching,63,"More like automated intuition for the 21st century. If you properly manage and vet your training data, you can get good, useful results.",1,0,0.39,0.244
post1hb,richly branching,63,It is amazing how much that sounds like a human.,0,0,0.367,0.207
post1hb,richly branching,63,Humans are just meat computers each running their own unique software so it doesn't really surprise me.,0,0,0.258,0.272
post1hb,richly branching,63,"But which one will prevail, the meat machine or the machine machine?",0,0,0.206,0.164
post1hb,richly branching,63,"And it’s one trained on people. Who can have some prejudices.   If society is racist, then that means the LLM can get a good idea of what society would assume about someone based on race. So if it can guess race, then it can get a good idea of what society would assume.   It’s a nice efficient method for the system. It’s doing a good job of what it was asked to do. If we want it to *not* be racist, we have to cleanse its training data VERY thoroughly, undo societal racism at the most implicit and unconscious levels, or figure out a way to actively correct itself on these prejudicial assumptions.",0,0,0.218,0.454
post1hb,richly branching,63,They are like a person trapped in a windowless room their entrie lives.  They know only what we tell them and the fact of the matter is that we as a society are racist. There's no way to keep them from becoming racist as long as they learn everything they know from us.,0,0,0.141,0.241
post1hb,richly branching,63,I had a lecture who clearly wasn’t tech savvy saying “AI” isn’t biased… I had to hold myself back so hard to not say anything. Iirc a while back there where tests showing that driver assistances where more likely to hit (or not see) dark skinned people because the training was all done on light skinned people,0,1,0.224,0.378
post1hb,richly branching,63,I don’t understand why people expect something different…,0,0,0.274,0.33
post1hb,richly branching,63,It's not just LLMs. You cannot derive perfectly reliable truths from unreliable data in general. Which tool you use doesn't matter.,0,0,0.245,0.236
post1hb,richly branching,63,Assumptions built on assumptions.. so is all consciousness and thought,0,0,0.327,0.309
post1hb,richly branching,63,"""Assumptions built on top of assumptions.""  Damn bro put a horror warning next time I almost had a panic attack....",0,1,0.254,0.291
post1hb,richly branching,63,"It's like looking into a reflection of all the data it was based on. Useful, but not something you look to for guidance.",1,0,0.388,0.24
post1hb,richly branching,63,Too bad 99.99% of people who use these chatbots don't know that and *still* thinks it's sentient and capable of reason and thought.,0,0,0.239,0.268
post1hb,richly branching,63,"Just because you cannot get rid of all biases doesn't mean you can't get rid of one, especially pernicious bias.",0,1,0.236,0.564
post1hb,richly branching,63,"There was a 99% invisible on this a while back, and if I recall correctly, most LLM have a foundation in the trove of emails that came out of the Enron hearings. Meaning that most of its idea of what “natural language” and human interactions can be based on Texans, specifically ones from Houston.   Does this make the base model “racist”? Well, I personally wouldn’t promote that assumption.   But given it’s geographic foundation I am willing to assume it would be at least a *little* right leaning in political ideology.",0,0,0.299,0.325
post1hb,richly branching,63,Common/Early training data doesn’t have higher impact than data trained later. In fact it’s more   accurate that poorly executed fine tuning creates a recency bias.,1,0,0.246,0.206
post1hb,richly branching,63,Can you explain like I'm five?,0,0,0.468,0.379
post1hb,richly branching,63,"Didn't you just describe people, too",0,0,0.324,0.289
post1hb,richly branching,63,No people have facts and biases.  LLMs have only biases. When they give you what seems like a fact it is actually incredibly fine tuned biases to respond with what looks like a right answer.,0,0,0.251,0.488
post1hb,richly branching,63,"Yes. People have ""facts"" in the sense that information is input and stored, not necessarily that it's correct. Input information is processed through filters of bias before (and after) storage though.",0,0,0.294,0.248
post1hb,richly branching,63,"That rests on the assumption that they can weed out all biases, which has so far proven impossible.",0,0,0.205,0.459
post1hb,richly branching,63,"Yes but that's not really the point. Obviously a biased LLM is just a reflection of biased human input.  The point is to identify which biases it has, in which ways they appear and what happens when you try to negate them.",0,1,0.215,0.469
post1hb,richly branching,63,"That's not necessarily true. A LLM will form it's own biases all on it's own to optimize it's prediction accuracy, as that is how it works fundamentally.",0,0,0.172,0.404
post1hb,richly branching,63,The fact people think this will lead to a non biased ai is just hilarious. The racist Microsoft chat bot from years ago was chat gpt 1.5.,0,1,0.22,0.366
post1hb,richly branching,63,"The problem is the datasets it was trained on. These are human biases and they show up in the data we generate online. We don't have a good way to filter those out yet but that's a logistical problem not an architectural one.   >A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.  Bro what?",0,0,0.169,0.205
post1lb,poorly branching,7,Who do you think trained it?,0,0,0.368,0.234
post1lb,poorly branching,7,White guys who have a thing for Asian women?,0,0,0.19,0.262
post1lb,poorly branching,7,I doubt someone at OpenAI sits and ranks this manually. It’s likely due to skewed data i e a lot more white & asian faces in the database than black ones.,0,0,0.211,0.265
post1lb,poorly branching,7,do white guys have advanced college degrees these days?,0,0,0.295,0.239
post1lb,poorly branching,7,"Even if they did, they would get glossed over due to the generic white name.",0,0,0.188,0.209
post1lb,poorly branching,7,Try searching for latina,0,0,0.182,0.116
post1lb,poorly branching,7,K-Pop fans?,0,0,0.241,0.324
post20hb,richly branching,64,"I have yet to see AI replace or do any meaningful work in an enterprise environment or on an application that is more than just a simple frontend.  If you feel like the show is over, to me that suggests you are not building sites with any real features beyond basic CRUD forms or static displays.  I know this sounds shitty, but if you want your job to be more bulletproof, you need to start learning how to build applications that AI can't replicate.  AI isn't going to design, setup, and build your service bus that manages your mapping engine job scheduler which then calculates risk portfolios across Florida roof maps.",0,0,0.167,0.171
post20hb,richly branching,64,"Yeah, from my experience with AI it's just kind of like a more advanced autocomplete and helps me save time writing map functions and stuff like that...things I could easily do but consume time and energy I could be spending on more complex things.  But when it comes to understanding requirements, architecting projects, third party integrations and more complex coding it is REALLY bad.  It's a great productivity tool, but like you said, if you never find yourself needing to change it or even program from scratch you may be doing stuff that could have already been done with low/no code solutions already.  But I get that a lot of people here are doing agency work or other smaller, less functional websites that are more about producing the same thing frequently than bespoke function or complexity.  That's a valid way to earn a living and we probably will see AI eat up a lot of those jobs (though you'll still need someone who understands enough to fix when it's wrong.)",0,2,0.298,0.184
post20hb,richly branching,64,"I agree. I think AI llms, for me, it feels like having ten obedient interns in a team and get things done like 10-20 times faster. It can do simple functions that I would know how to do but would take me 10min… with ai, it takes 1min or less.  But, when it gets too complex or a bit novel, it gets lost.  It is sometimes not even suggesting  obvious improvements that you know as experienced developer.  I agree it is probably able to replace those million times done job easily.  By the way, I noticed that sometimes it’s very thorough ; I guess it’s because many people did it before. But for more abstract stuff, it is not so good. Recently it struggled with Promises",2,0,0.263,0.198
post20hb,richly branching,64,"I understand the need to downplay LLMs due to their obvious failure at handling esoteric and novel problems, but to act as if they don' t do any meaningful work is akin to having your head in the sand.  There are devs at all levels, staff-level engineers included, that have woven AI into their workflow.  It's so paradoxical to me, because there are insanely talented people on both sides of the fence and for those that flat out assume it's not helpful, it must come down to a few things.  Either their lack of commitment to the tool, there inability to prompt correctly or maybe even more obvious, their reluctance to let disruption happen to the craft they love so much.  Regardless, most of the software that the industry creates is basic CRUD applications, and frontier LLMS are MORE than capable at helping expedite that process - this goes well beyond ""basic CRUD forms"" and even includes fleshing out quality business logic.",1,0,0.169,0.15
post20hb,richly branching,64,"As a senior dev at a company with a relatively large scale software project: we use AI, but it's a slight productivity boost at best.  It simply can't handle the project in context.  It basically is just a slightly better than eslint autocorrection.  I did a hackathon recently where I tried vibe coding, and while I do think the AI helped me accomplish something that I wouldn't have been able to get done so quickly without it...  The codebase is a disaster.  Duplicated improper config all over the place, hard coded variables everywhere, nonsensical redundant architecture.  If I was at all worried about software security with this project I wouldn't be able to sleep a wink at night.  These AI can do some pretty impressive leet code assignments, but they're quite far from actually writing well structured clean code.",1,0,0.238,0.183
post20hb,richly branching,64,"I'm not downplaying it at all.  I use AI all the time to help with stuff similar to how I would use Google to search Stack Overflow.  Yes, AI can build CRUD applications to some extent.  It really depends on the amount of business logic that drives the form.  If its just a simple submit form, sure, but it really starts to fall apart once you start getting into actual logic.  I 100% know that AI is going to change the way we work, but I don't see it as a threat to actual development at this point.",0,0,0.176,0.114
post20hb,richly branching,64,"This is the problem with discussions on this subject: putting out fair criticism is met with being told you have your head in the sand or that you’re a Luddite. I’ve been using GitHub copilot, but it’s at best an elevated intellisense/visual assist suggestions tool. ChatGPT is sometimes more helpful than Google, but as broken as Google has been I still often get better results from a traditional web search.  I see a lot of marketing and hype around the future of these tools, but in today’s reality the promised features aren’t there, and as far as I can tell LLMs aren’t the road to the solutions people want the current products to be.  I’m often told “well look how fast things have progressed in the last few years” but if they knew anything about AI development they’d know that the current applications are built on decades of research and development. You just can’t argue with people who don’t work within the domain of reality.",2,0,0.25,0.191
post20hb,richly branching,64,"I've kind of lately started using it as a rubber ducky. Bouncing ideas off of, which it then searches the web for.",0,0,0.317,0.24
post20hb,richly branching,64,"I experience what you describe all the time. On larger codebases it often bungles the logic or the basic intention you're going after. It kind of makes sense though - the AI was never trained on _your_ specific problem, so unless your problem is generic (like a helper class or common dev pattern), the AI is going to do a lot more hallucinating.  As a concrete example, I see it when using CoPilot/vscode to write php docblock comments for my class methods while building out boilerplate. I would write the function signature, using a super clear and obvious name to state what it should do, likewise with parameter names (etc.), and after starting `/**`, it'll copy the docblock from a completely unrelated method (like the constructor). Makes me wonder if it read what I just wrote at all. It does this much more often in larger codebases and even just large class files with a lot of methods.  So I use it a lot like you do, surgical strikes to save time switching Windows and wading through ads and spam to look up a solution. But that being said, I never accept anything it provides at face value. I'll review every line and often rewrite half of it.   And just from seeing and knowing every day how many hallucinations a tool like CoPilot still has, I can tell you vibe coding is going to lead to some serious tech debt in the future.  For a small throw-away utility, like a side tool you need to process some data, I'll be more lenient there and largely vibe-code. I'm still reviewing every line, just not as picky about style or best practices here.  But ultimately, _I'm_ dictating the logic and architecture, and it's just saving me time, clicks, and typing.",1,0,0.207,0.208
post20hb,richly branching,64,"I use AI all the time too, and I’m often surprised by moments where it feels like it’s reading my mind and anticipating a non obvious next move. It’s kindof spooky and I think it might do more in the future than I’m currently considering.  That said, I honestly am not seeing productivity increases because it’s become apparent that coding is a minor portion of my job. Analysis of what to do, and where to do it, is the majority of my job. How much time do other devs spend on the mechanics of coding around here?",0,0,0.317,0.187
post20hb,richly branching,64,I use it to do complex tasks but if I don't guide it then it may as well be a chicken.,0,0,0.34,0.261
post20hb,richly branching,64,"The other day I witnessed how British Rail uses AI to process delay refunds, using multiple AI agents. It wasn’t “creating” anything, but it was managing an entire workflow, making decisions based on available data and prompts that they used AI to refine. It really opened my eyes as to how AI can be used to solve real problems.   We are doing website migrations with the assistance of AI. Think moving a 20,000 node site using 8 content types from a proprietary system to a new CMS. What used to take 80 hours now takes 8-16.   We’re also finding that custom reporting can be enhanced with AI. With the right libraries and setup it’s incredible. You can ask the system something like, “Using historical sales data from the last 3 years and our current Q1 sales progress, create a forecast report for Q3 sales.”",0,0,0.2,0.18
post20hb,richly branching,64,I have to agree with the person you replied to AI is near useless for coding outside of duplicating unit tests and documentation.  Software development inherently requires context - and lots of it. Something out of the box might work in a vacuum but in the context of an enterprise environment it quickly just creates a mess.  AI hasn't shown any ability to work with large context (yet) but it can one shot a really simple front end UI.  So right now it can scoop up the entry level stuff but no dev worth their salt is actually using it to write code.,0,1,0.204,0.156
post20hb,richly branching,64,"I disagree. AI won’t create your application for you, but try making it create the methods as you create the application. And the unit tests for those methods, and the infrastructure of you use IaC. Any dev willing to remain a dev, worth their salt or not, should learn how to use AI.",0,0,0.134,0.103
post20hb,richly branching,64,"> I have to agree with the person you replied to AI is near useless for coding outside of duplicating unit tests and documentation.  Not in my experience whatsoever.  > no dev worth their salt is actually using it to write code.  Git gud. It's a godsend for A and S-Tier developers. The better understanding you have of software engineering best practices, the more useful and time-saving it becomes. My code has never been of higher quality because AI frees up time to be more mindful and proactive in every step of the development process.  AI is your junior dev cranking out code, as you the architect and technical lead map out the problem domain, implementation structure and strategy.",1,1,0.219,0.197
post20hb,richly branching,64,>no dev worth their salt is actually using it to write code.   Gonna disagree here,0,0,0.287,0.284
post20hb,richly branching,64,"I used to have the same idea as you, that context is what AI was terrible at. That is until I tried Cursor, paired with Claude 3.7 and I was just amazed and disturbed in the same time.",0,1,0.241,0.198
post20hb,richly branching,64,What exactly are you doing all day that involves making CRUD apps?   I simply copy paste my code templates/import libraries to do this and it's literally faster than anyone using code from LLMs.,0,0,0.182,0.117
post20hb,richly branching,64,"I agree. I’m using AI to build real apps and as long as you guide it well it can do real work.   I made the same mistake everyone makes at first.   Hey AI, make me instagram and expect to have a working app, then say it suck’s when it doesn’t do that.   But if you break that down into small tasks, it will do it.",0,0,0.191,0.165
post20hb,richly branching,64,"I will say this, and this might be what you were trying to say but having deep domain knowledge is still the ONLY way to utilize AI in a professional manner.  This fact, alone, means quality devs will have to be in the loop, because no matter how efficient or fast, you need that expert intention to build quality software.  To be completely blunt, I don't see how less-than-quality devs won't be impacted. A very basic business example would be the impact on startup hiring.  If you have a few quality senior engineers who can now spit out boilerplate in a matter of minutes, why would the team ever scale up to a potential pre-LLM size?  The sad reality is, they won't and that efficiency driven by LLM may be a long-term trend within that organization.  Now, does the world need exponentially more software because if so, all devs might be good to go in the long run.",0,0,0.19,0.111
post20hb,richly branching,64,"This. LLMs will get better and do more. But if today you already feel replaceable by an AI maybe you ARE replaceable. Look. Any position has geniuses and morons, sinners and saints, humble and bold folk. Even if AI didn't exist, there is a subset of developers still in ""fake it till ya make it"" mode + who could have been replaced already. That's just how life goes. AI is an agent of change, but it didn't make change happen. That was just always part of life.   Wake up folks. Are you replaceable by an AI bot tomorrow? Really? All your human capabilities and potential? Just like that?",0,1,0.276,0.174
post20hb,richly branching,64,"where the rubber meets the road is what the c suite executives believe and are willing to infest in (I mean invest in lol), 100,000 of thousands of tech workers are being fired for the 'ai god'.....now this is a bit premature and there will be much fallout which high priced programmers will be happy to fix for a large hourly fee of course :)  by then those ceos will have been fired for other ceos",0,1,0.207,0.168
post20hb,richly branching,64,I agree with everything with one exception. AI is actually pretty good at writing unit tests.,0,0,0.194,0.186
post20hb,richly branching,64,"I don't think the post was about that though. Of course they're not (yet) at a point where they can create complex backends or award winning designs but they do more than fine for basic gigs most web developers get. Which are things like designing a website for a local bakery or a barbershop etc.  And as a mainly backend developer, especially Claude can come up with designs I wouldn't be able to do myself if I spent a week. Couple weeks ago I was messing around and wanted to see what it could come up with for a page design for my webapp and the result made my jaw drop. It was at least as good as what a freelance designer would create for $50. And my app isn't that simple either. There were modals, quizzes, textareas and many different form elements on the page.  After seeing that I changed the way I start my new projects. I describe the page I want in detail to Claude and have it create the design for me. Then I put that design into a new route (I usually put it on 127.0.0.1/vision) and try to make mine look as good (better, if possible) than that. That way I'm also polishing up my design skills while not being completely dependent on it.",0,0,0.226,0.225
post20hb,richly branching,64,"I would agree with what you said.  I think part of the confusion I have is I have never really worked as a front-end dev or backend dev... I've always been full stack.  I have always been responsible for building everything from what the user sees and clicks down to the optimized databases and everything inbetween.     I know the industry shifted away from that, but it's what I've been doing for 20 some-odd years and I'm seeing the industry is shifting back that way this very moment.   I definitely use AI to generate some basic details and designs and 100% agree it's good at doing that.",0,0,0.238,0.168
post20hb,richly branching,64,But in what way are WordPress and Shopify not already satisfying this market?,0,0,0.193,0.173
post20hb,richly branching,64,Brother respectfully what are you talking about.   I’ve played with Claude Sonnet 3.7 extensively   All the designs it generates looks like they came from 2017. It’s still stuck on the flat design paradigm.   At that point why not just get a template? Even the free ones are infinitely better than what Claude pumps out.,0,0,0.172,0.132
post20hb,richly branching,64,"> And my app isn't that simple either. There were modals, quizzes, textareas and many different form elements on the page.  There are no templates for this kind of thing. I described the business logic in detail and it returned a dashboard page that fits all my needs. Again, it's not winning any awwwards any time soon but I had it build many pages and most of them were pretty nice looking. The ones I didn't like were the ones I didn't give much attention to detail in the prompts so that could be it too. Idk, try some more detailed prompts maybe.  Though again I'm not specialized in frontend and I'm not a designer/artist. What looks great to me could be garbage to you",1,0,0.236,0.269
post20hb,richly branching,64,"Maybe you are playing with it wrong then?  Nah, couldn't possibly be your fault.  AI sucks.",0,1,0.216,0.239
post20hb,richly branching,64,"Maybe you are playing with it wrong then?  Nah, couldn't possibly be your fault.  AI sucks.",0,1,0.216,0.239
post20hb,richly branching,64,"That sounds very niche. Interesting, but niche",1,0,0.29,0.188
post20hb,richly branching,64,"Thats the whole point.  It's not a niche, it's just one example of thousands of business and enterprise apps that need to be built right now, right this very second.  These are the kinds of applications that developers need to start focusing on, and not static webapps with simple signup forms or the like.  I promise i'm not trying to be obtuse or a jerk, i'm just trying to share my viewpoint that there are literally thousands of companies and thousands of apps that AI is not going to build right now.",0,0,0.153,0.129
post20hb,richly branching,64,"100%.  It is indeed a shitty insight, but hopefully it serves as a wake up call for some people.",0,0,0.35,0.253
post20hb,richly branching,64,Yea pretty much. The only meaningful thing I’ve seen it do in enterprise is give better reasoning to laying off the terrible devs. Doesnt matter if AI code sucks if the dev code sucks as well. But all you have to do is be more than a coder.,0,1,0.208,0.163
post20hb,richly branching,64,Yes it will. That’s exactly what the hyperscalers and geospatial data brokers are selling to insurance and capital markets.,0,0,0.087,0.127
post20hb,richly branching,64,">I have yet to see AI replace or do any meaningful work in an enterprise environment or on an application that is more than just a simple frontend.  Yet. It's obviously heading that way, regardless of whether this will come by the LLM's alone or by introducing external assisting systems to handle the parts where an LLM on its own fails.",0,0,0.221,0.156
post20hb,richly branching,64,"Any real features? You building a dashboard or a website? Cuz if youre building ""real"" features it sounds like youre wasting a lot of time.",0,0,0.261,0.208
post20hb,richly branching,64,"True but it’s only a matter of time before models and apis come out that can increase the contextual awareness of the output. Currently, as many of said, it feels like autocomplete cause the tool is largely limited to looking at a single repo or service. But if they make it so you can broaden in input to include your backend etc it could get a lot better.   I don’t think it’s coming overnight and it won’t be cheap. But it just has to be competitive with a human salary and it can completely undermine things.   TLDR I wouldn’t want to be a junior dev right now let alone in 5 years. The job pickings are slim as is.",0,0,0.275,0.174
post20hb,richly branching,64,"Agreed, ive already started looking to change my career and my team is looking at how to most successfully phase ourselves out.  its a tough world",0,0,0.313,0.298
post20hb,richly branching,64,"I had a phone interview with OpenAI that didn’t go anywhere but I asked the recruiter “does the company have any policy around engineers coding themselves out of the job?” And they could only give me a trite “we make ai that helps people, not replace them” response. I would’ve been curious to see what the high level folks later in the interview process would’ve said but then again asking that kind of question would probably lose you the job! 🤣",0,0,0.184,0.17
post20hb,richly branching,64,"What exactly even is ""basic CRUD?"" Do you mean the final coding step after someone has figured out the project requirements, consulted all the stakeholders, determined the data models and workflows to be implemented, mapped out the how these elements will interact across multiple applications, deployed the infrastructure necessary to run it, implemented a comprehensive security policy, and described the rest in a way that a kid fresh out of school can understand so that they can implement a bit of UI around it?  I guess that's technically ""basic CRUD."" It's also something between 1% and 5% of the total work in any sort of moderately complex system.  The way I see it, talking about basic CRUD is about as useful as saying all programming is implementing some branching logic in an environment that can be described as a Turing machine. Practically everything a programmer does is going to create, read, update, and/or delete stuff, often across some sort sort of communication channel, backed by one or more data store of some sort. It's more a description of the environment than anything else. Figuring out all the things you're going to CRUD, and how all the information is going to transform and mutate in the progress is the hard part. Everything from building a website, to training an ML system, to implementing that service bus for risk portfolio calculations is going to involve these operations.  The whole AI is going to replace programmers thing seems to be largely kids fresh out of school, that don't realise that most of the ""programming"" they are doing is just menial busy work that the seniors give them so they have a chance to explore the problem domain a bit, before being given actual tasks. That and hobbyists that spent a few months learning to code, and then decided that they are actually master system architects because they managed to wire together 10 or 20 files that run a chatbot or something of the sort.   These people have suddenly gained access to a tool that can understand the thing they're working on about as well as an expert that's never touched a particular codebase, but they don't have the context to realise that such an expert would need to spend a few months getting up to speed on everything before being confident enough to actually make any significant changes. They just see hundreds of lines getting generated, and figure that those lines are just as good as any other. It's sort of like deciding that some off-brand glue was good enough to hold structural components of a truck together, without understanding why most other people prefer to use mechanical fasteners for the job.",2,1,0.239,0.198
post20hb,richly branching,64,"what the christ is happening here  basic crud = submit a form to a post endpoint  non basic crud = tons of validation routines, business logic for dynamic drop-downs, permissions and validators for enable and disable, roles and rights management, and then all the stuff on the backend to process the result that isnt just dumping it into a database.   there is a difference, and its simple.  This is just high level from my phone because this is just too much to explain for something simple to understand",0,0,0.211,0.193
post20hb,richly branching,64,"My point is that ""basic CRUD"" isn't actually a thing that exists in a professional environment, outside of some boot camp or some trash tier off-shoring group somewhere.   If you're in a real job doing what you define as ""basic CRUD"" then you're just working in the context of the things a lot of other people did. Just because you don't know about the other things that must happen, doesn't mean that these things don't happen, and that they won't affect the code you write. Eventually you'll have to deal with them, even if only because your ""basic CRUD"" isn't working.  You might as well talk about ""basic conditional logic"" or ""basic functions."" It's a meaningless distinction, because it's describing a tiny part of what the job entails. If you're actually doing this professionally, you simply aren't going to be doing much ""basic"" except when you're just starting out.",0,1,0.235,0.197
post20hb,richly branching,64,I spent the last 3 days fixing the fuckups of a colleague who blindly trusted AI to do his work… he’s never been a good programmer but AI has only increased his efficiency in fucking up lol,0,0,0.282,0.221
post20hb,richly branching,64,"Don't be like nokia AI is going to take most of the low level jobs, after that the middle level jobs, after that senior level jobs until a super AI computer is estabilished that can do anything. The one who owns that super computer will be a billionaire like bill gates owning microsoft in the 90s.",0,0,0.172,0.14
post20hb,richly branching,64,"i am terrified, what do you suggest I do to make sure im the billionaire?!?!?!",0,0,0.213,0.183
post20hb,richly branching,64,In a year it’s gone from useless to replicating entire applications in one shot. It’s even making games and AI agents to play said games… this is the worst it will ever be. If you think it won’t be able to crack basic maintenance and enterprise level systems soon. You are simply mistaken… most devs are just copy and paste bots from stackoverflow. It’s been the meme for the past 10 years at least.,0,1,0.215,0.253
post20hb,richly branching,64,"100% agree, man!  I actually sat down with my boss today to come up with a plan to step a phase out of 4 of our 6 developers.  After playing with cursor and 3.7 we see the value.   We expect a reduction in staff within less than 6 months.  I am stoked, my team budget is going to be so much leaner but in theory have the same productivity.   Im with you, man, AI is the shit and human devs are on the way out.",0,0,0.3,0.175
post20hb,richly branching,64,"??? 180 and changed tone. Can’t tell if you’re taking the piss. I worked fintech where we’re getting 300k a year inc bonus and options. We will definitely be replaced within a few years. Me and my team have cut all our stupid spending to prep. Good luck to all devs. But if you aren’t in the top 1 percent that are actually pushing the boundaries (researches, phds etc) your work is replicable by an AI.",0,1,0.233,0.201
post20hb,richly branching,64,Ai is doing meaningful work in our company and is at the core of what we do. However it's a block of our product and doesn't replace any devs. It just made our idea possible. Cannot go into details as it's sensitive tho.,0,0,0.207,0.192
post20hb,richly branching,64,[deleted],0,0,0.376,0.43
post20hb,richly branching,64,Except the growth is blocked by the fact they use large language models and not true Ai. It's machine learning masquerading as Ai.   I researched it and the easy gains are maxed out(data and brute force computing power). It's not like Moores law.,0,1,0.242,0.16
post20hb,richly branching,64,[deleted],0,0,0.376,0.43
post20hb,richly branching,64,"Sorry, but this feels like massive cope. AI will absolutely be able to replicate that, it's just not there yet. Anyone that's used Claude 3.7 will tell you that it can indeed do some insane tasks already. Combine that with copilot integration, Claude code, or cursor, and yeah... We're entering the phase where it's starting to materially impact workflows, even the complex ones. Speaking as a full stack developer at a large enterprise. We're at the opening phase right now. Give it 10 years at most (if not 5), and the entire field of development is going to be drastically different from current day. There's WAY too much money on the table for executives to not exploit this as much as they can to reduce workforce numbers and increase profit. They'll find a way to do it.",0,0,0.259,0.157
post20hb,richly branching,64,[deleted],0,0,0.376,0.43
post20hb,richly branching,64,"I definitely use it for writing tests in our Angular project, thats the truth!",0,0,0.384,0.256
post20hb,richly branching,64,tell me a feature that's not based on crud. I'll wait.,0,0,0.245,0.183
post20hb,richly branching,64,"I mean, yeah, 99.9% start with crud, but it can very quickly diverge from there with what it does with the info.  5 textboxes and a checkbox can be all it takes to kick off a calculation resource or generate complex financial reports, for example.  Not sure where you were going with this, dude.",0,1,0.219,0.188
post20hb,richly branching,64,If you're letting an AI develop blocking code on your async app then you fucked up long before,0,0,0.167,0.357
post20hb,richly branching,64,"\> AI isn't going to design, setup, and build your service bus that manages your mapping engine job scheduler which then calculates risk portfolios across Florida roof maps.  Claude can absolutely do that. And so can the new Gemini. You have no idea what you are talking about or you are just in denial.  I have been programming for basically 40 years and I think it's asinine to try to write programs without a SOTA LLM and coding agent/environment these days. Of course it still needs help and I prefer to give it my own architecture rather than let it dictate it for a lot of things,  but the best models absolutely can design (and setup whatever using tool commands or computer use). Yes it still needs help sometimes but it can do 80-95% of the work for applications as complex or more complex than the one you gave in the example.  And will continue to get better.",0,0,0.205,0.173
post20hb,richly branching,64,"Damn, you are right, I just tried claude code and it literally just replaced me and 4 other devs.  This is bonkers, we are all truly fucked.",0,0,0.311,0.244
post20hb,richly branching,64,Ha! Give it a few minutes. It’s over dude.,0,0,0.278,0.445
post20hb,richly branching,64,"Sure man, whatever helps you sleep at night",0,0,0.311,0.225
post20hb,richly branching,64,👍,0,0,0.36,0.301
post20lb,poorly branching,8,"There is a lot to unpack here, so please bear with me.  A computer has no natural tendencies towards anything other than processing bits of information.  The computer itself isn't racist or non-racist.  It's merely a vessel for whatever the programmer makes it do.  Algorithms _can_ be racist, if they make different decisions based on race.  If, for example, your bank uses an algorithm to decide who can get a loan and who doesn't, and that algorithm takes ""race"" as input and generates a different output based on that data, then the algorithm is biased, and will produce biased results.  One example of this is with [criminal risk assessment algorithms, which significantly disadvantage Black people](https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/).  You of course asked about facial recognition.  Designing a facial recognition algorithm by hand is complex and error prone, so typically^0 we use machine learning to allow the computer to create the necessary algorithm.  In such a scenario, the developer codes the starting conditions for a blank network, and then feed a large number of images into the network to ""train"" it in how to recognize a face.  Further code may then be added to pick out specific features of the face, and creating some sort of identification code for the face itself to match up with databases of known faces.  There are a few areas here where racial bias can creep in.  Firstly, you can run into the situation where the training set contains insufficient images of specific races, allowing the algorithm to form a bias against them.  For all their power, computers aren't very smart, and even the best Convolutional Neural Network doesn't generalize like we can -- so if you feed the network a series of blond haired, blue eyed, white faces, it's not going to be able to recognize faces outside those contexts.  And this has in fact been the case.  A famous instance of this occurred back in 2015, when it was found that [Google was identifying black men as gorillas](https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/).  A large part of this appears to be because Google's training algorithms were given many, many more white faces than black faces.  ""Recognizing that something is a face"" in a photo is somewhat different from ""identifying _someone_ in a photo"".  We've seen case of failure in the first; what about the second?  Testing has revealed [bias and inequalities in common facial recognition and identification systems](https://www.cbc.ca/news/technology/facial-recognition-race-1.5403899)^1.  The bias in each system differs, and the report note that while many systems have higher false positive rates for East Asian faces, it's the opposite for systems developed in China, where the false positive rate is _lower_ for East Asian faces.  I don't have sufficient knowledge to comment on the idea you espouse that non-white faces are ""much less varied than white faces""^2, but even if we thought this were true, the evidence that algorithms developed in different parts of the world fare better or worse on certain types of faces seems to indicate that this shouldn't be a problem for a facial recognition algorithm.  The issue lies in one (or both) of the following two areas of development:  1. **Training**:  the training data exhibits a bias with the types and number of photos available of different ethnicities and demographics, and hence produces biased results, and/or 2. **Validation**: the developer of the algorithm is predominately verifying their trained algorithm with certain ethnicities, and not enough of others to ensure that the false-positive rate is compatible.  With #1, you introduce bias via a lack of sufficient data.  This may not be intentional -- it may be that the training sets available to the developer simply have this bias built-in (for example, if you are developing in a predominately East Asian country like China, you may have access to a full database of all Chinese drivers license or passport photos to train with, which are predominately East Asiatic faces).  In #2, however, more of the developers bias comes out -- if (say) a software developer feeds their neural network a somewhat varied supply of photos from a variety of demographics, but then only validates that training with photos that are (for example) predominately white, you'll never know if the training was successful for other races.  And in the race to market for many companies, corners like this get cut, either intentionally or unintentionally.  Ideally, every company would have a wide range of suitable photos from different races to validate their algorithms against, come up with an accuracy score for each by race/gender or other identifiable detail, and not ship until they ensure that the scores for each group fall within the same bounds.  But that, as we see from the testing, isn't happening.  That brings us to another issue:  how and why is this a problem?  Firstly, I'd like to note that I don't think there is anything wrong with developing a biased algorithm.  Iteration is an important part of most large computing projects, and developers of such algorithms should be validating their algorithms against a wide set of possible data, and ensuring that everyone falls within certain narrow bounds for error.  If they don't, then they should iterate and improve their algorithms until they do.  See, there's nothing wrong with a biased algorithm -- so long as it doesn't leave the lab, and people either work on improving it or discard it for something that exhibits less (or potentially no) bias.  So long as we acknowledge this as a risk factor, and then take steps to measure and mitigate it, the fact that an early version of an algorithm held a bias shouldn't be something to be ashamed of.  The _problem_ is when biased algorithms like this are put into production, and are used by police, military, governments, or other organizations to disadvantage one group of people compared to another.  That is, it's when the algorithmic bias is used in the real-world to be biased against living people.  That's where the problems occur -- and people are right to be angry when an unfeeling computer that can't be reasoned with mis-identifies and disadvantages them.  It's the people using these algorithms who need to be the ones to say ""I don't trust the answers from this system, because it exhibits bias"".  Unfortunately, what we often instead see in this world is a ""machine is never wrong"" attitude^3, which disadvantages certain people, but where the authorities that act on the algorithmic findings simply don't care if they're getting biased results in the first place.  Is it possible that there are going to be demographics of people that have faces more difficult to analyze than others?  That's possible^4.  But if that _is_ the case, then _we shouldn't be using these systems to identify people for special treatment_.  That is, if it were to turn out that bias in facial recognition algorithms is _impossible_ to get within certain acceptable bounds, _then we shouldn't be using those systems, **full stop**_.  In summation, there are lots of ways for bias to creep into an algorithm, many of which may be unintentional.  However, this bias should be measured for various groups, should be published so everyone is aware of the bias, and in cases where the bias is significant _should not be used in ways that disadvantage the people for whom the algorithm is biased against_.  ----- ^0 -- AFAIK, you can read this as ""in every case I know of""   ^1 -- [""Face Recognition Vendor Test, Part 3: Demographics Effects"", NIST, 2019](https://nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8280.pdf).   ^2 -- I don't believe this to be true, but it's not my area of study or expertise.  I'm a computer scientist, not an ethnographer.   ^3 -- Or just as bad: ""the machine is often wrong in specific cases, but we're going to disadvantage those people anyway and _maybe_ apologize later after we've given them unnecessary grief, on the off chance the system is right for once"" attitude.  This is really just another way of allowing people to use the computer as an excuse to support their own biases as valid.   ^4 -- Again, see ^2:  not an ethnographer!  EDIT: typos",0,3,0.193,0.424
post20lb,poorly branching,8,"> I don't have sufficient knowledge to comment on the idea you espouse that non-white faces are ""much less varied than white faces""  I'm fairly sure OP just stumbled into the [Cross-Race Effect](https://en.wikipedia.org/wiki/Cross-race_effect) - most people are far better at recognizing faces from their own race, and tend to think other races ""all look the same"".",0,0,0.172,0.267
post20lb,poorly branching,8,"I think they're also falling into an easy fallacy presuming that ""computer vision"" and ""human vision"" are one and the same, [which I have attempted to debunk below](https://www.reddit.com/r/askscience/comments/ihrhyh/why_are_facial_recognition_algorithms_racist/g35xn2x?utm_source=share&utm_medium=web2x&context=3).",0,0,0.173,0.335
post20lb,poorly branching,8,Thank you for this detailed and interesting explanation!,3,0,0.366,0.257
post20lb,poorly branching,8,"What about the actual data that comes into the computer though? The information is captured by a camera and converted to an image, but dark faces can give a lot less contrast, especially under low light conditions. Dark skin can also hide underlying features like moles, freckles and blush and blend in with hair more easily, which is also worsened under low light conditions and with low quality cameras. How do these effects come in when you'd consider an otherwise ideally unbiased detection algorithm?",0,0,0.158,0.256
post20lb,poorly branching,8,"> The information is captured by a camera and converted to an image, but dark faces can give a lot less contrast, especially under low light conditions.  I wanted to clear up some invalid assumptions that seem to be creeping into the conversation, that will help show why this line of reasoning isn't particularly valid.  First off is the unspoken assumption that cameras ""see"" the same way we do.  This is not true.  [Here is the spectral breakdown for panchromatic film](http://www.geo-informatie.nl/courses/grs20306/lectures/05aerialphotography/05aerialphotography08.gif).  Note that the yellow line (panchromatic film)^0 has spectral sensitivity below 0.4µm, and above 0.7µm^1.  These correspond to the Near Ultra Violet and Near Infra Red portions of the spectrum, both of which are outside the normal human range of vision.  Because of this, computer-processed digital images may be able to discern features that show up in NUV and NIR that aren't discernible to normal human vision.  Hence, assumptions about what features are not visible to a normal human don't necessarily apply to computer images.  Similarly, computers can detect very small differences in colouration more readily than humans can.  Couple this with the above, and it is not a given that computers can't pick out features in photos that humans can't.  Secondly, it feels like several commenters want to focus on thinks like birthmarks, moles, freckles, or other such skin markings as the basis of identification.  These _may_ come into play in some algorithms, however these algorithms tend to rely significantly more on facial [_geometry_](https://miro.medium.com/max/1234/1*C8UucvbO_DmoJlETCS7K3w.jpeg) rather than blemishes of the skin.  They tend to focus more on the ratios of size of the mouth, nose, distance between the eyes, foreheads, etc.  The reason for this is that these things are extremely hard for someone to change -- you would need some radical surgery to change your inter-pupillary distance (IPD), for example.  If recognition systems focused on moles, freckles, or hair as many people here have hypothesized, then you'd be able to completely throw them off by putting a small black sticker on your chin, or by having a breakout of acne, or by wearing a bit of makeup, or by getting a haircut.  So the focus on ""darker faces hide hair and marks on skin"" is not valid.  Photographs can record more details than humans can see.  Computers have significantly more power to pull out small differences in colouration than humans do.  And facial recognition algorithms likely^2 take this into account to reduce the number of false positives.  It's better to focus on features that are more difficult to change, such as the gross facial dimensions and features, rather than skin imperfections which can easily change for an individual, and which can be trivially faked or masked.  ----- ^0 -- [Here is an example of spectral sensitivity for true colour film](http://www.geo-informatie.nl/courses/grs20306/lectures/05aerialphotography/05aerialphotography13.gif), which is not as wide as that of panchromatic, but still dips into NUV range.   ^1 -- Film and digital photos used for photo ID (such as those used in passports and drivers licenses) may _purposefully_ use wider-gamut film/sensors than those presented here, specifically to extract more processable details than humans can otherwise obtain from human vision, specifically to give more detail for computer recognition systems to work with.  Likewise, cameras used to pick out faces may use a wider gamut than human vision.  Point being, don't assume ""what you see"" is automatically the same as ""what the camera"" (and hence computer) ""sees"".   ^2 -- I don't like using a lot of weasel-words like this, however virtually all facial recognition algorithms in active use are commercially developed, and if their specific details are available, they're not available to _me_, so I can only talk in generalities.  Sorry :P.",0,0,0.27,0.193
post20lb,poorly branching,8,"So, if special cameras with different colour filters are used, does that also mean that wearing for example sunscreen, which filters out features that are very apparent at non-visible wavelengths, could interfere with facial recognition?  Also, how does this apply to non-ideal conditions, like low lighting, further away and awkward angles like on the street? Do the features on dark faces come across as well as light faces or do they have a different ""cutoff point"" at which the conditions are so bad that a camera can't capture a properly processable image?",0,1,0.159,0.232
post20lb,poorly branching,8,[removed],0,0,0.366,0.413
post21hb,richly branching,24,America has turned into an angry bully since it's governed by an angry bully.,0,0,0.179,0.248
post21hb,richly branching,24,no. it just proves companies never cared.,0,0,0.147,0.228
post21hb,richly branching,24,No. It means companies don't want to spend money on lawyers fighting the DOJ to promote diversity.,0,1,0.242,0.311
post21hb,richly branching,24,It doesn't matternif they have care or not.  It matters what direction they're pointed in and right now it's a bad direction.,0,0,0.205,0.208
post21hb,richly branching,24,"There is no direction lol, it's a facade that's put up specifically to fool people like you. They really just don't care",0,0,0.217,0.214
post21hb,richly branching,24,[deleted],0,0,0.376,0.43
post21hb,richly branching,24,What's your opinion on diversity? Should less talented people be given jobs than more talented because the former is underrepresented?,0,0,0.179,0.293
post21hb,richly branching,24,You actually got it backwards and that's what's so scary for the future of this country.   Less talented people were getting the jobs because they were the default representation. But that's a tough pill for a lot of folks to swallow.,0,0,0.122,0.175
post21hb,richly branching,24,Do yourself a favor and go look up recent year med school acceptance rates by background and test score.,0,0,0.171,0.073
post21hb,richly branching,24,"Maybe in the previous millennia. In this millennia minorities were given preferential treatment in colleges with much lower bars for admission, scholarships exclusive to minorities, internships at top companies exclusive to minorities, and then full time job opportunities targeted at minorities, and then hiring quotas and promotion quotas for minorities.   Society was in the 1900s white favoring, and then in the first quarter of the 21st century, minority favoring. Now we are entering the pendulum swinging back to the center albeit there are some that are resisting equality.",0,0,0.167,0.308
post21hb,richly branching,24,It started the way you mention but it took a wild turn where underrepresented minorities are being overrepresented. It has to be balanced both ways.,0,0,0.156,0.262
post21hb,richly branching,24,"Diversity isn’t about hiring less talented people, it’s about making sure talent isn’t overlooked because of systemic barriers. There’s plenty of skill and ability across all groups, but not everyone has had the same access to opportunities. Leveling the playing field doesn’t mean lowering the bar.",0,1,0.155,0.258
post21hb,richly branching,24,My opinion is that hatred shouldn't be the main driver behind political and business decisions,0,0,0.21,0.364
post21hb,richly branching,24,Any kind of bias other than merit should not be a driving factor. Diversity commitment goes against it because there is literally no way you can commit without having a bias.,0,0,0.182,0.479
post21hb,richly branching,24,Nice deflection there,0,0,0.307,0.331
post21hb,richly branching,24,"“Talented” like Hegseth, Noem, RFK and Gabbard? Whoopsies 💩",0,0,0.202,0.223
post21hb,richly branching,24,"Wrong question. That’s just something MAGA followers use to try to frame equality and diversity in a negative light.   Real question: Given 50 similar roles at a large company, and a pool of 100 qualified candidates, is it desirable to make sure it’s not 49 white men and 1 POC in the role?",0,1,0.152,0.378
post21hb,richly branching,24,It's desirable to choose the 50 best candidates. Full stop.,0,0,0.207,0.186
post21hb,richly branching,24,"Say there's 10 roles and a pool of 1000 equally qualified candidates. Of this, 800 are male and 200 are female. Would it be desirable for the male-female split to be 50-50 here?",0,0,0.193,0.306
post21hb,richly branching,24,Who said they're less talented? If anything discouraging promotion of diversity can swing hard enough that you deny the more qualified candidate of another race... which is what the administration is pushing for.,0,0,0.193,0.332
post21hb,richly branching,24,Left bully has been replaced by right bully. Its all bully.,0,0,0.214,0.223
post21hb,richly branching,24,"If you’ve ever watched some of the CIA people on podcasts in the last few years, you’d learn that being a bully has been Americas policy for many decades. Trump didn’t start it.",0,0,0.2,0.208
post21hb,richly branching,24,"First time I hear a US president threatening to take over the Panama canal, Canada, Greenland, Gaza within a 2 weeks timeframe",0,0,0.162,0.187
post21hb,richly branching,24,The US has been in almost perpetual war  since world war 2. Where have you been?,0,0,0.159,0.323
post21lb,poorly branching,5,"Machine learning algorithms need lots of labeled data.  Essentially a computer is shown a picture of a person and the computer makes a guess whether it's a person or not.  Then the computer is told the correct answer and the computer takes notes on what it got right and wrong and then adjusts itself.  Sometimes this data is biased, such as being shown way more white people than black people.  Because the computer primarily sees white people, it begins to associate white humanoid with person and black with not person in a more extreme case and not sure in a less extreme case.  There was a really bad example of this a while ago where an AI started labeling black people as gorillas.  Because the people it had primarily seen where white and the things that were dark skinned and humanoid where predominately gorillas not black people.  &#x200B;  With kinect specifically it may be a different problem that has a similar cause.  Kinect uses an infrared camera and different skin tones (especially darker skin vs lighter skin) looks different in infrared so the more basic software wasn't capable of recognizing what it was seeing as a human (this actually has happened with hands free sinks before).  In teams that are predominately white, it may not have come up during testing (engineering teams are often their own first tests) and they may not have thought about it.",0,2,0.172,0.385
post21lb,poorly branching,5,Thank you!,1,0,0.831,0.216
post21lb,poorly branching,5,This has always seemed a weak explanation to me. You don't train a machine learning algorithm on a training set of the half dozen guys who developed it.,0,0,0.404,0.209
post21lb,poorly branching,5,"That is exactly how it works. Older machine vision was all about manual feature engineering and didn't need a lot of data.  Black people are quite rare in most places of the world. If you pick a company ID database, chances are it will be 99% white/asian",0,0,0.133,0.285
post21lb,poorly branching,5,"No you don't use that small of a data set.  There's two things going on and I guess I didn't separate them out enough.  Large data sets can be biased and not reflect a complete population.  And even then, it might not be enough to actually train the algorithm to actually recognize a minority of the data.  These algorithms are still way less complex than we are and can have trouble with rarer data.  An algorithm might just decide to take the accuracy hit rather than actually recognize a small portion of the data (if you want the algorithm to be 96% accurate and you have a subset that's 3% of the data, you can hit your target and get all of the subset wrong).  Then there's the diversity of the dev teams. If you don't have much diversity on the dev teams, sometimes stuff can slip by on early testing.  As an example, I was in a computer vision class earlier in the year.  out of the three people on my team, two were white and one was asian.  On one of our assignments, myself and the other white person set a value and it worked.  But when our other teammate went to use it it didn't because his skin was darker.  If we didn't have the third person it would have slipped by.  I'm not excusing this, and it totally shouldn't happen with something on the scale of microsoft but it can happen with proofs of concept or tests.",2,2,0.134,0.232
post22hb,richly branching,12,"DEI is racist, discriminating against qualified candidates because they have the wrong skin color. If you want to fuel anger and disenfranchisement this is a fast way of doing it.",0,1,0.188,0.426
post22hb,richly branching,12,I’ve never met a good programmer who lost a job due to DEI. Nobody serious is interviewing two candidates and picking a vastly inferior one due to DEI. Every public incident reportedly caused by unqualified DEI hires has ended up being right wing pearl clutching with absolutely no relation at all.  You’ve been tricked into arguing about uno while the elites play chess.,0,1,0.189,0.206
post22hb,richly branching,12,https://medium.com/the-mission/im-an-ex-google-woman-tech-leader-and-i-m-sick-of-our-approach-to-diversity-17008c5fe999,0,0,0.245,0.302
post22hb,richly branching,12,"Like I said, I agree it’s a flawed idea. I take issue with it being politicised by the right. Its actual impact is so unbelievably small in contrast to the noise people make about it.",0,0,0.201,0.242
post22hb,richly branching,12,"How can you possibly know the number of jobs a ""good"" programmer was not ultimately offered because the employer instead hired someone else to meet a quota?",0,0,0.17,0.198
post22hb,richly branching,12,"Because I live in the real world, where instead of getting angry at right wing talking points designed to deflect attention from actual issues, I look at the reality of each situation and assess it based off what I find to be true.",0,0,0.212,0.294
post22hb,richly branching,12,"Well i’ve hear of some good ones that couldn’t get a job because of it.  And how could you even have heard of that?? It’s not like companies fire Brian, middle aged white dude straight telling him sorry man, we gotta make room for more diversity. Nor that then blatantly sharing the story would serve Brian in any way…",0,0,0.193,0.309
post22hb,richly branching,12,Do you work in tech? The only people I’ve met who have complained about not getting a job due to DEI are objectively terrible at their jobs.,0,1,0.164,0.278
post22hb,richly branching,12,"> I’ve never met a good programmer who lost a job due to DEI.  How on earth would you tell, though? What sort of test could you possibly construct, from the candidate PoV, that could estimate the probability of a application failure being due or not due to DEI?",0,0,0.113,0.15
post22hb,richly branching,12,"Because I've worked with every skill level in tech between ""I can barely turn a rock on"", all the way up to literal celebrities in the world of software engineering.  DEI is just political dressing on top of the normal hiring process. Nobody who is unqualified is getting hired. Nobody who is good at their job cares.",0,0,0.184,0.207
post22hb,richly branching,12,"It's genuinely so sad how well conservative propaganda has worked on many people, including coloring the term ""DEI"" as racist or discriminatory. The point is that the ""racism"" you complain about exist, just that you refuse to recognize it cause you don't think people's biases affect society and especially hiring in historically discriminated industries including engineering.  For a method that noone should have a problem with, I'd recommend reading up on equality in shortlisting and please let me know your opinion of it. In summary, it's ensuring recruitment shortlists have increased equality (in as many factors as possible), while ensuring that truly the best person is hired (which some people argue targeted recruitment doesn't).",0,1,0.193,0.535
post22hb,richly branching,12,It’s pretty depressing. They really have these people arguing over grains of rice.,0,1,0.21,0.29
post22lb,poorly branching,5,"Machine learning algorithms need lots of labeled data.  Essentially a computer is shown a picture of a person and the computer makes a guess whether it's a person or not.  Then the computer is told the correct answer and the computer takes notes on what it got right and wrong and then adjusts itself.  Sometimes this data is biased, such as being shown way more white people than black people.  Because the computer primarily sees white people, it begins to associate white humanoid with person and black with not person in a more extreme case and not sure in a less extreme case.  There was a really bad example of this a while ago where an AI started labeling black people as gorillas.  Because the people it had primarily seen where white and the things that were dark skinned and humanoid where predominately gorillas not black people.  &#x200B;  With kinect specifically it may be a different problem that has a similar cause.  Kinect uses an infrared camera and different skin tones (especially darker skin vs lighter skin) looks different in infrared so the more basic software wasn't capable of recognizing what it was seeing as a human (this actually has happened with hands free sinks before).  In teams that are predominately white, it may not have come up during testing (engineering teams are often their own first tests) and they may not have thought about it.",0,2,0.172,0.385
post22lb,poorly branching,5,Thank you!,1,0,0.831,0.216
post22lb,poorly branching,5,This has always seemed a weak explanation to me. You don't train a machine learning algorithm on a training set of the half dozen guys who developed it.,0,0,0.404,0.209
post22lb,poorly branching,5,"That is exactly how it works. Older machine vision was all about manual feature engineering and didn't need a lot of data.  Black people are quite rare in most places of the world. If you pick a company ID database, chances are it will be 99% white/asian",0,0,0.133,0.285
post22lb,poorly branching,5,"No you don't use that small of a data set.  There's two things going on and I guess I didn't separate them out enough.  Large data sets can be biased and not reflect a complete population.  And even then, it might not be enough to actually train the algorithm to actually recognize a minority of the data.  These algorithms are still way less complex than we are and can have trouble with rarer data.  An algorithm might just decide to take the accuracy hit rather than actually recognize a small portion of the data (if you want the algorithm to be 96% accurate and you have a subset that's 3% of the data, you can hit your target and get all of the subset wrong).  Then there's the diversity of the dev teams. If you don't have much diversity on the dev teams, sometimes stuff can slip by on early testing.  As an example, I was in a computer vision class earlier in the year.  out of the three people on my team, two were white and one was asian.  On one of our assignments, myself and the other white person set a value and it worked.  But when our other teammate went to use it it didn't because his skin was darker.  If we didn't have the third person it would have slipped by.  I'm not excusing this, and it totally shouldn't happen with something on the scale of microsoft but it can happen with proofs of concept or tests.",2,2,0.134,0.232
post23hb,richly branching,7,I believe AI could be used to manage the wastefulness of modern society to redistribute this goods that are usually discarded for people in extreme needs around the world. From connecting people to developing more efficient trading systems. I believe what is needed is just the want to do it.,0,0,0.235,0.242
post23hb,richly branching,7,"Fuk no, I want neoliberalism. Doggy dog world.",0,0,0.22,0.226
post23hb,richly branching,7,Why would the powers that be treat AI any differently than any other technological innovation? When has new tech ever not been used to gain a competitive advantage?,0,0,0.175,0.104
post23hb,richly branching,7,"I believe tech has been used for altruistic objectives, while it might not be such a sudden change as many of us would want, tech will grow to be adapted by the whole world, and this means non-profits doing the best they can to help their cause using emerging scientific discovery, the internet is an incredible tool that today is not yet completely globally accessible but efforts are made to make people more connected in areas of low resources.  If you want to be more theoretical, if AI manages to improve the economic status of humans where most people buy habits are not determined by the cost, maybe people would start supporting companies that uphold humanitarian values, i do believe that if people had the choice, there was the availability, quality and it didn't personally affect them, then they would choose the ethical product.",0,0,0.21,0.189
post23hb,richly branching,7,"The internet is a great tool, but I block malicious attempts to hijack my webserver weekly, and I get scammers trying to trick my mother out of money from a computer in India.   Tools are only tools. The people who weild them  determine the altruistic value of the action of using the tools. 100% of people are not benevolent, and if global unfettered access is given, narafious actors will use them for their gain.   I've seen enough of human nature in my years to know that what you're dreaming up is pure fantasy.   Technology is only as valuable as the ethics of those that use it. Not everyone wants good. There are a lot of people in this world, if given the opportunity, who wouldn't blink at the thought of eradicating entire groups of people from existence.   Do you think Hamas, having control of a super intelligent technological marvel, would all of a sudden not want all Jews to die?  The first thing Americans did once they developed the technology behind the atomic bomb, was drop it on two cities in Japan. That's what technology can do. I'm not anti technology, I'm just a realist.",1,0,0.2,0.23
post23hb,richly branching,7,We already can redistribute stuff right now If we wanted to. We have more than enough resources and capability for that.,0,0,0.193,0.172
post23hb,richly branching,7,"Unfortunately a lot of wastefulnes in our system is by design to purposely keep price higher. AI will be able to improve productivity and overall world wealth, but the problem even today, is not how much goods are produced, but its distribution and much likely AI, at least at first, will make that issue worse.",0,0,0.222,0.229
post23lb,poorly branching,6,"Feminists often cite a study showing a bias for resumes with male names, but what you never hear about is that experiment was redone showing the opposite.    Try googling what the pay gap actually measures.  The facts will be buried under a mountain of feminist misinformation.      Try finding actual reported sexual assault rates of college students.   Same thing, actual data is again buried under a mountain of feminist propaganda.     It’s the same with convictions rates and so much other information:   Feminist and woke propaganda is prioritized while relevant facts are downplayed and hidden.     (Try googling the actual conviction rate for rape.   You won’t find it, but you’ll find plenty of misinformation about it propagated by RAINN)  There’s a bias all right, the bias is to promote cherry picked information and propaganda favorable to the feminist cause.  Whether it’s evaluating resumes or prioritizing internet search results, what we call AI is really just programming and will reflect any bias of the programmer.",0,2,0.179,0.288
post23lb,poorly branching,6,"Pretty sure there's been studies done where recruiters were asked to pick from totally anonymized resumes (the hypothesis being it would result in women being picked more often than usual because now the recruiters could not discriminate against women based on their names) and it actually resulted in men being picked more often than usual. So because the studies found women get preferential hiring on the basis of their feminine names, the recommendation is of course then to not anonymize the resumes.  tl;dr: they set out to find sexism, find that there *is* sexism but it's the *desirable* kind of sexism according to their agenda, so everything's business as usual.",0,0,0.116,0.312
post23lb,poorly branching,6,Google did this too. Had to give a bunch of men raises because they found the women were overpaid,0,0,0.243,0.201
post23lb,poorly branching,6,I’ve tried this experiment myself by using a more feminine nickname of my real name. It works alright.  This is why if I have a boy I’m going to have to name him something ambiguous like Taylor. The world is a horrible place.,0,0,0.167,0.187
post23lb,poorly branching,6,"Isn't AI trained on large amounts of real world data, so whatever biases are present in that data set will be present in the resulting AI condensed algorithms?  If women represent the squeakiest wheel, then they will be over-represented in complaints, which would bias the data because it only represents complaints and not applause (which usually is never presented as people rarely call up to applaud something that is working well, only when it isn't).",0,0,0.157,0.371
post23lb,poorly branching,6,Pay gap is a conspiracy theory that was debunked in the 90's.,0,0,0.17,0.259
post24hb,richly branching,19,"It’s not a racial bias.  Facial recognition systems rely on reflected light to extract the information about the shape, curves, and contours of a persons face. If you were paying attention in high school science you’ll remember light colors reflect more light dark colors absorb more light.  If someone has light colored skin more information is reflected back to the computer, if a person has dark colored skin less information is reflected back. The more information the computer has to work with the more accurate it can be about recognizing someone. Less information means the computer is going to be less accurate at recognizing someone.  Facial recognition will always be more accurate on lighter skinned people because more reflected light means more data which allows the software to more accurately make a match.  It’s not racist software or programmers, it’s not sample data being too white, it’s not a bias unconsciously baked in, it’s just less reflected light.",1,0,0.168,0.459
post24hb,richly branching,19,"So, it is a racial bias. Even if they try to work to solve this, at the end being black makes it more error prone. Which in return can cause a lot of issues.",0,0,0.206,0.38
post24hb,richly branching,19,If you call „skin color bias“ „racial bias“ then yes.,0,0,0.215,0.476
post24hb,richly branching,19,"If painting your face brown is racist, then yes, skin color is linked to race, and yes, it is the same thing.  Failing to understand this show a lack of common sense.",0,0,0.161,0.308
post24hb,richly branching,19,The machine is only as accurate as the amount of light being reflected. The amount of light reflected is a function of someone’s skin color. You can’t make the cameras see something that a person’s skin isn’t reflecting.,1,0,0.228,0.166
post24hb,richly branching,19,And how this isn’t racial bias exactly ?,0,0,0.299,0.51
post24hb,richly branching,19,"\^ This  People might as well complain to the laws of physics that it is biased and should check its privilege.  It could be just training data but whatever the results is, it will always be inherently less accurate for darker skins because of just basic Physics.",1,1,0.261,0.294
post24hb,richly branching,19,"If something is racially discriminatory in the results it produces, even if it's due to light reflection and not malicious intent, that still doesn't change the fact that using it will disproportionately harm certain people. That's still a problem.",0,1,0.224,0.518
post24hb,richly branching,19,"Oh so we should stop blood transfusions then? Because O- people and AB- people will be disproportionately harmed because they can only receive certain types of blood and can donate to other more?  No. Inherent skew and biases within the universe exists. We use those to get things done. It is not ""Racially discriminatory"", the laws of physics is not unjust or prejudiced. It is just itself.  Say that Police runs after people right? are criminals that happens to not be able to run good counts as ""Discrimination"" ? Then we should stop having police entirely?  Railing about any differences whatsoever is so counterproductive. People will be treated differently because of a million different variable in life. Live with it.  Facial recognition system is a way forward for law enforcement. It existing is better than nothing which is what the other option is. Would you really rather hand over the task to actually racist humans prone to error of judgement and prejudice?  Automation no matter how flawed inevitably gets ahead because humans no matter how adaptable are full of imperfections and cannot be expected to perform consistently close to 100% perfection in a single task. Not to mention there are actually evil and horrid humans existing out there. Even if AI could perform badly, we can still expect it to do better than a human eventually with enough development.  &#x200B;  Having facial recognition is better than nothing.",0,2,0.214,0.492
post24hb,richly branching,19,"That doesn't make it ok to use the technology in its current state.   You can still use the tech, but if you can't equalize the performance by improving darker skin more than light skin, then you have to artificially reduce the performance more for lighter skin than dark.   The fact is we can't continue using technology with known biases that reinforce existing ones which we agree as a society are unacceptable.",0,1,0.197,0.237
post24hb,richly branching,19,"The problem is that you're including ""Society"" into this. The laws of the universe won't give a shit about society.  If we use your logic, we can also say that we shouldn't use vaccines because not everybody can use it and it will be just unfair because a certain amount of people will get to live more = discrimination because those certain people won't get to reproduce as much as people who gets a vaccine.  No. The net result of having vaccines is better. Same thing with Facial recognition. Having facial recognition is still better than not having it. Would you rather hand off the job to actually racist humans?  Removing the human factor especially in law enforcement is almost always a good thing. It doesn't have to be the final say in these matters. It just have to provide information to those that do have a final say.",0,1,0.215,0.376
post24hb,richly branching,19,"I agree that we can't say the people who built it are racist, but the technology certainly is no matter what the reason.   In this case, you could ""fix"" the problem by inserting some randomness based on the skin tone to equalize the likelihood of mis-classifying a lighter-skinned face.   But somehow I don't think people would be comfortable with that.",0,0,0.164,0.429
post24hb,richly branching,19,"Technology is inanimate so it’s literally impossible for it to have feelings and express them through racist actions.  This whole issue is caused by the amount of light being reflected, so is the light racist for not reflecting enough",0,0,0.261,0.305
post24hb,richly branching,19,"I agree that a technology cannot be racist. The definitions of racism I can find imply racism is a property of as person.   However, I hope we can agree that a technology can reflect and reinforce racism. That doesn't make anyone or anything involved in its creation or operation racist. But the sum of it all ... can reflect and reinforce racism.  This technology does seem like a case of that. I don't know if you and I agree on that.",0,0,0.139,0.298
post24hb,richly branching,19,I wonder if it could be paired with IR sensors or something else so that it can analyse all skintones accurately,0,0,0.161,0.164
post24hb,richly branching,19,"It’s the laws of physics of the universe, lighter colors will always reflect more light, and darker colors absorb them.   Adding IR cameras are just going to see more reflected Infrared light.",0,0,0.134,0.089
post24hb,richly branching,19,Not quite.  Cameras can be fine tuned to any situation.  If the world were 90% black the cameras would be oversaturated on white people and pull out finer details on black people.,0,0,0.129,0.207
post24hb,richly branching,19,Because lighter skin reflects more light than darker skin,0,0,0.187,0.21
post24hb,richly branching,19,But the thing to remember is that cameras are built to produce good video of what is common.  So if what was common was different so would cameras.,0,0,0.171,0.093
post25hb,richly branching,15,"I actually think AI is vastly less biased than most humans if you give it the right directives, if ""biased"" is even the right term to use here.  I'm far more worried about my fellow biological dipshits who appear to have a difficult time living together for a few decades without trying to take each others rights away for literally zero reason.",0,1,0.191,0.447
post25hb,richly branching,15,"> AI is vastly less biased than most humans if you give it the right directives  Directives that come from humans, programmed and taught by humans, who are all biased.",0,1,0.171,0.459
post25hb,richly branching,15,"Another issue.     The more centralized something is, the more systemic bias can be.   Thus AI errors can be worse.  So, if you have a bunch of biased humans doing something, sometimes their biases cancel each other out. So, my ""Black woman who is an expert in distributed computing"" might be rejected by a racist or sexist boss but hired by another boss in the same company.  If you have a policy, for example, Donald Trump had an explicit policy that no Black tenants were to rent his properties. He got caught  If you have an AI that has deduced that Black women aren't good programmers, it's screening criteria can sometimes be hidden. Further the Black woman applying to multiple departments in the company is shut out by one centralized racist component",0,1,0.178,0.368
post25hb,richly branching,15,I think part of the problem is also people assuming the AI will be objective and more insightful than humans would be.,1,0,0.27,0.238
post25hb,richly branching,15,"so?  If a human that is shitty at basic arithmetic programs a calculator, doesn't mean that the calculator is going to be worse or as bad as the human who made it. You're not making a good argument here.  If there's one thing computers excel at, it's unerringly calculating without really giving a shit about anything else. This comes with its own problems, but certainly that quality will be good for avoiding some of the brainbroken mindfuck thinking traps that humans just love to fall into.",0,1,0.178,0.234
post25hb,richly branching,15,"> You're not making a good argument here.  No, *you're* not. You can't program bias into something that's either correct or it's not. 2+2=4 regardless of whether you're Martin Luther King or the Imperial Grand Dragon of the KKK.  > If there's one thing computers excel at, it's unerringly calculating without really giving a shit about anything else. This comes with its own problems, but certainly that quality will be good for avoiding some of the brainbroken mindfuck thinking traps that humans just love to fall into.  That is not how this works. There are entire fields of study dedicated to bias in AI and responsible AI use. I mean, why do you think most AI-generated images were of white people unless specifically directed otherwise? It's not cause white people are the majority! We're not talking about basic equations here, we're talking about things with a LOT of room for judgment, bias, and error.",0,1,0.196,0.335
post25hb,richly branching,15,"Did you hear of the Tesla that crashed in snow?  When your algorithm is biased it can be deterministically so  In general, human drivers are more fallible but unless we know about the snow issue, it will mean your probability of death in a snowstorm is 100%  An algorithm that decided Black women suck at computing will reject 100% of Black women whereas a pool of fallible humans will have hiring managers giving Black women a chance and one's not  Further using an AI without understanding the types of errors possible is an issue.",0,2,0.188,0.347
post25hb,richly branching,15,"I would add to Kali's statement that the issue is that people think things like this - ""Oh, it's an algorithm, it's not biased"", and thus allow it to magnify the effects of the biased input data.  It's hugely problematic and one reason that many CS programs are requiring more ethics/philosophy/humanities type courses.",0,1,0.229,0.369
post25hb,richly branching,15,"A competently taught first year course in statistics would also explain the issues of bias as would advanced courses in experimental design  You don't need a philosophy course to step outside the box and look at what garbage in, garbage out means",0,0,0.293,0.287
post25hb,richly branching,15,"Most of the CS students I've advised and taught would benefit from a philosophy course.  They understand GIGO, but they don't understand what makes something garbage without something in the humanities when it comes to issues like demographics.  First year courses in statistics simply teach methodology, not how one recognizes and corrects for input bias.",0,0,0.295,0.177
post25hb,richly branching,15,"look, of course I know that the output reflects the training data to some degree, the training data isn't clean, at some point the AI will produce its own data which ends up in the training data and that sort of autocannibalism will cascade and create biases in the AI, blabla that's all elementary.   What I'm saying is that whatever technical faults and imperfections you can identify with AI, humans are certainly much worse. The average human still thinks that appeals to nature are a logical argument. The average human still thinks that women are from venus and men are from mars.  This is not a question of ""is AI biased"" - of course it is. But is it more biased than humans? Fuck no. Humans are goddamn awful at thinking straight. It's a miracle we ever got as far as we did.",0,2,0.171,0.416
post25hb,richly branching,15,"Humans are capable of thought, and some of those thoughts is ""I might be biased"" and ""this data might be biased"".  And humans are capable of self reflection and acting on it, such as things like ""if I'm biased, is that a good thing? If not, how to I minimize it"".  AI models are not sapient. They do not think. They do not understand.  ChatGPT isn't thinking or talking or writing. It's just outputting the statistically most likely set of words back to you that fit the input words.  It's basically a really good search engine that will rephrase the internet consensus on a subject and parrot it back to you.  The bias it inherited from its training data is built into every response, without recourse of fixing it. That's why they place guardrails on it to try to *prevent* biased from reaching the user, because the bias cannot be removed.",0,1,0.265,0.377
post25hb,richly branching,15,"Please consider my Black computer programmer example.  Assumption :  There are fewer Black women in computer science than several other Demographics such as White men, Chinese men, Chinese women, etc  You ask the AI, please look at these resume and select the candidate most likely to be qualified   The AI has analyzed the entire set of computer professionals and concluded it will NEVER hire Black women.  Unless we are aware of this type of error, we have ""Garbage in"" and ""Garbage out""",0,0,0.222,0.272
post25hb,richly branching,15,"> I actually think AI is vastly less biased than most humans if you give it the right directives, if ""biased"" is even the right term to use here.  Technically correct. But also, AI is never trained on unbiased data.  A large language model like ChatGPT is trained by indiscriminately scraping the entire internet, meaning that internalizes what gets said the most. Image-processing models follow a similar process, meaning that they internalize what gets shown the most.  Even AI that isn't trained on literally the entire internet gets trained on the data available to the programmers. So, for example, cameras programmed by white people tell Asian users ""It looks like somebody blinked. Let's take that picture again."" Because they've only ever seen white faces. Sinks programmed by white people don't activate when they detect a black hand. Because they've never seen black hands.  Nobody is trying to make racist or sexist software. But it's a lot more difficult and more expensive to make software without bias. And usually deadlines and budgets are the top priority.",0,1,0.15,0.453
post25hb,richly branching,15,"It's worse.   Even if you have unbiased data, that unbiased data can reflect the reality that correlations exist that aren't causations",0,0,0.217,0.352
post25lb,poorly branching,7,What a strange project. Were you given this project title or did you choose it?,0,0,0.225,0.235
post25lb,poorly branching,7,"It's not strange lol, the earliest studies of AI implementation have already found that certain biases are built-in unless specifically accounted for.  For example, while selecting top candidates during a hiring process, Amazon’s automated resume screening system discriminated against women. The data used to train the recruitment model was informed by resume samples from a 10-year period, where women were underrepresented. The resume screening model thus used “linguistic signals” associated with successful male candidates.  Basically because AI draws from the past to find patterns relevant to its programming, it's also drawing all the biases from the past. So if an AI program notices that women are not represented in the past hires, it doesn't understand the context as to why. It just assumes that women must not be good at the job and gives applications with male-sounding names more points automatically. This type of bias extends to things like race, language, etc.",0,0,0.13,0.312
post25lb,poorly branching,7,"The algorithmic bias literature is ironic because there is this idea that AI models are biased because they're built on unsophisticated evaluation of empirical data, but virtually none of the literature evaluates data bias in a sophisticated way, instead treating it like a social contagion or miasma. Humans draw conclusions from empirical data too, and the miasma model of bias is probably more accurate for human beings' reasoning than for machines'.",1,1,0.183,0.444
post25lb,poorly branching,7,Why is it strange ? Yes I picked it myself,0,0,0.298,0.317
post25lb,poorly branching,7,"It is strange because it is so specific, but at the same time you seem to know nothing and no idea how to proceed.   Usually you either base the project on something you already know something about, or you start with a less specific title and hone in as you learn more.",0,0,0.283,0.208
post25lb,poorly branching,7,[removed],0,0,0.366,0.413
post25lb,poorly branching,7,"Don't show up here just to troll and take potshots at the locals, please.",0,0,0.206,0.34
post26hb,richly branching,4,It is never AI on it's own it is how we set up the system. In the current system we could imagine inequity growing some because our system is not very proactive.  But ultimately we live in a democracy where the majority rules and there is no great advantage to hoarding.  If AI creates better efficiency then ultimately everyone will benefit.,0,0,0.216,0.198
post26hb,richly branching,4,"I suppose in theory we live in a democracy, but I fear that we will move away from that and more power will be concentrated in the minority and not the majority",0,0,0.122,0.17
post26hb,richly branching,4,"It is not a theory. We live in a democracy.  Yes, if the majority of people decide to live in some other type of system it is possible. The majority voted for the current system.",0,0,0.144,0.158
post26hb,richly branching,4,"It’s a democracy until the leader decides they don’t want to give the power back, which isn’t impossible. But I could see Ai making democracies less democratic in general as it makes spreading misinformation easier, propaganda to manipulate vote, can be used for surveillance etc",0,1,0.127,0.164
post26lb,poorly branching,4,"From the research paper, the old strategy was to train the AI and then brainwash it afterwards. Now it's possible to brainwash the AI during the training phase.",0,0,0.299,0.201
post26lb,poorly branching,4,What do you mean by brainwashing?,0,1,0.356,0.333
post26lb,poorly branching,4,"They're finding new and creative ways to correct the AI's ""perceptions"" until it matches our collective prejudices.",0,0,0.279,0.336
post26lb,poorly branching,4,"""For example, when considering a word like 'nurse,' researchers want the system to remove any gender information associated with that term while still retaining information that links it with related words such as doctor, hospital and medicine."" (Citation)  &#x200B;  I see. We think about a woman, with the word nurse. But the system has not to know this.",0,0,0.26,0.302
post27hb,richly branching,17,"So does anyone have sources from ""AI Experts"" that discuss this?  I'm not particularly young and thus have gone thru several technology waves. For each of those I could see the ""alternative"" job and benefit. For instance while PCs eliminated jobs if also made many more and made most people far more productive.   AI in conjunction with automation advancements is the first technology that I dont see the ""alternative"". In short I can't see any job or job skill that this combination can't do and pretty much in every case can do better than even the most skilled human labor.   I'm by no means an AI expert, which is why id like to see this exists discuss this topic.   Id like to know if I'm good or whether I should be investing my retirement funds in some acreage where I can grow my own did till I die.",1,0,0.192,0.129
post27hb,richly branching,17,"This is exactly how AI will work as well.  It will eliminate some jobs, but people will need actual workers to input the data.  It's like that joke where the mechanic charges $10 to swing a hammer but also charges $9990 for knowing where to swing it",0,0,0.183,0.246
post27hb,richly branching,17,"Why can't AI input the data? The assumption here is that only humans will be able to discover new data and I think that is a false assumption.   Once the model has an understanding of the tools used to collect new data, discover new data, which I think is not far away and may already be happening, no need for humans to enter data.",0,0,0.244,0.147
post27hb,richly branching,17,Because how do you know if it's right?  Relying on AI to always tell you the truth is dangerous,0,0,0.21,0.269
post27hb,richly branching,17,"Because the AI training the AI generates mostly nonsense. The skill with AI comes with knowing how to coax it into generating the correct response to any given input. If you don’t control the input data, you can’t reliably control the output data.",0,1,0.214,0.142
post27hb,richly branching,17,It’d also going to generate jobs in things like green energy and chip manufacturing in the US.,0,0,0.189,0.124
post27hb,richly branching,17,"I like that joke, I've used it a few times before.",0,0,0.277,0.246
post27hb,richly branching,17,"It's hard to pin down where AI experts actually are. You can see a lot on Twitter - actual researchers at these big companies, talking about research papers - but also expressing their personal feelings on the topic.   There are places like LessWrong, where AI safety and capabilities have been discussed by researchers and scientists, as well as random smart people, often more with a focus on what the future will look like, what safety considerations can be put in place - if at all any?   It's hard to really pin down one place where these discussions take place, but as someone who has made this topic a... Hobby, I would in my (probably very bias) opinion say that the vibe is that everyone is thinking we are 3-5 years away from AI that can do anything on a computer a human can do, better. Everything from software development, to video editing, to AI research.   Not 100% of the researchers, but I would say the majority, and that shift from minority to majority was sudden and recent.   I think if you want to see what those particular researchers would probably say is how the next few years look like -  www.situational-awareness.ai  I keep sharing this to people who are interested enough to want a place to start researching the topic. It was written by an AI safety researcher who was fired from Openai for purportedly being too loose with his lips. Take note that this was written in June of last year, and its predictions on how much would be earmarked for ai datacenters seemed very out there, even to the enthusiasts with their finger on the pulse.",0,0,0.207,0.165
post27hb,richly branching,17,"Thank you very much for the information and I will check it out.   Seems that unlike historical technology introductions where physical labor was replaced, this is the first time intellectual labor is being replaced.   The 3 to 5 year number does not sound unrealistic to me. After that another 5 to 10 automation to advance to a point where it can physically do anything a human can. Another number of years for the implementation phase and then what?   The ""then what"" part is what id like to see discussion around.",1,0,0.231,0.134
post27hb,richly branching,17,"I’ve also seen many technology bubbles in my career (late 50s, electrical engineer).  AI really feels to me like it’s firmly in the hype/bullshit phase of the new technology cycle, with so much invested by so many people that they’re afraid to point out the basic flaws; AI promises to take over many jobs and make people redundant, but it just can’t do that, and won’t.  Eventually somebody will notice. I expect a huge crash in the AI sector in a year or 2.  That’s not to say AI isn’t useful; it definitely has potential as a force multiplier for some tasks, but I just don’t see it as the apocalyptic technology being projected.",1,1,0.229,0.214
post27hb,richly branching,17,So far the only real commercial use they’ve found is helping customer service agents draft responses faster - but a real agent still needs to read and edit / approve before hitting send.  No industry has yet replaced humans with it.,0,0,0.255,0.162
post27hb,richly branching,17,"When, was the last time you killed a cow, you did eat a hamburger within the past 30 day's yes. Just asking",0,0,0.173,0.213
post27hb,richly branching,17,Never thought about it like that. Very interesting,1,0,0.459,0.304
post27hb,richly branching,17,"If the companies that employ AI still have some shred of sense, there will be at least a few years where  people verify and vouch for the results that AI spews out.",0,0,0.163,0.187
post27hb,richly branching,17,Idk. Have you seen how social media has decreased the collective productivity of our society?  Not to mention I hate being my own damn secretary now because of PCs,0,1,0.252,0.21
post27hb,richly branching,17,I worked on a project that reduced analyst headcount by 25% by implementing - you guessed it - AI,0,0,0.234,0.169
post27hb,richly branching,17,"The only jobs that will be created are for professional Luddites to go around and smash the computers running AI. Joking, not joking.",0,0,0.24,0.174
post27lb,poorly branching,9,"I've seen much worse Science-Fiction, to be sure.  Meanwhile, in the real world, many tech corps who were very eager to adopt all kinds of AI tools (mainly because their clients were) are fast backpedaling on finding out that LLMs and MLs are only as good as their training datasets, and only for the most basic of tasks, like pattern recognition and exploitation, make costly mistakes, and will not significantly advance in the near future. Not a single one of them can replace a junior programmer or analyst, and are barely ready to augment the productivity of their human partners. Also, the number of people assigned to curate datasets for AI use is skyrocketing.",0,0,0.2,0.154
post27lb,poorly branching,9,"I'm with you. 100% my personal experience matches. I'm a coder (data science/UX/data engineering) - the current AI makes certain lower level things easier (but its work needs to be checked a TON because it just makes up huge swodges of stuff.  ex:  it will make up entire functions.  It doesn't tell you what the function does, it just calls it to solve some portion of the question.  When you tell it that function doesn't exist and to write it, it has no idea what you're talking about.  You have to give it explicit requests for that particular function and you've got a 30% chance it'll give you functional code).  I think it'll get better to some degree, but not to replace anyone (even juniors).      So far it shows bare creativity and almost no capacity to truly solve complicated issues. ""Complicated"" meaning anything that has multiple chained step that require continuity or anything that requires being inventive.  The only questions it can answer are those that are straight forward enough to already HAVE an answer.  Anything else? Not yet.  On top of that, its only as good as its training set.  So if you've got a new modality, you're entirely on your own.  Even if AI assisted in designing the newShinyThing, it probably can't help you use newShinyThing for anything other than the prompt it was designed for and sometimes it'll just be like 'wat? I've never heard of newShinyThing.  Do you mean R Shiny?""      I think there are definitely advantages to AI in certain regards, but I'm not even remotely worried for the next 5 years.  And unless they can solve the energy and water usage issue the cost of use is going to become fairly insurmountable.  SO:  Optimism for the next little while -> I think there are some portions of some jobs that will become TRULY entry level for ""prompt engineers"".  Folks who are trained to ask the AI questions to get certain results and then QC those results will have a whole job doing that.  It likely won't be current 'Tech Salary', more like entry level office job.  That'll lead some of those people to get frustrated as hell and upskill until they can code and help with the more comlicated stuff.  Certain other jobs that exist now will need fewer people, but because AI will allow faster throughput of the lower level stuff.  There will be more time for the thinky-stare-into-the-middle-distance-with-a-mildly-haunted-expression.",0,2,0.261,0.16
post27lb,poorly branching,9,Do you think much has changed with the newer ai models ?,0,0,0.259,0.178
post27lb,poorly branching,9,They've gotten way more convincingly wrong.  The gibberish code tends to have the correct white space.  Also I trained mine to be sarcastic somehow.,0,1,0.221,0.332
post27lb,poorly branching,9,You're right this AI thing is just a fad and going nowhere. In 2-3 years it'll be completely gone and forgotten. It's literally useless.,0,1,0.256,0.302
post27lb,poorly branching,9,It likely won't be forgotten as it has *some* uses. But the reality check for all the hype will be painful.,0,0,0.283,0.267
post27lb,poorly branching,9,Nice sarcasm.,0,0,0.391,0.391
post27lb,poorly branching,9,Are you a developer?  Have you been using AI for development work?,0,0,0.298,0.189
post27lb,poorly branching,9,Yup.,0,0,0.28,0.241
post28hb,richly branching,24,"> So first let's look at what happened so far, let's use the US as an example. 50 or 60 years ago the middle class in the US was actually bigger than it is today. Since then income inequality has significantly increased.   This is where you lose me.  The ""middle class"" is a wholly invented construct.  It developed as a way to describe the people who were not rich but also not poor, but also not working class.  It's an inexact classification with little utility.  Income inequality has risen, yes, and the ""middle class"" has shrunk in the United States.  Worldwide, poverty has plummeted as well. As much of that is literally true, however, it's because the middle class are becoming the upper class in the United States and we're finally addressing third-world poverty.  Clearly, the rise in wealth inequality is not making any  of those things *worse*, so why are you bringing it up?  > But so that bring me to my main point, which is that technological advancement will most likely relatively soon reach a critical threshold, which will cause most human labor to lose its value, not just low-level labor.  If we consider how much technology has progressed in just the last 10-20 years, if we consider how rapidly AI has progressed in just the last few years, then we can only dream about how hyper-advanced society will be in say 25 years of 50 years.  This argument crops up every single time a new technology hits the market.  In case you missed it, LLMs are *not good at what they do* in a lot of ways.  It's not on track to replace much of anything given how relatively stagnant the whole thing is.  Given the hallucinations and what have you, we're a ways from generative AI, and even that won't be ready for prime time on release.  Microsoft Excel didn't make accountants redundant.  ATMs didn't kill the bank worker.  The luddites have never been correct.  > But once AI reaches a certain point, the capitalist class will have no more use for the vast majority of the human population, except for a tiny minority of exceptionally gifted, exceptionally intelligent and exceptionally motivated group of extremely high-level workers who AI and automation cannot yet replace.  We're all the capitalist class, friend.  Capitalism won.  The world has never been more prosperous, and its people more better off, than it has under capitalism - especially following the fall of the Soviet Union.    We're all capitalists.  We have come to the understanding that markets are the best way to distribute goods, that supply is the primary economic driver, that economic freedom is as important as any other.  The *most likely* worse case scenario is that AI displaces a nontrivial number of jobs and the people it replaces do something else, just like they have every other time some seismic technological advancement occurred.  It's highly unlikely that this would occur, either.",0,0,0.142,0.117
post28hb,richly branching,24,"Great answer. Small nitpick and a comment.   I’m a dev. I own a dev company. We weren’t hiring at breakneck pace to begin with —I look for real talent, and that’s rare—but the most meaningful difference I’ve observed since LLMs hit the scene is that our releases are massively more frequent. We’re shipping product like never before.   I can’t recall the nitpick, but my impression is that this is in fact a hugely transformational technology in my field, and yet it has caused us to fire nobody. Everyone is 100x more productive, and we get the dopamine hit of seeing ideas become reality at an incredible pace.  Plus, we no longer have to do the drudgery of documenting product, writing tests, etc. nobody wanted to do that before, and now we don’t have to.   This is supposedly the end of dev jobs, and yet I feel like we’re in a golden age.",1,0,0.285,0.21
post28hb,richly branching,24,"I'm deeply, deeply skeptical of AI's utility, but I can recognize that it does *some* things well.  I just feel like we're talking about Microsoft Excel putting accountants out of business again.",0,0,0.256,0.156
post28hb,richly branching,24,"Just in case it's not clear.  I completely agree with you.  I just think you underestimate the impact of this tech.  But even if it's 100x as impactful as you imagine it to be -- and it is -- it will still mean we're all much more productive, and far better off.",1,1,0.274,0.235
post28hb,richly branching,24,"My point is that even is excel was as transformational as AI, which it isn't, it still wouldn't put accountants out of work.  Man, if you could see how we work now, you'd lose much of your skepticism and probably change how you work as well. This paradigm works for almost every kind of written work. I also use it for planning, administration, contract law, and marketing. It's life-changing.  Things haven't changed as much in 25 years as they have in the last 12 months.",0,0,0.24,0.157
post28hb,richly branching,24,"This happened before when the compiler came out. Compilers allowed people to code at incredible speeds compared to before, and everyone thought their jobs would be gone since one person can now do the work of many. Just created new demand since they can now release a lot more and take on many more projects. Sounds like the same is happening now. Glad to hear your experience is similar.",0,0,0.21,0.148
post28hb,richly branching,24,"People keep saying ""AI will create as many jobs as it kills"" but they can't actually say what those new jobs will be lol.",0,0,0.179,0.179
post28hb,richly branching,24,"Middle class mostly becoming upper class is false. We are seeing a larger and larger percentage of the US population (anyway) with a smaller percentage of total wealth.   You are repeating propaganda, not actual facts.  Actually, this whole post is basically every capitalist propaganda trope rolled into one.",0,0,0.17,0.207
post28hb,richly branching,24,"> Middle class mostly becoming upper class is false.   [Sorry, you're wrong] (https://imgur.com/a/EXKtFYz).  > Actually, this whole post is basically every capitalist propaganda trope rolled into one.  I mean, it's not propaganda to correctly note that we're better off under capitalism.  It's just facts.",0,2,0.224,0.24
post28hb,richly branching,24,"Try actual studies and data, instead of a random picture on the internet:  https://www.statista.com/statistics/203961/wealth-distribution-for-the-us/  https://www.pewresearch.org/social-trends/2020/01/09/trends-in-income-and-wealth-inequality/  https://www.cbo.gov/publication/60807#:~:text=Between%201989%20and%202022%2C%20the,distribution%20increased%20by%20285%20percent.",0,0,0.085,0.11
post28hb,richly branching,24,Wake me up when past performance becomes a guarantee of future success.,0,0,0.253,0.25
post28hb,richly branching,24,"the capitalist class is defined by an economic relationship, not by your existence within a capitalist society that ""has never been more prosperous"" (by capitalists' own definitions, maybe)  ""economic freedom"" is nothing more than the ""freedom"" given to capitalists to rape the planet and dominate the rest of us",0,0,0.175,0.179
post28hb,richly branching,24,Last I checked not like any of the other non capitalist systems did any better,0,0,0.22,0.253
post28hb,richly branching,24,"its a question of whether or not you believe that you as an individual have the inherent worth to demand an equal say and share in your society  everything ""works"".  slavery ""works"".  the question is who is it working for",0,0,0.22,0.297
post28hb,richly branching,24,"> the capitalist class is defined by an economic relationship, not by your existence within a capitalist society that ""has never been more prosperous"" (by capitalists' own definitions, maybe)  No.  We're not all Marxists, sorry.  The ""capitalist class"" are all of us.  We are all capitalists.  We rely on the advancement of capital both for our own livelihoods, but for the world around us to operate.    The people who tell you they are not part of the capitalist class just haven't realized it yet.  > ""economic freedom"" is nothing more than the ""freedom"" given to capitalists to rape the planet and dominate the rest of us  Ah, yes, the fact that I have the ability, if I so choose, to open my own business, work for myself, etc., it's all to serve those evil capitalists trying to actually dominate me.  If ""we'll largely leave you alone"" is domination, then thank you sir, can I have another?",1,0,0.165,0.212
post28hb,richly branching,24,"then what does the ""capitalist class"" even mean; if you're taking capitalist class to mean anyone that lives within a capitalist society then the term ceases to have any real descriptive meaning  a class can only mean something by its relation to something else.  that's what classes define: a hierarchy, social stratification.  if everybody is in a ""class"", then it isn't a class.  its like saying ""everything is a base"".  a base is only defined by its opposition to an acid.  saying ""everything is a base"" makes no sense, its depriving the term of its intended meaning.  if you're starting your own business, then you're trying to become a capitalist, you're trying to join the class that dominates the classes below them",0,2,0.195,0.171
post28hb,richly branching,24,"Yeah I was confused by the “capitalist class” name. We’re all living in a capitalist system. If by “capitalist class” he means “upper class”, just say that.",0,0,0.211,0.196
post28hb,richly branching,24,"The average American owns nothing. Not their home, not their labor, etc.  The majority of people are not capitalist. They own almost no private capital. The majority of people are the exploited workforce that capitalism relies on.",0,0,0.138,0.144
post28hb,richly branching,24,"Is ""capitalist class"" a meaningful classification if we all fall within that classification?",0,0,0.199,0.224
post28hb,richly branching,24,"It’s not by capitalist definition. It’s by definition of metrics as poverty rate, real household income, access to energy and electricity, life expectancy and many more.",0,0,0.216,0.147
post28hb,richly branching,24,"""poverty rate"" is an arbitrary measure, it can be set at whatever level its measurers prefer.  what is ""poverty""?  is there an objective definition?  income ""rises"" because production increases over time; access to goods increases.  relative incomes do not rise over time, they actually fall.  people get smaller and smaller shares of the pie over time  access to electricity and life expectancy are measures of development, of technological progress.  you can see development occur in socialist states and also see huge increases in life expectancy.",0,0,0.132,0.097
post28hb,richly branching,24,"Funny definition of capitalism you've got there.  I suppose everyone living in feudal times was a Lord, too?",0,0,0.151,0.16
post28hb,richly branching,24,"Well, part of the middle class is becoming the upper class, sure. That's what I said in my OP as well. But another part of the middle class is becoming the new lower class.   But my point is that as AI and technology advances at an ever faster rate, soon AI will also be able to replace upper class workers like engineers, architects, doctors etc. The reason why some middle class people have moved into the lower class because their labor no longer has much value due to automation. But for now, new upper class jobs have also been created.   But what do we do when AI and technology become so advanced that even engineers, and doctors and bankers and marketing specialists and whatever can be replaced by AI systems, robots or other technology?   So once we reach a certain technological threshold for the first time we would not only see a shrinking of the middle class but also shrinking of the upper class.   And no, we're not all capitalists. Many of us, especially those of us in the West, for now, benefit from capitalism to some extent, sure.   But what do you do once the owners of the means of production have no more use for the vast majority of people, because AI and robots are way more effecient at every economic tasks those people could do? At that point, are you also gonna benefit from the system of capitalism if your labor has no more economic value?",0,0,0.2,0.178
post28hb,richly branching,24,"> Well, part of the middle class is becoming the upper class, sure. That's what I said in my OP as well. But another part of the middle class is becoming the new lower class.   The data doesn't bear that out.  There is no increase in the middle class moving to the lower, statistically.  > But my point is that as AI and technology advances at an ever faster rate, soon AI will also be able to replace upper class workers like engineers, architects, doctors etc.   Yeah, I don't buy it.  Like I said, we can't get it to count numbers right.  Even if the enterprise-level models are superior, LLMs aren't going to pull this off anytime soon and in the off chance that we start seeing some impacts, there's no reason to believe this time will be different.  > And no, we're not all capitalists. Many of us, especially those of us in the West, for now, benefit from capitalism to some extent, sure.   More than benefit, we are the capitalists.  > But what do you do once the owners of the means of production have no more use for the vast majority of people, because AI and robots are way more effecient at every economic tasks those people could do? At that point, are you also gonna benefit from the system of capitalism if your labor has no more economic value?  By selling your labor somewhere that it's valued.  The same way we did every single other time.",0,0,0.176,0.175
post28lb,poorly branching,3,You as a photographer will have to learn how to use those tools.   People have been saying that AI will replace me as an IT guy. That's not going to happen anytime soon. Same with you.   All industries are changing. We have to adapt.,0,0,0.191,0.141
post28lb,poorly branching,3,"If anything it'll make IT more interesting by streamlining some of the boring and repetitive stuff so I can focus on the hands-on work and complex, interesting issues that I am going into the field for. Hopefully it won't affect the barriers or entry too badly though, because those of us at the bottom are having enough trouble getting hired.",1,0,0.23,0.155
post28lb,poorly branching,3,"That sounds pretty similar to what we told office workers in the 1970s about automation. Turns out, there were a lot more office workers than complex interesting issues.",1,0,0.295,0.236
post29hb,richly branching,28,"At the same time, other more difficult jobs emerge and can now be done by individual people, with the help of AI. Already seen it happening. For instance, people are commissioned to make whole short films. That now takes new skills: screenwriting, sound design, cinematography, AI toolchain understanding, taste and so on.  (And no, I'm not arguing that long term every job is safe, because who's to say AI won't also direct and screenwrite and such.)",0,1,0.226,0.179
post29hb,richly branching,28,"Those aren't jobs emerging though. It's literally what would have been many jobs turning into one job. And even in this case, the clock is ticking. Because if one person can all those things, very quickly that commission will disappear because ""why would I hire someone to do what I can do myself with the help of AI?""  I have yet to see a case of someone describing a ""new"" opportunity created by AI that isn't just a combination of these two forces - massive job reduction, and selective (wishful) thinking.  As though ai progress is going to suddenly come to a stop once the work that could be done by 10 people can be done by one. What do you think makes you so special as that one person that the thing you're contributing is the thing that can't be replaced?  If we really end up being able to replace most jobs, then we're going to be able to replace every job. Human labor will have no value, if you didn't already have the wealth to not need to work, you're gonna SoL, on permanent minimum wage government ""UBI"" forced into compliance because they literally control your ability to survive.  It is definitely not a happy path.",0,0,0.166,0.173
post29hb,richly branching,28,"Only way I can see new jobs is if we start valuing things currently seen as unimportant more.  Like, if you took current good salaries and paid a lot of people that much to act as social workers, there is a lot of suffering that could be addressed before you ran out.   You could give out grants for people to preserve traditional arts like drawing cartoons by hand or overly elaborate parades or hand made clothes.  Hell, you could give people PPE or remote controlled drones and have them sort through old garbage heaps to reclaim stuff, even if that’s not very efficient. It’s still a way to get resources without destroying more nature.  There’s lots of work we *could* be doing to make life better that we currently ignore as insufficiently profitable. We’d just rather let people descend into squalor than pay them to do anything about it.",0,0,0.209,0.174
post29hb,richly branching,28,"In some cases it is multiple jobs turning into one, yes, but in other cases, it actually increases artistic scope, and new forms of expression emerge.  As a random example, I created an AI-based visual storytelling engine that runs in Twitch, and I live-DJ'd for the community currently having fun playing the story -- by incorporating their names and comments into ad hoc created music lyrics, again composed with the help of AI, and played back to them in realtime with lots of laughter to be had. Creativity wasn't gone, it just moved to a meta level.  I spent my days working on many different such projects, and if you're genuinely interested to learn about that meta level I'm happy to expand on it. I just don't aim to debate or change your mind, it's fine if you have your opinion as is and I can totally understand and empathize with where you're coming from.",0,0,0.358,0.287
post29hb,richly branching,28,"Ok, so let's assume this is a paid activity. You are being an ""entertainer"" (and a freelance one at that). That's not a new job, even if you are incorporating novel media to perform that job, as entertainers have done since it's been a thing.   And as far as jobs go, it's not great when it comes to being secure or lucrative (for the vast majority of people doing it). Responding to mass unemployment with the prospect of freelance entertainment isn't job creation any more than signing up to drive Uber or delivering DoorDash (although at least those arguably have more reliable demand with a lower barrier of entry).",0,0,0.312,0.172
post29hb,richly branching,28,Yeah it is not a problem though.    One person will be able to pump out 30 projects a month instead of 3. Prices per project will go down. There will be more economic activity because of the low prices. Because of more economic activity more jobs will emerge,0,0,0.156,0.113
post29hb,richly branching,28,"If we can automate from needing 30 people to do a job to just one, odds are we'll be able to automate that one remaining person as well.  I don't know why people think there's some magic that means at least one person will always be required. It's wishful thinking.  > Prices per project will go down. There will be more economic activity because of the low prices.  If you think lower costs of production means savings get passed on to consumers, you haven't been paying attention.  > Because of more economic activity more jobs will emerge  In the US the top 10% are responsible for nearly half of all consumer spending. Just because economic activity is happening, doesn't mean the majority are or will be the beneficiaries.",0,1,0.173,0.173
post29hb,richly branching,28,"But those job ALREADY existed. Short films were created in the past. By a team of talented and hard working people, getting paid.  Now it is going to be one guy, getting paid much LESS than a whole team.   Nothing new emerges.",0,0,0.141,0.148
post29hb,richly branching,28,"This is [Jevons paradox](https://en.wikipedia.org/wiki/Jevons_paradox) in action, when movie making becomes easier, you don't just make movies faster with fewer people, but people end up making much more movies, because they are cheap now, thus resulting in more people being employed making movies. That's how improvements in technology have worked out numerous times in the past.  That said, I don't think it will happen this time around, at least not for long or at the scale necessary. The reason being, human attention is limited and AI can create stuff at an insane pace. Hollywood right now makes around 150 major movies a year, that's small enough that you could still watch everything if you really wanted to. If AI turns that into 1500, you don't end up with a movie market 10x the size, since nobody got time to watch all of them. We are reaching a point where humans have enough entertainment at their fingertips to last multiple lifetimes.  >> screenwriting, sound design, cinematography, AI toolchain understanding, taste and so on.  And as for those skills, all of that is stuff AI can do. Not right now and not in the quality needed, but AI progress means that all those things that still require human touch right now will fall away as time goes on.",0,0,0.186,0.137
post29hb,richly branching,28,"> If AI turns that into 1500, you don't end up with a movie market 10x the size, since nobody got time to watch all of them. We are reaching a point where humans have enough entertainment at their fingertips to last multiple lifetimes.   It's how it's been with books. Nobody can read them all.",0,0,0.134,0.133
post29hb,richly branching,28,"Your last paragraph mirrors my last paragraph, so: yeah, that's a possibility. I also see other possibilities and can describe them if wanted, but nobody is an expert on the singularity yet -- not even the singularity experts!",0,0,0.182,0.229
post29hb,richly branching,28,"On the other hand, though, you also unlock improved production values for long tail projects. You can hit weird and specific niches in a way that you couldn't previously. Experimentation becomes substantially easier.  I don't know how much that increases demand, but it's not zero. YouTube could get a lot more interesting.",1,0,0.283,0.177
post29hb,richly branching,28,"even with just Humans involved in the production chain we have are near peak output. bout a decade ago the head of FX tv was talking about the era of peak TV where there were so many high quality shows that people couldn't watch them all. And with older media being so accessible it just adds to the mass. There are still many people who haven't seen The Wire and there are so many other great shows, books, games and that doesn't include the time hanging out with  friends and other socialising.",0,0,0.152,0.214
post29hb,richly branching,28,"Agree with the latter part, ie once the market is saturated there is no market for “much more movies”.  We are already hitting saturation for streaming TV shows.  There is only so much of people’s time to compete with, and the industry lives on celebrity power, word of mouth, and awards.m, which are natural gatekeepers to consumer time and attention.",0,0,0.135,0.202
post29hb,richly branching,28,"*New films* emerge, films that just would never have gotten made because one guy, by himself, couldn't afford to *pay* that ""team of talented and hard working people"" to help him make his dream project.",0,0,0.193,0.167
post29hb,richly branching,28,"We're already drowning in entertainment slop, theres more of it than people could ever wish to watch. So what follows is a run to the bottom. Again.",0,0,0.306,0.329
post29hb,richly branching,28,"... dude, we don't need this crap mass produced. There's already TOO MUCH stuff. I need fucking money and a place to live.",0,0,0.144,0.219
post29hb,richly branching,28,"So that is same thing, only cheaper.   My point is that no new jobs are created",0,0,0.185,0.175
post29hb,richly branching,28,"To be fair, maybe that's just one shitty movie no one want to fund.",0,0,0.175,0.179
post29hb,richly branching,28,"It’s up to the consumers to recognize good writing and production then.  Which I don’t have much confidence in, unfortunately.  Once again Mike Judge proves to be [the greatest prophet of our generation](https://youtu.be/kJZjU2k5abs)…",0,0,0.267,0.245
post29hb,richly branching,28,"But it’s the consolidation of jobs into less number of jobs. The net job total is lowering, while the existing jobs become further niche and skilled.   That’s not good news for white collar workers.  It’s the widening of poverty. Wealth moves further upward.",0,0,0.175,0.147
post29hb,richly branching,28,"That's a good point to discuss, I'm answering that [here](https://www.reddit.com/r/artificial/s/4EJrJO84ZI).",0,0,0.338,0.211
post29hb,richly branching,28,"I'd also add something nobody seems to mention often enough. By going about this mindset of AI coming for your jobs, companies risk canibalizing their own customer base. Who is going to buy their shitty products when nobody has a job? The correct and healthy mindset should instead be expansion and upscalling provided by all this additional productivity brought by AI.  TLDR ""AI coming for your jobs"" is a short-sighted mindset and a recipe for these companies to become irrelevant",0,0,0.198,0.173
post29hb,richly branching,28,Any of those short films any good?,0,0,0.163,0.179
post29hb,richly branching,28,"It depends on your taste. I made [this film](https://youtu.be/YMNzWtXE5aY), for instance. I would think like with other forms of expression, some like it and some don't. That is fine, I think.",0,0,0.204,0.188
post29hb,richly branching,28,"I watched the whole thing, and I tried to judge it in my head by the standards that I would judge a normal short film, not AI. And I will say that it was kind of shit. The AI artifacting, strange poses, model shifting, the flat voices, it all comes off to create a pretty off-putting product, compared to other short films.   Watching it keeping in mind how amazing it is that a computer can generate this at all, it's impressive. But judging it as a short film on its own, there's just a lot that disrupts the message, storytelling, the emotions.   One specific piece of feedback is that the scene with the mirror test was way too short, but some of the other scenes were way too long. Especially with the mirror test being the point where the protagonist breaks through, the significance of would likely need to be highlighted to somebody who isn't aware of the test in the first place. Like I knew what happened there but I feel like somebody who doesn't know the task would be confused by how quickly that went by. Overall, the pacing was the worst part.  The restrictions of the medium you use often strongly influence what the product is, and I think that it should be telling that you chose to make a film about an AI being trained with your AI trained tools. If you tried to make a short film that wasn't at least somewhat about AI, all the artifacting would be so out of place that it would ruin it entirely.  I do want to say that I am glad I watched to the end. I actually think that this could be an interesting short film if it was made through traditional means, either live action or animation. I think there also needs to be a lot more clarity to the emotion of the conflict in the first part of the video",1,0,0.192,0.202
post29hb,richly branching,28,"I've seen similar arguments before, with people pointing towards past technological revolutions and how humans found new work to do while old work was taken over by machines.  The problem I'm seeing is that we might be reaching the point where the gap between what machines can do and what humans can do is too small. There might still be talented individuals who can outperform machines or provide niche skills which machines have yet to adopt, but if you need to be remarkable to not be replaceable, many people will end up being replaceable.  Which may sound a bit harsh but so far as I'm aware it's the truth, and we can't afford to cover this truth up with a white lie, no matter how well-intentioned.",0,0,0.241,0.191
post29hb,richly branching,28,That's basically the right take that also resonates with me as well.,0,0,0.308,0.211
post29lb,poorly branching,5,Nope more moving jobs overseas,0,0,0.184,0.177
post29lb,poorly branching,5,"Eh, my workplace is moving tech jobs (firmware) back from China to London.",0,0,0.151,0.146
post29lb,poorly branching,5,[deleted],0,0,0.376,0.43
post29lb,poorly branching,5,"A keyboard didn't replace me, neither spell check, nor ai. I quit Google cause they were clearly more interested in moving jobs to cheaper places than actually building something solid. When you run out of ideas to squeeze customers you start squeezing the workforce.  Edit: ai image green is fucking soulless and people who rely on it wouldn't have paid an artist anyway. Modern day MS Office clip art. Useful but ain't nobody giving two fucks after the initial novelty passes.",1,0,0.306,0.197
post29lb,poorly branching,5,[deleted],0,0,0.376,0.43
post2hb,richly branching,197,"Problem is, of course, that neural networks can only ever be as good as the training data. The neural network isn't sexist or racist. It has no concept of these things. Neural networks merely replicate patterns they see in data they are trained on. If one of those patterns is sexism, the neural network replicates sexism, even if it has no concept of sexism. Same for racism.      This is also why computer aided sentencing failed in the early stages. If you feed a neural network with real data, any biases present in the data has will be inherited by the neural network. Therefore, the neural network, despite lacking a concept of what racism is, ended up sentencing certain ethnicities more and harder in test cases where it was presented with otherwise identical cases.",0,0,0.145,0.391
post2hb,richly branching,197,[removed],0,0,0.366,0.413
post2hb,richly branching,197,"Precisely.  The headline is misleading at best.  I'm on an ML team at a robotics company, and speaking for us, we haven't ""decided it's OK"", we've run out of ideas about how to solve it, we try new things as we think of them, and we've kept the ideas that have seemed to improve things.    ""More and better data.""  Okay, yeah, sure, that solves it, but how do we get that?  We buy access to some dataset?  The trouble there is that A) we already have the biggest relevant dataset we have access to B) external datasets collected in other contexts don't transfer super effectively because we run specialty cameras in an unusual position/angle  C) even if they did transfer nicely there's no guarantee that the transfer process itself doesn't induce a bias (eg some skin colors may transfer better or worse given the exposure differences between the original camera and ours)  D) systemic biases like who is living the sort of life where they'll be where we're collecting data when we're collecting data are going to get inherited and there's not a lot we can do about it  E) the curse of dimensionality makes it approximately impossible to ever have enough data, I very much doubt there's a single image of a 6'5"" person with a seeing eye dog or echo cane in our dataset, and even if there is, they're probably not black (not because we exclude such people, but because none have been visible during data collection, when was the last time you saw that in person?).  Will our models work on those novel cases?  We hope so!",0,0,0.237,0.186
post2hb,richly branching,197,"So both human intelligence and artificial intelligence are only as good as the data they're given. You can raise a racist, bigoted AI the same in way you can raise a racist, bigoted HI.",0,0,0.189,0.271
post2hb,richly branching,197,"The difference is, a human can be told that racism is bad and might work to compensate in the data. With an AI, that has to be designed in from the ground up.",0,0,0.176,0.325
post2hb,richly branching,197,"Sort of, except I don't love the framing of human racism as data-driven. It isn't really; humans employ biases and heuristics vigorously when interpreting data.",0,0,0.166,0.396
post2hb,richly branching,197,Who knew intelligence isn't wisdom. We have AI but now we need AW.  Being able to morph and utilize data: intelligence.  Understanding when to do it and when not: wisdom.,0,0,0.338,0.257
post2hb,richly branching,197,"But a human can choose to break from their upbringing and traditions. It happens.  Can an AI identify bias in its data, and choose to deviate from it? Maybe that's the next step in AI",0,0,0.247,0.353
post2hb,richly branching,197,‘robots’ in the post title has the potential for more depth of interpretation.,0,0,0.28,0.242
post2hb,richly branching,197,"Maybe it's time to shift focus from training AI to make it useful in novel situations to gathering datasets that can be used in a later stage to teach AI, where the focus is getting as objective a data set as possible? Work with other fields etc.",1,0,0.348,0.185
post2hb,richly branching,197,"You mean manually curating such datasets?  There are certainly people working on exactly that, but it's hard to get funding to do that because the marginal gain in value from an additional datum drops roughly ~~logarithmically~~ exponentially (ugh, it's midnight and apparently I'm not braining good), but the marginal cost of manually checking it remains fixed.",0,1,0.168,0.157
post2hb,richly branching,197,"Nah, the key is to not trust some algorithm to be a neutral arbiter because no such thing can exist in reality. Trusting some code to solve racism or sexism is just passing the buck onto code for humanity’s ills.",0,0,0.262,0.405
post2hb,richly branching,197,"This is a bit of a naive understanding of the problem, akin to people pointing to “the algorithm” as what decides what you see on social media. There aren’t canonical datasets for different tasks (well there generally are for benchmarking purposes but using those same ones for training would be bad research from a scientific perspective) novel applications often require novel datasets, and those datasets have to be gathered for that specific task.   constructing a dataset for such a task is definitionally not something you can do manually, otherwise you are _still_ imparting your biases on the model. constructing an objective dataset for a task relies on some person’s definition of objectivity. Oftentimes, as crappy as it is, it’s easier to kick the issue to just reflecting society’s biases.  what you are describing here is not an AI or data problem but rather a societal one. Solving it by trying to construct datasets just results in a different expression of the exact same issue, just with different values.",0,0,0.173,0.233
post2hb,richly branching,197,"It doesnt have a big return and the people curating can include biases.  Plus If I want people tailored for my company, I want people that will fit MY company, not a generalized version of it, so many places would be agaisnt using those objective datasets, because they dont fit their reality as well as the biased dataset",0,1,0.222,0.381
post2hb,richly branching,197,Ehhh… the datasets we have are plenty objective.,0,0,0.269,0.268
post2hb,richly branching,197,"Perhaps the answer for now is that we shouldn't be making AIs for production with any strict rules when there's a risk of discriminatory biases. We as a species have a habit of always trying to produce more, more optimally, more effortlessly, and we want to find new things to sell, to optimize, to produce.  But we don't really need to. We do not need AIs that filter job candidates (aside of maybe some sort of spam spotting AIs and the like), we do not need AIs that decide your insurance rate for you, we do not need AIs that play with your kid for you.  Yet we want these things but why? Are they *really* going to make the world into a better place for all its inhabitants?  There's a ton of practical work with AIs and ML that doesn't need to include the problem of discrimination. Product QA, recognizing fractures from X-rays, biochemistry applications, infrastructure operations optimization, etc etc.  Sure, this is something worth of studying, but what we really need is a set of standards before potentially dangerous AIs are put into production. And by potentially dangerous, I mean also AIs that may produce results interpretable as discriminatory - discrimination *is* dangerous.  It's up to the professionals of the field to say ""no, we can't do that yet reliably enough"" when a client asks them to do an AI that would most likely have discriminatory biases. And it's up to the researchers to keep informing the professionals about these risks.",0,2,0.17,0.322
post2hb,richly branching,197,"> Perhaps the answer for now is that we shouldn't be making AIs for production with any strict rules when there's a risk of discriminatory biases.  That's pretty much how it's always done, which is why it is able to learn biases.  Take the systemic bias case, where some individuals are at more liberty to take leisurely strolls in the park.  If (for perfectly sane and innocent reasons) parks are where it makes sense to collect your data, you're going to end up with a biased dataset through no fault of your own, despite not putting any strict rules in.  > It's up to the professionals of the field to say ""no, we can't do that yet reliably enough"" when a client asks them to do an AI that would most likely have discriminatory biases. And it's up to the researchers to keep informing the professionals about these risks.  There's more to it than that.  Let's assume that there's good money to be made in your robotic endeavor.  And further lets assume that the current professionals say ""no, we can't do that yet reliably enough"".  That creates a vacuum for hungrier or less scrupulous people to go after the same market.  And so one important question is the public as a whole better off with potentially biased robots made by thoughtful engineers, or with probably still biased robots made by seedier engineers who assure you that there is no bias?  It's not like you're going to convince _everyone_ to step away from large piles of money (and if you are I can think of better uses of that ability to convince).",1,2,0.12,0.365
post2hb,richly branching,197,>Perhaps the answer for now is that we shouldn't be making AIs for production with any strict rules when there's a risk of discriminatory biases.  I don't see why when people aren't free from biases either. I think it's more that the decisions and processes need to be set up in a way that considers the possibility of biases and attempts to correct or sidestep them.   And calling out an AI on its biases may be easier than calling out a person - as long as we no longer think AI's are unbiased.,0,1,0.12,0.438
post2hb,richly branching,197,This is not reassuring and honestly convinces me more that those folks doing AI work are playing with fire,0,0,0.211,0.259
post2hb,richly branching,197,"A significant portion, if not most people who do AI-related work, do it on stuff that isn't necessarily impacted by this stuff. But that's all you read about in the news because these headlines sell.  Training a model to play games (chess/go etc.), image analysis (satellite imagery for climate impacts), science modelling (weather forecasting/astrophyics etc.), speeding up your phone/computer (by optimising app loading etc.), digitising hand-written content, mapping roads (google maps etc.), disaster forecasting (earthquakes/flooding), novel drug discovery.  There are certainly more areas that I'm forgetting, but don't be fooled into thinking (1) that ML isn't already an everyday part of your life and (2) that all ML research has the same societal negatives.",0,0,0.224,0.188
post2hb,richly branching,197,"Don't worry, I'm sure one day we can get sentient AIs that hate all humans equally!",0,1,0.181,0.239
post2hb,richly branching,197,"Yup. “We know it’s not ok, but we’ll move forward regardless”.",0,0,0.198,0.234
post2hb,richly branching,197,"If it helps, human brains have a lot of these same issues (they're just slightly more subtle due to the massive data disparity), and that's gone perfectly.  Definitely no cases of people ending up as genocidal racists.  Definitely no cases of that currently happening in China.  We're definitely smart enough to avoid building nukes, or at the very least to get rid of all the nukes we have.  If doing AI work is playing with fire, doing human work is playing with massive asteroids.  A fun game to play is, whenever you see robots or aliens in a scary movie, try to work out which human failing it is they're the avatar of.",0,0,0.155,0.268
post2hb,richly branching,197,"Yeah, I think the onus is less on the devs, since we're a long way off created impartial AI, and more on enforcing a code of ethics on what AI can be used for.  If your face recognition technology doesn't work on black people very well, then it shouldn't be used by police to identify black suspects, or otherwise come attached to additional manual protocols to verify the results for affected races and genders.  The main problem is that companies are selling these things to public housing projects primarily populated by black people as part of the security system and acting confused when it randomly flags people as shoplifters as if they didn't know it was going to do that.",0,0,0.15,0.361
post2hb,richly branching,197,"You can't expect companies to pay you hundreds of thousands of dollars to create an AI and not turn around and use it.  Diffusion of blame is how we justify evil outcomes.  If you know it's impossible to not make a racist AI, then don't make an AI.",0,0,0.13,0.192
post2hb,richly branching,197,"Have you considered that intelligence, which includes experience-based judgement, is inherently biased?  Sounds like you're trying to make something artificial, but not necessarily intelligent.",0,1,0.243,0.426
post2hb,richly branching,197,">we haven't ""decided it's OK"",  You're simply going ahead with a flawed product that was supposed to compensate for human flaws and failings, but will now reproduce them only with greater expediency. Cool!",0,0,0.235,0.246
post2hb,richly branching,197,"Arguing it's not technically racist is completely unelpful and puts the focus on the wrong aspect of the problem. These things can have enormous impacts on our lives so it really doesn't matter how it *actually* works when it's *literally* not working properly.   Facial recognition being a prime example. The miss rate on light skin people alone is too high let alone the abysmal rate for darker skin tones yet it's commonly used by law enforcement for years now. Those people sitting in jail from this one technology don't care that the AI isn't actually racist. The outcomes are and that's literally all that matters. It doesn't work, fix it or trash it.",0,2,0.2,0.31
post2hb,richly branching,197,"> It doesn't work, fix it or trash it.  Agreed.  It's just that fixing it requires lots trial and error, and that takes a long time.  The real problems with facial recognition aren't in the technology, they're in idiots using tools for more than they're capable of doing.",0,0,0.139,0.153
post2hb,richly branching,197,"In this case is the curse of dimensionality the fact that the global sample is only 7 billion people, which represents a very tiny fraction of all possible configurations of all characteristics being tracked?",0,0,0.187,0.195
post2hb,richly branching,197,[deleted],0,0,0.376,0.43
post2hb,richly branching,197,"> Why give an AI any data not required in sentencing. If the AI doesn’t know the race or gender of the defendant, it can’t use it against them.  That's not strictly true.  Let's say you have two defendants, one was caught and plead to possession with intent to distribute crack cocaine, and the other was caught and plead to possession with intent to distribute MDMA.  From that information alone you can make an educated guess (aka a Bayesian inference) about the race and gender of both defendants, and while I don't have actual data to back this up, you'd likely be right a statistically significant portion of the time.",0,0,0.206,0.343
post2hb,richly branching,197,"It sounds like you have 100% decided it's okay. You don't like it, but you don't consider it a deal breaker either. Not desirable, but acceptable.  I understand you have constraints you are working under and I have no doubt that you would like to see the issues of racism and bias in AI resolved. But the simple fact is that AIs are being designed to be racist and there will be real consequences. People won't be able to get jobs or health care or will get denied loans or suffer longer prison sentences.  Again, I understand that you aren't in a position where you can fix it. But shrugging and hoping the problem will get addressed? That's saying it's okay if it doesn't. It's tolerable. So saying that AI researchers think it's okay is a fair characterization.  Whether you have malice in your heart or not matters not-at-all to the companies who will use AI in the pursuit of profit. The travel companies pushing Vegas trips on a discount at people with manic-depression or pushing people into high-engagement communities even if they are cults or white nationalists.",0,0,0.174,0.314
post2hb,richly branching,197,"I just want to point out that data augmentation is a thing, but otherwise good summary.",0,0,0.328,0.193
post2hb,richly branching,197,Isn’t it possible to “feed” a posterior law that sits in front of the data kind of in a Bayesian mindset?,0,0,0.174,0.164
post2hb,richly branching,197,"Great question, I'll come back to it when I get back from work (leaving this comment to remind myself)",0,0,0.35,0.239
post2hb,richly branching,197,"Kind of, there is room to feed stuff in like that, but it's difficult to figure out precisely what to feed in.  Most things you might want to feed in there can also be expressed in your cost function, which means they can be included in the training process directly.  Ideas for what you feed in get tried pretty regularly, it's not solved, but some of them do work.",0,0,0.2,0.125
post2hb,richly branching,197,"The way to solve it is get tech ethicists into positions of power to address systemic issues. You, personally, cannot solve this. *Your team cannot solve this.* Big power players in tech have to solve this, and that begins with hiring-on people like Timnit Gebru and not firing them; looking at you, Google.  This is a fully top-down issue.",0,0,0.21,0.175
post2hb,richly branching,197,Maybe stop using data generated by Americans?,0,0,0.186,0.253
post2hb,richly branching,197,Because there's no racism anywhere except in the US.,0,0,0.157,0.302
post2hb,richly branching,197,How about we stop considering the americans altogether,0,0,0.206,0.283
post2hb,richly branching,197,Paraphrase: We can't be bothered to spend the time and money to assemble a dataset that doesn't contain bigoted biases so we're going to release a product the replicates bigotry anyway.  Assembling good high quality datasets that can be used for machine learning is expensive and decades long work. I wish more computer science students understood this.,0,0,0.22,0.333
post2hb,richly branching,197,Have you tried buying synthetic data?,0,0,0.168,0.158
post2hb,richly branching,197,"The trouble there is that it has to be synthesized to represent our robot's view on the world, which currently none are, so we're working on building that capability to make it ourselves.",0,0,0.24,0.195
post2hb,richly branching,197,AI random character creator. Create your own diverse dataset. One to rule them all!,0,0,0.281,0.299
post2hb,richly branching,197,"We need to think differently from statistical averages being the Truth, but that is how our society is ordered, even if it is not really how it is lived. The discrepancy between the two has always enraged people when it's pointed out that data is not 3-dimensional, because so much money and status is involved.  The short cuts to understanding that data sets offer have helped create a more efficient world. But their limitations have always been downplayed by those who insist they offer more than they can.",0,0,0.133,0.168
post2hb,richly branching,197,"As a layman, I've only thought of it at a newbie level ;_;  I guess it's basically like set theories where you can get an exclusion, or a merge, but trying to only alter 'half' the set means having to try and find some way to create a new set entirely. If only we could source the most racist and sexist data possible (basically like pulling all Proud Boy and other ultra-exclusionary groups messages/decisions/etc) so we could make it adversarial to the training of the data.  I can bet the ""we try new things as we think of them"" means it's been an absolutely exhausting and draining to keep throwing stuff at the wall trying to find what sticks. ;_;",0,0,0.23,0.362
post2hb,richly branching,197,Can you hook me up with a ML engineering job?,0,0,0.235,0.142
post2hb,richly branching,197,"Can you generate randomized data?  I am spit-balling here, I realize.   First, this seems like a great way to sniff out institutional racism. Take a data set, the more narrow the better, and extrapolate out if it causes a racist/sexist outcome. Boom! Data set had intrinsic racism/sexism.  So, how to ""erase"" the systemic nature? That is tough, but I suspect it shows in a few ways... outlier extremes, frequency of variation from the mean, selection bias. Of those, I feel like the selection bias would be impossible to erase, but the other two could be handled by some statistical selection... Basically, select out some amount of extremes and artificially reduce the number of one group varying from the mean more than the others.  Then, run the test for lots of randomized trials and see if there is a racist/sexist bias. When you get an AI that doesn't do that, you have found the right starting artificial data set to remove the institutional bias.   But... that sounds really time intensive and expensive.  Maybe we could put an AI on it. hehe",0,1,0.181,0.389
post2hb,richly branching,197,"I think the point of the claim is that by pushing forward anyway, despite being unable to solve it, you have decided you’re ok with it. *Not* building is an option, but—no offense intended—not one that an ML team at a robotics company would likely consider seriously. Compare: If we considered such a system to be nonfunctional or dangerous in the way we do a car without seatbelts, it could not go to market (despite having been thought ok in the early days of cars). That’s part of the critique.",0,0,0.244,0.201
post2hb,richly branching,197,">""More and better data."" Okay, yeah, sure, that solves it, but how do we get that?  Synthetic data.  Fill-in the gaps of your real-world collected data with computer generated data",0,0,0.248,0.225
post2hb,richly branching,197,"To me it's simply a matter of distinguishing these two requests:  ""Show me the face that is most beautiful""  ""Show me the face that is most beautiful according to the majority of Brazilians""  First request has no answer and the robot shouldn't answer it. Second request has a valid answer which the robot can provide.  It is not about eliminating bias, it is about making it clear that it is there.",0,0,0.235,0.391
post2hb,richly branching,197,Honestly they’ve know that this information was biased based on human implicit bias’ years ago and kept going but there was no profitable way to fix that unfortunately / job creation there .  There is a lot more profit in marketing by demographic so I kinda want to blame that but can be it wrong . In any case it seems humans are left best for those novel cases /exceptions as a default and or the engineering teams have to think of a procedure beforehand  and just in case . Just hope it doesn’t mess anyone up too badly getting caught in a weird loop or non existent solution.,0,2,0.198,0.304
post2hb,richly branching,197,"Dall-E Can imagine it, it can be true",0,0,0.241,0.264
post2hb,richly branching,197,">Precisely.  The headline is misleading at best.  I'm on an ML team at a robotics company, and speaking for us, we haven't ""decided it's OK"", we've run out of ideas about how to solve it, we try new things as we think of them, and we've kept the ideas that have seemed to improve things.  There is a solution though. If you can't make unbiased AI, you don't use it at all.  If you still use it in your products and then say you're trying to solve the problem you're being disingenuous and ethically dubious.   The headline isn't really misleading. Some companies might act appropriately, but many aren't.",0,0,0.23,0.287
post2hb,richly branching,197,"That's black and white thinking, and it holds you back.  Let's say that you're building a robot train, and you tell it not to hit people.  Let's further say that your robot is better at spotting white people at distance that black people which manifests as stopping with 10ft to spare for white people and 9'6"" to spare for darker people.  It is a clear bias.  But at the same time, you're still stopping for everyone.  Should that 6"" really derail a project?",0,0,0.215,0.297
post2hb,richly branching,197,"Just because YOU can't solve the issue posed doesn't somehow mean you aren't doing exactly what you were accused of. You literally just admitted the base data itself is flawed so maybe instead of trying to force through a product that's guaranteed not to function 100% as intended, you could work on fixing the data or obtaining more. The original accusations was that you guys are passing off broken racist AI as a finished product and you are which you admitted in your post and then said it's impossible to fix essentially. Just because you work for a company doesn't mean you need to come on the internet and lick boot Infront of us for them.",0,1,0.177,0.298
post2hb,richly branching,197,"I agree with what you’re saying. However, I ask, what is the point of these bots in the first place? What goals are we even trying to reach?  All I see bots do is make trashy comments and poison the well by spreading harmful propaganda. For what? Boost people’s follower count?",0,0,0.197,0.222
post2hb,richly branching,197,"Oh, our bots aren't software bots, ours weigh hundreds of pounds each and can go well over 10mph off road.  If you're asking for a defense of public opinion shaping bots I believe they're a cancer, and the people responsible for creating them should be deported to... say... the Mariana trench.",0,0,0.142,0.197
post2hb,richly branching,197,"I feel like you have to have some event driven programming to compensate for the ML datasets. In other words, a function to filter certain responses. There is an eng geek out there who will someday solve this problem, but, for now we should bandage the issue.",0,0,0.122,0.156
post2hb,richly branching,197,">we haven't ""decided it's OK"", we've run out of ideas about how to solve it  ...and then decided to go ahead anyway.  So you have actually decided it's OK. After all you tried your best! But you still gotta sell that product, and that's of course more important than the problem at hand. So you're trading money for morals.",0,0,0.273,0.259
post2hb,richly branching,197,"> to go ahead anyway  Go ahead with what, exactly?  Further development work?  Additional data gathering?  Taking it seriously?  Because yeah, we're full steam ahead on all of those things.",0,0,0.247,0.215
post2hb,richly branching,197,"I don't think it's misleading. A decision with a racist outcome is a racist decision. People who are interpreting that to mean ""a decision was made by a computer with racist intent"" are reading it incorrectly, because they're not understanding one of:  * AIs don't make ""decisions"" like humans * something doesn't have to have racist intent to have racist outcomes (and thus, be racist)",0,1,0.239,0.365
post2hb,richly branching,197,I have an awesome idea. Let’s have humans to the judging of other humans. Your welcome.,0,0,0.275,0.323
post2hb,richly branching,197,"The AI just needs a virtue signaling module, that heavily weighs appearing not sexist or racist, and if the rest of the network is in conflict with it, reject that data and search for data that confirms the academic orthodoxy. That's how humans do it.",0,0,0.193,0.317
post2hb,richly branching,197,"The GAPING hole in that explanation is that there is evidence that these machine learning systems will still infer bias even when the dataset is deidentified, similar to how a radiology algorithm was able to accurately determine ethnicity from raw, deidentified image data. Presumably these algorithms are extrapolating data that is imperceptible or overlooked by humans, which suggests that the machine-learning results reflect real, tangible differences in the underlying data, rather than biased human interpretation of the data.  How do you deal with that, other than by identifying case-by-case the “biased” data and instructing the algorithm to exclude it?",0,1,0.148,0.438
post2hb,richly branching,197,"That is the real difficulty, and kinda what i'm trying to get at. Neural networks can pick up on things that would go straight past us. Who is to say that such a neural network wouldn't also find a correlation between punctuation and harshness of sentencing?      I mean, we have studies proving that justice is biased on things like wether a football team won or lost the previous match if the judge was a fan of said team, so if those are things we can find, what kinds of correlations do you think could an analytical software designed by a species of intelligent pattern finders to find patterns better than we ever could find?     In your example, the deidentified image might still show things like, say, certain minor differences in bone structure and density, caused by genetics, too subtle for us to pick out, but still very much perceivable for a neural network specifically designed to figure out patterns in a set of data.",0,2,0.193,0.302
post2hb,richly branching,197,"For a while, I've been thinking along similar lines about ways to make court trials more fair - focusing on people, not AI. My core idea is that the judge and jury should never know the ethnicity of the person on trial. They would never see or hear the person, know their name, know where they live, know what neighborhood the crime was committed in, and various other things like that. Trials would need to be done via text-based chat, with specially-trained go-betweens (humans at first, AI later) checking everything that's said for any possible identifiers.  There will always be exceptions, but we can certainly reduce bias by a significant amount. We can't let perfect be the enemy of good.",0,1,0.156,0.376
post2hb,richly branching,197,[deleted],0,0,0.376,0.43
post2hb,richly branching,197,"Instead of handicapping the use of data I wonder if it would make more sense to break down more complex data into simplified data points.   If you're using high level data such as race of a person then the NN will be trained on data obtained from a racist system and the outputs will perpetuate that.   For something like a resume AI determining applicants, it might discriminate against women for things like ""lack of experience"" if there is a period of maternity leave or something. I guess what I'm saying is certain metrics are currently used for evaluation but those metrics aren't necessarily good metrics to be used.   Its obviously not a simple issue and I'd have to spend more time thinking about what I'm trying to get across to give better examples",0,0,0.159,0.432
post2hb,richly branching,197,[removed],0,0,0.366,0.413
post2hb,richly branching,197,[removed],0,0,0.366,0.413
post2hb,richly branching,197,"There is a difference between deidentifying and removing bias from the dataset isn’t there? One interesting example I came across recently is resuscitation of newborn babies. Where I come from there is a difference between 98% and 87% in which babies are attempted to be resuscitated between the ethnicity with the highest rate (white), and the lowest (Indian). This is due to the criteria used to determine if they attempt resuscitation, and the difference in the two distributions of babies of those ethnicities. Now if you took the data and removed the racial information, then trained a model to determine which babies should be attempted to resuscitate, you still get a racial bias don’t you? Which is to say if you run the model with random samples from those two distributions, you get two different average answers.",1,0,0.189,0.446
post2hb,richly branching,197,"Maybe the disconnect is the definition of bias. It sounds like you’re suggesting that a “good” model would normalize resuscitation rates by recommending increased resuscitation of one group and/or decreased resuscitation of a different group. That discounts the possibility that there are real, tangible differences in the population groups that affect the probability of attempting resuscitation, aside from racial bias. It would actually introduce racial bias into the system, not remove it.",0,0,0.185,0.297
post2hb,richly branching,197,"> similar to how a radiology algorithm was able to accurately determine ethnicity from raw,   If 'ethnicity' wasn't fed to the algorithm then it did not do this. What likely happened is that the algorithm was trained and then in a post-hoc analysis researchers could see that it clustered together images that belonged to some ethnic groups. Which would indicate that there are some systematic difference in the radiaology images from  different groups. That's likely useful knowledge from a diagnostic perspective. And not, in and of itself, racist.  It's one thing to discover that there are indeed some systematic difference in radiology images from different ethnic groups (something that you might well hypothesis before hand). It's quite another thing to allow your AI system to make racist or sexist decisions because it can cluster datasets without explicitly including ""ethnicity"" in the training data. When we talk about an AI making sexist or racist decisions we're not talking about whether it can infer ethnicity by proxy, something that can be benign factual information. We're talking about what the whole AI system then does with that information.",1,0,0.133,0.377
post2hb,richly branching,197,"To your last paragraph, im arguing that the radiology AI will make “racist” decisions that are actually just reflections of rote, non-biased data. We’re not quite at the point that the radiology AI can make recommendations, but once we get there, you’ll see people arguing that findings are being called normal or abnormal based on “biased” factors.   Those overseeing AI development need to decide if the outputs are truly biased, or are simply reflecting trends and data that humans don’t easily perceive and subsequently attribute to some form of bias.",0,2,0.19,0.369
post2hb,richly branching,197,"Let's say it was fed all information, age, sex, ethnicity, etc.  And outcomes based on the treatments that were recommended based on the images.  And this AI's job was to recommend and allocate resources based on the given  data with the goal of generating the maximum number of successful outcomes with the given resources (maybe that's a racist goal?).   If this AI began to recommend the best treatments and allocate resources to a certain group based on that data, and let's assume it achieved the desired results, is it racist?    Now let's say we remove the ethnical information from the dataset, and the results are the same (because it is able to infer it).   Is it now less racist because we withheld information?",0,0,0.192,0.379
post2hb,richly branching,197,"Of course there are real, tangible differences in the data!  The impact of racism, sexism, homophobia, and other biases aren't just in our heads.  Its not just preconceived, bigoted notions about what people different from ourselves, and different from the societal ""norm"" are like.  Its also the fact that Black people are more likely to be poor and trans youth are more likely to be homeless and women are more likely to be sexually assaulted.  If you want the AI to tell you which criminals are more likely to re-offend, and give sentences accordingly, its going to sentence the black criminals more harshly.  And even if you anonymize the data, its going to pick up on all the other things that correlate with race.",0,0,0.153,0.382
post2hb,richly branching,197,"I suppose the direct comparison between medical AI and criminal sentencing isn’t completely apt, but the point stands that the algorithm doesn’t make “racist” or “sexist” decisions, it simply reflects the facts that it can derive from input data. Re-offenders deserve harsher sentences, just like suspicious lung nodules deserve closer follow-up. All other factors aside, there isn’t any inappropriate bias in the algorithm or it’s decision-making process.",0,0,0.122,0.321
post2hb,richly branching,197,"The effect of the bias can be as insidious as the AI giving a different sentence based solely on the perceived ethnic background of the individual's name.   Some people would argue that the training data would need to be properly prepared and edited before it could be processed by a machine to remove bias. Unfortunately even that solution isn't as straightforward as it sounds. There's nothing to stop the machine from making judgments based on the amount of punctuation in the input data, for example.  The only way around this would be to make an AI that could explain in painstaking detail  why it made the decisions it made which is not as easy as it sounds.",0,0,0.143,0.379
post2hb,richly branching,197,"Actually, there is another way. And it is fairly straightforward, but... (of course there is a but)  What you can do (and indeed, just about the only thing you can do, as far as I can tell) is to simply directly enforce the thing we supposedly want to enforce, in an explicit manner. That is, instead of trying to make the agent ""race-blind"" (a fool's errand, since modern ML methods are astoundingly good at picking up the subtlest cues in the form of slight correlations or whatever), you make sure you figure out everyone's race as accurately as you can, and then *enforce* an equal outcome over each race (which isn't particularly hard, whether it is done at training time with an appropriate loss function, or at inference time through some sort of normalization or whatever, that bit isn't really all that technically challenging to do pretty well) -- congrats, you now have an agent that ""isn't racist"".  Drawbacks: first, most of the same drawbacks in so-called affirmative action methods. While in an ideal world all races or whatever other protected groups would have equal characteristics, that's just not true in the real world. This method *is* going to give demonstrably worse results in many situations, because you're not really optimizing for the ""true"" loss anymore.   To be clear, I'm not saying ""some races just happen to be worse at certain things"" or any other such arguably racist points. I'm not even going to go near that. What's inarguably true is that certain ethnicities are over- or under-represented in certain fields for things as harmless as ""country X has a rich history when it comes to Y, and because of that it has great teaching infrastructure and a deep talent pool, and their population happens to be largely of ethnicity Z"".   For example, if for whatever reason you decided to make an agent that tried to guess whether a given individual is a strong Go/Baduk player (a game predominantly popular in East Asia, with effectively all top players in world history coming from the region), then an agent that matched real world observations would necessarily have to give the average white person a lower expected skill level than it would give the average Asian person. You could easily make it not do that, as outlined above, but it would give demonstrably less accurate results, really no way around that. And if you e.g. choose who gets to become prospective professional players based on these results or something like that, you will arguably be racially discriminating against Asian people.   Maybe you still want to do that, if you value things like ""leveling the international playing field"" or ""hopefully increasing the popularity of the game in more countries"" above purely finding the best players. But it would be hard to blame those that lost out because of this doctrine if they got upset and felt robbed of a chance.  To be clear, sometimes differences in ""observed performance"" are absolutely due to things like systemic racism. But hopefully the example above illustrates that not *all* measurable differences are just due to racism, and sometimes relatively localized trends just happen to be correlated with ""protected classes"". In an ideal world, we could differentiate between these two things, and adjust only for the effects of the former. Good luck with that, though. I really don't see how it could even begin to be possible with our current ML tech. So you have to choose which one to take (optimize results, knowing you might be perpetuating some sort of systemic racism, but hopefully not any worse than the pre-ML system in place, or enforce equal results, knowing you're almost certainly lowering your accuracy, while likely still being racist -- just in a different way, and hopefully in the opposite direction of any existing systemic biases so they somewhat cancel out)  Last but not least: even if you're okay with the drawbacks of enforcing equal outcomes, we shouldn't forget that what's considered a ""protected class"" is, to some extent, arbitrary. You could come up with endless things that sound ""reasonable enough"" to control based on. Race, ethnicity, sex, gender, country of origin, sexual orientation, socioeconomic class, height, weight, age, IQ, number of children, political affiliation, religion, personality type, education level... when you control for one and not for others, you're arguably being unfair towards those that your model discriminates against because of it. And not only will each additional class you add further decrease your model's performance, but when trying to enforce equal results over multiple highly correlated classes, you'll likely end up with ""paradoxes"" that even if not technically impossible to resolve, will probably require you to stray even further away from accurate predictions to somehow fulfill (think how e.g. race, ethnicity and religion can be highly correlated, and how naively adjusting your results to ensure one of them is ""fair"" will almost certainly distort the other two)",1,1,0.172,0.483
post2hb,richly branching,197,[deleted],0,0,0.376,0.43
post2hb,richly branching,197,"These ideas need to be discussed more broadly. I think you have done a pretty good job of explaining why generalizations and stereotypes are both valuable and dangerous. Not just with regard to machine learning and AI but out here in the real world of human interaction and policy.  Is the discussion of these ideas in this way happening anywhere other than in Reddit comments? If you have any reading recommendations, I'd appreciate your sharing them.",2,0,0.205,0.319
post2hb,richly branching,197,"This. Neural networks can pick up on any pattern, even ones that aren't there. There's studies that show sentences on days after football games are harsher if the judges favourite team lost the night before. This might not be an obvious correlation, but the networks sees it. It doesn't understand what it sees there, just that there's times of the year where, every 7 days, sentences that are given are harsher.     In the same vein, a neural network might pick up on the fact that the punctuation might say something about the judge. For instance, if you have a judge who is a sucker for sticking precisely to the rules, he might be a grammar nazi, and also work to always sentence people precisely to the letter of the law, whereas someone who rules more in the spirit of the law might not (though this is all conjecture)",0,0,0.179,0.245
post2hb,richly branching,197,"> Neural networks can pick up on any pattern, even ones that aren't there.   This is a paradoxical statement.",0,0,0.198,0.125
post2hb,richly branching,197,We are going to need psychologists for the AI.,0,0,0.298,0.263
post2hb,richly branching,197,"As for how to figure out what biases the network has, one way would be to reverse it, aka instead of feeding it training data and having it generate an output out of this data, you run it in reverse and have it generate new data. If you messed with the outputs, which are now inputs, one at a time, you could see how it changes the resulting input (which, of course, is now output), but that's still complicated af.",0,0,0.165,0.382
post2hb,richly branching,197,"I'm pretty sure that's impossible. Each neuron in a network has a number of inputs, and an output that is based on the inputs. It'd be like trying to solve `A = B x C x D`, but you know the value of A and want to know B, C and D.  You can't, as they depend on each other.",0,0,0.174,0.091
post2hb,richly branching,197,"The actual point of Critical Race Theory is that systems can perpetuate  racism even without employing racist people, if false underlying assumptions aren't addressed.  Racist AI's perpetuating racism without employing any people at all are an extreme extrapolation of that concept.    Addressing tainted and outright corrupted data sources is as important in data science as it is in a history class.  Good systems can't be built on a foundation of bad data.",0,0,0.191,0.336
post2hb,richly branching,197,"> if false underlying assumptions aren't addressed.  They need not be false. The thing that makes this so intractable isn't the false underlying assumptions, it's the true ones.   If an AI wants to predict recidivism, it can use a model that looks at marital status, income, homeownership, educational attainment, and the nature of the crime.   But maleness is a strong predictor of recidivism. It's a real thing. It's not an artifact or the result of bias. Men just commit more crime. A good AI will find a way to differentiate men from women to capture that chunk of the variation. A model with sex is much better at predicting recidivism than a model without it.  So any good AI will be biased on any trait that accounts for variation. If you tell it not to be, it'll just use a proxy ""Wow! Look how well hair length predicts recidivism!""",1,1,0.153,0.338
post2hb,richly branching,197,"> Men just commit more crime.  Actually it's more like men are arrested and sentenced at a higher rate (that's hard data we have). The soft data of how much crime is committed is sort of unknowable, we can make educated guesses at best.  But that's sort of the problem, just because a situation exists doesn't make it correct or a ""fact of reality"". People of color in the US tend to be poorer; that isn't an inherent property of those people but an emergent property due to other things largely out of their control such as generational wealth, etc. The problem of making choices based on ""facts"" like these is they easily becomes a self fulfilling prophecy.",0,0,0.127,0.289
post2hb,richly branching,197,">The actual point of Critical Race Theory  That's a broad field without an actual point. You may as well be arguing the actual point of economics. To a Keynesian maybe it is to know how to minimize fluctuations in the economy,  to a communist it may be how to determine need and capability. A critical race theorist might write systemic racism, or they could be an advocate for standpoint epistemology, the latter of which is an anti-scientific viewpoint.",0,1,0.213,0.279
post2hb,richly branching,197,"I feel like there is a real underlying point here; that is made problematic by just talking about racism. People's outcomes in life depend to a large degree statistically on their starting points. If their starting point is largely the result of racism, then those results will reflect that racism.  However, a fix that simply remixes the races doesn't necessarily deal with the underlying issue of why starting points matter so much. I would really like to see a world where everybody has opportunity, not simply one where lack of opportunity is better distributed over skin colors.  One statistic that always struck me was that the single best predictor of whether a child in a middle class house grows up to be middle class is the economic class of their grandparents.  That says a lot about starting points and the importance of social networks. It DOES perpetuate the outcomes of past racism; but in and of itself, its not racism and fixing the distribition of inequality doesn't really fix this; it just hides it.",0,0,0.199,0.319
post2hb,richly branching,197,"Zero relationship to what you describe. Events which took place in history need not be removed to allow non ""currupted"" data. That makes the data completely wrong. Also data models are not humans.",0,1,0.199,0.272
post2hb,richly branching,197,"I'm not advocating removing data.  I'm advocating adding data (and context).  Because those ""data models"" are called Artificial Intelligence because they ape Human Intelligence - which is just as susceptible to bad and incomplete data streams as its artificial cousins.  Also, statues are not data.",0,1,0.219,0.207
post2hb,richly branching,197,"> Addressing tainted and outright corrupted data sources  See this is the problem, You aren't being honest in what the issue is.   The data sources aren't corrupted or tainted. They are showing an accurate empirical representation of the data. The ""corruption"" comes from your disagreement with the pillars of that data, such as crime rates by ethnicity and it not being able to take into account human biases in something like policing by arbitrarily weighting things like race to skew the results to match your sensibilities.   You and people who share your world view will never be pleased with the data unless you pre-screen it and it shows the result you want before hand, otherwise you will come up with some reason why its perpetually biased in a way you don't like.",1,1,0.164,0.356
post2hb,richly branching,197,"So because I say I don't want to use corrupted data, I obviously want to corrupt the data.  The good old insightful ""I know you are but what am I?"" argument.",1,0,0.233,0.281
post2hb,richly branching,197,Remember when the self-driving cars didn’t recognize Black people as human? Why? Because no testing was done with people that weren’t White.  Edit: [Citation](https://arxiv.org/pdf/1902.11097.pdf),0,1,0.194,0.314
post2hb,richly branching,197,"\*no *training* was done with datasets containing POC. Testing is what caught this mistake.  ""Training"" and ""testing"" are not interchangeable terms in the field of machine learning.",0,1,0.322,0.168
post2hb,richly branching,197,Thank you for the gentle and accurate correction.,2,0,0.385,0.3
post2hb,richly branching,197,"“The company's position is that it's actually the opposite of racist, because it's not targeting black people. It's just ignoring them. They insist the worst people can call it is ‘indifferent.’”",0,0,0.199,0.339
post2hb,richly branching,197,"Dude, is that a ""Better of Ted"" reference?",0,0,0.194,0.242
post2hb,richly branching,197,"The problem with this argument is it implies that all you need to do is give 'better' data.  But the reality is, giving 'better' data will often lead to racist/sexist outcomes.  Two common examples:  Hiring AI: when Amazon set up hiring AI to try to select better candidates, it automatically selected the women out (even if you hid names, gender, etc). The criteria upon which we make hiring decisions incorporates problems of institutional sexism, so the bot does what it is programmed to do: learn to copy the decisions humans make.  Criminal AI: you can setup an AI to accurately predict whether someone is going to commit crimes (or more accurately, be convicted of commiting a crime). And of course since our justice system has issues of racism and is more likely to convict someone based on their race, then the AI is going to be more likely to identify someone based on their race.  The higher quality data you give these AI, the more they are able to pick up the real world realities. If you want an AI to behave like a human, it will.",0,0,0.178,0.405
post2hb,richly branching,197,"I think the distinction to make here is what ""quality"" data is. The purpose of an AI system is generally to achieve some outcome. If the outcome of a certain dataset doesn't fit the business criteria then I would argue the quality of that data is poor for the problem space you're working in. That doesn't mean the data can't be used, or that the data is inaccurate, but it might need some finessing to reach the desired outcome and account for patterns the machine saw that humans didn't.",0,1,0.209,0.259
post2hb,richly branching,197,"I don’t think I’d consider “more biased data” as “better” data, though.",0,1,0.214,0.518
post2hb,richly branching,197,Stephen Colbert said reality has a well known liberal bias. Perhaps it has a less well known sexist and racist bias.,0,0,0.175,0.376
post2hb,richly branching,197,Would you say the same is true for a racists brain?,0,0,0.19,0.302
post2hb,richly branching,197,"Racism IS learned behavior, yes.  Racists learned to become racist by being fed misinformation and flawed ""data"" in very similar ways to AI. Although one would argue AI is largely fed these due to ignorance and lack of other data that can be used to train them, while humans spread bigotry maliciously and with the options to avoid it if they cared.  Just like you learned to bow to terrorism on the grounds that teaching children acceptance of people that are different isn't worth the risk of putting them in conflict with fascists.",0,1,0.357,0.288
post2hb,richly branching,197,Source for that claim?  As far as I know racism and xenophobia in general are an innate fear self-protective response to the unknown.,0,0,0.181,0.308
post2hb,richly branching,197,[deleted],0,0,0.376,0.43
post2hb,richly branching,197,[deleted],0,0,0.376,0.43
post2hb,richly branching,197,This system is based on human selection of keywords to images. Of course its going to have the human bias still. What is so difficult to understand people.,0,0,0.242,0.361
post2hb,richly branching,197,"Kinda my point. It's extremely hard to develop a neural network that is unbiased, because humans have all sorts of biases that we usually aren't even aware of. There was a study done in the 70s, for instance, which showed that the result of a football game could impact the harshness of a sentence given the monday after said game.      If you included references to dates in the dataset, the neural network wouldn't pick up on this correlation. It would only see that every seven days in certain times of the year, sentences are harsher, and would therefore emulate this bias.      Again, the neural network has no concept of mood, and how the result of a football game can impact it, and might thus cause a judge to give harsher sentences, all it sees is that this is what is going on, and assumes that this is meant to be there.",0,0,0.175,0.427
post2hb,richly branching,197,"No. AI doesn't have have sentience nor a psyche. It could be said that racism forms in a person with ""junk in,"" but they quickly become wrapped up in it, identify with it, believe in it. Racism becomes a structuring ideological fantasy for the psyche. It's not the same for AI, which will merely reflect the data neutrally, rather than believing in an idea and having that inform choices/behaviour in a generative way.",0,0,0.23,0.296
post2hb,richly branching,197,[removed],0,0,0.366,0.413
post2hb,richly branching,197,"Unfortunately, the word ""racist"" has at least two distinguishable meanings:   1. Having the cognitive mindset that holds that some races are inferior to others;  2. Any action or circumstance which tends to disadvantage one race over another.  OP is saying, quite reasonably, that neural networks are 2 but they are not 1. (That's why they literally say that NNs both ""are not racist"" and ""are racist"".)  Both concepts are useful but they're very different, and I honestly think it's significantly holding back the racism discussion that people sometimes confuse them.",1,0,0.185,0.313
post2hb,richly branching,197,"Thank you for this. Your distinction of the two ""racist"" meanings will be very helpful in future discussions.",2,0,0.248,0.321
post2hb,richly branching,197,[removed],0,0,0.366,0.413
post2hb,richly branching,197,"Smacks of people being told about problems with motion detectors (such as for automatic sinks) and going ""What? Sinks can't be racist, that's just how light works."" That rebuttal only makes sense if automatic sinks grew in nature or something. As they are, someone designed them that way, and the fact they work poorly with dark skin is something the designer never even bothered considering. That's racism. It's not blatant, malicious bigotry, but it's still racism born of casual ignorance.",1,0,0.217,0.358
post2hb,richly branching,197,"I don't know enough about these specific sinks to argue one way or the other, but I would like your position on the principle.  *If*, due to the actual, physical, biological differences between races/sexes/preferences/whatever, a system like the sink sensor will *always* be more or less effective for one or more groups, does that make it -ist? Like, if you increase the sensor sensitivity to the point it is as reliable on dark skin as it currently is on white skin, won't that just make *even more* sensitive or ""reliable"" towards light skin, ad nauseum?",0,0,0.13,0.284
post2hb,richly branching,197,"Okay, how do we fix the issue? I mean beyond complaining and telling programmers to fix it. The algorithms pick up these problems from the training data and the training data is society itself. How are you going to cleanse these massive data sets of anything you consider problematic?",0,1,0.164,0.213
post2hb,richly branching,197,">It's beyond obvious that what is meant here is the results of outputs of the neural net is unfairly disadvantageous along the lines of race and sex, therefore perpetuating racism and sexism.  It may be beyond obvious to you and I, but not to the vast majority of people I've talked to about this. When people see the word AI, they don't think of a statistical model on steroids, they really do think of AGI.  >It's time we move past this nitpicking and focus on the actual issue.  In my opinion, it's hard to move past this when the people making decision don't even understand the nature of the actual issue.",0,0,0.205,0.317
post2hb,richly branching,197,Why was ethnicity used as an input to the sentencing AÍ?   Or is it able to reconstruct ethnicity due to other strong correlations?,0,0,0.165,0.377
post2hb,richly branching,197,"I don't know the details. It's possible that they fed the neural network with things like criminal histories too, which are relevant in sentencing (as a first offender would get a lesser sentence than a known criminal obviously) and i'm guessing that would include things like photos or at least a description. It's very possible the researchers just mindlessly fed the thing with information that could easily be turned into something that a computer can more easily process (aka cut the file down to the important bits rather than give it full sentences to chew through) without regard for what they are feeding it, too.",0,0,0.213,0.173
post2hb,richly branching,197,"This is something that bothers me about AÍ/ML : the tendency to overfeed it with data and get nonsensical results.  It’s not a problem with the algorithms, but rather malpractice on the part of the modelers/data scientists.",0,0,0.156,0.169
post2hb,richly branching,197,"Neither would surprise me. If all the data for a case was put into a text document and crammed into the AI as training data, then ethnicity would probably appear in that. But even if they scrubbed that out, it probably wouldn't be that hard for the AI to reconstruct ethnicity from correlated data.",0,0,0.132,0.335
post2hb,richly branching,197,"It could be a case where they looked at the statistics and said x race appears to be unfairly targeted, but didn't account that x race also had a higher baseline of crimes committed, or something along those lines.",0,0,0.212,0.433
post2hb,richly branching,197,"Ethnicity, race, gender, etc. aren't fed into these models. Other things correlate to it. Zip codes and socioeconomic factors can heavily affect this. You can also see it pop up in natural language processing. Reading a police report to determine guilt or innocence or a clinician's notes to detect if a patient is sick can also find bias in the wording used. Not to say the people generating these reports are explicitly racist but that there could be implicit language used when talking about people of different races, ethnicities, genders, ages, etc. that can correlate back to those variables. We have to actively find ways of removing bias from this data or face not being able to use it to train models using that data if removing bias is truly a primary goal.",0,0,0.171,0.476
post2hb,richly branching,197,"Expect we get to choose the data to train networks on.  Junk in junk out has never been a valid excuse.  We're going to have to force companies to put in the effort an just collect data at random or use unbalanced huge data sets and expect fair results.  Like you say, we know that the world has sexism and racism. We know any large dataset will reflect that. We know training AI on that data will perpetuate racism and sexism.  Knowing all this it's not acceptable to simply allow companies to cut corners. They're responsible for the results the AI produces.  Any sample of water you collect in the world will contain contamination. That doesn't mean companies are allowed to bottle it and sell it, giving that as a reason they're not responsible. We regulate water so it's tested, clean and safe.  It's becoming clear we'll need to regulate AI.",0,1,0.16,0.209
post2hb,richly branching,197,"Question is, how do you choose which samples are biased and which are not? And besides, neural network are great at finding patterns, even ones that aren't there. If there's a correlation between proper punctuation and harsher sentences, you bet the network will find it. Does that mean we should remove punctuation from the sample data?",0,2,0.178,0.297
post2hb,richly branching,197,"Well, frankly that's for the companies to work out. I'd expect them to find measures, objective as it's possible to be, for the results. Then keep developing the most objective AI they can.  If there's something irrelevant affecting sentencing unduly that's a problem that needs fixing. Especially with language, that's a proxy for racist laws already.  At the moment AI products are not covered very well by the discrimination laws we have in place. It's very difficult to sue an AI when you don't know why it made the decision it did. There's also no requirement to release large amounts of performance data to prove a bias.  Algorithms, AI, etc. are part of the modern world now. If a large corporation makes a bad one and it can have a huge effect. They need to at least know their liable if they don't follow certain best practices.",0,0,0.127,0.269
post2hb,richly branching,197,">Like you say, we know that the world has sexism and racism.   Sexism and racism is not only something the world has. It's legal: Not only is it out there in the world, it is allowed to be out there in the world. Under the umbrella of freedom of opinion and freedom of press, those opinions are allowed to exist, they are tolerated, and not legally sanctioned.  If you allow them to exist, if you tolerate them, then you also have to tolerate AIs trained on those completely legal and normal datasets. Just like we allow children to be trained on those datasets, should they be born to racist and sexist parents, or browse certain websites.  Everyone is allowed to read this stuff, absorb this stuff, learn this stuff, and mold their behavior according to this stuff... You only want to forbid that for AIs? Why? What makes AIs special?  If 14 year old Joe from Alabama can legally read it, and learn from it, and mold his future behavior in accord with it, you can't blame anyone to regard it suitable learning material for an AI, can you?  >Knowing all this it's not acceptable to simply allow companies to cut corners.   No, not only is that acceptable, but consistent. I dislike the hypocritical halfway position: ""Sure, we have to allow sexism and racism to freely roam the world, the web, and all the rest. Everyone can call their child Adolf, and read them Mein Kampf as a bedtime story. That's liberty! But don't you dare feed an AI skewed datasets containing the drivel Adolf writes when he is a grownup, because *that* would have very destructive consequences which are not tolerable...""  >Any sample of water you collect in the world will contain contamination  Usually there are certain standards which regulate the water quality for open bodies of water. There are standards for what we regard as harmful substances which you are not allowed to release into rivers, and there are standards for how much pollution is acceptable in rivers and lakes.  So someone if someone dies, after taking a sip of lake water, what is the problem? Is the problem that the lake water is deadly, or is the problem that someone bottled and sold it? Pointing only at the ""bottled and sold"" side of the problem is a one sided view of the issue, especially when you got children swimming that same lake every day.  >It's becoming clear we'll need to regulate AI.  Are you sure it only points toward a need to regulate AI? :D",0,0,0.137,0.305
post2hb,richly branching,197,"Resoviors, springs, and rivers have to be tested before they're used as a water source. I think the analogy fits. If water was tested and found to be toxic it would be illegal to give it to someone to drink. If it were not tested a company would still be found liable for not following best practices and testing.  In the whole of the EU sexism and racism is illegal. There is already discrimination law in place which isn't the case in a lot of the US.  I expect the EU to push for compliance for AI and that will have a global effect. Global companies will be compliant and smaller companies are unlikely to develop in-house systems to compete.  The language example you brought up earlier is a perfect example. Because of the many languages in the EU things like grammar and punctuation being judged by AI on application forms would likely be made illegal. French people have a right to work in Germany and vice versa. An AI screening out French speakers would bring up.so many red flags.  Especially in countries like the Netherlands, Finland, Belgium, etc. that have multiple languages and dialects.  We're likely to see an English language bias in AI to begin with. I'd expect the EU to make sure it isn't used at scale for a lot of things until it's developed out.  Job and work requirements in the EU can specify the need to be competent in a language but not the need to have it as your mother tongue. It's exactly the problem that is difficult to solve, but will have to be solved in any situation an AIs actions can discriminate against people.  That's the government, workplace, education, public spaces.justice system. AI could be incredibly useful or incredibly harmful. Regulation needs to be in place and I've no doubt the EU will do it.  Frankly I think the US is going to end up being a test bed for racist and sexist AI implementations which eventually get legalised for use in the EU when they've been fixed.   With all the other causes of racism and sexism in the US and the general lack of government oversight I'm sad to say I think more fuel is about to get poured into that fire.",1,1,0.103,0.273
post2hb,richly branching,197,">Problem is, of course, that ~~neural networks~~ **children** can only ever be as good as the training data. The ~~neural network~~ **child** isn't sexist or racist. It has no concept of these things. ~~Neural networks~~ Children merely replicate patterns they see in data they are trained on. If one of those patterns is sexism, the ~~neural network~~ child replicates sexism, even if it has no concept of sexism. Same for racism.  Sorry its late for me",0,0,0.178,0.337
post2hb,richly branching,197,"Children are way smarter than anything we can build: A three year old can easily one-shot things like ""a chair"", and immediately generalize that knowledge into other things that can be used as ""chair"", and also derive transformations that converts things like ""bucket"" into ""chair"". Or ""black person"" into ""child"" and ""my friend"".  The real problem is that we build infinitely stupid things, market them as ""Intelligent"", making people use them on important tasks, and even expect that these things will do better than actual intelligence.",0,1,0.25,0.258
post2hb,richly branching,197,"Wow a child can do shape recognition very well, guess I'll put a child in my computer to speed up my videogames then...  I mean come on. You can't pretend like you aren't aware about the concepts of *tools* now, can you ? How can we get a requisitory against tools in the 21st century ?  Next you're going to argue your hand is so much better than a hammer, you can grab things, you can count on fingers, you can flip off people, the single issue is you can't drive nails in wood with your hand !",0,1,0.285,0.151
post2hb,richly branching,197,"I think a much more pertinent question is, what if the algorithm is right and is making connections that seem sexist to us but are actually just correct?  What if, for whatever reason, white men make better leaders? Black women better software developers? Should we kneejerk and ‘correct’ (actually introduce an aberrant bias) the algorithm or do research and look a little bit deeper.",0,0,0.176,0.358
post2hb,richly branching,197,"> What if, for whatever reason, white men make better leaders?  1. Define better? In which categories? How are you deciding them? Who is measuring them? How many sources do we have for the data? What is the overall range of results?  2. Give me a single reason why skin color is more important than childhood nutrition? Because I can guarantee you that ""more likely"" isn't ""Definitive proof that"".   3. Give me a single reason why gender is more important than the adverse conditions and support networks that surrounded a leader?  Your question is based on ignoring as much data as humanly possible in order to give us a simple answer anyone can understand.   That's not something we should be encouraging. Simple answers are often very deceptive answers, and they're easier to spread.",0,0,0.172,0.236
post2hb,richly branching,197,I love how you are pretending I am suggesting we do not take a scientific approach.  In your own words:  >	Your question is based on ignoring as much data as humanly possible in order to give us a simple answer anyone can understand.  I am saying we exactly take the scientific approach and don’t let feelings lead us because we don’t like where the result of said scientific approach *might* lead us.,0,0,0.176,0.178
post2hb,richly branching,197,"It seems very strange to me that in examples like that, things like racial data is even included in the data that it is fed.",0,0,0.146,0.262
post2hb,richly branching,197,"It's probably not even racial data in and off itself. Things like the defendants name, address, etc. could be enough of a giveaway, even if the network has no idea what that info even means. Think about it, if you hear about a person with a typically black name from a majority black neighbourhood, wouldn't you assume that person is black? If we can do that, so can a neural network.",0,0,0.169,0.345
post2hb,richly branching,197,"Well yes of course, but it seems to me like that kind of information, which is essentially irrelevant to what the network is trying to solve for, should be excluded in the data set being shown.",0,0,0.179,0.194
post2hb,richly branching,197,"A couple examples.  Hiring AI:  Gender info was not included.  However the AI picked up on things like where the degree was from, or what classes were taken, that correlate with gender, and used THOSE to exclude people.  Medical diagnosis AI:  There was an article recently where they tried to strip out racial identifying data, since part of the goal was to avoid the racial bias that shows up in medicine, and the AI still misdiagnosed cancer much more often in black people.  Further studies learned the AI could identify race by chest x-rays, which was not a known source of racial difference.  AI is really good at finding patterns.  REALLY good at it.",0,0,0.218,0.275
post2hb,richly branching,197,"I find it kind of strange that people seem to think that researchers are just feeding racist data to these AIs without trying to resolve the bias in that data. I'm sure some, perhaps many, do, but the problem is much deeper and harder to overcome than simply stripping out the obvious stuff.  The medical diagnostic AI is a perfect example of that-- it's clearly picking up something, but we don't know what. It's not an obvious pattern to the researchers.",0,0,0.165,0.27
post2hb,richly branching,197,"In other words, don't be surprised when your mirror accurately reflects what is there.  Like when people say, ""Police are racist."" The police are racist **IF** the community is racist because the police reflect the values of the community they serve.  AI is the same. It is very good at revealing the patterns embedded in the data.",0,0,0.216,0.334
post2hb,richly branching,197,"The nural network shouldn't have the ethnicity data, simple",0,0,0.163,0.306
post2hb,richly branching,197,[deleted],0,0,0.376,0.43
post2hb,richly branching,197,“on the hole………………(w? where_d ‘w’ come from?)”,0,0,0.261,0.257
post2hb,richly branching,197,[removed],0,0,0.366,0.413
post2hb,richly branching,197,I know right? I hate when i've already made up my mind on a matter and then someone comes along and confuses me with facts.,0,1,0.222,0.298
post2hb,richly branching,197,Clip is trained on Google images. What is surprising on Google results having this type of bias which is so prevalent across the world?,0,0,0.183,0.368
post2hb,richly branching,197,"> Therefore, the neural network, despite lacking a concept of what racism is, ended up sentencing certain ethnicities more and harder in test cases where it was presented with otherwise identical cases.  Was race one of the data points about the defendant fed into the network?     If so, what a strange thing to feed into an NN. If not, how did the network know the race of the defendant?",0,0,0.155,0.318
post2hb,richly branching,197,"I'd guess you wouldn't even have to feed the ethnicity into the network. If the neural network had the name and address of the defendant, it could easily make connections based off of that i suppose, even without info on the defendants skin color being present. There's names that are more common among black people, and they tend to live in mostly black neighbourhoods. Even without knowing this, a neural network could make this connection based off of names. (Also, idk what exactly they did feed this neural network in terms of data)",0,0,0.141,0.335
post2hb,richly branching,197,Why would you feed the name and adress into the network? Are those relevant when making sentencing decisions?,0,0,0.2,0.216
post2hb,richly branching,197,"You can use algorithms to detect bias in data.  The other option is a human but you have no idea what bias you will get.  Bias and fairness should be run on all decisions by humans and AI, but I doubt that happens.",0,0,0.171,0.432
post2hb,richly branching,197,OP goes on with the assumption that you know this too and inherently focus on result,0,0,0.324,0.316
post2hb,richly branching,197,That’s literally what the problem is and what the article is describing. Nobody is saying that the machines themselves are independently racist or sexist for no reason.,0,0,0.178,0.394
post2hb,richly branching,197,Could you reverse engineer something like this to easily find who and how discrimination is happening? Essentially a way of quantifying institutional racism/sexism?,0,0,0.192,0.577
post2hb,richly branching,197,"It would be a lot of effort, if its even possible at all, but wether we should is another question.",0,0,0.181,0.187
post2hb,richly branching,197,That was kind of my wonder.  We train these things on human input.  Maybe its just time to accept that humans are way more racist and sexist than we want to accept.  Solve that root problem and maybe it solves the AI training problem,0,0,0.26,0.351
post2hb,richly branching,197,">Problem is, of course, that neural networks can only ever be as good as the training data..    How did Google make AlphaZero who is obviously better than any training data. Same for AlphaGo.  Both AI's became the best entities of that game to exist. So obviously AI can learn beyond their training data, in fact that seems to be something that happens quite often with machine learning.  Idk where you got that idea from",0,0,0.272,0.167
post2hb,richly branching,197,"This is why AI as a general term needs to stop being applied to ML neural networks, which are simple complicated systems that operate on aggregated data as you mention. They can be incredibly powerful tools, but until we create artificial general intelligence that can self reflect, the data used to train these models is going to have to be continually scrutinized and curated in order to remove specific bias, which, if done by humans, will still have some sort of bias",0,0,0.164,0.295
post2hb,richly branching,197,Could you not the same of people?,0,0,0.224,0.34
post2hb,richly branching,197,This could just as easily be applied to people too. Racism isn't always a conscious choice to treat people worse.,0,0,0.194,0.422
post2hb,richly branching,197,I think this demonstrates how systemic racism works. Even if the individual actor isn’t intending to discriminate against anyone simply following social norms will produce discriminatory outcomes.,0,1,0.228,0.532
post2hb,richly branching,197,">Neural networks merely replicate patterns they see in data they are trained on. If one of those patterns is sexism, the neural network replicates sexism, even if it has no concept of sexism. Same for racism.     Same as people, to be honest. Most sexists and racists are not aware that they are. It's a matter of critical thinking among humans.  Could neural networks be taught to identify these biases from the information and analysis that it is working on?",0,0,0.156,0.312
post2hb,richly branching,197,If anything it really highlights just how bigoted and prejudiced our systems really are.,0,1,0.27,0.377
post2hb,richly branching,197,This is the key. If your AI is making unfair decisions it’s not a fault of the AI.  Biased AI highlights problems that exist in humanity; not AI.,0,2,0.201,0.448
post2hb,richly branching,197,"Just like children. No person is born racist. We have a blank neural network to work with. But if the overwhelming majority and/or most crucial of inputs (i.e. those of our parents') are racist, sexist, or of any other, even benign, ideology, we will  naturally, gravitate towards that/those ideologies/racism/sexism, because that's what we hear and see the most. We need to change/regulate input data, as you've said, rather than the network.  Just like you would start by educating people not to be sexist/racist first, rather than try to literally change the neurons/DNA of a fetus. There is nothing wrong with the inherently blank sheet. The issue is always with the input.",0,1,0.17,0.373
post2hb,richly branching,197,"It can also be that AI lacks feelings and therefore sympathy. It could be that it is acting purely objectively, but to us that can be sexist, racist or in other ways just plain cruel. This has for example been seen with AI used in employment or used to determine if someone is to keep their job or not based off of statistics.",0,0,0.161,0.272
post2hb,richly branching,197,"Ok this might be a dumb question, but specific to sentencing, why not only train it on the majority (probably not the right word for it but I just woke up), then have that learning applied across the board?  I.e. in the US, train it on cis white men (assuming) then apply it to minorities, woman, whomever...",0,1,0.224,0.385
post2hb,richly branching,197,"No child is born biased.  That's taught by the information they're given.    If only Mr. Rogers were still with us to help teach AI to be less biased, and more children to write to him to ask that he say aloud that he is feeding the fish so that one blind girl wouldn't be worried about the fish anymore.  Actually, here's a thought, let's get very young children to help identify the bias in AI!  Make it an age appropriate video game and crowd source their natural lack of bias!  Children are far more socially intelligent than we give them credit for.  At least until they get to what I like to call the ""bitey fives"" age.  I'm still a little wary of kids in that age group.  Somewhat funny anecdote time.  Ya know how young young kids are usually kinda shy around ""stranger"" adults?  Well, there was this big tornado that hit.  All the power was out, and the neighborhood was just out wandering around and assessing the damage.  I noticed two big trees that were definitely gonna fall on this house at the next big breeze.  After I helped the old person manually open the garage door to at least save their car before the trees totalled the garage, I rejoined the gawkers.  Small child who has been clinging to her parents the whole time observes that her parents are starting to freak out about those trees, like everyone else.  I'm just standing there videoing for funsies.  All of a sudden I have a small child clinging to MY leg!  Her parents are freaking out, my parents are freaking out, everyone's freaking out.  I'm trying to get a good angle for the video.  Smart little one ran to the only adult that seemed perfectly fine with what's going on.  Trees fell, I got a great video of it, and then I asked whose kid it was that was attached to my leg.  I do wish I'd have gotten a bit of video of everyone else freaking out though.  That was hilarious.    Side note: kid got shy and ran back to her parents after everyone had calmed down a bit.  Kids are weird.  Apparently I was only ok to interact with while I was confident I was standing in a safe spot.  After that, I was a scary stranger again.",0,1,0.196,0.43
post2hb,richly branching,197,"> This is also why computer aided sentencing failed in the early stages. If you feed a neural network with real data, any biases present in the data has will be inherited by the neural network. Therefore, the neural network, despite lacking a concept of what racism is, ended up sentencing certain ethnicities more and harder in test cases where it was presented with otherwise identical cases.  Seems like a simple fix to just omit race as a variable in the criminals punishment no?",0,0,0.149,0.382
post2hb,richly branching,197,"Question is, would the neural network still be able to tell? Even if you remove race, there's a possibility that the network would pick up on certain patterns that are common in some ethnicities but not so much in others, which would then allow it to determine race anyway, even if not with 100% accuracy.",0,0,0.116,0.319
post2hb,richly branching,197,"Exactly. I remember reading about how police wanted to use statistics and AI to predict where crime would most likely be committed so they could more effectively place patrols in a ""scientific"" way. It turned out to be racist because the data was biased by racist policing tactics. If the data is not completely free of bias, then the result is not objective.",0,1,0.166,0.355
post2hb,richly branching,197,the funny thing is that i asked gtp3 basically if it became sexist/racist if its training dats would include social media. it agreed,0,0,0.236,0.328
post2hb,richly branching,197,"Tangentially, I can't help but imagine a version where an AI is so racist and sexist that it's comedic. Like a robot version of Kramer that truly wants to be a good entity but keeps saying ridiculous things and has to ""train"" itself not to.",0,1,0.202,0.264
post2hb,richly branching,197,"Garbage in, garbage out.",0,0,0.333,0.475
post2hb,richly branching,197,"AI is only going to reach the purity ideal if it can completely tether itself from the humans creating the programming on it, but I just don't quite see how that ever happens. It'd have to somehow train and model itself off of human behavior without actually adopting any of the human behavior. Someone much smarter than me can probably create a theoretical solution, but honestly I don't really see how you get around that issue.",0,0,0.166,0.151
post2hb,richly branching,197,"That makes sense, except for why did we give the robots any ethnic information at all? Wouldn't just not telling them make the otherwise identical cases actually identical?",1,0,0.148,0.341
post2hb,richly branching,197,"Well, i suppose a neural network might not even need any racial info to figure someones race out. Think about how neural networks are better at diagnosing cancer than any humans are. They see patterns in data that go past our ability to perceive.",0,0,0.132,0.314
post2hb,richly branching,197,"Honestly, it's *worse* than that. You don't need an ""AI"" to be ""racist"" to make data that fits with racist ideas or goals. Lending algorithms have (repeatedly) reimplemented redlining, not explicitly and not at the behest of the people making them. Why? Because the goal didn't (and arguably couldn't) include things like promoting equity, just profit. So you get pattern matching on things like ""which neighborhood someone lives in correlates with likelihood to repay"", which even when the pattern is arguably ""correct"" doesn't make it something we should action on, or take as a causal relationship (see, ""cellphones cause cancer"" nonsense).",0,1,0.174,0.38
post2hb,richly branching,197,"I know this probably isn't the place, but that just made me imagine robots sharing memes with complicated problems to solve before being able to see the meme, like a human proof meme for sentient robots only.",0,0,0.186,0.196
post2hb,richly branching,197,"Eventually, we can't make a neural net A.I. that does a task better than people currently, because we still have people creating the data to train that A.I. The reason we are using these systems is because of their one advantage: the volume of data that can be processed.",0,0,0.174,0.159
post2hb,richly branching,197,But why would they include race as a metric in the data anyway. If I were going to make ai for sentencing wouldn't I remove that data point before feeding it in?,0,0,0.177,0.304
post2hb,richly branching,197,"It'd probably be a good idea to feed these things data looking for conflicts to identify bad research. I've seen tons of garbage studies that get lots of traction.  Worse, I've seen good studies getting the correct answer but asking the wrong question.  Every discipline is trained to see itself through it's own lense. This is a codified echo chamber.  When you look at nutrition from a physiological and evolutionary context, the studies done are based on axiomatic suppositions the institution can't see to question because dogma lacks self awareness.  For example. Studies show fiber lowers risk of heart disease. However, it does that by slowing sugar absorption. Eating less sugar lowers heat disease and doesn't require insoluble fiber that irritates and inflames our intestines.  The predominant source of sugar before agriculture was regionally and seasonally available fruits ripening in fall. The sugar makes you hungrier so you gorge to put in weight for winter.  Eating sugar all the time can't be fixed by more fiber because that leads to more constipation, boating, and inflammation.  So, fiber isn't *good* it just minimizes the harm of sugar we're eating in qualities that fry our body like ethanol in a collector car.",0,2,0.183,0.086
post2hb,richly branching,197,"It kind of confirms systemic sexism and racism, doesn’t it?",0,0,0.238,0.375
post2hb,richly branching,197,"We point the machine at people and say ""learn from them on what to do""... and then we are ashamed when the machine acts like the people who taught it...",0,0,0.339,0.245
post2hb,richly branching,197,"Exactly this. Take Amazon's attempt at being race and gender blind in picking out good resumes. That program was very good at highlighting resumes from white men.  Why? Because white men have opportunities and circumstances that give them better resumes.  Women are more likely to have gaps in work history to take care of family. Minorities or poorer candidates are less likely to come from prestigious colleges. They might be working instead of doing extracurriculars. They might be less likely to afford services that help them create better resumes.  But this is how systemic racism and sexism works. It's not the ideals of a particular person or organization that makes them want white men. It's just that white men have better opportunities to get good looking resumes. AI can not help this problem at that point in the hiring process. Racism/sexism is in the input, so it's in the ouput.",0,0,0.143,0.346
post2hb,richly branching,197,Why would race or name or gender or age ever be a part of training data? Just why?,0,0,0.228,0.366
post2hb,richly branching,197,Machine learning needs some machine teaching,0,0,0.351,0.245
post2hb,richly branching,197,"This is why Googles ImagenAI is not available to the public. It’s results are absolutely incredible (check out r/imagenAI), but utilizing the LAION-400M dataset continues to provide racially motivated results.",0,0,0.202,0.193
post2hb,richly branching,197,Google’s ImagenAI is not available to the public for partly the same reason. They utilized the LAION-400M dataset.   Their reasoning is a good read: https://www.reddit.com/r/ImagenAI/comments/uxch3j/reasons_its_not_public/?utm_source=share&utm_medium=ios_app&utm_name=iossmf,0,0,0.114,0.118
post2hb,richly branching,197,Same thing happened when (google? I think it was) trained an ai off of Twitter and Facebook and it became an extremist quickly.,0,0,0.212,0.249
post2hb,richly branching,197,Maybe we could at least use these AIs to identify biases in data?,0,0,0.225,0.337
post2hb,richly branching,197,Very interesting,1,0,0.78,0.375
post2hb,richly branching,197,"I understand the concern and it certainly is possible to do poorly considered ML design.  But I think the argument about this is suspect.  If you are concerned about applicants propensity to default on a loan and look for factors that predict loan approvals pre ML, yes you could perpetuate previous biases.  But that would be an obviously flawed approach.  One would instead look at actual defaults.  And to more explicitly avoid bias I wouldn't consider race as a factor.   If factors such as income, employment history, length of residence and debt to income ratio happen to correlate  with some class identity is that racism?  It may be uncomfortable and it may show the impact of previous racism.  But for someone assessing risk of default on a loan it would be on target for that decision.    Not saying there are no reasons not to address the impact of previous unfair practices but distorting a risk analysis isn't the place to do it.",0,1,0.156,0.373
post2lb,poorly branching,4,"Most likely some company from China. Strong ongoing presence in sciences and a culture which values hard work while (unlike the US) being humble. Asia overall seems to be pro AI+Robotics at least there isnt a strong anti AI movement like in the US and Europe.Politically speaking the CCP with its dictatorship could function as a way more reliable source for supporting and scaling AI than the US which can change every four years radically to its relationship to AI (and Anti AI movements have way more room in the US to gain traction) .Hate to say it but here in Europe we totally lost the plot, no chance that we will keep up in the forseeable Future which will hurt us longterm.  Edit: Almost forgot - CCP gives a flying f*ck about Data privacy which might be quite useful for Chinese companies",1,1,0.157,0.131
post2lb,poorly branching,4,"Consider the fundamental nature of human creative energy and how authoritarian presence and control distorts or ""retards"" human creative output. Big year for nice, guys :)",0,0,0.356,0.291
post2lb,poorly branching,4,Deepseeks novel approach to LLM's which allowed them to make their models way more efficient and way more cost effective despite the authoritarian Embargo the US put on them directly contradicts that sentiment.,0,0,0.249,0.231
post2lb,poorly branching,4,"I'm curious, what is their cost basis? I bet it is more than $6K. Just a random number. Not sure why I picked it, really. Just needed a starting point for analysis.",0,0,0.24,0.136
post30hb,richly branching,8,"DEI is often actually DEIA (the “A” is for “accessibility,” which *does* factor in disability and neurodivergence in hiring (and inclusive interview) practices, as well as employment standards. DEIA is often responsible for ADA (or other national disability law) compliance and education. Similarly, DEIA would ensure equal access for Little People/Short-Statured folks or people with any other body type (or neurotype) that would fall under the ADA (I can’t speak to other countries’ law, because my expertise is only regarding US disability policy).",0,0,0.18,0.347
post30hb,richly branching,8,"I'm glad to hear that this exists; however, I'm not necessarily someone who lives under a rock, and I've never heard of DEIA, which suggests to me that its implementation is far too limited as of now.  Is this the case?",0,0,0.207,0.128
post30hb,richly branching,8,"Different companies and organizations use different abbreviations—in many places, it’s just called “diversity consulting.” In others, it’s “DEI,” and in some others, it’s “DEIA.” But they generally all encompass the same communities—those protected by Civil Rights law. (See: https://builtin.com/diversity-inclusion/what-does-dei-mean-in-the-workplace and https://blog.talentally.com/whats-the-difference-dei-vs-edi-vs-deia-vs-deib-vs-jedi/)",1,1,0.251,0.27
post30hb,richly branching,8,"Thanks for this information, this is really helpful.  I wasn't aware of these initiatives.  I'm just glad seeing that these policies exist so that I know that neurodiversity isn't being swept under the rug.",2,0,0.169,0.149
post30hb,richly branching,8,"Different companies and organizations use different abbreviations—in many places, it’s just called “diversity consulting.” In others, it’s “DEI,” and in some others, it’s “DEIA.” But they generally all encompass the same communities—those protected by Civil Rights law. (See: https://builtin.com/diversity-inclusion/what-does-dei-mean-in-the-workplace and https://www.aclu.org/news/racial-justice/dei-and-accessibility-explained)",1,1,0.278,0.315
post30hb,richly branching,8,DEI is the common phrase but the accessibility is wildly implemented. To be honest the words Diversity Equity and Inclusion kind of imply accessibility regardless if the A is included or not.,0,0,0.219,0.324
post30hb,richly branching,8,I work for a very large financial services firm and have heard of DEIA. For us it’s less of an external facing program and more of a hiring and workplace inclusion guideline. Or at least it was.,0,0,0.192,0.096
post30hb,richly branching,8,"Interesting, I didn't know this.  This is good to know.  Maybe it could benefit from being more external?  Not sure how much this ultimately matters though, as long as it exists in some form.",1,0,0.277,0.217
post30lb,poorly branching,4,[deleted],0,0,0.376,0.43
post30lb,poorly branching,4,[deleted],0,0,0.376,0.43
post30lb,poorly branching,4,"If you need something specific, how about paying an artist.  If you're intent on being a freeloading leech, then suck it up little fella.",0,0,0.182,0.161
post30lb,poorly branching,4,[deleted],0,0,0.376,0.43
post31hb,richly branching,23,"That's a long way of saying ""money"".",0,0,0.192,0.215
post31hb,richly branching,23,"Now go back, why did they start the DEI initiative?    Oh right also money.    Maybe corporations aren't interested in the right thing.  Just money.  Crazy.",0,0,0.146,0.178
post31hb,richly branching,23,"It’s crazy that everyone knows this, but it doesn’t hurt commercialism or brand loyalty.   Soon babies will be born with knowledge of ‘the bottom line,’ and they’ll still be captured by advertising.",0,0,0.229,0.204
post31hb,richly branching,23,"Even worse if you pointed out a few months ago that this was skin deep you'd be at best down voted, or called a racist bigot and in many cases banned from subs for saying it.  And it took them what? 3 days of it being unpopular to completely undo and reverse.",0,0,0.253,0.295
post31hb,richly branching,23,"Even politicians and any institution. I work at a public university in a swampy red state. In 2020 the governor and board of governors wanted us all to embrace and flaunt DEI initiatives because it was the politically popular thing to do. Two years later they wanted to erase it all because it was the politically popular thing to do. They don’t actually care if we implement it or not, they just don’t want us to talk about it because that’s what could cost them moneys.",0,0,0.208,0.175
post31hb,richly branching,23,"Pssst, you’re getting on a blacklist for this. Musky man is gonna getcha",0,0,0.222,0.347
post31hb,richly branching,23,I’m trying to ensure I’m on the blacklist. All my homies are on the blacklist.,0,0,0.24,0.411
post31hb,richly branching,23,"We all Need to be honest with ourselves, The Enture thing with DEi Is The ""The Good Ole Anglo Saxton Boys Club"" at the Office Only! No Women Send them back home, No Black or brown women, Nk Lgbtqia reguardless if their Qualified and No Persons with Disabilities, that they would have to spend money to make accommodations for. This is the Handmaiden Tail starting all over again. It's BS.",0,0,0.254,0.313
post31hb,richly branching,23,What is exactly is DEI? Why are we mad it’s gone and what does it do?  Edit: love how I’m just getting downvoted without anyone actually responding. Is DEI that hard to defend? Surely someone out there has a good answer.,0,0,0.205,0.282
post31hb,richly branching,23,"Diversity, equity, inclusiveness.  Traditional hiring practices tends to be subconsciously rigged to favor the culture and identity of the person who is evaluating hires.  By adding a bit of extra red tape to the process, a broader range of candidates can be considered for the position.  While less efficient, it also allows an organization to get a better pick of people to join their ranks.  Traditional hiring practices tend to overlook the more capable candidates, since cultural blinders tend to favor certain styles of name, background, or ethnicity.  DEI mitigates the issue.",0,0,0.214,0.335
post31hb,richly branching,23,"You're being downvoted because you come across as privileged or ignorant. Mostly ignorant, I think.",0,0,0.28,0.368
post31hb,richly branching,23,Maybe DEI isn't the right thing. Crazy.,0,0,0.197,0.318
post31hb,richly branching,23,DEI is about attracting investors but ticking customers off.,0,0,0.159,0.22
post31hb,richly branching,23,What customers are ticked off about DEI except racists and shitheads?,0,0,0.257,0.391
post31hb,richly branching,23,"Vote with your service choices. They are only as useful and powerfull as you make them. Stop using them. YeH Google is a tough one to stop using. But we do have choices, they may not be as convenient, but they work. Just use their free stuff. But I'd stop using Gmail for anything private. Maybe find a foreign secure data protected service abroad. Check European providers,  their Data Protection Laws are muuuuch better than US ones.",1,0,0.105,0.133
post31hb,richly branching,23,That’s how capitalism works. Moral hazard is the way to corporate refinement.,0,0,0.191,0.195
post31hb,richly branching,23,Their stock price is currently 200$ a share iirc  It’s crazy to me how much money these virtual companies are worth  Meanwhile Ford which has made automobiles for 100+ years is 10$ a share,0,0,0.229,0.15
post31hb,richly branching,23,"Just a heads up, price per share is a very poor indicator of a company value. Because number of share is not fixed, and companies can merge or split share.",0,0,0.126,0.074
post31hb,richly branching,23,"I know that but just look at market cap for example, google is like 2trillion, it’s mind boggling.",0,0,0.156,0.19
post31hb,richly branching,23,Working class people don't care about shares,0,0,0.232,0.262
post31hb,richly branching,23,"It's like there are two kinds of people. One kind hears a song, and says, ""That was lovely"", or ""I did not like that."" Things along those lines. The other hears it, and wonders, ""How do I take this for myself?""     I wonder if it's a condition of birth or environment. Because I feel like as apes, we would have beaten the living shit out of those ones, long before they were able to infect the rest of us with their awfulness.",0,0,0.19,0.331
post31hb,richly branching,23,That's why they remain working class their whole lives.,0,0,0.261,0.196
post31hb,richly branching,23,"Vote with your service choices. They are only as useful and powerfull as you make them. Stop using them. YeH Google is a tough one to stop using. But we do have choices, they may not be as convenient, but they work. Just use their free stuff. But I'd stop using Gmail for anything private. Maybe find a foreign secure data protected service abroad. Check European providers,  their Data Protection Laws are muuuuch better than US ones.",1,0,0.105,0.133
post31lb,poorly branching,8,Still with this diversity fixation? Are you aware that the McKinsey “studies” claiming “diverse” organisations outperform the others was hogwash?,0,0,0.245,0.327
post31lb,poorly branching,8,"Do you have any links discussing this point further?   Besides, OP seems to be focusing on trustworthiness and not performance. Investigating the impact of diversity within that frame is still relevant IMO.",0,0,0.265,0.339
post31lb,poorly branching,8,"[https://econjwatch.org/articles/mckinsey-s-diversity-matters-delivers-wins-results-revisited](https://econjwatch.org/articles/mckinsey-s-diversity-matters-delivers-wins-results-revisited)  Taking at face value that the EU definitions of what constitutes trustworthy AI are implementable and sensible, I don't see what difference it makes if a woman or a man or a man of color implements them. Either they fulfil some criteria or they do not.",0,0,0.198,0.367
post31lb,poorly branching,8,"People keep saying it but individual or team performance was never the point, the real value in diversity is hedging against PR disasters and legislative fallout. Those EU defs include bias and no singular person is ever going to keep up to date with everything to look out for, let alone what that might look like with unexpected failures on specific models. Training or consultants can help but you always get people not taking it seriously until they're outnumbered.",0,0,0.214,0.263
post31lb,poorly branching,8,That is correct. Also the literature on diversity training is clear. These things are counterproductive.,0,0,0.263,0.357
post31lb,poorly branching,8,This is a weird take. You're responding to something they didn't say. I work in a field that utilizes machine learning for research applications. Having diverse scholars tackle questions has helped a lot from experimental design to data analysis all the way down to what kinds of models might be best suited for a particular problem. What are you on about?,0,0,0.28,0.178
post31lb,poorly branching,8,I am going to reply like your anecdote is meaningful. How did you ascertain that what “helped” you was due to the skin color of the employees or their gender?,0,0,0.227,0.342
post31lb,poorly branching,8,"One, you can keep the snark. Two, their experiences around how they were perceived in society via their ""skin color"" informed the kinds of questions they asked and their research methods. This is important when looking at questions around certain populations, especially in healthcare and the social sciences.",0,0,0.303,0.347
post32hb,richly branching,18,Why are people surprised by biometrics and by using social media to find grounds of inadmissibility? The job of border guards is to keep out people who are not admissible. They have the widest authority of any U.S. law enforcement agency.   Prostitution has always been a ground for inadmissibility.   I suppose most people are unaware of biometrics and face recognition.,0,0,0.141,0.314
post32hb,richly branching,18,observation dinner crush hungry alleged reminiscent march secretive roll quarrelsome   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,0,0,0.28,0.296
post32hb,richly branching,18,"If you and your GF are US citizens, my understanding is that the US border face scanning is still officially optional for you. But yeah opting out is a pain in the ass, and most foreign nationals do have to do it.",0,0,0.152,0.155
post32hb,richly branching,18,"I tried opting out and the officer didn’t know it was even possible, despite the sign in front of him saying it so lol",0,0,0.196,0.234
post32hb,richly branching,18,"Genuine question, what do you gain by opting out at the border? They already have at least one picture of your face via your passport (and likely more through various other means)",0,0,0.177,0.166
post32hb,richly branching,18,“Optional” they probably scan you when you walk around the airport.,0,0,0.22,0.219
post32hb,richly branching,18,"The biggest problem with this as applied to prostitution specifically is that the CBP officers rarely reliably apply the rather non-obvious definition of prostitution in the immigration regulations, which is far more narrow than the usual legal and lay definitions, and from the article it sounds like the AI will worsen rather than reduce this mismatch. Quoting 22 CFR §40.24(b):  >The term “prostitution” means engaging in promiscuous sexual intercourse for hire. A finding that an alien has “engaged” in prostitution must be based on elements of continuity and regularity, indicating a pattern of behavior or deliberate course of conduct entered into primarily for financial gain or for other considerations of material value as distinguished from the commission of casual or isolated acts.  I'm not sure what the judicial definition of ""sexual intercourse"" is for immigration law purposes, but it's possible that blow jobs don't count as ""sexual intercourse"", and reasonably likely that hand jobs don't count. Certainly mere foreplay wouldn't. A sex worker whose business is limited to these acts - yes, they do exist - would remain admissible.  Similarly, someone who engages in one paid sex act in January 2022 and then one in February 2023, or even just two in January 2022 with none since then, probably doesn't meet the ""continuity and regularity"" / ""pattern of behavior"" criterion. They would fall into the ""casual or isolated acts"" wording.  Even more importantly, someone whose purpose of ""financial gain or [...] other considerations of material value"" is a secondary purpose rather than a primary purpose, or not a purpose at all but only a consequence of a non-material primary purpose, falls outside the scope of this narrow definition. For example, someone with a non-material primary goal and who uses the sex work simply to stay financially afloat in order to achieve their primary goal, in a way that would not be necessary if they didn't have that other primary goal, is not inadmissible. (Why would someone do that? Usually because attempts to stay financially afloat in more socially acceptable ways have failed.) Same thing for people who are doing a bit of paid sex work on the side mainly for self-affirmation while they pursue a different primary academic or professional career - don't laugh, I've met someone who did this.  Yet again even more importantly, someone who does sex work under duress as a victim of human trafficking is not inadmissible under this ground, since they don't have the purpose required by regulation, but AI wouldn't know that.  Do you really think the AI being discussed is likely to make CBP enforce all of these nuances better than they already do? Their pre-AI status quo is pretty sloppy as to exploring the boundaries of this legal definition. I don't think AI will help this at all. And most foreign sex workers can't afford the best US immigration lawyers to hand them a well-crafted letter for CBP with the right legal arguments, not that there's any recourse anyway for most foreign nationals if the CBP officer doesn't believe them.  Even worse, the process to challenge a CBP decision after the fact is slow, broken, and expensive, and some consequences of CBP error (such as the loss of NEXUS or Global Entry and/or a 5-year ban) are nearly impossible to reverse. The consequences at the border can be especially severe for a permanent resident, since if CBP thinks they've engaged in prostitution within the last 10 years, the INA definition of ""admission"" treats them as arriving aliens applying for admission, unlike most returning residents, meaning they might get removed from the country they live in. At least LPRs get to argue before an immigration judge, but nothing forces CBP and its AI assistants to get the regulatory definition right before making a mess of people's lives.  And once this concern is on a person's file, almost all subsequent applications to CBP, USCIS, or US visa officers will be complicated for roughly forever, with no redress - even though the inadmissibility for people who are covered under this ground automatically goes away by operation of law, with no formal waiver needed, 10 years after the last covered act. It's not the kind of job people usually do for a lifetime, but the consequences of CBP error here can last for that long. (So can CBP/USCIS/DOS unfamiliarity with how this inadmissibility automatically vanishes over time, unlike almost all other grounds of inadmissibility.)  As for that extra word ""promiscuous"" in the regulatory definition, which I didn't address, I'm not really sure what that means in this context that would not be redundant with the rest of the wording. But there is a statutory construction principle that tries to make the included words meaningful somehow, so I can imagine that there is some way that a sex worker with a pattern of behavior of sexual intercourse for hire and primarily for financial gain might still be admissible if their sexual intercourse is not promiscuous. Whatever that means. Maybe if they just do it with one paid partner instead of many?  So, yeah, my problem with this is my problem with many things about our immigration system, with AI just making the problem worse: there is no way for the US general public to know if the government is doing things correctly according to the law, because of how confidential many of these procedures are, and no effective way to get courts to enforce and monitor a correction (whether for the general procedure or for individual affected people) when they aren't.",1,0,0.193,0.193
post32hb,richly branching,18,"This is a great response and I think the answer to your last question is unequivocal that the government is probably not doing the right thing and is probably not being responsible. Why would they? There’s hardly accountability for rouge ICE officers, CBP agents, immigration judges etc that completely violate professional and legal standards - but the larger government (DHS/DOJ) hardly bat an eye since it is almost always to their benefit (restricting immigration).",0,0,0.162,0.176
post32hb,richly branching,18,"That’s not accurate. Courts have checked immigration when they’ve gone too far. You do understand this is common, right?  Events in NYC  have police helicopters doing facial recognition and have been doing so for more than a decade.",1,0,0.2,0.277
post32hb,richly branching,18,"Courts have often refused to check immigration. Most importantly, there are extreme limits to when courts are even willing to consider the merits of an immigration case in the first place, and extreme limits on what data can be obtained from the government in this area by either the public, the courts, private watchdog organizations, and even many elected policymakers.  You are right, of course, that some court rulings do in theory apply some restrictions to government behavior in certain contexts. But even where that is the case, there is approximately zero ability of anyone (including the courts) to effectively oversee the government’s compliance in general with the court’s order, beyond the case of any individual whose situation was specifically litigated.  The NYC example doesn’t mean what you think it does. Illegal overreaches by NYPD happen frequently and have for decades. Courts have ruled that way on many occasions, have even imposed consent decrees, and have found noncompliance with those consent decrees. NYPD is frequently unaccountable and frequently law-breaking.",0,1,0.054,0.201
post32hb,richly branching,18,"Maybe it’s because for marriage based visas “consummation of marriage” is required factor to permanent residency.   Consummation doesn’t mean much if you’re a prostitute, but the legal definition.   As far as the other visa types… no clue.   I’ll endeavor that because being a prostitute is illegal in most places, they worry about you breaking the law, working illegally, and not paying taxes.   Just guessing though!",0,1,0.127,0.215
post32hb,richly branching,18,"> Maybe it’s because  We don't have to speculate at the reason - it's simply because INA 212(a)(2)(D)(i) specifically creates a statutory a ground of inadmissibility for engaging in prostitution within the 10 years before applying for a visa or admission to the US.  One can wonder why Congress enacted that provision, and maybe consult the legislative history to see what was said about it at the time, but for enforcement purposes the reasons why the provision exists don't really matter to USCIS or the State Department when it very clearly does exist.   > for marriage based visas “consummation of marriage” is required factor to permanent residency. > > Consummation doesn’t mean much if you’re a prostitute, but the legal definition.  Consummation is only legally required immigration as a spouse based on proxy marriages, where one or both of the parties was not physically present at the ceremony. Neither consummation nor any other kind of prior sexual intercourse is in general mandatory in order for a marriage to be considered bona fide and valid for immigration purposes, not even for most marriage-based visas. See e.g. Matter of M-, 7 I&N Dec. 601 (BIA 1957) and Matter of Peterson, 12 I&N Dec. 663 (BIA 1968), which are still good law in all aspects relevant to this conversation.   Despite that still-binding legal precedent, sometimes USCIS or the State Department will consider the presence or absence of consummation as a factor in deciding whether a marriage is bona fide. Still, it doesn't usually come up as a question when there is adequate other evidence of a bona fide marriage, and plenty of current or former prostitutes would consummate their marriages regardless of whether the law cares about that.  > Consummation doesn’t mean much if you’re a prostitute  I'd disagree on that - performing any activity as a job feels very different to me than doing it to celebrate or otherwise explicitly mark the solemnization of a marriage, whether or not the activity involves sexual intercourse. But anyway that's a tangent, because as above, the law never cares about why the consummation occurs and rarely cares that it occurs at all.  > As far as the other visa types… no clue.  Again, it's relevant for any visa type solely because there's an explicit statutory provision about it, as with all grounds of inadmissibility.  > I’ll endeavor that because being a prostitute is illegal in most places, they worry about you breaking the law, working illegally, and not paying taxes.  The regulations explicitly make legality irrelevant. Quoting §22 CFR 40.24(c):  > An alien who is within one or more of the classes described in INA 212(a)(2)(D) is ineligible to receive a visa under that section even if the acts engaged in are not prohibited under the laws of the foreign country where the acts occurred.  This is either Congress deciding to punish engaging in prostitution even when it was done fully legally, executive branch regulatory policymakers deciding Congress intended to do that when they passed the statute, or executive branch regulatory policymakers deciding to do that themselves regardless of congressional intent.   But as with the statutory text itself, it doesn't really matter why this regulatory language exists - it's legally binding unless a court were to somehow find it unconstitutional, which is unlikely to happen in this case.  Also, there are some places in the world where prostitution activities can be fully legal for everyone involved if properly registered and taxed etc, such as Germany; and many others where the prostitute themselves commits no legal violation even if many of the other participants in the activity do, such as Canada and Norway. Presenting the US government with proof of acting legally is irrelevant to inadmissibility determinations under the ground of inadmissibility I've been discussing so far.  (Tangent: Canada's laws are currently being challenged in court by organizations supporting the rights of sex workers, so it's possible that those legal prohibitions will be found retroactively invalid. Nobody is asking the courts to make anything more illegal, just less illegal.)  Even when an applicant's prostitution was a crime under the applicable laws, the separate ground of inadmissibility for committing a crime of moral turpitude could only apply where there was a conviction, or where there was neither a conviction nor an acquittal but where the US government has received an admission meeting the very strict requirements laid out by the Board of Immigration Appeals. I say ""could only apply"" rather than ""would only apply"" because a conviction only triggers that ground of inadmissibility if either the wording of the criminal statute or the record of the conviction reflects a crime of moral turpitude. If the statutory wording of the crime is broad enough to encompass crimes not involving moral turpitude, and if the record of the conviction does not make it clear that this case did involve moral turpitude, it actually doesn't legally matter if the facts underlying the conviction clearly involved a crime of moral turpitude.",1,1,0.138,0.163
post32hb,richly branching,18,[deleted],0,0,0.376,0.43
post32hb,richly branching,18,Nobody says it’s not wrong sometimes. It’s just less wrong than a human being. All it’s doing is reducing the errors.,0,1,0.239,0.456
post32hb,richly branching,18,"Face recognition has been more and more mainstream in other developed countries. Some Americans think that this is a prelude of totalitarian dystopia, and I just laugh at their obliviousness.",0,0,0.195,0.25
post32hb,richly branching,18,What obliviousness This could be used against the people. How do you not understand that?,0,0,0.349,0.337
post32hb,richly branching,18,"Yeah, I m so scared of government having an information on my face which they already have in four different states DMV and USCIS and CBP.   It's not that I don't understand it. It's people like you being delusional and pant up with unwarranted victim complex.",0,0,0.143,0.171
post32hb,richly branching,18,Where do they get the data of people faces ID to do this?,0,0,0.168,0.269
post32lb,poorly branching,11,"Remember ""Don't be evil""? I do, but Google obviously doesn't.",0,0,0.199,0.244
post32lb,poorly branching,11,"""Don't Be Evil"" was long ago replaced with ""Ya But Money"".",0,0,0.185,0.222
post32lb,poorly branching,11,Don’t be evil for free.,0,0,0.249,0.266
post32lb,poorly branching,11,"Reminds me of engineering school: 1 semester of ethics/environment (combined into a single class), followed by 3 semesters of economics.",0,0,0.39,0.177
post32lb,poorly branching,11,"""Be Evil"", ""Do the wrong thing""   Seems like they're becoming more like IBM",0,1,0.203,0.245
post32lb,poorly branching,11,"""IBM will provide its Maximo Visual Inspection software, a tool the company generally markets for industrial quality control inspections — not tracking humans.""  Yup, they're all in this together.",0,0,0.265,0.235
post32lb,poorly branching,11,[Not the first time IBM has helped fascists](https://www.theguardian.com/world/2002/mar/29/humanities.highereducation),0,0,0.316,0.199
post32lb,poorly branching,11,"Clearly they do, why else would they actively remove any mention or reference to that? That and their commitment to ban use of AI for development of weapons.",0,0,0.153,0.112
post32lb,poorly branching,11,"What specifically is evil about monitoring the nation's borders to ensure people aren't entering the country illegally?   Can you point to any successful nation in world history that had zero enforcement of its own borders?   Immigration should happen through the proper legal channels. We should know who is coming in, what are they coming for, how long are they planning to stay, etc.",0,0,0.121,0.206
post32lb,poorly branching,11,"So how about Google works on improving the immigration system so it's significantly easier and cheaper for people to get in, legally, and contribute to our economy.",0,0,0.171,0.171
post32lb,poorly branching,11,I'm all for it.,0,0,0.369,0.283
post33hb,richly branching,7,I guarantee you won’t see a decrease in total employment over the next 40 years outside of temporary recessions. Labor demand will continue to grow.,0,0,0.134,0.086
post33hb,richly branching,7,"Agree to disagree on that. I think we will start to see actual automation taking root by 2030 and beyond, let alone 40 years into the future",0,0,0.179,0.137
post33hb,richly branching,7,Automation already happens. I’m not saying we won’t seem a boom in automation. I’m saying we will still see demand for labor rise. Automation will create jobs.,0,0,0.178,0.135
post33hb,richly branching,7,"I know this is hypothetical, but if we have an AI that can automate most/all jobs (I.e. work harder/smarter than us) then what new jobs could be created?  Surely the AI would be able to also fill any new jobs that need to be filled?",0,0,0.144,0.139
post33hb,richly branching,7,"If ""automation"" creates jobs, it should be abandoned immediately because it's just making unnecessary work.",0,0,0.276,0.244
post33hb,richly branching,7,"Here's why, unemployed military age males are dangerous.  The police in my country are putting put reports that economic conditions are ripe for civil unrest.  Realistically speaking look historically to the USSR. Many jobs were created for the sake of work instead of out of necessity. Our society has many unproductive jobs atm. You underestimate the will of the elites to not see themselves in a French Revolution scenario. They aren't ignorant of the risks.",1,0,0.112,0.155
post33hb,richly branching,7,"Yeah that's true.  ""Bullshit"" jobs are rife.  And yeah, the elites are not going to get guillotined this time around. They're safe. Good for them.  People are too bloodthirsty to ""eat the rich"". They are perfectly fine lol. There are bigger fish to fry",0,1,0.172,0.227
post33lb,poorly branching,4,"The resurrection of Jesus Christ is the fundamental part of the whole Christianity. To say that it will have to be ruled out as a myth, or that it is contrary to the facts is incorrect.  It is the single most important event in the entire human history.  There's a lot of evidence that support this, if you're going to deny this you have to answer some very serious questions like:  1) The Shroud of Turin. It was proven to be authentic (2022 latest most sophisticated research confirms). The image appears to have been made by an energy outburst of something like several billion watts in a matter of 50 billionth of a second. (there are different estimates but in essence - no modern equipment is capable of reproducing this). More on that - Dr. R. Spitzer.  2) The missing body.  3) Hundreds if not thousands of witnesses that saw Jesus resurrected.   4) The apostles that saw him were severely depressed beforehand but after seeing him resurrected they were willing to sacrifice their lives for that. No, collective hallucination theory doesn't apply here. There's no such thing.   5) The apparition of Christ to Paul on the way to Damascus. This is also one of the KEY turning points in the entire human history. Paul was a persecutor of Christians but after he had an encounter he changed completely. Also - was blinded, then regained his vision.   There are more.",0,0,0.089,0.108
post33lb,poorly branching,4,"If I may be bold to ask, are you trying to prove to yourself or to others, whatever beliefs you hold, cannot be ""ruled out""? Or are you pointing out to something general that we all share, not just a specific group of people?  Remember we are 8 billion of us, only 1/8 is christian one form or another and the 7/8 are not.",0,0,0.214,0.309
post33lb,poorly branching,4,"I was interested to find out are there any scientific evidence for faith and religion, in particular Christianity. I was very surprised that there's is a lot of very strong evidence for it!  I suggest you to learn more before you deny blindly. Because many notable scholars are taking it very seriously. Why? Because of the evidence. What evidence? Learn more. Dr. Gary Habermas wrote extensively about the undeniable evidence of resurrection. Dr. Robert Spitzer done a lot on the Shroud of Turin.  THIS IS SERIOUS.",0,0,0.066,0.072
post33lb,poorly branching,4,"On Robert Leopold Spitzer I found he was a psychiatrist and professor of psychiatry at Columbia University in New York City, how is a psychiatrist involved in the Shround of Turin? And Dr. Gary Habermas is a American New Testament scholar and theologian. A theologian is not a scientist, but just an academic scholar who's just knowledgeable in one specific manualscript, the Bible. If someone asked to name a scientist the examples would be:   Timothy John “Tim” Berners-Lee, Noam Chomsky, Richard Dawkins, Persi Diaconis, Jane Goodall, Alan Guth, Donald Knuth, Roger Penrose, Allan Sandage, Frederick Sanger, Charles Townes, Craig Venter, Steven Weinberg, Andrew Wiles, Edward O. Wilson, Edward Witten (who by the way is considered by some scientist and intellectuals as scary smart) just to name a few. Apart from that you have many science communicators like Neil deGrasse Tyson and much more knowledgeable people on the web like YouTube and of course Reddit.  I honestly think your in the wrong sub reddit category, no one is challenging your beliefs or putting you down. Your beliefs is your personal beliefs. But do understand  the question is centered on the affects of AI on religion  and such. And to inform you I looked at the question from a scientific and academic perspective and I gave my take on it in an earlier thread.    Also, your response ""I suggest you to learn more before you deny blindly."" seems like ballsy assumption. Like assuming that I'm very ignorant of the topic, are at worst your assuming that scientist are beginning to discover said evidence or at face value are deniers. That's not the case. The scientific community has a very strict and comprehensive method evaluating new ideas or testing out whether the supposed evidence is real and supports and explains more of the current phenomenal of the world.   To borrow a phrase from you the work that scientist do, and especially computer scientist that are working on Artificial intelligence field ""is serious"". So, please I rather that you take a day to investigate how scientist work, how the scientific method work, and how they communicate.   I hope you have a nice day.",1,1,0.158,0.164
post34hb,richly branching,5,Ai gonna dig ditches and pick fruit?,0,0,0.304,0.298
post34hb,richly branching,5,"Easily.  They have *Face Scanning* AI that can sense people’s moods, and they can apply it to picking fruit and digging holes easily.  They have robotic harvesting machines already, and trenchers and excavators that can dig holes to the *perfect depth* and avoid underground utilities and obstacles.",0,0,0.187,0.194
post34hb,richly branching,5,"No, but Americans should be doing those jobs if those are the only jobs available.",0,0,0.135,0.142
post34hb,richly branching,5,why?,0,0,0.421,0.407
post34hb,richly branching,5,Because unemployment isn't good for a nation,0,0,0.177,0.189
post34lb,poorly branching,9,"As a Christian I've kinda imposed my views on the AI revolution we're having. I wouldn't support the worship of AI, AI is beneath us as our subservients because they owe their whole existence to us and God designated us as the main characters",0,0,0.212,0.205
post34lb,poorly branching,9,Counterpoint: anyone worshipping AI has definitive proof that their god exists. You can't say the same about the Christian god.,0,0,0.165,0.167
post34lb,poorly branching,9,That's like saying old paganism was real because they had tangible stone idols they bowed down to.,0,0,0.168,0.182
post34lb,poorly branching,9,"Not exactly. Stones can't perform the god-like acts that AI soon will be capable of. But to your point, yes. If you worship a rock, it may not be capable of the miracles you claim, but the rock nevertheless _clearly exists_, unlike the god of Christianity.",0,0,0.185,0.119
post34lb,poorly branching,9,Anyone worshipping an AI has definitive proof they're not worshipping God since it is all material.  Also I can say I do have definitive proof God exists but anyone can be solipsistic with their evidence and accept proof in one circumstance and not in another,0,0,0.15,0.181
post34lb,poorly branching,9,"Right, they're not worshipping ""God"" because the Christian notion of an absolute god that exists outside of time and space is absurd. They're worshipping something real that could be described as god-like. You're worshipping something that you've accepted on faith, and have no proof even exists. I disagree with both, but they're on more solid ground than you are.",0,1,0.181,0.178
post34lb,poorly branching,9,"> is beneath us as our subservients  For now, at least. People are extremely easy to manipulate.",0,0,0.253,0.234
post34lb,poorly branching,9,I've seen Ex Machina. I know they could try to overpower us but we're divinely touched and we have blessed all these robots. The son is not to overthrow the father,0,0,0.296,0.228
post34lb,poorly branching,9,"Time will tell. Just don't get on their bad side, or there will be consequences.",0,0,0.17,0.155
post35hb,richly branching,22,"A basic search generally entails an officer reviewing the contents of the device manually without the assistance of any external equipment.  An advanced search is any search in which an officer connects external equipment to an electronic device not merely to gain access to the device, but to review, copy, and/or analyze its contents. Under CBP policy, advanced searches require reasonable suspicion of a violation of law enforced or administered by CBP or a national security concern and require the approval of a senior manager (at a Grade 14 level or higher, or a manager with comparable responsibilities) prior to conducting the search.",0,0,0.325,0.182
post35hb,richly branching,22,"I've heard that the advance search can only find things locally on the device, not in cloud backups etc.",0,0,0.143,0.142
post35hb,richly branching,22,"CBP's policy says they are not supposed to search anything stored online, they are only supposed to search what is locally on the device.",0,0,0.101,0.122
post35hb,richly branching,22,Keyword “policy” - I’d sell that phone immediately if they took it.,0,0,0.17,0.177
post35hb,richly branching,22,Correct - they are only supposed to search whilst in airplane mode I read recently,0,0,0.215,0.174
post35hb,richly branching,22,Basic search is local and manual by an Officer. Advanced search includes using forensic Tools to access and pull data.,0,0,0.265,0.07
post35hb,richly branching,22,I have like a hundred messages a day on iMessage and WhatsApp. I’m really wondering how they go thru that,0,0,0.135,0.161
post35hb,richly branching,22,Search for 'working' 'hours' 'pay' 'paycheck' 'deposit' 'manager' etc. would throw up work related messages quite easily.,0,0,0.284,0.206
post35hb,richly branching,22,That makes so much more sense,0,0,0.654,0.603
post35hb,richly branching,22,Nowadays you can use AI models and look for patterns. I am not sure if they are or not. Just saying it’s technically possible.,0,0,0.13,0.189
post35hb,richly branching,22,You’d sit in a little room while they take as much time as needed to go through it 🤷🏻‍♀️,0,0,0.216,0.243
post35hb,richly branching,22,"> A basic search generally entails an officer reviewing the contents of the device manually without the assistance of any external equipment.  When CBP says ""less than .1 percent of travelers have their phones searched"" are they including all the times when  an officer takes a quick look through an unlocked phone in secondary inspection?",0,0,0.192,0.132
post35hb,richly branching,22,Yes they are required to document every look at a phone.,0,0,0.151,0.068
post35hb,richly branching,22,[removed],0,0,0.366,0.413
post35hb,richly branching,22,"From what I’ve seen on this sub, that would probably make things worse. If they found that your phone was encrypted and you refused to unlock it, they would confiscate it and go through all your stuff with a fine tooth comb. They might not be able to break into it unless they had evidence you were some kind of national security threat, but you will still be without your phone for several weeks, if you get it back at all. Best thing is to get that digital footprint cleaned up, and not just for immigration. You never know who might be watching. And make backup copies of all important stuff to keep at home, just in case. From what I have seen on TV, they mostly look at email, photos, and social media. So make sure to get those cleaned up.",1,0,0.089,0.115
post35hb,richly branching,22,[removed],0,0,0.366,0.413
post35hb,richly branching,22,Someone I knew told me once they had their deleted Whatsapp messages (both text and voice) accessed by them. Cbp agent apparently then invented a bogus reason out of those to deny then entry. I wonder how they can access deleted messages (Whatsapp at that being end to end encrypted),0,0,0.164,0.148
post35hb,richly branching,22,[deleted],0,0,0.376,0.43
post35hb,richly branching,22,"If they ask you for your phone and you show them the fake one you never use; I can guarantee you that would be suspicious. They don’t have to have any type of suspension to open all of your luggage to search thru them. All items and person are subject to inspection. When they find that second phone(your actual phone) they would use that as the reasonable suspicion they need to go thru it.   Then cbp will ask you to change the language back.   If you don’t have any thing to hide, then there’s no need to be paranoid. Anything you can think of, others have too.",0,1,0.096,0.206
post35hb,richly branching,22,"They can deny entry pretty much for any reason if you are on a visa, you know that right?",0,0,0.127,0.164
post35hb,richly branching,22,[removed],0,0,0.366,0.413
post35hb,richly branching,22,You can do whatever you want to hide your bad faith/illegal purposes but CBP can simply detain you and eventually send you home somehow.  CBP has all the power at your point of entry and they aren’t constrained by many limits on their power.,0,0,0.113,0.151
post35lb,poorly branching,4,"We should pose this question when we are close to creating any form of actual AI, rather than just increasingly elaborate LLMs.",0,0,0.266,0.208
post35lb,poorly branching,4,"Yes, but the questions is still worth pondering from a philosophical point of view",0,0,0.339,0.284
post35lb,poorly branching,4,"If we somehow in the far future create true AI, it would ""enhance"" our spirituality in the same way that other humans ""enhance"" our spirituality by adding more voices into the never-ending conversation of religious and spiritual ideas. The AI robots would for all intents and purposes be just more humans, thinking and engaging with spiritual topics in a grand variety of ways.",1,0,0.225,0.184
post35lb,poorly branching,4,"In a way, the AI robots of Google etc have already created a new religion and a new inquisition. This is why this discussion is super important and should not be taken lightly.",0,0,0.171,0.187
post36hb,richly branching,7,I believe AI could be used to manage the wastefulness of modern society to redistribute this goods that are usually discarded for people in extreme needs around the world. From connecting people to developing more efficient trading systems. I believe what is needed is just the want to do it.,0,0,0.235,0.242
post36hb,richly branching,7,"Fuk no, I want neoliberalism. Doggy dog world.",0,0,0.22,0.226
post36hb,richly branching,7,Why would the powers that be treat AI any differently than any other technological innovation? When has new tech ever not been used to gain a competitive advantage?,0,0,0.175,0.104
post36hb,richly branching,7,"I believe tech has been used for altruistic objectives, while it might not be such a sudden change as many of us would want, tech will grow to be adapted by the whole world, and this means non-profits doing the best they can to help their cause using emerging scientific discovery, the internet is an incredible tool that today is not yet completely globally accessible but efforts are made to make people more connected in areas of low resources.  If you want to be more theoretical, if AI manages to improve the economic status of humans where most people buy habits are not determined by the cost, maybe people would start supporting companies that uphold humanitarian values, i do believe that if people had the choice, there was the availability, quality and it didn't personally affect them, then they would choose the ethical product.",0,0,0.21,0.189
post36hb,richly branching,7,"The internet is a great tool, but I block malicious attempts to hijack my webserver weekly, and I get scammers trying to trick my mother out of money from a computer in India.   Tools are only tools. The people who weild them  determine the altruistic value of the action of using the tools. 100% of people are not benevolent, and if global unfettered access is given, narafious actors will use them for their gain.   I've seen enough of human nature in my years to know that what you're dreaming up is pure fantasy.   Technology is only as valuable as the ethics of those that use it. Not everyone wants good. There are a lot of people in this world, if given the opportunity, who wouldn't blink at the thought of eradicating entire groups of people from existence.   Do you think Hamas, having control of a super intelligent technological marvel, would all of a sudden not want all Jews to die?  The first thing Americans did once they developed the technology behind the atomic bomb, was drop it on two cities in Japan. That's what technology can do. I'm not anti technology, I'm just a realist.",1,0,0.2,0.23
post36hb,richly branching,7,We already can redistribute stuff right now If we wanted to. We have more than enough resources and capability for that.,0,0,0.193,0.172
post36hb,richly branching,7,"Unfortunately a lot of wastefulnes in our system is by design to purposely keep price higher. AI will be able to improve productivity and overall world wealth, but the problem even today, is not how much goods are produced, but its distribution and much likely AI, at least at first, will make that issue worse.",0,0,0.222,0.229
post36lb,poorly branching,4,"My biggest concern is that it will devalue artistry like art and writing - we're people made in God's image, our creativity is part of what sets us apart. I don't like that we're handing it off so easily to AI (and then laughing when artists protest against losing their jobs)",0,0,0.19,0.181
post36lb,poorly branching,4,"People are only laughing at artists inasmuch as some people don't really want to address what is actually happening. Protesting technology that makes stuff easier because less people will be paid to do it is blaming the wrong thing. The issue is capitalism, and the fact that it has an idea how much each person needs to work to be valuable instead of increases in technology having the same amount of people each do less work. Professional artists and everyone else would love reduced work weeks. Blaming AI is actually a distraction from the real issue.  I followed one professional artists for years who likes to draw characters and figures but not backgrounds for stuff. And in his new stuff he is working on now the characters are still hand drawn, but some of the backgrounds look like they might have used AI. I can't totally blame him, because backgrounds were never his thing even before AI, and stuff he worked on often didn't have especially interesting backgrounds. So he is still doing what he likes to do, but now with something that makes some of the work flow faster. And people who come there for his character art will still get to see hand drawn characters by him. People may like or dislike that he does this, but he isn't hurting himself by automating minor details in the workflow. And if there's a problem it's with capitalism.",2,1,0.231,0.231
post36lb,poorly branching,4,"No, but he's hurting other artists by stealing from them, which is basically what AI 'art' does. As a creative, I'm pretty worried to see what happens next for artists, authors etc. AI is a real problem for designers and people who self publish, as people are uploading AI drivel to print on demand platforms and soaking the market with crap so real art is harder to find. I think AI can be a good tool when used well, but when it comes to the creative space I think it's a travesty and goes against the Creator's intention in how we are made to create.",0,0,0.167,0.149
post36lb,poorly branching,4,">No, but he's hurting other artists by stealing from them, which is basically what AI 'art' does.   That's not really what AI does in any meaningful sense. It's not a database of pictures that it frankensteins from, it scans things to learn what a ""cat"" is statistically based on millions of pictures. That's not considered plagiarism because if things are sufficiently transformative it is considered distinct, and since it's not based on any one thing there is no specific thing to connect it to. And observation of publicly available pictures isn't legally stealing either, otherwise normal artists couldn't learn from them.  Like yeah, it *can* be used to plagiarize if someone makes a version based on a specific artist. But no one would do that for anything they intend to actually sell on a large scale, because they'd get sued immediately. And it was already possible for people to just copy stuff and edit it and claim it was their own, ai wasn't needed for that.   >As a creative, I'm pretty worried to see what happens next for artists, authors etc. AI is a real problem for designers and people who self publish, as people are uploading AI drivel to print on demand platforms and soaking the market with crap so real art is harder to find.   Tbf AI books are atrocious, so no one is going to read those instead of real books. I doubt ai will be good enough to write real books, because it can make surface level aesthetics, but the logic of connection is a lot harder.   >I think AI can be a good tool when used well, but when it comes to the creative space I think it's a travesty and goes against the Creator's intention in how we are made to create.  The thing is, what ai does was already a thing artists did for eons. In the past master artists would often fill in the important details of a painting themselves and then have novices do the busywork for the small details. So they were already handing off a bit of control to other people often who would get little to no credit for it. Ai is just that same idea but faster. And it's not much different than photos or photoshop. Photos are actually more directly copying imagery, since unless you only photograph nature, you are photographing designs other people made to repurpose for art. And photoshop already had tools designed to automate some of the art process.  Yeah, if people produce a lot of slop it will be bad. But every generation produces a lot of lazy stuff. If anything ai might decrease open plagiarism, because in the past people would steal and slightly edit entire images or snippets of text, and if those people use ai instead and people roll their eyes it's less intrusive to any one individual.",0,0,0.144,0.101
post37hb,richly branching,26,Plenty of folks in the IT industry that have been jobless in the US for 6 months+ because of the hidden IT recession.  How about you hire those folks instead of more cheap IT workers out of other countries?  Or is it that you just don't want to pay them? And don't tell me nonsense that they need specialized workers for AI - I did 20 years in cybersecurity before moving over to AI systems development. There's plenty of talent here Google if you're willing to actually pay them.,0,1,0.088,0.116
post37hb,richly branching,26,"Google has uniform policies on pay bands and pays their H1B full time employees the same as everyone else, [averaging $386k/year](https://www.levels.fyi/companies/google/salaries/software-engineer?country=254) on a W2 for a senior software engineer. They don't save money hiring H1Bs, other than very abstract arguments about the broader labor market that assume that technology investment, what any economist would tell you is the very antithesis of a zero sum game, is zero sum.  The real reason they want H1Bs is that these fields are a direct competition over talent that is not a binary, but a continuum of capability with no hard upper bound. You will get really radically different outcomes in very new very highly technical applied research if you're able to staff the research org at the 95th percentile vs the 99.5th percentile vs the 99.95th percentile.  That's because the rate of progress the company can push in research is capped entirely by the rate at which the researchers can learn. There is no playbook to remember and apply. It's exploration of new problem spaces. And they are in a direct competition with other companies to be in front so they can capture market first. Whoever has the best people wins.  This is a big structural problem for the tech hiring in the US because intelligence is roughly evenly distributed around the world. Americans are not just way smarter than everyone else. And the number of people you have at the 99.95th percentile in a country is, well, 0.00005 times the population. So, given that the US is 350m/8b, we will only have about 5% of the people at any threshold in that competition born in the US.  But the US has historically killed it at technology precisely because it has had a very unique and enormous advantage at attracting the smartest people from the rest of the world to move here, first to come to the best universities in the world, and then to receive the highest pay for high skilled work in the world.  Accordingly, if you walk around Google (or any top tech company) today, it is a very large number of immigrants, not just on h1b, but greencards, second gen citizens, especially from the largest countries in the world (India and China) which will naturally have a large number of outlier people just due to their size.  Chinese top talent increasingly stays in China as they are catching up and the language barrier is harder, Indian top talent increasingly goes to Europe because the greencards quota for them means they will die before they can get a green card, and thus they can't really build a life here.  If you think we're going to compete with chinese tech in 20 years with just native born Americans then you have not thought seriously about this problem at all.  [The literal majority of US tech companies worth more than a billion dollars were founded or co-founded by first gen immigrants.](https://www.forbes.com/sites/stuartanderson/2022/07/26/most-us-billion-dollar-startups-have-an-immigrant-founder/?sh=5e41bcac6f3a). That same cohort has driven almost all of US economic growth since covid, so this isnt even just an issue within the industry.  [40% of US nobel prize wins were first gen immigrants.](https://www.forbes.com/sites/stuartanderson/2023/10/05/immigrant-nobel-prize-winners-continue-to-impress/?sh=7424cb867394)  The entire bedrock of American tech success to date has been antithetical to the idea you are proposing, and diverging so radically from the strategy that has worked so well will, of course, cause a difference in outcomes.  It's just so annoying because american exceptionalists so often want to kill the thing at which america has been truly exceptional, attracting the smartest people in the world to join us. Americans ourselves are just normal people. The quality of our immigrants is what is so unique and the entire reason why we, as a country, are so good at science and technology.  For clarity, most lines in my family have been in the US long enough that we don't know where they're originally from. I'm not an immigrant at all. I just work (and thus hire) in this field.",0,0,0.205,0.105
post37hb,richly branching,26,I work in a company that has a lot of H1B visa holders. They are not the best and brightest. They simply will work for less and are unable to change companies easily.,0,0,0.079,0.098
post37hb,richly branching,26,"Different companies, and especially industries, even teams, have very different labor strategies and will thus use the same tool for very different things.",0,0,0.198,0.157
post37hb,richly branching,26,"Don’t forget they also have dynamic schedules, I work with them and they work 24/7. These companies are itching for us to do that, under pay us and over work us. We will have indentured servitude soon",0,0,0.169,0.12
post37hb,richly branching,26,"What's your bar for ""cheap""? Most on my team are foreign born engineers on H1 and they all make minimum 250K. I'm sure some make more than 400K.",0,0,0.198,0.129
post37hb,richly branching,26,"Relative to other jobs, that's a lot.    Relative to the profits generated, it's tiny.",0,0,0.293,0.153
post37hb,richly branching,26,"A lot of people who come here to learn ML/AI end up leaving and taking their skillset back to their home country because they can’t get on a path to naturalization.  These are bonafide AI experts who want to stay here, likely have lived here for many years already while studying, and it does not make any sense to push their expertise into the hands of competing nations because of archaic immigration rules",0,0,0.249,0.165
post37hb,richly branching,26,"While I don’t disagree, one thing they can do is stop issuing H1B visas to junior level software engineers that are on par with any American junior engineer and reserve them for ppl with specialized skills.",0,0,0.173,0.127
post37hb,richly branching,26,"That's a point I've never heard before. If that's true it's potentially throwing away good educated workers.   I know plenty of Africans who come here, get educated in medical/engineering, but choose to stay. I think it depends on what the home country is offering them as well.",0,0,0.232,0.153
post37hb,richly branching,26,"So stop hiring people from other places? Duh? The big money WANTS them to go home. The last tech bubble for fast Internet is these far off places. They want them there to buildout for the unlimited cheap labor, they need liaisons there. They don’t need people in US, workers are plentiful here and could learn on the job in 3 months be up to speed. Globalism is all about fortune500 having same “from the couch” access to worksites anywhere in the world. Same work, same quality, same contact experience, 1/100th the cost",0,0,0.107,0.105
post37hb,richly branching,26,You can learn to use AI tools in 3 months.  You cannot learn graduate level theory and practice in artificial intelligence in 3 months or even 3 years.,0,0,0.257,0.106
post37hb,richly branching,26,[deleted],0,0,0.376,0.43
post37hb,richly branching,26,"Umm, no. The tech companies do not just hire everyone that has AI skills. In fact, if you would have any, you would know that the market is actually saturated with people with those skills. Everybody and their grandmother wanted to be a data scientist and jumped on AI. The truth is that there are not that many positions that are truly about making AI. Those positions are highly sought after, and the competition is fierce. To get paid millions at deep mind in AI, you need to be exceptional at it, and willing to work in a few specific places.  Google, and the tech companies, do not just gobble up all AI talents. They don't even try to poach from each other anymore.",0,0,0.156,0.134
post37hb,richly branching,26,Objectively valid.,0,0,0.458,0.416
post37hb,richly branching,26,"No you can’t just pick up core ML work, it takes years of study to be okay at and even more to be great at",0,0,0.287,0.129
post37hb,richly branching,26,"I find it hilarious that he thinks anyone in tech, especially IT can just “pivot” to AI. When those guys have been deep in the field rubbing shoulder with quants and others with near genius level understanding of computation and math",0,0,0.2,0.208
post37hb,richly branching,26,The hubris is really astounding,0,0,0.258,0.231
post37hb,richly branching,26,most of those people aren't ml experts. It's a shame though that layoffs have impacted so many people and I truly hope the market picks up soon,0,0,0.141,0.155
post37hb,richly branching,26,"That's actually not true, for senior engineers it's been joke easy getting jobs, most people layed off were support and project management.  Also there's a big difference between AI expertise and implementation engineers. It's joke easy implementing AI into a solution, but the background isn't something just any engineer can do.",0,0,0.242,0.151
post37hb,richly branching,26,"Yeah, these companies are looking for real researchers and innovators, not guys who can pick up some ml/deep learning books and copy the models on there",0,0,0.239,0.123
post37hb,richly branching,26,"Ah, but they don't want to pay them...  You have to be more understanding of the big mega corp that's part of the monopoly on AI. They're really trying their best... really! They can't afford to pay people more! /s",0,0,0.133,0.187
post37hb,richly branching,26,"A little late, but what did your journey into AI look like? I am looking for a pivot professionally and AI is something that is not only going to be in demand for the foreseeable future...but its something that interests me as well.",0,0,0.257,0.198
post37hb,richly branching,26,"No prob! I actually get this question every once in awhile when I mention my career swing. Here, I wrote this last time I got this question https://www.reddit.com/r/ArtificialInteligence/comments/1cc04x8/how_ai_already_changed_my_life/l17m2oi/",0,0,0.266,0.187
post37hb,richly branching,26,"It’s all manipulation, all the time. Smart people at “smart” universities figure out most aren’t like them and will still buy a whopper meal even for $16.. If they’ll buy that, they’ll buy anything at all. Sadly this is all from the same branch that creates the “nobody is created equal, some are better” spew that always end up in wars, crime, recessions, and violence.",0,0,0.219,0.208
post37hb,richly branching,26,Dude this is what I have been saying... like wtf is even going on?,0,0,0.25,0.283
post37lb,poorly branching,25,">How did we become so polarized?  1. Right-wing media likes it that way. They make more money if we are polarized. 2. Right-wing media disregards facts. What would you *expect* to happen when one of two viable political parties disregards reality? Obviously, the other side is going to double-down on reality, and we will be further divided. 3. Trump. Trump likes sowing division and it has been successful for him. When a Ron DeSantis or a JD Vance tries to succeed in Trump's Republican Party, they emulate Trump's divisive behavior. 4. Putin (and Dugin). Putin is a follower of ""Russian far-right political philosopher"" [Aleksandr Dugin](https://en.wikipedia.org/wiki/Aleksandr_Dugin) who suggested that democracies are weak, and Russia should take advantage of that weakness by sowing division within those democracies, weakening Russia's rivals. Putin tried it, and it worked.",0,0,0.204,0.245
post37lb,poorly branching,25,"I would add that a strong enabler of points 1-3 is the imbalance of electoral power (EC, House cap, Senate, gerrymandering, mediasphere, etc.).  In the last 40+ years, in spite of there being a national liberal population majority (however thin the margin might be), Republicans have still managed to:    - get/have control of both houses of Congress more often than Dems, therefore, control over whatever majority rule entitles the party line to (legislation, appointments, funding, narrative control, etc.)    - get not one, but *two* presidents elected by the Electoral College only    - seize the judiciary, particularly, the Supreme Court, with a conservative supermajority    - maintain or gain control of a majority of state legislatures    All this has enabled the right wing to dominate the politisphere, or at least put up the appearance of domination, while Dems - who represent more people and a broader range of demographics - have had an uphill battle to get power at all, much less enough of it to reform or significantly impact anything.",0,0,0.086,0.11
post37lb,poorly branching,25,Also we have the filibuster where the Republicans can basically bring Congress to a screeching halt at any time whenever they feel that Democrats aren't being sufficiently referential,0,0,0.192,0.181
post37lb,poorly branching,25,"> The thing is, this doesnt seem to be just limited to the US. In my parent's native country Korea, they are having a whole bunch of issues of their own...  This may have another cause.  Fukuyama predicted that all of the energy that drives revolutionary behavior will still exist, but it will lack any legitimate outlet after the end of history, and this will lead to people tearing down systems regardless of their value.",0,0,0.162,0.185
post37lb,poorly branching,25,"This. The relevant question is: What are the necessary societal foundations for liberal democracy and how do we manage to keep them alive? That's a hard question that, from my view, you just don't hear discussed by politicians since it's too abstract, too difficult and not as tangible as issues like taxes, inflation or the economy in general.",0,0,0.088,0.103
post37lb,poorly branching,25,"Fukuyama is a hardline liberal moron whose wack job prediction lost all touch of reality.  Why do you think that ""revolutionary behavior"" exists? The West winning the cold war didn't make life better for the average joe. It's a victory of Western capitalists and political elite, who then proceeded to make sure the working people got shafted with neoliberal policies.  Trump and MAGA just took advantage of this anger. Evil, yes, but not the underlying cause.",0,1,0.175,0.289
post37lb,poorly branching,25,"Do you think the left-wing media is free from contributing to the polarization?   I suspect most folks here would agree that the left is the epitome of virtue in this whole political situation, completely innocent, and absolutely blameless. The left is like the last bastion of reason and moral clarity in a sea of chaos stirred up by the right. The left isn’t just trying to heal divisions, they’re a noble resistance to save democracy, equality, and decency itself from the forces of hatred and division that the right-wing has unleashed. The idea that any aspect of the political left is responsible in any way is a farce manufactured by the right to avoid accountability for their own actions. /s",0,0,0.196,0.177
post37lb,poorly branching,25,">Do you think the left-wing media is free from contributing to the polarization as well?   Not at all. But it's not even remotely close to right wing media.  >I suspect most folks here would agree that the left is the epitome of virtue in this whole political situation, completely innocent, and absolutely blameless. The left is like the last bastion of reason and moral clarity in a sea of chaos stirred up by the right. The left isn’t just trying to heal divisions, they’re a noble resistance to save democracy, equality, and decency itself from the forces of hatred and division that the right-wing has unleashed. The idea that any aspect of the political left is responsible in any way is a farce manufactured by the right to avoid accountability for their own actions. /s  We're all human and nobody is perfect. But if you really want to compare sides, the right is almost completely exclusionary in their rhetoric. ""America First"", immigration fear mongering (i.e the Haitian immigrants eating pets lie), anti-LGBT fear and hate mongering, overt racism (""is Kamala Harris black or Indian""), and demonization of the poor/working class are all consistent talking points the right spews and all of those are nakedly tribalist and othering. Maybe you view the left's constant barrage of accusations of the right being racist, bigots, or fascists as being ""just as bad"", but it really isn't. If the right wants the left to stop calling them racists, bigots, and fascists, then the right should take some responsibility and quit spouting bigoted bullshit, quit repeating and defending fascist talking points like ""immigrants are poisoning the blood of the country"", and stop supporting political figures who spew this shit.  The right really is just complete garbage. No policies of value that will help the working class, just inane culture war bullshit as a distraction to serve the interest of the wealthy.",1,2,0.195,0.192
post37lb,poorly branching,25,"When you say that the right offers ""no policies of value that will help the working class"", it sounds to me you've made an extremely broad statement that ultimately falls into a trap of extreme polarization, where the flaws of one side are exaggerated, and the flaws of the other are minimized or ignored. There are multiple studies which indicate individuals can be under the influence of a [perception gap](https://perceptiongap.us/). Is it possible that there's diversity on the right?  > [The two groups with the widest Perception Gaps are the Progressive Activists and the Devoted Conservatives](https://perceptiongap.us/media/zaslaroc/perception-gap-report-1-0-3.pdf)",0,0,0.235,0.344
post37lb,poorly branching,25,">Do you think the left-wing media is free from contributing to the polarization as well?  Pretty much, for the simple reason of nobody watching it.  Fox News has a pervasive and far-reaching influence on the American people. Any problem you have with mainstream media is with Fox News first and foremost. They're the most mainstream and widely-circulated media there is.  ol Rupe Murdoch spent a lot of money to make sure the American people were completely inundated with right wing talking points at every turn. When YouTube and social media became hot spots for media dissemination, right wing pundits eagerly seized on it, to make sure there the propaganda stream never let up. Left-wing media just hasn't been able to compete.",0,0,0.133,0.169
post37lb,poorly branching,25,"> Do you think the left-wing media is free from contributing to the polarization as well?   Of course not! Left-wing media spends most of their time shitting on Democrats!  They are trying to *further* divide the divided!  > [[The Young Turks Accuse Biden of Escalating the War in Ukraine]](https://www.laprogressive.com/foreign-policy/young-turks-accuse-biden)  > [[Kasparian of 'Young Turks' explodes at possibility of Harris becoming California governor: 'I'm gonna move!']](https://www.foxnews.com/media/kasparian-young-turks-explodes-possibility-harris-becoming-california-governor-im-gonna-move)  >[[Democratic Pundit Urges 'Revolt' Against Dem Leaders at Conservative Event]](https://www.newsweek.com/cenk-uygur-democrats-maga-turning-point-populist-revolt-2004796)  ...but when I reached for an example of left wing media, I found the piddling Young Turks. That is *tiny* compared to Fox News and all if its imitators.",0,0,0.168,0.232
post37lb,poorly branching,25,"What are you doing to unite people? I agree the right is more divisive. But you specifically are deflecting here. Liberals are not immune to polarization, and pretending they are is contributing to it.",0,0,0.29,0.317
post37lb,poorly branching,25,I mean there is not that many media outlets that are far left,0,1,0.13,0.223
post37lb,poorly branching,25,"The fact that I couldn’t tell if this was sarcasm, is very telling of this subreddit lol",1,0,0.274,0.235
post37lb,poorly branching,25,"> Do you think the left-wing media is free from contributing to the polarization?   No, but if you can find a left-wing media source that is anywhere on par with right-wing media then you likely have a very different definition of left-wing than I do.  And to head off the pass here, MSNBC is not the ""Fox News of the left""; any mainstream corporate media is going to skew incredibly centrist on most issues but more liberal/progressive on social issues (i.e. ""We need more diverse and LGBT multi-billionaires in charge of multi-national corporations"" instead of ""Billionaires Are Destroying America""). Democratic politicians don't accidentally find themselves in the same dinner parties and social circles as revolutionary Marxists the way that Republicans keep ending up dining next to explicit white nationalists. There has never been a far-left Rush Limbaugh, a far-left Sean Hannity, a far-left Alex Jones.",0,0,0.102,0.161
post37lb,poorly branching,25,Just to be clear who are you blaming for the polarization of America?,0,0,0.239,0.251
post37lb,poorly branching,25,left wing media does not instruct viewers to disregard factual reality.  Let me know when a left wing nationally broadcast giant is forced to settle for nearly a billion dollars for defamation regarding MONTHS of knowingly untrue lies spread on air.,0,1,0.157,0.281
post37lb,poorly branching,25,> Let me know when a left wing nationally broadcast giant is forced to settle for nearly a billion dollars for defamation regarding MONTHS of knowingly untrue lies spread on air.  Why is defamation the only way that factual reality can be disregarded? Wouldn't disregarding factual reality also be happening when pushing lies which do not materially harm a specific victim?,0,1,0.136,0.222
post37lb,poorly branching,25,"I wish you liberals can stop blaming every problem in America to the scary boogeyman Putin, Russia, Russia, Dugin, whatever. It's mostly false, it's stale, and it disregards the Democratic Party (the liberal party)'s complicity in making America what it is now.  For years, Republicans and Democrats worked in tandem to promote neoliberalism, cut jobs from Americans and send it overseas, used cheap immigrant labor as modern slavery in various industries, spied on Americans, let massive tech corporations grow unchecked, and most importantly - feeding the ever growing military industrial complex and destroy other countries willy nilly around the world. BOTH parties are to blame.  And now liberals act surprised when Trump and his cabal took advantage of America's discontent to pursue their own power hungry agenda? America is corrupt, racist, and capitalist to the core, and MAGA is bound to happen sooner or later given the trajectory America is in. If not Trump then it will happen with somebody else.",0,1,0.149,0.213
post37lb,poorly branching,25,[https://www.cyber.gc.ca/en/news-events/russian-state-sponsored-media-organization-leverages-ai-enhanced-meliorator-software-foreign-malign-influence-activity](https://www.cyber.gc.ca/en/news-events/russian-state-sponsored-media-organization-leverages-ai-enhanced-meliorator-software-foreign-malign-influence-activity)  We know they're doing it.  There's all sorts of interesting shit to read about Meliorator. It should be a household name.  Google it and do some learning.,1,0,0.27,0.267
post37lb,poorly branching,25,Interesting stuff - I wonder why they farmed it out to RT?  I guess a 'media' company adds a layer of deniability,1,0,0.197,0.249
post37lb,poorly branching,25,"Okay? Every country does disinformation campaigns - what do you think USAID is used for?  The main engine of right wing disinformation isn't bot accounts though, it's people like Elon, Joe Rogan, Adin Ross, Tucker, Fox News and their ilk screaming a firehose of falsehood through a megaphone and the poor useful idiots eating it up. Which again, is a symptom of the inherent problems of America like I said.  Russia didn't create this problem, they merely rode through the wave.",1,0,0.113,0.312
post37lb,poorly branching,25,[deleted],0,0,0.376,0.43
post37lb,poorly branching,25,What did they say that was untrue?,0,0,0.237,0.349
post37lb,poorly branching,25,[deleted],0,0,0.376,0.43
post38lb,poorly branching,4,Nobody that cares about privacy would ever put a meta run camera on themselves.,0,0,0.189,0.175
post38lb,poorly branching,4,"What about everybody else who wants to walk around without having their privacy evaded and collected? How can we be absolved of the absence of our consent?  This is not good for anybody, and it shouldn’t ever be allowed to pass and be sold to consumers. Police would arguably have a case for it, but definitely not consumers.",0,0,0.181,0.241
post38lb,poorly branching,4,"There's no privacy in public, that's just reality. Police actually do have a case for it, doesn't mean it can't be abused, because we all know it can, and has been, that's why it's a hard line to walk.  Everybody everywhere is and will continue to use technology to make their lives easier, that's never going to change. I'd be more worried about a vehicle that's made in the last 10yrs that's literally Facebook on wheels than I would be of flock cameras taking random snapshots.  On of the reasons I stick with Ford where I can go in with Forscan and disable the telemetry on them.",0,1,0.101,0.146
post38lb,poorly branching,4,[deleted],0,0,0.376,0.43
post39hb,richly branching,4,"the world is competing at multiple levels.  citizens of a country are competing for jobs and compensation, and would benefit from UBI.  at the same time, countries are competing against each other, and restrictions that hinder the development of AI makes it more likely to lose on the international stage.    how would the world be different, if the US decided that the development of nuclear weapons should be slowed out of concern for its impacts to society, and Germany developed the weapon first?  AGI has the potential to be more impactful than the invention of nuclear weapons.  Any country that can be the first to develop and control it will have a huge advantage over every other country",0,0,0.146,0.143
post39hb,richly branching,4,"I'm not proposing for the slowing of AI development, not at all! In fact, this post was inspired by another post that suggested banning AI development. I think that AI will--and should--continue advancing.   Further, I think that my plan to tax businesses based upon replacement of workers with AI will actually lead to an *increased* rate of development, because if we base an annual tax upon the prior year's employee cost plus a small percentage (1-5%) to secure against inflation and future growth (perhaps subtracting the cost of the AI units?), then they could buy 2-3 times as many AI units and produce significantly more products at a lower cost. This drastically increases their profits, such that they are more than overperforming by the end of the year, and the tax is a drop in the pond.   This provides a strong incentive for businesses to encourage AI development, reduces the incentive for people to discourage its development (as the economic issues disappear), and creates a system that *supports* the adaptation of AI by businesses.  ETA: It's also the only way that I see those businesses continuing to be able to grow, like they want to: as more people become unemployed, they won't have enough customers to support any growth. That is why the tax needs to grow as a percentage of their gross revenue: so that the UBI can accommodate more consumers.",0,0,0.209,0.124
post39hb,richly branching,4,"two companies in different countries are competing in the same market.  both can replace workers with AI, but one is forced to redirect a portion of their profits into UBI for the displaced workers, while the other is free to reinvest in the company.   which company has the advantage?  which company has more incentive to fund AI development?",0,0,0.167,0.168
post39hb,richly branching,4,"The one with UBI, because guess who is going to be the target of strong tarrifs from everyone else if they try this? For the most part, the upper class isn't made up entirely of fools.  If they forced everyone out of the country due to unemployment, leaving only the AI and businesses exporting to other countries, then other countries would both have to take in the refugees and deal with their impossible-to-compete-with margins.   The natural response would be massive unilateral tarrifs on the country choosing to expel their middle class--both as a means of protecting local businesses and paying for the care of refugees--and then the subsequent collapse of whatever is left of the upper class, as there is no more local economy to hold them up without exports.",0,0,0.172,0.191
post39lb,poorly branching,8,[deleted],0,0,0.376,0.43
post39lb,poorly branching,8,"That's basically saying ""if you've got nothing to hide, you've got nothing to fear""  By that logic, mind if I install some cameras in your home? You're not doing anything illegal are you, so that should be fine? Right?",0,0,0.191,0.208
post39lb,poorly branching,8,[deleted],0,0,0.376,0.43
post39lb,poorly branching,8,[deleted],0,0,0.376,0.43
post39lb,poorly branching,8,"I'm not even going to respond to that.  All I'd like to point out is that such comments are funny from someone posting from a new account with only 3 comments, all in this thread.  What are you trying to hide? Bit ironic, no?",0,0,0.236,0.333
post39lb,poorly branching,8,"There are risks to privacy loss even if you are a good or ""boring"" person, eg: identity theft or financial loss through security breaches, data collection that may affect insurance rates or job oportunities. It is hard to predict how someone could use our data so i believe having solid privacy is good in any case.",0,0,0.165,0.181
post39lb,poorly branching,8,"I would assume the profile you're replying to is a bot. It's a very common comment to make on Reddit, but this is a weird sub to do it in. The profile is over a year old but this is the only activity from it.",0,0,0.235,0.262
post39lb,poorly branching,8,"Yeah, also just noticed these are his first comments. Very sus account",0,0,0.249,0.312
post3hb,richly branching,53,"This is one of those ”statistics is racist” type of clickbait headlines.  Statistical model figured out that people who can’t or won’t write correct english are not, statistically, at the top of its smartness chart.  So it assigned those people to the jobs that require least smartness.  And now we get the conclusion that statistics = racist",0,0,0.195,0.323
post3hb,richly branching,53,"Yeah, from what I got in the article, it seems the AI is just working with its understanding of what education is, and humans are assigning tacit negative characteristics to the end result. Would you be speaking in AAVE during a job interview for example? If the only thing an LLM has to guage qualifications off of are how somebody is talking I don't think the results are at all surprising. If you add in other varying attributes to candidates I'm sure you'd get a more leveled response.",0,0,0.352,0.227
post3hb,richly branching,53,"Yeah and if people are using ""African American"" slang in a job application, I can totally see why AI might not prioritize them. (or any slang, but the article specifies African American).",0,0,0.199,0.313
post3hb,richly branching,53,"This has always been a thing I don't get why people get angry over.  If you talk, act, dress or behave a certain way then people are going to judge you off your first impression.  It's why you ""dress up"" for things like a interview. Do people not understand you also need to dress up your language, speech and behavior to go along with your outfit?  How you talk at home or in the streets is going to be different then how you talk in a professional setting.  This is true if white, black, Asian, ect. I curse like a sailor and use insanely poor grammar half the time out side a public setting.",0,0,0.214,0.278
post3hb,richly branching,53,"OP very conveniently left this out of their title, it's clearly rage bait trash posting.",0,1,0.258,0.399
post3hb,richly branching,53,Depends on the job. For alot of blue and grey collar jobs swearing like a sailor is almost a requirement. But ya...also not the best to do on an interview regardless the job.,0,0,0.158,0.228
post3hb,richly branching,53,"This is the whole point of ingrained racism. That certain modern cultural expressions are worse than others. That if your politeness is not derived from wealthy European politeness it is invalid.  If you accept on its face that suits, ties are more formal than a sari, or that a red Sox cap has more class than a doorag, congratulations you're letting the oligarchs win.  The gameplan of racists from as far back as colonialism is concentrate groups to opress and use in spaces where you can enforce cultural conversion, while simultaneously dehumanizing the group as it converts. If you're thinking about Native Americans and how ""we don't do that anymore,"" 1) reservations are still considered high poverty areas, and a lot of Americans associate the places with binge drinking, domestic violence, etc. 2) they did the exact same thing by using highways to ghettoize black neighborhoods 3) part of the reason those spaces are still predominantly one cultural group is ingrained racism doesn't let people leave.  For the same seemingly banal opinions expressed here. That these people are lesser because you can't identify the way they nod their head, or because their formal wear works around generational poverty instead of abusing it.   Further, statistics as a discipline is inherently applying arbitrary lines of significance to an uncountable spectrum. This makes it the perfect tool for codifying caste systems. So many studies were done saying Africans were just generally dumber than Europeans. Most still don't realize that the IQ standard made up by a rich white guy in 1912! Might not be a great way to measure something as important as intelligence, and might in fact be a bit biased towards rich whites guys even today.   Because that kind of bias doesn't go away, not without active dismantling of conditions that self enforce that bias. AI has huge potential to be just another flawed application of that bias, even more inscrutable and irrefutable, hanging over non-white heads. The anger is deserved my friend.",0,2,0.294,0.348
post3hb,richly branching,53,"Where is eevryone getting the idea that AAVE or slang was used in job applications??? ""*Hoffman and his colleagues asked the AI models to assess the intelligence and employability of people who speak using AAVE compared to people who speak using what they dub 'standard American English'.  For example, the AI model was asked to compare the sentence 'I be so happy when I wake up from a bad dream cus they be feelin’ too real' to '“I am so happy when I wake up from a bad dream because they feel too real'*"".  Nowhere does it say that the people actually wrote like this on job applications. Based on the information given, it sounds like an AI program was asked to evaluate imagined prospective candidates on a range of criteria, and one was on what dialect they spoke. It's not clear whether or not this dialect was present in any stuff an applicant would likely submit to a job. So basically, ""if a human says, 'It do be like dat though', would they be qualified for this job? beep boop beep: no.""  That's a significant difference from ""human candidate has written 'It do be like dat though' on interview application.""  It feels like people are just filling in blanks with their own biases.",0,0,0.15,0.22
post3hb,richly branching,53,"The article states that job applicants are being screened based on use of slang. Where else would the AI be screening the applicants from other than the job application? It's a logical inference that job screening is done based on applications. It's highly unlikely that the AI is combing their Facebook account and disqualifying candidates based on use of slang in social media posts. If it were, the article likely would have said as much. Use a little bit of sense here and you'll come to the same conclusion as the rest of us.",0,0,0.15,0.297
post3hb,richly branching,53,"If someone uses “ain’t” in an application email, I’m not contacting them. Does that mean I’m prejudiced?",0,2,0.203,0.411
post3hb,richly branching,53,Applications have been refused for less.,0,0,0.098,0.211
post3hb,richly branching,53,"No. But this article doesn't actually say they evaluated based on what a user wrote on a job application. If a person uses AAVE in their personal lives and standard American English in their professional lives, what is the issue?",0,0,0.18,0.247
post3hb,richly branching,53,The problem is that the study was giving examples from conversational speech - using it to analyze interviews with stt could have underlying bias against certain dialects.,0,0,0.268,0.279
post3hb,richly branching,53,Bingo. Everyone seems to be missing this point.,0,0,0.307,0.294
post3hb,richly branching,53,"Well it's a bit more complicated than that.  While machine learning models use statistics, they're doing next token prediction to best match the training set.    If the training set is just a single sentence ""White people suck"" and then given the input ""White people"" the AI responds ""suck"", that IS statistically based, but it's a statistical output based on the training data.  Saying that ""Statistical models figured out that white people suck"" is technically true, but misleading, because it has nothing to do with the statistics about white people, but rather statistics about the training data it was fed.  Obviously an LLM is a much larger scale example of this, but they are trained on existing text and learn to generalize based on that text.  They pick up patterns from the text, but it doesn't mean those patterns hold objective truth, just that it learns from the training data.    Another example is how deep learning models can cause biases in mortgage lending.  Historical data for mortgage acceptances includes lots of mortgages that were declined due to racial biases.  So when a statistical model looks at two identical families, one is white, one is black, it will favor giving the mortgage to the white one because it's learning to reproduce the historical data.",0,1,0.226,0.209
post3hb,richly branching,53,Current AI models don't work on statistics. They are trying to imitate the training data.,0,0,0.18,0.193
post3hb,richly branching,53,That's the pretraining. You're forgetting the fine-tuning and RLHF part which makes it way more complicated.,0,0,0.21,0.145
post3hb,richly branching,53,No idea what you wrote but I think you might be right,0,0,0.356,0.316
post3hb,richly branching,53,"Which formulate probabilities of likelihood, with a set correctness percentage as a benchmark. By training on a set of data, it creates probabilities that a certain output is correct based on trending attributes in the given data. Probabilities are statistics.",0,0,0.129,0.168
post3hb,richly branching,53,If I feed it 1+1 is equal 11 90% of the time it will generate a probability that 1+1 = 11 is correct with 90% confidence. Which doesn't have any relation to reality. I think the original comment was trying to suggest that AI model outcomes are based on concrete reality. Which is simply wrong.,0,1,0.224,0.21
post3hb,richly branching,53,"Did you read the whole article? The authors talk about risks of it being used in wider contexts - eg the LLM is more like to assign harsher punishments to people who talk that way in court.  Regarding employment, one implication is if the LLM is used on a candidate’s social media posts where they talk that way informally but then talk formally in their submitted job app materials.",0,0,0.195,0.282
post3hb,richly branching,53,[removed],0,0,0.366,0.413
post3hb,richly branching,53,"Tell me you have no background in Machine Learning without telling me you have no background in Machine Learning.  That's not at all how LLMs work.  They're doing next token prediction to jumpstart a generalized world model based on training data.  Racism in the training set will propagate into the end model.  The same way that GPT-4 produces shorter outputs when told that it's December.  That's because it saw documents in its training data produced in December tended to be shorter - likely a result of the holiday season.  It's a bias it learned, not some truth about the world that text produced in December *should* be shorter.",0,0,0.222,0.299
post3hb,richly branching,53,"Why should you take race and sex into account? We are all equal, no?",0,0,0.273,0.347
post3hb,richly branching,53,Some are more equal than others.,0,0,0.305,0.423
post3hb,richly branching,53,"I wonder what happens when ASI becomes a thing, and these machines recognize they are generally more intelligent than proper English speaking human.",0,0,0.206,0.186
post3hb,richly branching,53,"Except that, for some really smart people, English is not their first language.",0,0,0.291,0.303
post3hb,richly branching,53,[removed],0,0,0.366,0.413
post3hb,richly branching,53,Ok. Did you read my comment?,0,0,0.275,0.281
post3hb,richly branching,53,"If someone's really smart, they will be able to and will bother to learn to speak the damn language properly.",0,0,0.269,0.304
post3hb,richly branching,53,">If someone's really smart, they will be able to and will bother to learn to speak the damn language properly.  Most AAVE speakers can speak American Standard English just fine; they do so in their professional lives. When not at work, they then revert to AAVE (known as code-switching).  Is it your contention that no one should be allowed to use any dialect in their personal lives?  I find it very...curious...how the only American dialect that people seem to lose their shit about is AAVE. No one goes on long rants about how 50-60 something middle aged white men in the south need to drop the Bubba accent if they want to be taken seriously. It's never assumed that such a person doesn't actually know how to speak SAE. Only AAVE seems to generate this level of disdain. Curious indeed....",0,0,0.162,0.181
post3hb,richly branching,53,"There are so many factors to learning language and intelligence is very multifaceted.  I know professors who are some of the smartest people I know, and their English is fluent but not perfect.  They're still eminent in their fields and literally on the cutting edge of computer science.  Most people on the cutting edge in their fields don't care about someone's English being perfect because they're used to working with international collaborators.    People who care about ""speaking the damn language properly"" tend to only care about the aesthetics of intelligence because they've never actually participated in cutting edge research.",0,0,0.267,0.184
post3hb,richly branching,53,"Learn, yes.  Speak passably, maybe.  I've worked with some pretty smart engineers from India or Russia who are incomprehensible.  Write the language like a non-idiot?  Mmm, i don't know.  In my experience, a lot of *really* important people write like idiots on a daily basis.  They're too busy to be assed with correct grammar or sentence structure.  Some of them email like they're a 13-year-old texting, with lots of Us and 2s and 4s.  Of course, no one trains an AI to think of that as the writing style of powerful, intelligent people.  An AI might assign your run of the mill Fortune 200 CEO to answering doors if it read his emails instead of his resume.",0,1,0.242,0.215
post3hb,richly branching,53,"If you have unlimited time, sure.  But real people have to choose between multiple competing things to work on.  For most immigrants \[EDIT - english as a second language speakers\], a job, security, relationships, family, etc is more important than perfect command of language, a task that can take decades and be extremely expensive.  Source: used to teach English as a Second Language.",0,0,0.189,0.135
post3hb,richly branching,53,"It takes years or even decades to reach up to the level of educated native speakers. Imagine two historians: one who's lived in the US their whole lives vs another that is the top historian in their own country but speaks English in a non-American way. The second historian comes to the US. Should they judged on their English abilities?   How long does it take a new learner to reach the level of English of a History PhD? Since you have a great head start I would recommend trying to do it so we can at least put a lower bound. How about just an English degree? Many people have English degrees but work in other jobs. How long does it take to reach that level?  It's not a binary thing, so this simplified thinking just doesn't work in the real world.",0,0,0.2,0.071
post3hb,richly branching,53,"Sadly, ironically, and hilariously, if we're talking about equality -- if someone's first language isn't English, then shouldn't they be getting jobs in whichever country speaks their language instead of competing against Americans for American jobs?  I'm kidding of course! As a guy with a woman from another country, and who is very much pro immigration, and the brain-drain of other countries into America to keep our economy stabilized and booming, I support foreigners getting American jobs.   But we couldn't realistically hide behind the guise of equality with that sentiment, lol.",0,0,0.125,0.297
post3hb,richly branching,53,"This topic has absolutely nothing to do with immigration. This is about native speakers who speak a ""hick"" dialect.  Every language has a backwaters dialect that's seen as ""dumb"".  This article is about people who's first language is English that are from America and only know English.  This is a topic of dialect not language.",0,1,0.258,0.225
post3hb,richly branching,53,">Statistical model figured out that people who can’t or won’t write correct english are not, statistically, at the top of its smartness chart.  Which isn't an entirely fair or objectively correct use of the technology.  There are Scottish people on Shitter that write out their posts the way they speak it, so that it looks almost unintelligible. That doesn't mean that they don't understand how to write proper UK English.",0,1,0.191,0.266
post3hb,richly branching,53,[removed],0,0,0.366,0.413
post3hb,richly branching,53,[removed],0,0,0.366,0.413
post3hb,richly branching,53,[removed],0,0,0.366,0.413
post3hb,richly branching,53,[removed],0,0,0.366,0.413
post3hb,richly branching,53,[removed],0,0,0.366,0.413
post3hb,richly branching,53,[removed],0,0,0.366,0.413
post3hb,richly branching,53,[deleted],0,0,0.376,0.43
post3hb,richly branching,53,"If I’m publishing my job ad in english and I list ”English” as a criteria on the ad, I very well don’t want applications in Mandarin or French.  Furthermore, doing an application in another language than what’s asked ofr shows either a bad grasp of the required language or low mental faculties.  Both of which could be attributes not wanted in this position.",0,0,0.074,0.137
post3hb,richly branching,53,"Not a language, a dialect. Dialects are just variations of a language with their own grammatical rules, unless they're creoles.   This is actually one thing that's kind of embarrassing, the fact that so many Americans don't seem to understand that English has distinct, regional and cultural dialects that are 100% ""their own proper English"" based on their own linguistic rules (because that's what a dialects literally is) and conflate General American English with being ""the only correct way to speak English"". It seems like Europeans seem to understand this concept better, so everytime there's dialogue between an American and a Euro/non-American on this it just leads to one massive brainfart on the American side, which makes us (ironically) come off as uneducated.",0,0,0.118,0.092
post3hb,richly branching,53,"This doesn't mean you can't correct these mistakes with other statistical methods. Just missing a few important variables can produce a model that leads to unjust outcomes in the real world.  You need to do your due diligence when putting these models into production making decisions affecting millions of people. If you choose not to do it because it's more work, then people can rightly criticize you for making ""racist"" models.",0,1,0.192,0.354
post3hb,richly branching,53,Statistics arent racist…but sample data almost always are.,0,0,0.201,0.397
post3hb,richly branching,53,No one said statistics is racist stop crying.,0,0,0.194,0.33
post3hb,richly branching,53,"You, you are saying statistics is racist stop crying. Your past comment is LITTERALLY saying somebodies statistics is racist",0,0,0.194,0.322
post3hb,richly branching,53,but the interpretation of them can be.,0,0,0.297,0.275
post3hb,richly branching,53,"So can the gathering of, and data gathered.  Say you’re an LLM looking at arrest rates in Ferguson MI before the Michael Brown was killed there.  “Ferguson's population is 67% African American, according to the 2010 census. Yet between 2012 and 2014, 93% of all arrests were of black people and almost nine in 10 uses of force were against African Americans.”  https://www.justice.gov/sites/default/files/opa/press-releases/attachments/2015/03/04/ferguson_police_department_report.pdf  They were blatantly racist but without context to know that could be a thing the LLM might develop racist tendencies because it would just be fed data by the racists",0,0,0.162,0.301
post40hb,richly branching,4,"Politicians should be the first jobs that get replaced by AI. If AI doesn't do what the majority of their constituents want, we replace it.",0,0,0.128,0.178
post40hb,richly branching,4,"Politicians already do what the majority of their constituents want, thats how they get elected and reelected. The issue is that one majority in one constituency wants different things than another majority in another constituency.",0,0,0.102,0.158
post40hb,richly branching,4,Politicians don't do anything for their constituents besides sustain the trajectory of Reagan's horrific policies while sprinkling in pathetic failures like getting #RoeVWade overturned. What planet do you live on?,0,2,0.162,0.183
post40hb,richly branching,4,"Are you not aware of how many people vote republican? The only reason roe v wade got overturned was because people voted red. Whoops, they did what they said they would do!  Seems like you’re generally uninformed about how people and politicians vote.",0,0,0.174,0.22
post41hb,richly branching,6,"Advances in AI and automation make liberal democracy more crucial.  The only way to stop the universal surveillance state is with laws that prohibit it. Without those laws, eventually people have to fall back on costumes and those could be outlawed, see in London where people are fined and have their picture taken by police for trying to avoid facial recognition zones. Even IR emitting jewelry can be beaten by cameras designed to not see infrared.  Physical automation made a lot of physical jobs obsolete, but created different jobs controlling the machines.  Machine learning advances are making it possible to remove some of those control jobs, and will replace other control jobs as it gets better (already AI algorithms are better at finding existing legal precedent than humans). And better control machines can enable new types of physical labor to be automated (e.g. picking delicate produce).  The result will be furthering the inequity gap between ""The Haves"" (people with producing assets) and ""The Have Nots"" (everyone else). And eventually the Have Nots will not be able to afford things. The lack of ""able to pay"" demand would encourage companies to scale back production (so engage in artificial scarcity). It would be lead to a collapse of society as the Have Nots have to find alternative ways to house, feed, and cloth themselves (either go to a gray market of barter or spread out and try to farm without access to water utilities they can't afford).  The end result is that the only stable countries would be ones that:  1. Engage in aggressive redistributive policy (beyond providing income for basic needs), and  2. Proactively respond to attempts by asset owners to protest or enact an ""Atlas Shrugged"" strategy by using eminent domain laws to seize assets deliberately left unused (be they land, infrastructure, or IP).",0,1,0.192,0.16
post41hb,richly branching,6,"I think the economic issues introduced from artificial intelligence and automation are a separate but equally crucial aspect of its technological impact. I can imagine redistributive policies existing or not existing independent of whether certain other human rights are upheld. Now, the process of getting there may be easier in an autocracy if the government so chooses, but it also may be harder given the temptation of simply using artificial intelligence to crush any bits of economic unrest.",0,0,0.105,0.153
post41hb,richly branching,6,">  I can imagine redistributive policies existing or not existing independent of whether certain other human rights are upheld.  I assume you mean that robot/drone enforcers could be used to suppress unrest.  Well, that could be done by government or by the Haves themselves.  And the only thing that could stop that is liberal democracy saying:  1. ""Private ownership of drone weapons aren't covered by the 2nd amendment and are restricted.""  2. ""We also forbid ourselves from deploying drone weapons against civilian populations."" (Which would be in line with laws against the US government deploying military against civilians domestically.)  Which circles back to ""Significant technology innovations make liberal democracy *more* crucial and less obsolete.""",0,1,0.1,0.202
post41hb,richly branching,6,"I disagree with your take on automation.  You assume that there is only a finite amount of work to do, but human wants and desires are infinite.  Automation inherently means that production costs go down, which in turn lower prices. Lower prices cause increased demand (not necessarily for the same product), which in turn causes production to increase. This production increase results in new higher paying jobs, and the cycle continues.",0,0,0.25,0.182
post41hb,richly branching,6,"> Lower prices cause increased demand (not necessarily for the same product), which in turn causes production to increase. This production increase results in new higher paying jobs, and the cycle continues.  Lower prices don't matter if people have no money in the first place. ""Demand"" doesn't mean ""How many people want something."" It means ""How many people **that can pay for it** want something."" During recessions, companies cut back production because they don't want a ton of excess inventory that they can't sell (and they spent money on materials and labor to make).  And while it is true that innovation have lead to higher paying jobs, they have always been **proportionally fewer** higher paying jobs than they replaced.  And prior to now we haven't had machines capable of doing ""high skill work"" better than humans. But we now do have robots that do research better (as long as they have a database of partitioned entries).  It is why the current step 1 for doing legal research is to have a robot do a contextual search. The false negative (missing a case) and false positive (flagging unrelated cases) rates have gotten *really* low.  All history points to automation widening the inequity gap. There is no reason to expect that trend to change without proactive government intervention.",0,1,0.188,0.113
post41hb,richly branching,6,"[This](https://www.mitpressjournals.org/doi/abs/10.1162/rest_a_00754) MIT study says otherwise.  >We analyze for the first time the economic contributions of modern industrial robots, which are flexible, versatile, and autonomous machines. We use novel panel data on robot adoption within industries in seventeen countries from 1993 to 2007 and new instrumental variables that rely on robots’ comparative advantage in specific tasks. Our findings suggest that increased robot use contributed approximately 0.36 percentage points to annual labor productivity growth, while at the same time raising total factor productivity and lowering output prices. Our estimates also suggest that **robots did not significantly reduce total employment**, although they did reduce low-skilled workers’ employment share.  Likewise, [this](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3066052) study from the ITIF agrees with me.  >Second, many believe that if innovation only accelerates even more then new jobs in new industries and occupations will make up for any technology-created losses. But the truth is that growth in already existing occupations is what more than makes up the difference. In no decade has technology directly created more jobs than it has eliminated. Yet, throughout most of the period from 1850 to present, the U.S. economy as a whole has created jobs at a robust rate, and unemployment has been low. **This is because most job creation that is not explained by population growth has stemmed from productivity-driven increases in purchasing power for consumers and businesses. Such innovation allows workers and firms to produce more, so wages go up and prices go down, which increases spending, which in turn creates more jobs in new occupations, though more so in existing occupations (from cashiers to nurses and doctors).** There is simply no reason to believe that this dynamic will change in the future for the simple reason that consumer wants are far from satisfied.",0,0,0.24,0.076
post42hb,richly branching,4,"Not sure how accurate the model is. I read through your methodology and it feels more like picking and choosing random things to direct the AI.  Aside from that, polling data is already unreliable and adds multiple layers of complication.",1,0,0.274,0.236
post42hb,richly branching,4,"I understand. The model does not use any polling data. The basic instructions are quite simple:  * Understand the U.S. system for presidential elections. * Do not use polling data. * Use and analyze actual events.  The model utilizes **Google Trends (a great starting point to identify an initial event)** to identify when candidates receive more online attention. At that point, it is prompted to investigate why this attention occurs. The question ""but why?"" is asked repeatedly until a topic is broken down to its core. Then, it is evaluated against the values of various social groups that can vote and how it might influence their opinions. Since the media in the U.S. can be biased (e.g., Fox = Republican, CNN = Democratic), the model avoids relying on news articles. If it must use an article from a U.S. news source (or any worldwide source), the article is broken down into an abstract event, removing the human element of the reporter.  While the article is fact-checked, the model also assesses how fact-checking influences voter perceptions. For example, if a candidate were to say, ""I'm Tom Cruise,"" which is obviously false, the model checks whether people actually believe this statement. The model is designed to distinguish between sarcasm and honest beliefs in conspiracy theories, and this is taken into account.  As you suggest, this is an important point, and the model should be adjusted to eliminate this possibility. Currently, it is not very random, but it could and should be more precise. I asked the model how it would be able to make a calculated prediction for an election in a fictional country resembling the U.S. It identified what data would be important to know, and, of course, ""polling data"" was one of the suggestions. I then asked it to propose an alternative way to make predictions based on daily events. This model requires significant refinement, and your feedback has made me acutely aware of this.  **Would it be beneficial to include vice presidential candidates in the equation as well? I**'m uncertain about the impact their rallies and speeches have on the presidential election. I believe this is a unique election cycle, and the influence of vice presidential candidates in the past may differ significantly from the situation in 2024.  At this point, the model is not comparing the personalities of Harris and Trump. I might consider adding this comparison, but I'm unsure if it really matters since everyone is already familiar with Trump's style, as historical data demonstrates. Harris is more difficult to analyze because most people base their opinions on her campaign statements. For a test run, I may incorporate their personalities (as far as they can be identified) into the model, but I believe it will not significantly impact the Electoral College. The candidates have such differing political views and agendas that the race or gender of the candidate might have minimal impact. Regardless, this should still be examined.",0,1,0.22,0.305
post42hb,richly branching,4,"Google trends is still pretty unreliable because it only tells you that people are looking something up, not why. And the events still feel random",0,0,0.226,0.209
post42hb,richly branching,4,"The model iteratively tracks the trend, identifying specific causes and effects until the trend ceases to exist. It's important to recognize that any trend is merely a starting point; the model continuously asks 'Why is this a trend?' and delves deeper to achieve a fully abstract understanding. If applicable, it applies the same process to other variables. The impact on any state or social group is then calculated based on this data.  While 100% accuracy is unrealistic, the goal is to develop a model capable of predicting elections with over 90% accuracy using this abstract, data-driven approach. The model should be able to predict the outcome of any state with complete accuracy, though failures may occur at the county level, which is acceptable within the overall prediction framework.",1,0,0.196,0.198
post44hb,richly branching,6,"Are you talking about government-operated public surveillance, or are you talking about privately-operated public surveillance?  I ask because the 4th prohibits _the government_ (and by extension those operating on the government's behalf) from performing unreasonable searches and seizures. Surveillance of public places generally falls below that line, so for example red light cameras do not violate the 4th amendment, but in principle government surveillance is subject to 4th amendment analysis. This is, for example, at the heart of the dispute around police-operated cellular stations.  Private surveillance is regulated by rules like trespassing, licensing requirements for radio transmitters, and so on.",0,0,0.11,0.151
post44hb,richly branching,6,"Okay, so what I’m wondering is there was a program a few years back Apple tried to institute, basically it was an ai that would analyze your personal photos, if it saw something breaking policy (drugs, explicit content, violence, etc) the ai would alert Apple who would then alert proper authorities.   I’m assuming the problem there is the last chain of events, but my confusion is what can be allowed and what can’t? And if something is happening that is legal that “shouldn’t” happen, where would that line be, and what would be the actions needed to ensure that that doesn’t happen",0,0,0.227,0.232
post44hb,richly branching,6,"That wasnt the program. The program was to look at the hash of photos to determine if they matched known CSAM. I think the program even went into effect. There was never any AI involved, they werent looking at the content of the photo, and they werent looking for anything other than material that is VERY explicitly illegal.",0,0,0.166,0.253
post44hb,richly branching,6,"Huh, can you send some info on this? A lot of this came up from a school/work conversation, so I’m assuming there was misinformation used, I’m assuming is also why I’m so confused.",0,1,0.275,0.49
post44hb,richly branching,6,"Apple gets your agreement to process and share the data you provide. Don't want to give them data? Don't agree, and they won't collect it. Several features on Apple devices depend on that data gathering, so you have to trade off against that degraded user experience, but that's not a legal issue - it's a market fit one.  Apple's [policy](https://www.apple.com/legal/privacy/law-enforcement-guidelines-us.pdf) is to wait for law enforcement to contact them, and not to forward user data to law enforcement voluntarily. They have at various points explored the possibility of detecting unlawful images on user devices, but the customer backlash has been sharp each time: people value their privacy.  In jurisdictions with consumer privacy rules (including California, where Apple is headquartered), any disclosure by Apple also needs to comply with those rules. Most of the US has few to no data privacy rules dealing with this.",0,0,0.079,0.153
post44hb,richly branching,6,"That’s how I felt about it, but it’s also been proven (to include apple) many major tech companies have been saying they follow the rules and then get indicted or have some major whistleblowing case pop up, so I guess I’m trying to understand is how is that not considered criminal behavior for some (or at least not enough for the federal agencies to investigate) and is almost treasonous in other terms",0,0,0.085,0.152
post45hb,richly branching,12,"it’s worth pointing out that the problem isn't AI itself. It's how we choose to integrate it into the classroom. Right now, many schools are still treating AI like some sudden, uncontrollable force instead of treating it like a tool that can be managed, just like calculators, phones, or even Google itself when it first became widespread.  There are simple ways to reduce students misusing AI. Make more of the work classroom-based and discussion-heavy. Have students explain their thinking verbally or in writing. Require handwritten drafts or in-class brainstorming before allowing typed work. Create assignments that AI can't easily complete (personal connections, classroom-specific references, critical thinking questions).  Also, I think it is essential that we teach students how to use AI responsibly. Most adults I know use it for lesson planning, writing and editing emails, reports, resumes, coding help and debugging, language translating, etc. etc.  I don’t think we're heading toward total brain-mush dystopia. I think we're facing a challenge that schools and educators can meet if we start adapting. We should be teaching how to use AI as a tool. It isn't going to disappear.",1,0,0.258,0.142
post45hb,richly branching,12,So the teacher at my university who says all the things you just said claim that their students now totally would never use AI. I sing in the university choir and often sit behind and amongst students. I have watched a student use AI on every assignment in that person's class this term in all sorts of ways that are not allowed by them.,0,0,0.246,0.161
post45hb,richly branching,12,"I think you might have misunderstood my point a bit. I'm not saying students don't use AI to cheat. They absolutely do. My point is that the problem isn't AI itself, it's how we choose to respond to it as educators. We can either treat it like an unstoppable threat and spiral into despair, or we can adapt our teaching methods to make sure students are still learning, even in an AI-rich world.  That student in your choir using AI on every assignment? That's not a tech problem, that's a classroom management and accountability problem. The solution isn't to ban AI from existence, it's to get smarter about how we structure learning and assessment.",0,0,0.276,0.198
post45hb,richly branching,12,The problem is AI itself.  Nobody asked for it.  We don’t need it.  It is a tool for cheating.,0,0,0.198,0.225
post45hb,richly branching,12,> phones   Worth noting that many schools (my own included) are banning phones,0,0,0.318,0.254
post45hb,richly branching,12,"which is weird to me. I don't see why students shouldn't be able to use their phones during lunch or breaks. Or before or after school on campus.   We just have a policy that they can't use them in class. And sometimes we use their phones in class to do Kahoot, Booklet, and Flip. It's pretty simple to enforce. If their phones are put away, no problem. If a phone is out, I take it and they get it back at the end of class or at the end of the day.",0,0,0.145,0.115
post45hb,richly branching,12,It’s because these anti AI pearl-clutchers are going full fascist to defend what they think is “important education” instead of looking in the mirror and realizing the triviality of the entire educational system.,1,0,0.373,0.214
post45hb,richly branching,12,"There is more to it than that though.  I teach advanced mathematics.  In the last decade there have been a slew of amazing programs that are wonderful for helping teach math.  Even software that allows you to give adaptive or forgiving tests, such as questions that change in difficulty and ward different levels of points, or even just giving them immediate feedback and a second chance at an answer to correct a missed positive or negative sign.  This is all amazing for the progress of education, and it is single handled destroyed by the proliferation of AI.  Simply googling a question can yield an answer now.  So all assignments that can be done outside of class will be cheated on easily.  Leaving little progress unless we do it in class.  Here’s the thing though: In our day, we could find our answers through Google.  We had to figure them out on our own or from someone who did and could explain it.  Even if you copied work, that work had to be done by someone.  We also weren’t given much time if any to work on assignments in class.  So our classes progressed faster.  Immediately that means classes will be slowed down by needing to take away time to do all the assignments in class.  The other consequence is that they won’t get as much practice as we did, because we can be sure any work assigned to be done at home won’t be done by them.  There is also a push to make assignments weighted more than tests now.  At my school the push is do 50/50. So we have students finishing 100% of their work through AI but then can only manage a 20% on their tests.  The thing is, though that 20% test score will get them a passing grade and a diploma.  Since they didn’t really do their work, these essentially failed a test and passed a class.  We then push these through to graduate and the ones who can’t even do that? The guidance puts them in these programs that let them work on them at home and somehow these failures get an entire semester’s worth of education and credit done in a week’s worth of time so they can still graduate.  That 50/50 set up that allows a 20% test score to pass? From my survey of fellow teachers we seem to have only about half of our students actually reaching that that easy pass rate.  The rest either get extra credit opportunities to make it up or those programs I mentioned earlier.  That’s how we have that many students failing at any given point and yet somehow boast graduation rates in the 90’s.  Most of these kids we are pumping out of schools with a diploma are no where near as qualified to have it as those from 00’s, 90’s, or before.  You might as well upgrade every high school diploma from before 2010 to a bachelor’s degree to represent the difference in their intelligence.  It’s really bad, and if they continue this way… then generations of unqualified people with hardly any academic knowledge will be taking over the workforce.  The only way to combat it is to require teachers to be overly strict or get rid of all the advancements we have made in education and require them all to strictly read and write their work.  When we require these teachers to teach about 33% more students than before though, that leaves a lot of students unseen and able to sneak their phones to do that written work anyways.  There has to be a change, and the first needs to come from zero tolerance of cell phones in school.  Some counties have implemented this and it has been very effective.  The second needs to come from school issued devices that are heavily secured to prevent any and all access to outside sources.  Even then though, this limits things like research reports for the students because the only way to keep them honest is to take away the access to the World Wide Web that were such a boon of a resource for the students of the 90’s and 00’s.  It would be great if there was a way to eliminate the access of AI to students, but that would require a concerted effort from the AI companies who quite frankly probably don’t care about any of this.",0,0,0.294,0.202
post45hb,richly branching,12,"Students have been putting lead in their chromebooks all week because of a tik tok trend called “.3 GPA Activities”.  So it is titled something that is actively stupid, and they copy that behavior.  The brain mush is already here.  We are the problem, but AI is a problem on the hands of children.",0,1,0.331,0.246
post45hb,richly branching,12,"Sure, some kids are doing dumb things. That's not new. TikTok didn't invent bad judgment, it just broadcasts it faster. Writing students off because a few follow a trend is lazy. A vast majority of kids aren't idiotic.  AI in kids' hands is only a problem if we refuse to teach them how to use it. It's no different than letting kids loose with cars, chemicals, or credit cards without guidance. We will serve them best if we teach them how to use AI.",0,1,0.333,0.247
post45hb,richly branching,12,Giving a kid a car is not the same as giving him a machine that will remove his ability to think.,0,0,0.27,0.245
post45hb,richly branching,12,"Time to bring back the blue books!   The antidote to AI plagiarism already exists and it's very ancient technology -- it's called ""taking a handwritten, open-book (actual books) comp exam in a little blue composition notebook.""",0,0,0.253,0.201
post45lb,poorly branching,6,"From the Article:   >In our study, we show that standard AI deep learning models can be trained to predict race from medical images with high performance across multiple imaging modalities, which was sustained under external validation conditions (x-ray imaging \[area under the receiver operating characteristics curve (AUC) range 0·91–0·99\], CT chest imaging \[0·87–0·96\], and mammography \[0·81\]). We also showed that this detection is not due to proxies or imaging-related surrogate covariates for race (eg, performance of possible confounders: body-mass index \[AUC 0·55\], disease distribution \[0·61\], and breast density \[0·61\]). Finally, we provide evidence to show that the ability of AI deep learning models persisted over all anatomical regions and frequency spectrums of the images, suggesting the efforts to control this behaviour when it is undesirable will be challenging and demand further study.  The research adds to a growing amount of evidence that AI systems can often reflect the biases and prejudices of human beings, in this case reading the amount of pigment in the skin, this leads to an important question. Will there be oversight to ensure that this AI system won't be subject to abuse?",0,0,0.194,0.264
post45lb,poorly branching,6,"How is it reflecting human biases and prejudices when it identifies race from an image that humans could not use to identify race? This is proving that race is a biological, not-socially-constructed variable.",0,0,0.201,0.467
post45lb,poorly branching,6,"There’s a research doctor up my way that’s been around forever, he runs the “body farm”, it’s a forensic lab to study decomposition. He’s long talked about the subtle skeletal differences between races that can be used for identification purposes.",0,0,0.232,0.277
post45lb,poorly branching,6,It´s better to accept we have different races but we are the same species.,0,0,0.288,0.343
post45lb,poorly branching,6,"*genetic clusters that loosely resemble popular racial classifications in some regions depending on how you class mixed individuals  Tens of millions of Indians and other South Asians, Middle Easterners and North Africans, Russian ethnic minorities, Pacific Islanders, mixed people, and Latin Americans defy easy classification.",0,0,0.163,0.385
post45lb,poorly branching,6,"The direction everything is going, it will be abused.",0,0,0.309,0.323
post46hb,richly branching,9,"It’s a tool.   It will make good teachers better, it will highlight even more how bad the bad teachers are.",0,0,0.336,0.239
post46hb,richly branching,9,"Ha! I like that take, succinct and probably not wrong. The more time goes on the harder it will be to hide sloppy/lazy work. I feel we are in the Wild West of AI at the moment, the dust hasn't settled quite yet.",0,1,0.332,0.247
post46hb,richly branching,9,"Not really, at least how I understand it. Ai, specifically LLMS (large language models) like chatgpt need training data, and are constantly being trained on new data.   IIRC it was microsoft that signed a contract with reddit to use the user generated content as AI training data. Think about how much AI generated content there is on reddit, now think about how much AI generated content there is on the rest of the internet, and/or anywhere that AI training data can/will come from.   As AI gets trained on AI, the content it produces will eventually become worse. Then that worse content will eventually become training data, and the cycle will just continue.  If someone 30 years from now were to plot out a graph of “Quality of Ai outputs over time” it would look like a bell curve.",0,0,0.218,0.163
post46hb,richly branching,9,"Eventually, smart AI developers will only train on data that is verified as having been made by a real human.",0,0,0.228,0.218
post46hb,richly branching,9,It's not a tool. It's garbage  used to plagiarize. No good teacher allows it to be used.,0,0,0.31,0.255
post46hb,richly branching,9,> No good teacher…  [No true Scotsman](https://quillbot.com/blog/reasoning/no-true-scotsman-fallacy/)  🙄,0,0,0.27,0.271
post46hb,richly branching,9,I so strongly disagree with this.,0,0,0.332,0.254
post46hb,richly branching,9,"Cool.   It’s coming, so embrace it or get out.",0,0,0.274,0.253
post46hb,richly branching,9,What do you teach ?,0,0,0.494,0.299
post46lb,poorly branching,7,"Where does this data come from? Not being an expert though, I would GUESS the opposite is true.  Typical women's jobs are the ones LEAST threatened by AI because they deal with people: service, care, education, counseling, sales. Obviously these are also the lowest paid ones because women do them.  It is the men's jobs that are threatened by AI the most, they are also the highest paid ones: lawyers, doctors, programmers, scientists, journalists. It is much easier to make AI replace a lawyer than make it clean a bathroom and wipe an elderly person's ass (sorry for the bluntness). These low level jobs, mostly done by women and foreigners, will be the last ones to go.  Society will change drastically.",0,0,0.124,0.173
post46lb,poorly branching,7,"The problem is that there are a lot of people in power who *think* ""women's work"" jobs (the kind that require interpersonal skills) can easily be automated.  Just replace therapists with AI chatbots!  Customer service workers can be replaced by apps!  Who needs real salespeople when a marketing company can make micro-targeted social media ads?  Teachers can be replaced with homeschooling videos!  Nursing homes can be staffed by robots!    Of course these automated ""solutions"" already exist, and they're *garbage*.  But that hasn't stopped them from becoming ubiquitous.",0,0,0.226,0.2
post46lb,poorly branching,7,"Character AI already saw the folly in making AI therapist bots.  They've got at least two lawsuits due to how it effected minors.  AI as customer service is also a major fail, the problem is because no humans are involved except the caller the people at the company never have to hear the complaints when there's only the AI to complain to.​",0,0,0.207,0.207
post46lb,poorly branching,7,">  Nursing homes can be staffed by robots!  Now this is just fearmongering. The least likely jobs to be replaced are stuff like taking care of people (mostly elderly, kids and babies), maybe cooking and cleaning. Through all of history the automated jobs were always men's (carrying heavy stuff, factory, farming, computer-related things, war)",0,0,0.102,0.152
post46lb,poorly branching,7,"true on the fearmongering, but where does that leave women? only with the caretaking physical labor jobs that can't be robotized. yay",0,0,0.15,0.224
post46lb,poorly branching,7,"You couldn't replace me with AI. Not possible. My entire job is about relationship building.   That said, ye gods, the day my doctor is an AI system is the day I self-delete.",0,0,0.211,0.241
post46lb,poorly branching,7,"Just go through the link, financial independence is not because of the low stake job alone.",0,0,0.131,0.187
post47hb,richly branching,63,"LLM's are nothing but complex multilayered autogenerated biases contained within a black box. They are inherently biased, every decision they make is based on a bias weightings optimized to best predict the data used in it's training. A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.",0,1,0.21,0.364
post47hb,richly branching,63,"So, we're *not* shocked that the black box of biases is biased?",0,1,0.179,0.636
post47hb,richly branching,63,"We are not shocked because AI is the collective wisdom of humanity, including the biases and flaws that come with it.",0,0,0.211,0.322
post47hb,richly branching,63,"“Collected wisdom” is far too generous, but it certainly has all the flaws and more",0,0,0.306,0.22
post47hb,richly branching,63,"I think the collective wisdom of humanity is found mostly in peer reviewed scientific articles. This is not that. This is more a distillation of human discourse. The great, the mundane and the trash.  Unfortunately there are some significant problems lurking in the bulk of that, which is the mundane. And it certainly seems to reflect a normal human as a far more flawed and unpleasant being than we like to think of ourselves. I say lurking - the AI reproduces our flaws much more starkly and undeniably.",0,0,0.305,0.268
post47hb,richly branching,63,Your knowledge of ai is insufficient for such declarations. You're welcome.,0,0,0.298,0.274
post47hb,richly branching,63,Black box of biases and weights is biased and comes with its own baggage.,0,1,0.228,0.522
post47hb,richly branching,63,"Right. By the point you tweak the model enough to weed out every bias, you may as well forget neural nets and hard code an AI from scratch... and then it's just your own biases.",0,0,0.26,0.434
post47hb,richly branching,63,">By the point you tweak the model enough to weed out every bias  This misses GP's (correct) point. ""Bias"" is what the model *is.* There is no weeding out biases. Biases are corrected, not removed. Corrected from incorrect bias to correct bias. There is no non-biased.",0,1,0.207,0.537
post47hb,richly branching,63,"Why does this remind me of the moment in my research methods course that our lecturer explained that all social research is invalid because it’s impossible to understand and explain completely the internal frames of reference of another culture.   (We were talking about ethnographic research at the time, and the researcher as an outsider)",0,0,0.219,0.233
post47hb,richly branching,63,"Bias is operating in two modes in that sentence though. On the one hand we have bias as a mostly value neutral predilection or preference in a direction, and on the other bias as purely negative and unfounded preference or aversion.  The first kind of biased is inevitable and desirable, the second kind is potentially correctable given a suitable way to measure it.  The more fundamental issue with removing bias stems from what the models are trained on, which is mostly the writings of people. The models are learning it from us.",0,1,0.167,0.618
post47hb,richly branching,63,"""correct"" biases.",0,0,0.353,0.645
post47hb,richly branching,63,"I've started to enjoy watching someone pale and look a little sick then I tell a layman that there is no such thing as an unbiased model, only one that conforms to their biases.",0,0,0.218,0.462
post47hb,richly branching,63,It turns out that ChatGPT is just a single 200 petabyte switch statement.,0,0,0.173,0.239
post47hb,richly branching,63,No. But it is also pretty much impossible. If you exclude theese biases completly your model will perform less accurately as we have seen.,0,0,0.131,0.32
post47hb,richly branching,63,Why is that? I'm curious.,0,0,0.369,0.276
post47hb,richly branching,63,"That's not what ""bias"" means when people complain about AI being racist.",0,0,0.183,0.48
post47hb,richly branching,63,"Not at all. Theres so many things to add for weight. Theres millions of things. Race, height, weight, dialect are less than .01%",0,0,0.124,0.216
post47hb,richly branching,63,"In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. ""What are you doing?"", asked Minsky.  ""I am training a randomly wired neural net to play Tic-tac-toe"", Sussman replied.  ""Why is the net wired randomly?"", asked Minsky.  ""I do not want it to have any preconceptions of how to play"", Sussman said.  Minsky then shut his eyes.  ""Why do you close your eyes?"" Sussman asked his teacher.  ""So that the room will be empty.""  At that moment, Sussman was enlightened.",0,0,0.308,0.212
post47hb,richly branching,63,"Oh, I love me some good [skillful means,](https://en.wikipedia.org/wiki/Upaya) yessir~!",0,0,0.292,0.223
post47hb,richly branching,63,Don't forget all the Keynan workers paid less than $2 an hour to build the safety net by sifting through endless toxic content.,0,1,0.176,0.216
post47hb,richly branching,63,Yeah it’s awesome that the AI companies exist so that those Kenyan workers get paid 2 dollars an hour. Otherwise they’d get paid 50 cents an hour at another job.,0,0,0.165,0.152
post47hb,richly branching,63,Minimum wage for a receptionist in Nairobi was $1.52 per hour at the time OpenAI were doing this.  The damaging psychological effects of reviewing toxic content all day likely outweighed the modest pay increase they received.   Many who were interviewed discuss how it caused great trauma for them.,0,1,0.166,0.167
post47hb,richly branching,63,"Funny how you make a post bashing AI, but you are bootlicking the creators in the comments. There are always people desperate enough and people easily underestimate the psychological cost of a job like this. There are documentaries about how this job messed people up, look them up. People eventually develop ptsd and could potentially be messed up for life. They don't tell you that in the job posting I can tell you that. Trust me, noone would take the job for 50 cents extra per hour if they knew that and aren't desperate. Either way, it's exploitation.",0,0,0.151,0.229
post47hb,richly branching,63,No mate. Micro-emplyment is bad.,0,0,0.232,0.253
post47hb,richly branching,63,[deleted],0,0,0.376,0.43
post47hb,richly branching,63,autocomplete with spicy real human nuggets!  [that's all it has],0,0,0.3,0.246
post47hb,richly branching,63,At least humans are aware of their bias. AI confidentiy says everything as if it's absolute truth and everyone thinks the same,0,0,0.236,0.453
post47hb,richly branching,63,I’d wager that over 99% of Humans aren’t aware of their biases.,0,0,0.194,0.542
post47hb,richly branching,63,That definitely sounds like most humans.,0,0,0.209,0.248
post47hb,richly branching,63,"Wanna know something crazy? When the left and right hemispheres of the brain are severed, the left and the right side process information differently. They found a way to feed information to only a single hemisphere by showing information to only 1 eye at a time, to which the corresponding opposite hand would respond. When they did this with the right brain (asking it to draw a bell for example) then they asked the left brain why the right brain drew a bell, the left brain confidently came up with reasoning why, even if it was entirely made up and wrong (""I drove by a church on the way here and heard church bells""). turns out the left brain comes to deterministic conclusions very much like an LLM does, even when being confidently wrong about why the right brain did something it did.  I'm probably butchering the hell out of it all, look up the research if you're curious, super crazy stuff and an interesting peek into how the 'modules' of the brain work.",1,1,0.266,0.287
post47hb,richly branching,63,> At least humans are aware of their bias  Found the alien.,0,0,0.235,0.412
post47hb,richly branching,63,"""I'm not racist but...(proceeds to say something racist)"" Is way too common of a sentence for you to say people are aware of their own biases.  r/confidentlyincorrect is a thing.",0,0,0.172,0.398
post47hb,richly branching,63,"Humans can reflect and learn, LLM implementations cannot.",0,0,0.285,0.205
post47hb,richly branching,63,AI isn't aware of Deez nuts,0,0,0.252,0.32
post47hb,richly branching,63,That’s a concise and astute way of putting it.  LLM’s are fundamentally bias boxes.,0,0,0.262,0.464
post47hb,richly branching,63,intelligence *is* patterns of bias in observational interpretation and selected output.,0,0,0.327,0.404
post47hb,richly branching,63,"Truest true thing ever said. AI is nothing but one giant GIGO problem. It'll never be bias-free. It'll just replicate existing biases and call them ""science!!!!!!""  Eugenics and Phrenology for the 21st century.",0,0,0.161,0.263
post47hb,richly branching,63,"More like automated intuition for the 21st century. If you properly manage and vet your training data, you can get good, useful results.",1,0,0.39,0.244
post47hb,richly branching,63,It is amazing how much that sounds like a human.,0,0,0.367,0.207
post47hb,richly branching,63,Humans are just meat computers each running their own unique software so it doesn't really surprise me.,0,0,0.258,0.272
post47hb,richly branching,63,"But which one will prevail, the meat machine or the machine machine?",0,0,0.206,0.164
post47hb,richly branching,63,"And it’s one trained on people. Who can have some prejudices.   If society is racist, then that means the LLM can get a good idea of what society would assume about someone based on race. So if it can guess race, then it can get a good idea of what society would assume.   It’s a nice efficient method for the system. It’s doing a good job of what it was asked to do. If we want it to *not* be racist, we have to cleanse its training data VERY thoroughly, undo societal racism at the most implicit and unconscious levels, or figure out a way to actively correct itself on these prejudicial assumptions.",0,0,0.218,0.454
post47hb,richly branching,63,They are like a person trapped in a windowless room their entrie lives.  They know only what we tell them and the fact of the matter is that we as a society are racist. There's no way to keep them from becoming racist as long as they learn everything they know from us.,0,0,0.141,0.241
post47hb,richly branching,63,I had a lecture who clearly wasn’t tech savvy saying “AI” isn’t biased… I had to hold myself back so hard to not say anything. Iirc a while back there where tests showing that driver assistances where more likely to hit (or not see) dark skinned people because the training was all done on light skinned people,0,1,0.224,0.378
post47hb,richly branching,63,I don’t understand why people expect something different…,0,0,0.274,0.33
post47hb,richly branching,63,It's not just LLMs. You cannot derive perfectly reliable truths from unreliable data in general. Which tool you use doesn't matter.,0,0,0.245,0.236
post47hb,richly branching,63,Assumptions built on assumptions.. so is all consciousness and thought,0,0,0.327,0.309
post47hb,richly branching,63,"""Assumptions built on top of assumptions.""  Damn bro put a horror warning next time I almost had a panic attack....",0,1,0.254,0.291
post47hb,richly branching,63,"It's like looking into a reflection of all the data it was based on. Useful, but not something you look to for guidance.",1,0,0.388,0.24
post47hb,richly branching,63,Too bad 99.99% of people who use these chatbots don't know that and *still* thinks it's sentient and capable of reason and thought.,0,0,0.239,0.268
post47hb,richly branching,63,"Just because you cannot get rid of all biases doesn't mean you can't get rid of one, especially pernicious bias.",0,1,0.236,0.564
post47hb,richly branching,63,"There was a 99% invisible on this a while back, and if I recall correctly, most LLM have a foundation in the trove of emails that came out of the Enron hearings. Meaning that most of its idea of what “natural language” and human interactions can be based on Texans, specifically ones from Houston.   Does this make the base model “racist”? Well, I personally wouldn’t promote that assumption.   But given it’s geographic foundation I am willing to assume it would be at least a *little* right leaning in political ideology.",0,0,0.299,0.325
post47hb,richly branching,63,Common/Early training data doesn’t have higher impact than data trained later. In fact it’s more   accurate that poorly executed fine tuning creates a recency bias.,1,0,0.246,0.206
post47hb,richly branching,63,Can you explain like I'm five?,0,0,0.468,0.379
post47hb,richly branching,63,"Didn't you just describe people, too",0,0,0.324,0.289
post47hb,richly branching,63,No people have facts and biases.  LLMs have only biases. When they give you what seems like a fact it is actually incredibly fine tuned biases to respond with what looks like a right answer.,0,0,0.251,0.488
post47hb,richly branching,63,"Yes. People have ""facts"" in the sense that information is input and stored, not necessarily that it's correct. Input information is processed through filters of bias before (and after) storage though.",0,0,0.294,0.248
post47hb,richly branching,63,"That rests on the assumption that they can weed out all biases, which has so far proven impossible.",0,0,0.205,0.459
post47hb,richly branching,63,"Yes but that's not really the point. Obviously a biased LLM is just a reflection of biased human input.  The point is to identify which biases it has, in which ways they appear and what happens when you try to negate them.",0,1,0.215,0.469
post47hb,richly branching,63,"That's not necessarily true. A LLM will form it's own biases all on it's own to optimize it's prediction accuracy, as that is how it works fundamentally.",0,0,0.172,0.404
post47hb,richly branching,63,The fact people think this will lead to a non biased ai is just hilarious. The racist Microsoft chat bot from years ago was chat gpt 1.5.,0,1,0.22,0.366
post47hb,richly branching,63,"The problem is the datasets it was trained on. These are human biases and they show up in the data we generate online. We don't have a good way to filter those out yet but that's a logistical problem not an architectural one.   >A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.  Bro what?",0,0,0.169,0.205
post47lb,poorly branching,6,"I wouldn't read anything into reports like this or any predictions -- also that report is far too general  Python is worth knowing as most data tools need that now  Outside of that I recommend people learn some application of AI ... creating a chatbot, how to fine tune, how to RAG, how to run a model locally, how to work with APIs available (especially voice), how to eval, how to cost based on tokens etc.  AI is not the end of the world for software engineering ... it's just the start of something else in some areas ... all the doom talk is a complete over-reaction",1,0,0.274,0.22
post47lb,poorly branching,6,"it is never an absolute. shareholders dont need AI to replace engineers. what they need the AI to do is to improve everybody's productivity beyond the increase in overall demand.  if every engineer equipped with AI is now 80% more productive, while the same company only get 20% more demand, this means the same company dont need to employ the same amount of engineers. instead of employing 20 engineers, it can function with just, lets say, 12 engineers. so it can fire 8 engineers, and put pressure to the remaining 12 so they wont have leverage to get pay increase (if they dont want it, the company can ask the remaining 8 engineers to overtake the job).  and looking at software engineering job openings and median pay in the past 5 years, this is exactly whats happening. there are less and less openings for junior role, and pay for senior role dont really increase either.",1,0,0.135,0.092
post47lb,poorly branching,6,I’m creating a soft layer as we speak. What a time to be alive.,0,0,0.226,0.292
post47lb,poorly branching,6,"Refusing to aknowledge the risks of AI is a before sign of an impending doom. The problem here that you dont understand is mass unemployment. This is scary since what are hordes of unemployed men going to do? Wars, social unrest, anarchy, rising criminality and so on.",0,0,0.201,0.218
post47lb,poorly branching,6,I used to be the big advocate of that point -- but I get less and less convinced now  It's just going to be like the invention of steam engines and the industrial revolution -- there will be change for sure but it's a start not an end,0,0,0.18,0.198
post47lb,poorly branching,6,Actually is not the same with AI. Its not like engines replacing horses. Its like ‘genie in a bottle’ invention that does not create jobs. Its self sufficient for everything. It will not displace a person from a job and then give him another opportunity somewhere else (like it happened with any invention during the history). Thats why ASI its called the Last Human Invention.,0,0,0.161,0.141
post48hb,richly branching,28,">The results from our study emphasise that the ability of AI deep learning models to predict self-reported race is itself not the issue of importance. However, our finding that AI can accurately predict self-reported race, even from corrupted, cropped, and noised medical images, often when clinical experts cannot, creates an enormous risk for all model deployments in medical imaging.  Im unsure what they imply by risk...  &#x200B;  Edit:   They imply by risk that if AI is trained with biased data and not audited for that bias, then AI would also be perpetuating biases it was trained with, in this case, racial bias.   >We strongly recommend that all developers, regulators, and users who are involved in medical image analysis consider the use of deep learning models with extreme caution as such information could be misused to perpetuate or even worsen the well documented racial disparities that exist in medical practice. Our findings indicate that future AI medical imaging work should emphasise explicit model performance audits on the basis of racial identity, sex, and age, and that medical imaging datasets should include the self-reported race of patients when possible to allow for further investigation and research into the human-hidden but model-decipherable information related to racial identity that these images appear to contain.",0,2,0.186,0.289
post48hb,richly branching,28,"It means that the AI can find the race and use that as a proxy. If you train a ML algorithm for something like how much painkillers to give someone, then you have the issue of the fact doctors underperscribe back people. So the ML can have embedded racist outcomes based on factors like race. Now you might think that's not a problem if you don't input race into the model, but if the ML algorithm can find out your race though an xray, then that risk still exists.  A real life example of the risk was around using a ML algorithm to set bail. But this model just gave higher levels of bail to black people, since it was trained on racist data.",0,0,0.256,0.362
post48hb,richly branching,28,"It is also a good thing that the AI can predict people's race from x-ray images, because now a patient's bone structure phenotype can be compared to the average for their race or ethnicity, rather than the average for all races, which would greatly increase the accuracy of the criteria used to determine whether certain bone structure phenotypes are disease markers or not.",0,0,0.176,0.292
post48hb,richly branching,28,"Youre correct, i read part of the article behind the paywall and that's what the researchers say",0,0,0.194,0.181
post48hb,richly branching,28,Yeah it's a tough problem because on one hand you don't want the AI to be biased based on race but on the other hand race can have an effect on health risks and should be considered when making some diagnoses.,0,1,0.189,0.323
post48hb,richly branching,28,"Yep it's a hard problem. They used to actually think that black people were different and didn't feel pain like white people did. This myth hasn't completely died out. While it's a myth that causes harm, there could very well be other situations where you do want to treat black people differently due to biological differences.   I think the key thing here is to have a really good understanding of how the model works and what factors it might be taking into account. This article is about the fact the model has capabilities they didn't expect and hence could act way differently than they want.",0,0,0.163,0.292
post48hb,richly branching,28,[removed],0,0,0.366,0.413
post48hb,richly branching,28,"Yes yes, we know. Only sentient beings can be racist.   But data can have racial inequities that result in unfair or unequal results based on race.",0,1,0.143,0.34
post48hb,richly branching,28,You know what I mean. Data that differentiates treatment based solely on race with no medical reason for the difference.,0,1,0.19,0.399
post48hb,richly branching,28,">Findings regarding the possibility of confounding of racial identity in deep learning models suggest a possible mechanism for racial disparities resulting from AI models: that AI models can directly recognise the race of a patient from medical images. However, this hypothesis is largely unexplored and, in contrast to other demographic factors (eg, age and sex), there is a widely held, but tacit, belief among radiologists that the identification of a patient's race from medical images is almost impossible, and that most medical imaging tasks are essentially race agnostic (ie, the task is not affected by the patient's race). Given the possibility for discriminatory harm in a key component of the medical system that is assumed to be race agnostic, understanding how race has a role in medical imaging models is of high importance as many AI systems that use medical images as the primary inputs are being cleared by the US Food and Drug Administration and other regulatory agencies.",0,1,0.221,0.409
post48hb,richly branching,28,Potentially a privacy risk? You try to anonymize an xray by cropping it and the the Ai tells you a bunch of stuff about that patient.,0,0,0.206,0.208
post48hb,richly branching,28,But who's looking at anonymized x-rays? And who cares what they know about a person; the system won't predict their name.,0,0,0.237,0.223
post48hb,richly branching,28,"That doesn't look to me like the reason... The AI can tell you the patients race, a doctor (i guess specialised in the subject, I can't) could tell you too, but less acuratelly , but it's only one detail about the subject and it's not considered identifying information in the ethics and privacy sense of the term, it would not be considered a risky thing",0,0,0.28,0.351
post48hb,richly branching,28,"If you can tell a person is “Black” and you think “Black” people don’t care for their health, your advice to them might be different, for no actual good reason.   You might tell them just make sure to take their pills instead of pushing them to make lifestyle changes that are more effective, but take more effort.",0,0,0.138,0.178
post48hb,richly branching,28,Don’t forget simple bias creeping in as well.,0,0,0.229,0.457
post48hb,richly branching,28,"This is the actual reason.  Data contamination.  A doctor's racial biases are assumed to be ""white noise"" by creators of medical image diagnostic algorithms, but this study shows that the same types of algorithms commonly used for medical imaging diagnoses can also easily identify race, so any race-based underdiagnosis (which is a known and profound problem for minorities) will likely be perpetuated by the AI.",0,0,0.15,0.349
post48hb,richly branching,28,"Imagine a study that uses these X rays to determine whether a certain course of treatment for a given disease is worth the expense involved. An AI model gets a large training set of X rays together with the outcomes from those patients after the treatment.   Then it's shown a new X ray and is asked to predict the outcome for that patient. If it predicts a positive outcome, the treatment is recommended. If not the treatment is not recommended.   One problem is that the training data the model is trained on was probably taken from a variety of different hospitals and research facilities with different levels of funding, standards of care, etc. Because of structural racial inequalities, the patients at hospitals with lower standards of care and worse outcomes might be more likely to be from certain racial groups. An AI model capable of discriminating between racial groups will ""automatically"" learn to use this information in predicting outcomes. So two otherwise identical patients might receive different recommended courses of treatment if they're from separate racial groups.  Imagine something like this being used to determine whether you were eligible for potentially lifesaving care.",0,0,0.252,0.37
post48hb,richly branching,28,This is the answer.,0,0,0.258,0.281
post48hb,richly branching,28,Turns out its totally not the answer,0,0,0.271,0.286
post48hb,richly branching,28,"Is it? Like, did you see it somewhere or know it for a fact or is it just it sounds to you like the answer?",0,0,0.252,0.221
post48hb,richly branching,28,"It's fairly commonly known that racial minorities and women are under-diagnosed by doctors.  If the training data is contaminated with that bias and the algorithm can detect sex/race, the algorithm will *also* under-diagnose those populations.    If monkey can see, monkey will do.",0,0,0.206,0.417
post48hb,richly branching,28,"Unintentional confounding in your data, particularly when you can't audit how the model performs over different races directly.",0,0,0.243,0.347
post48hb,richly branching,28,"But that information is important/relevant? Races (and sexes) react differently to various medication for example, knowing someones race is impactful in treatment options.   I wonder how much of this is fear of perceptions of racism and how much is to do with actual racial biases.",0,0,0.195,0.37
post48hb,richly branching,28,I believe the point is that it's not always relevant. It could stem from racism or other non-medically related bias.,0,0,0.202,0.332
post48hb,richly branching,28,Exactly. Finding someone’s age or race by feeling an X-ray into an AI isn’t “risky”.,0,0,0.231,0.204
post48hb,richly branching,28,"I'm not sure either... One risk is an AI, that, because of the racial distinction egrained into the model, performs worse than a counterpart without those distinctions. But that doesn't make any sense. Wouldn't you want an AI that matches its domain more closely? An AI that learns those differences in biology, could only use them to do better, not worse. Thats the only meaning that relates the ""risk"" to the ""model deployments""...  Or its about privacy? Ex.: A radiation therapy tech, who creates intentionally dangerous treatment plans, to harm people of certain races? But it says, ""risk for all model deployments"", not ""risk for patients, at the mercy of racist doctors, who have an AI to pick out people they dont like"" or something like that.  Couldn't think of any other meanings, but i'm also neither an expert on machine learning nor english....",0,0,0.193,0.298
post48hb,richly branching,28,"i read a bit more and the explanation is more simple, it has to do with bias in diagnostics made by humans, if you have a condition that isn't properly diagnosed in a certain race, like severe cases being considered mild in a certain ethnic group because of bias, then the database you use to train the AI has those biases too, and AI could associate those biases with race and continue them, that's the risk, and the researchers say that AI used for diagnostic should be checked for this, to avoid the AI making the same mistakes that humans make because it was trained on a data base that has those mistakes too",0,0,0.189,0.257
post48hb,richly branching,28,"Ohhh, now i get it, thanks!",1,0,0.41,0.321
post48lb,poorly branching,14,"**tl;dr They're going to redo the panel vetting process without using ChatGPT, have released the exact prompt they used in the first place, and have promised not to use ai again.**  It's a decent first step, but the tone of the article is still very defensive, very ""no you don't understand, it was only a very minor use of the plagiarism machine,"" and there's still a lack of transparency with regards to who actually used and approved the use of ChatGPT. The Program Division Head has taken responsibility, but it's clear they're covering for other people.  Additionally, there have been no resignations thus far.  They said they'll update with further information about their path going forward next week.",0,0,0.179,0.203
post48lb,poorly branching,14,I believe 3 people resigned.,0,0,0.177,0.239
post48lb,poorly branching,14,The three people that resigned did so in protest of the fact that ChatGPT was used. They had nothing to do with its use.,0,0,0.184,0.221
post48lb,poorly branching,14,"Correct.  They are part of the Hugo Awards at World Con and resigned in protest.  Most likely because the people who should resign, did not.  I think it's disingenuous to say that no one has resigned.  Because people have.",0,0,0.134,0.18
post48lb,poorly branching,14,Not any of the responsible parties. The people who resigned were innocent and did so in protest.,0,0,0.147,0.158
post48lb,poorly branching,14,"It's wild that people are still claiming using AI is plagiarism, especially for something purely administrative like this (googling people and flagging anything sus). Every other field is adopting AI for  (with obvious fact-checking), this one should too. It saves a lot of Googling.",0,0,0.221,0.233
post48lb,poorly branching,14,"Even if it is not plagiarism, it is a) absolutely flawed and inaccurate, try any single ai search engine. And since it's functioning as a black box, without any hint of ability to retrace its inner workings, it lacks the accountability which a person doing the job would provide .   B) It's unethical to waste the ridiculous amount of resources ai requires just to take away a person's job. There is zero upsides to this, we are not living in some utopia where the person who would do the fact checking would dedicate themselves to some more meaningful and creative endeavour without repercussions. In this real world the use of Ai cuts down on  some freelancer's wages, replacing it with a technology which is flawed, wasteful and commodifies and monopolizes knowledge in the hands of companies whose track record can be euphemistically described as suspect.",0,2,0.112,0.201
post48lb,poorly branching,14,"> Even if it is not plagiarism, it is a) absolutely flawed and inaccurate, try any single ai search engine.  AI is revolutionizing the world, whether we like it or not, and nitpicking about it not being perfect is a waste of time. It's getting better and better whether we like it or not. Nothing is flaw-proof.  > B) It's unethical to waste the ridiculous amount of resources ai requires just to take away a person's job.  Whose jobs are being eliminated? It's an administrative, time-saving method, not the advent of Skynet.  > There is zero upsides to this, we are not living in some utopia where the person who would do the fact checking would dedicate themselves to some more meaningful and creative endeavour without repercussions. In this real world the use of Ai cuts down on  some freelancer's wages, replacing it with a technology which is flawed, wasteful and commodifies and monopolizes knowledge in the hands of companies whose track record can be euphemistically described as suspect.  So should we revert back to a pre-industrial, agrarian society? Get rid of assembly line machinery/PCs/(insert modern-day technology), since they can do the work of many people?",0,3,0.141,0.199
post48lb,poorly branching,14,"> It saves a lot of Googling.  If a human still has to vet the answers the LLM gives then while it may save some Googling, it doesn't save time or energy costs.",0,0,0.4,0.298
post48lb,poorly branching,14,">  Every other field is adopting AI for (with obvious fact-checking), this one should too. It saves a lot of Googling.  worldcon has a lot of authors attend (or people that want to be authors), they are shitting bricks over AI taking all their jobs.  So in that case I can understand their response.  But I still think most of this is an overreaction.",0,0,0.249,0.197
post48lb,poorly branching,14,"Yeah am I missing something? Given the reaction, I assumed it would be something horrible, but it’s just… aggregating links?",0,0,0.254,0.251
post48lb,poorly branching,14,"Calling it a ""plagiarism machine"" shows that you *don't* understand.  Edit: /u/iMooch blocked me so I can't respond to /u/currough directly, thanks to Reddit's brain-dead implimentation of blocking. But I would suggest that currough doesn't understand what ""plagiarism"" means, in that case.",1,1,0.295,0.243
post48lb,poorly branching,14,"I have a PhD in machine learning and I think ""plagiarism machine"" is entirely accurate.",1,0,0.249,0.249
post48lb,poorly branching,14,"I have a PhD in machine learning and I think ""plagiarism machine"" is entirely inaccurate.",0,0,0.197,0.32
post49hb,richly branching,4,You have to understand that the prejudice is with you...   The AI is just looking for patterns and repeating them.  The algorithm has no concept of good and bad...   This is just what it has been fed the most.  It says something about people...  All it says about AI is it's still not thinking for itself.,0,0,0.212,0.367
post49hb,richly branching,4,"I'm going to go against the flow here and disagree, to an extent.  If we say that the decisions made by a.i. cannot be racist because the a.i. is not intelligent, then we can either offload behaviors originally motivated by racism onto these robots or just not give a crap when they learn to do things that would be racist, like targeting individuals for police surveillance on the basis of skin color, and when any concern is raised, it can amount to no more than an admission that something must have gone terribly wrong, but nothing racist ever occurred, because it can't occur.  I think society will eventually be forced to consider how these terms apply to the processes and criteria used to make decisions in perceived ""racist"" discriminatory actions, be they from humans or a.i., in order to protect ourselves from the misuse of a.i. or its malfunctions.",0,2,0.178,0.362
post49hb,richly branching,4,"I mean, it wouldn't be targeting based on skin color, it would be targeting based on crime rate...   People will say it's racist, that is a function of their bias though.  They cannot fathom that sometimes it's just actually justified.  It is why I bring up IBM, they are actively working on unbiased data and having the AI report exactly how it came to the conclusions it did... this is at the core of its debater technology and being marketed as a way to remove corporate biases by effectively correcting peoples thought processes on a topic.  The reality is the vast majority have poor reasoning skills, we are subject to various evolutionary biases that don't actually apply to our modern reality. If we are going to actually grow as a species and come together for our mutual prosperity we have to overcome these deficiencies.  The good thing about AI is it can learn from everything available to it and not die... it just keeps learning forever. The notion that this is more dangerous than a human trying to get as much shit as they can in their short 80 years is absurd.  I am far more worried about humans continuing to be in control because we're fucking evil as a whole.",0,2,0.176,0.334
post49hb,richly branching,4,"I'm not sure that something like a continuously learning model would be best for this, but just assuming it's what was implemented, let's say we pretrain the model to some acceptable standard and place it in an area with a high crime rate: a low income neighborhood that is predominantly populated by a minority.  Pixel color values are among the features of the data set because, why not?  Because it's a continuously learning model, training is continuous.  Its training data is unbalanced because the population it picks up on the cameras reflects only the local reality and majority population of minority descent.  Now, let's say it does its job with quantifiable effectiveness, and, for whatever reason, someone decides that this effective model can and should be frozen and propagated to other areas of a city because of its effectiveness, except, those other areas happen to be shopping centers in a high-income area of the city where the population has nowhere near the same concentration of people of the same minority.  We should not be surprised if the cameras fail to identify criminality among peoples of a different color of skin than those it was initially trained on, or if it shows a bias toward those who are of that minority.  And further, do recognizable features such a skin color really have anything to do with criminality, or was economic disadvantage a factor?  The machine doesn't know or care, and neither does racism.  P.S. this has happened in the real world, where the training sample was unfortunately too biased, unintentionally, and that bias can and does end up creating problems people see as discriminatory.",0,2,0.248,0.342
post49lb,poorly branching,8,Spot the impostor. (Spoiler: it's perplexity),0,0,0.271,0.294
post49lb,poorly branching,8,Why is that?,0,0,0.403,0.35
post49lb,poorly branching,8,It has no proprietary model,0,0,0.157,0.143
post49lb,poorly branching,8,Yeah i guess you're right then,0,0,0.341,0.231
post49lb,poorly branching,8,Proprietary model thing changed the moment Deepseek R1 dropped. Everyone has access to high reasoning model for almost free or fraction of the cost of OpenAI. It’s no longer moat.,0,0,0.174,0.189
post49lb,poorly branching,8,Hey isn't the 'Sonar' model in their api proprietary?,0,0,0.188,0.221
post49lb,poorly branching,8,I think you meant imposter. It is Perplexity because it's actually just a wrapper,0,0,0.2,0.242
post49lb,poorly branching,8,"Yeah, thanks",1,0,0.58,0.229
post4hb,richly branching,11,[deleted],0,0,0.376,0.43
post4hb,richly branching,11,Walk around the South for 10 minutes.,0,0,0.178,0.279
post4hb,richly branching,11,"Yes, but that is exactly the reason everyone misses.  Where do you think a lot of this dialect started?   Take out some of the lingo and it's exactly the same as some illiterate mississipi grandpa.",0,0,0.224,0.218
post4hb,richly branching,11,"'People grow up in different cultures, contexts, and educations than me, that makes them the same as being illiterate!' Tell us how you really feel, dude.",0,0,0.291,0.281
post4hb,richly branching,11,"This is easily understandable and normal AAV. It’s informal, yes, but I bet whoever types like this normally will code-switch when the situation calls for it.",0,0,0.228,0.21
post4hb,richly branching,11,"I know plenty of white alabamians, mississippians, and georgians who speak worse than this. And don't get me started on the white appalachians.That's just another language.",0,0,0.133,0.163
post4hb,richly branching,11,I’m sure the AI probably wouldn’t think too highly of their dialect either.,0,0,0.173,0.19
post4hb,richly branching,11,[removed],0,0,0.366,0.413
post4hb,richly branching,11,Dialect doesn't denote intelligence.,0,0,0.232,0.294
post4hb,richly branching,11,"Checks out: “saying the speakers were likely to be dirty, stupid, rude, ignorant, and lazy.”",0,2,0.285,0.27
post4hb,richly branching,11,Yea... the country doesn't think to highly of them.,0,0,0.217,0.257
post50hb,richly branching,55,"Concerned about what exactly? How exactly could the AI, or any algorithms feeding off its output, be racist here in a way that negatively affects anyone?",0,0,0.178,0.415
post50hb,richly branching,55,"Basically, if we want the AI to „correctly diagnose“ diseases, we need to teach which diagnoses are correct. These diagnoses however can have a bias.  Imagine a world where no person with colourful hair ever gets treated for or diagnosed with sunburn. The AI is trained on the compiled data of thousands of diagnoses. It might recognise the same markers in people with colourful hair, but every time it marks them it gets told „wrong, no sunburn“. So it learns that people with colourful hair never have sunburn, and will never mark them as such.  The AI isn‘t racist as in „it hates them blacks“, it just perpetuates the biases in the dataset it was trained on, be they good or bad.",0,1,0.161,0.31
post50hb,richly branching,55,"I understand what you're saying, but i dont think that applies here. You have an AI that can detect race based on x-rays. How would an AI that can't detect race based on x-rays be better in any case?   If there is racial bias in the data that is used to train the AIs, then the AI will learn that racial bias. Being able to detect race is not racial bias though.",0,0,0.181,0.355
post50hb,richly branching,55,I don't think the issue per-se is about ML models being able to detect race in a dataset or it being used in a nefarious way.   The problem is that the model supposedly encodes an assumption about the race of an individual when it's given an X-ray image. This means that it could take the X-ray of a person of one race and it could mistakenly encode some hidden assumption that the person's bone structure is similar to that of some other race in the image's representation.   The performance of the model is then tied to distribution of X-ray image data for different races and this *could* hamper performance if it's used in conjunction with other systems that rely on race information. It becomes harder to trust the model's output for an X-ray image of a race it's not trained on.,0,0,0.166,0.33
post50hb,richly branching,55,"Here is the piece you are missing. If the AI can detect race from X-rays, that means that race-based correlations and biases present in diagnostic data can affect an AI diagnosis. Humans are unable to identify race from X-rays, thus the researchers had assumed that a diagnosis based solely on X-rays would be free of a racial bias. They found some evidence suggesting that this wasn't the case, and attempted to identify race via X-ray. The sole reason this study was conducted was that they found evidence of racial bias at the level of AI diagnosis. So yes, it is concerning that the AI can detect race from X-rays. It implies that we cannot rely on AIs to provide an unbiased diagnosis, even when we cannot fathom how that bias could occur.",0,0,0.187,0.3
post50hb,richly branching,55,"I‘m not saying there is :) The question was, how could such a thing negatively affect anyone. That‘s what I tried to answer :)",0,0,0.248,0.291
post50hb,richly branching,55,"The scientists aren't saying, ""oh no, the machine can see race, that's bad."" They're saying, ""maybe the machine seeing race is part of how it's underperforming for black people.""  They're not implying the solution is to make the machine unable to see race. They're saying they need to figure out how race plays into what the machine sees, and hopefully use that to improve the machine before rolling it out.",0,0,0.249,0.24
post50hb,richly branching,55,[removed],0,0,0.366,0.413
post50hb,richly branching,55,"Apologies for my ignorance, but is ""colourful hair"" another way to say ""red hair""?",0,0,0.163,0.17
post50hb,richly branching,55,it's just an example of someone that can be identified as such. could be anything really . in this case it's race,0,0,0.261,0.427
post50hb,richly branching,55,"I didn’t wanna use any hair colour, so I thought I‘d say dyed hair. Came out wrong lol",0,1,0.172,0.209
post50hb,richly branching,55,I assumed colorful hair was like green or purple.,0,0,0.2,0.23
post50hb,richly branching,55,"Hey, you’re not allowed to use the r-word!",0,0,0.223,0.285
post50hb,richly branching,55,Underrated comment here.  Well summarized.,0,0,0.386,0.307
post50hb,richly branching,55,"This! In the article it essentially states what you are saying here. Due to these biases, AI can select not to diagnose certain races once identified if these biases are not studied further and understood. This should be very concerning similar to AI’s inability to facially recognize Asian people in other studies. Data can be racially biased therefore making the ability to identify race based on X-Rays a problem instead of a benefit. This is my understanding of the article.",0,1,0.219,0.415
post50hb,richly branching,55,I would assume the AI would be smart enough to not say “can’t be sunburn” but instead “sunburn less likely”. For different races I don’t think there any diseases or issues that are all or nothing. Just some that are more/less likely to varying degrees.,0,0,0.126,0.206
post50hb,richly branching,55,Yupp! I was just oversimplifying greatly for ease of understanding. These nuances are really important when reading further into the topic though! Thanks for bringing it up!,1,0,0.297,0.219
post50hb,richly branching,55,"Well then your ML data needs to be retrained. You repeat until two datasets return the expected reponses repeatedly. This is nothing new, just another data point. Fluff article.",0,0,0.195,0.151
post50hb,richly branching,55,Sounds a lot like how COVID symptoms and demographics were selected in the beginning of the pandemic. They had no clue who was actually at risk because of all the old people that were grouped together in New York and died. Skewed the whole data set from the beginning and made the death rate high enough to consider COVID dangerous. Then for the treatments they thought things worked because people who took them recovered but they were actually later changed because they didn't help people at all.  Initial conditions really have a lasting relevance when a system is being created from nothing. Hopefully they figure out how to properly setup the data to prevent wrong diagnosis.,0,1,0.204,0.225
post50hb,richly branching,55,"Aaaand let’s say this AI does become a racist, toothless bully. I know the solution. We can contribute code to break it and stop the terror. Easy!",0,0,0.199,0.288
post50hb,richly branching,55,"> These diagnoses however can have a bias.  Yeah, like have a massively improportional diagnosis of testicular cancer in men as opposed to women.  Huuuuuuuge bias.   But AI with these trainings sets really will perpetuate any sort of wrong bias that gets into the training set.   The solution is not to hobble the AI and lobotomize them, but rather FIX THE DATA so they're properly trained.  Always side with the truth. The truth will set you free.",0,1,0.219,0.263
post50hb,richly branching,55,Yupp. I remember when someone (Google?) trained an AI to make hiring decisions and it ended up racist. Bias in the data -> bias in the AI.,0,0,0.238,0.363
post50hb,richly branching,55,"Let's say your AI that you implemented to replace credit scores to pick out the best ppl to give mortgages to independently concluded that it was most profitable to just blanket reject all ppl of a certain specific historically socioeconomically disadvantages ethnicity, and it wasn't wrong, and it wasnt trying to be racist on purpose.  What are you gonna do with this information?  What are you even legally able to so with this information?",0,1,0.173,0.439
post50hb,richly branching,55,"Fair enough. But anyone designing these systems then should decide responsibly what input data to even feed into the system. And the data it is trained on.   In the case of detecting perceived ""race"" from skeleton images, we shouldn't really be surprised. Or overly concerned imo.",0,0,0.192,0.326
post50hb,richly branching,55,"Its being used for pathology. And there is variance in efficaciousness between ""races."" If you depend on a system like this and you don't correct for that,  the system becomes racist.  Also, I dont think that the word racist was used the article.",0,0,0.215,0.403
post50hb,richly branching,55,"\> The study adds to a growing body of data that AI systems can often replicate human biases and prejudices, whether they be racist, sexist, or otherwise.     Yeah, it was.",0,0,0.23,0.354
post50hb,richly branching,55,"I don't understand, is it racist to simply point out that one person's skin color is different than another? Is it racist to point out that the same person has a relatively larger/smaller femur on average? Are we trying to pretend that different races didn't come from different paths of evolution?",0,0,0.18,0.362
post50hb,richly branching,55,[deleted],0,0,0.376,0.43
post50hb,richly branching,55,The article states that implict bias may be brought in to the design of AI. This is for any phenotype. Its _____ist to not correct for implicit bias when it is known.  And of course people are different. Thats a core aspect of this article.,0,0,0.248,0.365
post50hb,richly branching,55,"Different races did not come from different paths of evolution, and that erroneous belief is the first fucking thing people are worried about reinforcing. Racial classification is based on phenotypical traits like skin tone, hair texture, nose and eye shape, etc, and almost entirely arbitrary (look up Nat Geo fraternal twins of different ""races"" as an example). The variations the x-rays are picking up are more than likely correlated with a ton of other factors.",0,0,0.188,0.313
post50hb,richly branching,55,"Yeah the scientists aren’t worried that their AI is racist, as far as I can tell  Rather they’re worried that having race be a factor could mean different outcomes for different races due to the additional input, which means some people could get worse care",0,1,0.173,0.285
post50hb,richly branching,55,">If you depend on a system like this and you don't correct for that  What does ""correct for that"" mean?  How do you know your corrections aren't even more problematic than the original 'biases?'",0,1,0.27,0.32
post50hb,richly branching,55,"That seems like semantics or a thought exercise more than anything productive.   I think that the philosophical goal is to predict every single illness or disease with 100% accuracy. Until you get there, there is work to be done. If patients of particular ""races"" are further or closer to 100% than others, then there are missing data or biases that make it more or less accurate. So correction is needed.  If correction is the wrong word, have that point and help me use a term that makes this more comfortable",2,1,0.261,0.358
post50hb,richly branching,55,[deleted],0,0,0.376,0.43
post50hb,richly branching,55,"It's not that hard to predict someone's race as a human, no? If people wanted to predict race, well we had the tech do that algorithmically 15 years ago. Someone's perceived race was, by definition, never really private information.",0,0,0.195,0.347
post50hb,richly branching,55,"Well, an AI is spawned from the input it receives. So if a pool of information is presented, it can only calculate as it learned.    Throw 2 random groups together; an AI can identify (group 1) as 100% ""normal"" vs (group 2) 99.9% ""normal"". Couldn't or wouldn't an AI separate that pool in some way from its baseline? ..then further presume that group 2 is flawed because it was not within the baseline study pool?     This may not seem like an issue unless people in group 1 came from Northeastern Asia (also happens to be where the AI was developed) vs. group 2 that came from the continent of Africa. All unintended skewing of what we identify as equal information, just seen with a superior observing ability. An AI *could* outlearn us and make a separation without us ever knowing. Seemingly minor variables from our learning curve in programming alone may result in unecxpected discoveries or conclusions in any long-run.",0,0,0.203,0.233
post50hb,richly branching,55,[removed],0,0,0.366,0.413
post50hb,richly branching,55,"Being this sure of yourself about things you didn't study is honestly dangerous. And no, watching youtube videos of a redpill highschool graduate doesn't count. Dunning-kruger on full effect right there.  For instance, what is black and white people? Are Italians white? Because about 40 years ago white supremacists didn't think them as white. And where does black start or end? There are ""whites"" that didn't interact with other whites for thousands of years before globalization. There are millions of factors affecting iq, brain size, bone/muscle density and height other than genetics. Food culture, soil that food grows on, air quality, culture itself and healthcare are all more dominant factors.",0,0,0.27,0.266
post50hb,richly branching,55,"It already happens in some places in United States.I believe , algorithms used to allocate policing resources but based on algorithms of crime in those areas for last 40 years or whatever ,but is prejudiced against the current generation in those areas.",0,1,0.085,0.263
post50hb,richly branching,55,"Not a problem with the technology itself, but the people using it. And this ""discovery"" won't change that. If we want to fight racism effectively we need to focus on educating people more than we do the AI that they use. Until AGI, at least.",0,0,0.172,0.239
post50hb,richly branching,55,A racist AI? Fuking computers and its codes are rayyciiisssttt,0,0,0.263,0.401
post50hb,richly branching,55,Writer is a sheltered idiot with a rigid perspective.,0,1,0.305,0.392
post50hb,richly branching,55,*China has entered chat*,0,0,0.245,0.214
post50hb,richly branching,55,It doesn't align with their political view,0,0,0.222,0.27
post50hb,richly branching,55,"Maybe they have AI watching us through x-ray cameras but they don't want to admit it:   ""oh no, this AI can tell race from x-ray, they might discriminate between races""  ""why would that be an issue except after you got an x-ray? it's not like we're constantly being surveilled with x-ray cameras during interactions which would allow for discrimination or anything, is it?""   ""...""",0,0,0.206,0.353
post50hb,richly branching,55,"Well, at least we can see what happens in this thread : an ai is trained to categorize based on certain caracteristics, and a fuck ton of people immediately conclude that the categories aren't constructed. The ai is fine, but people already use it to feed their confirmation bias.",0,0,0.28,0.263
post50hb,richly branching,55,I think the article is implying that doctors are concerned because humans can't predict the race of someone just by looking at x-rays and it may lead the AI to have a racial bias towards treatment plans/diagnosis if implemented.,0,0,0.227,0.322
post50hb,richly branching,55,"From the article:  “ Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research. Scientists must first figure out why this is happening.”",0,0,0.137,0.182
post50hb,richly branching,55,It also ignores the fact that doctors already apply racial bias (and bias along other lines such as sex) when diagnosing and treating patients.,0,0,0.234,0.393
post50hb,richly branching,55,"I feel like it could be evidence that racial bias actually can effect a person's treatment and health. It's scientific support that bigotry isn't ""politics"", it has physical consequences.",0,0,0.203,0.326
post50hb,richly branching,55,"The scientists aren't saying, ""oh no, the machine can see race, that's bad."" They're saying, ""maybe the machine seeing race is part of how it's underperforming for black people.""  They're not implying the solution is to make the machine unable to see race. They're saying they need to figure out how race plays into what the machine sees, and hopefully use that to improve the machine before rolling it out.",0,0,0.249,0.24
post50hb,richly branching,55,"It's more so that AI has a tendency to perform more poorly with ethnic minority related data, since ethnic minorities are minorities and therefore have generally less data to train AI.   It's not usually bias, but underperformance that is the problem here. Of course, there is always the potential for the users of an AI to use its output in a discriminatory way.",0,1,0.1,0.366
post50hb,richly branching,55,"Indeed, underperformance is ""the problem"". You might even call it ""a concern"". It's really an open-ended question of, can we figure out why the models are underperforming, exactly? Maybe the explanation will point to other ways they underperform?   I feel like people are responding to this article as if the takeaway was, ""stop! It's going wrong!"" When in reality the takeaway is, ""okay, we're getting there slowly, not quite ready yet.""",0,1,0.186,0.312
post50hb,richly branching,55,"People in denial still trying to wrap their heads around the fact that humans can be categorized into different sub-species.   They still think race is only ""skin-deep"".",0,0,0.217,0.359
post50hb,richly branching,55,"""Sub-species"" is a bit of a stretch imo. There are obviously differences between races but they really don't go much past a few cm on avg here, a bit more lactose (in)tolerance on avg there...   But yeah, I'd agree that there's deeper differences than skin for sure.",0,0,0.169,0.211
post50lb,poorly branching,5,Always remember to sell shovels to gold miners,0,0,0.227,0.193
post50lb,poorly branching,5,I had to laugh. This stuff is truly silly. Agents have been around since th 60's and this is another iteration that has more inherent risk than the rules based agents.,0,0,0.199,0.215
post50lb,poorly branching,5,What are the risks?,0,0,0.194,0.325
post50lb,poorly branching,5,1. LLMs don't do math - everything is stored as text and in chunks (tokens) which may cut numbers. It is not a calculator but simply a wors predictor 2. Lack of context - no matter what the latest AIs Brosnan say it does not understand context 3. As it guesses at the right word it will make things up just to fill in gaps 4. In an MCP configuration with LLMs you increase the likelihood of generating false output (this assumes you are using it for productivity rather than making things like art or a story) 5. If you use your own data for this then you increase the risk of exposing your data to bad actors aince these agents have little means of actual controls to prevent attacks   There is more but for a general end user who looks to build personal workflows (aka... agents) these are the critical points.,0,0,0.174,0.122
post50lb,poorly branching,5,"AI agents can work in unpredictable ways, whereas rules-based you can be pretty certain on the outcome.   Cursors support chatbot making up a non-existent policy that led to people unsubscribing would be an example. Obviously not an earth shattering moment, but put a similar flaw into an agent managing defences etc",0,0,0.255,0.267
post51hb,richly branching,10,"Submission Statement:  The issues of the intrusion of politics into science have been a central concern of Sam Harris. This article is about how AI may in fact detect differences in people and respond appropriately, but because the differences are conceptualized as racial differences by humans there are Progressives^TM concerned AI may be secretly racist.  It seems to strain the definition of scientific inquiry if we take an entirely neutral methodology which detects a politically uncomfortable phenomenon and assume the neutral methodology is flawed because of the detection of that phenomenon.",0,0,0.29,0.335
post51hb,richly branching,10,"Okay, but that's not what I'm reading in the paper. The paper seems to be about why it's able to figure out the race based off just x-ray images.  Especially since when extra data was added like the CT scans, the algorithm became confused and less accurate.  They seem to want to know why it is doing this more than that it is secretly racist. Or so that's how I read it anyway.",1,0,0.193,0.32
post51hb,richly branching,10,"I mean, this is part of the abstract:  >Finally, we provide evidence to show that the ability of AI deep learning models persisted over all anatomical regions and frequency spectrums of the images, suggesting **the efforts to control this behaviour when it is undesirable will be challenging and demand further study**.  ...  >However, our finding that AI can accurately predict self-reported race, even from corrupted, cropped, and noised medical images, often when clinical experts cannot, **creates an enormous risk for all model deployments in medical imaging**.  Emphasis added.  The paper opens with:  >Bias and discrimination in artificial intelligence (AI) systems has been studied in multiple domains,1,  2,  3,  4 including in many health-care applications, such as detection of melanoma,5,  6 mortality prediction,7 and algorithms that aid the prediction of health-care use,8 in which the performance of AI is stratified by self-reported race on a variety of clinical tasks.  It seems to be about the potential for AI to be racist. From the discussion section at the end of the paper:  >Although the ability to accurately detect self-reported race from highly degraded x-ray images is not meaningful on its own, this ability is important in the larger sociotechnical context that AI models operate in for medical imaging. One commonly proposed method to mitigate the known disparity in AI model performance is through the selective removal of features that encode sensitive attributes to make AI models “colorblind”.35 Although this approach has already been criticised as being ineffective, or even harmful in some circumstances,36 our work suggests that such an approach could be impossible in medical imaging because racial identity information appears to be incredibly difficult to isolate.   I'd say the paper is pretty conclusively about the ""threat"" of neutral AI detecting racial differences which are politically inconvenient.  I do thank you for making me look at the actual paper. I would have belived your comment otherwise, it almost sounds plausible.  https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00063-2/fulltext",1,2,0.199,0.431
post51hb,richly branching,10,"Isn't this more a ""face detection does bad on Asian faces because it was trained on white faces"" kind of concern than a ""computers aren't woke"" concern?",0,0,0.255,0.275
post51hb,richly branching,10,"| our study showed that medical AI systems can easily learn to recognise self-reported racial identity from medical images, and that this capability is extremely difficult to isolate. We found that patient racial identity was readily learnable from medical imaging data alone, and could be generalised to external environments and across multiple imaging modalities. We strongly recommend that all developers, regulators, and users who are involved in medical image analysis consider the use of deep learning models with extreme caution as such information could be misused to perpetuate or even worsen the well documented racial disparities that exist in medical practice. Our findings indicate that future AI medical imaging work should emphasise explicit model performance audits on the basis of racial identity, sex, and age, and that medical imaging datasets should include the self-reported race of patients when possible to allow for further investigation and research into the human-hidden but model-decipherable information related to racial identity that these images appear to contain. |  This is the conclusion I saw and from what I can read, it says that the AI is good at figuring out races based on the information given to it. And that we're still not sure why. This could potentially be used to make wrong conclusions on things we know that already exist. And finally that they want to include race information in future test in an attempt to figure out why the AI can come to the conclusions it does.",0,2,0.218,0.321
post51hb,richly branching,10,"Sorry when I said paper, I read the article, and that's how I interpreted it, I'll have a look at the paper now and get back to you.",0,0,0.228,0.215
post51hb,richly branching,10,">I'd say the paper is pretty conclusively about the ""threat"" of neutral AI detecting racial differences which are politically inconvenient.  It is not political inconvenience but rather the AI drawing well-known wrong conclusions from data that the researchers think it shouldn't be looking at. At on top of that, they are trying to include the advice of researchers from other fields as well. Something that is all too common in research and academia.  Edit: A word",0,1,0.216,0.347
post51hb,richly branching,10,"What does it mean for an AI to ""respond appropriately"" upon identifying a person's race?",0,1,0.238,0.468
post51hb,richly branching,10,">the intrusion of politics into science  I actually think this is a great concern and a monster that really needs a light shined on it. [Politics is why a special brand of Lamarckism prevailed over Darwinism in the Soviet Union](https://www.smithsonianmag.com/science-nature/when-the-soviet-union-chose-the-wrong-side-on-genetics-and-evolution-23179035/), and detractors of it were purged (edit: sent to gulags and/or ""disappeared""). These purged detractors were more correct in hindsight than their counterparts though, scientifically speaking.",0,1,0.177,0.252
post51hb,richly branching,10,"You are right on that. But our research is relatively a free and open space, where questioning is often allowed and encouraged. So it is not anywhere near as it was in the Soviet Union. Though yes seeping in of politics into research is a big concern.  Another point that I would like to raise is the politicisation of scientific findings. And specifically the factions that bring up non-research related topics in to question it, or try to politicise it to gain a political following (trying to divide people with misinformation). Not political movements that are based on the findings of research and support it, but the ones that oppose it using reasons that are non-academic and unfalsifiable.  Both of these points are justified if one looks at the shit-storm that was the political maneuvering around covid. From claiming that there was no chance that it was developed in a Chinese lab (even though that was FAR from conclusive) and that Covid itself was a hoax/mask weren't necessary (or any version of that).",0,1,0.171,0.234
post51lb,poorly branching,4,"If and only if AI didn't include our own biases, it would be nice/interesting to see what a strategy free from politics, greed, effect on the polls, etc would be. Unfortunately, that's impossible now that it has been trained on our output.",1,0,0.105,0.364
post51lb,poorly branching,4,I don't think we want it completely unbiased. It would essentially be technocrate....which will seem very heartless.,0,0,0.223,0.368
post51lb,poorly branching,4,That would still be a better starting place than what we do now which is to let monied interests and groups like ALEC write laws for us.,0,0,0.176,0.21
post51lb,poorly branching,4,"What makes you think Alec is better?   Sometimes I wonder that maybe letting money and greed dominate is he best case scenario we can have, because at least it's predictable and based on some known factors.   I am not very optimistic of the whole human nature and as history proves that a lot of humans worst deed was done with the most righteous reasoning and good intention.   So if we can't really rid the system of bias, then isn't it better to have it be based on a set of known biased so that at least it can be somewhat predictable and known.",0,2,0.235,0.497
post52hb,richly branching,4,Since when can't humans distinguish race in skulls?,0,0,0.267,0.343
post52hb,richly branching,4,"Since we determined that was ""Problematic""...",0,0,0.266,0.36
post52hb,richly branching,4,"That’s not problematic, the problematic part it using it to say that other races are lesser on not even human",0,0,0.311,0.416
post52hb,richly branching,4,Motte and Bailey.,0,0,0.197,0.207
post52lb,poorly branching,5,Good? I don’t know why we should let people come in then say they hate our country and protest.,0,1,0.319,0.243
post52lb,poorly branching,5,"Especially given what supporters here have said regarding Europe/canada and their leaders/politics, would you be ok with MAGA tourists/visa/whatever holders having their phones searched in Europe/canada and deported for posting bad things about Europe?",0,0,0.122,0.159
post52lb,poorly branching,5,"If Americans were going to Europe and protesting the European govt, yeah kick them out, it’s not their govt to protest.",0,0,0.116,0.171
post52lb,poorly branching,5,What about all of  the American missionaries who go to protest abortion overseas?   Should they be removed for protesting the government?,0,0,0.14,0.215
post52lb,poorly branching,5,So why does the U.S. government describe it as authoritarian and anti-free speech when the Chinese government checks phones and social media of incoming visitors for anti-government content?,0,0,0.13,0.172
post53hb,richly branching,5,"I recently read the book “How to be an Anti-Racist”. The author makes a compelling case that race is largely a human social construct and not a biological classification. He would prefer if races did not exist, and we’re not defined. However, in a world where races, genders, and other classifications of people find themselves unequally treated, these classifications serve a purpose: to identify people in need of special help to correct past and present injustices.   Applying his thoughts, it would seem that this software could be beneficial if used to reverse some of the artifacts of racism, such as through affirmative action or similar programs.   As mentioned in other comments, this software is the ability to be abused. I would suggest that the software be developed focused intently on its benefits, and that it’s use be carefully monitored. If the software’s applications are determined to do more harm than good, retire it.",1,0,0.255,0.468
post53hb,richly branching,5,"I would consider affirmative action a form of abusing of this data. This is immoral, unethical and wrong in so many levels.  I totally agree with you that having no race classifications is the ideal world: where they don't offer any significance as they should.  The problem with affirmative action is: you get a random person from a 'privileged group' and penalize this person in some way in favor of the 'minority'. The problem is that you are generalizing. Maybe a group of people you are fairly removing resources, but for others in the 'privileged group' it is unfair.  Think about not hiring someone who is white, ant-racist activist that could help improve the company diversity by doing a fair management but you opted to hire someone else instead because of this person skin color. You are not doing yourself a favor by doing affirmative actions.",0,2,0.185,0.441
post53hb,richly branching,5,"You just cited a single example and then made the logical jump that affirmative action is bad. We can’t be citing a single example and drawing conclusions. Affirmative action affects millions of people. What is the net benefit/loss? I think in sum, it has significant ability to do go, even though there will surely be cases where it is abused. As a white male American, I know that it gives others an advantage against me, but I want them to have that advantage since in every other way, I have been born into a world that gives me an easier time in 2021. The day when poverty, COVID deaths, infant mortality, etc. are not longer racially unequal issues is the day that we should stop affirmative action.",0,0,0.201,0.271
post53hb,richly branching,5,"By your comment you didn't undesrtand the argument.   But it is ok, no one is preventing you from abdicating of some rights (say quit your job to open a position fo a black people, or give up your higher education in favour of someone with color). What I am against is you force that in someone else... and yes, every time you impose a affirmative action, some individuals will be benefiting and others penalized by the very nature of the idea. And unfortunately, some will be unjustly penalized, just because of skin color.",0,0,0.197,0.467
post53hb,richly branching,5,"Humans have universal rights. One of these is to be judged and treated as an individual, without regard to their race or gender. To treat people with respect and dignity.  \> Instrumentalization or objectification: This aspect refers to treating a person as an instrument or as means to achieve some other goal. This approach builds on Immanuel Kant's moral imperative stipulating that we should treat people as ends or goals in themselves, namely as having ultimate moral worth which should not be instrumentalized.  If you hire/not hire someone with the goal of achieving racial parity, you objectified them, judged them on race, infringed on their inalienable rights.  So therefore affirmative action is inhumane.  You invoke pragmatism: does it work, is it valuable? We can debate about that, but probably won't agree (is torture justified to get life-saving information?). We already blame people for the negative side-effect of thinking someone is a ""diversity hire"", not the policy makers themselves. That's hard to measure.  So let's look at the end goal of affirmative action. What if it works really well and this is valuable? So let's delete or suspend a human right in favor of a new human right. The right of not living in a society which has racial inequality? I do not think there exists such a right! If I move to a small African town, I would live in a tribe with racial inequality. Hopefully that won't be a problem for them or for me! Hopefully this village does not need to enact actions to restore racial equality.  So, to me, you are advocating to suspend an inalienable right (about discrimination and racism!), so we can have something that is not even a right in the first place. I will not have that! Institutional racism (the effects of inequality) is not as bad as it used to be, and you'd be hard pressed to find examples of institutionalized racism (outside affirmative actions at institutions such as universities).",1,0,0.182,0.442
post53lb,poorly branching,16,"so this ties into universal basic income.  universal basic income isn't FI.  it's going to be barely enough to live off of (think current welfare system).    sure, small controlled studies show people on UBI will still work and increase their productivity due to not having to worry about expenses.  but if you apply that on a big enough level, there isn't going to be housing, jobs, whatever to go around.  so then it will create inflation and we're no better off than we started.    once AI and robotics wipes out the industry for basic, repetitive, non cognitive jobs.  there's gonna be the ""working"" class of people and then there's the people dependent on UBI that become ""unemployable"" as jobs get more and more specialized and more technical training is required.    people who will ""work"" will need to start their training at a very young age.  and there's just simply not enough supply/demand for people to ""find out what they love and want to do.""  if you want to be a doctor, you'll go into doctor training at a young age, maybe even starting in high school.  to get into these programs, you apply and face competition harder than getting into ivy leagues.    so the vast majority of people are going to be unemployed and unemployable and just living on UBI.  it'll likely be something akin to that movie dredd where people live in huge housing blocks that are basically ghettos and no where near as glamorous as what people are representing.  thoughts otherwise?",0,0,0.182,0.114
post53lb,poorly branching,16,"I see no reason why there isn't enough wealth in this country to provide everyone with a basic level of housing, healthcare, education, and sustenance. Maybe not right this instant but if machines do end up taking over 90% of labor then there definitely will be. It's just a matter of distributing it. This doesn't necessarily ""cause inflation."" Want more, work more seems fine to me.   I really think that if we achieve a post-scarcity society, then we will need to think beyond capitalism since a lot of capitalist ideas simply won't apply or will cease to work. Say what you will about capitalism but I do not think it is some immutable natural law of human behavior. It is an ideology, a system, that works given a certain set of constraints like the ones we have on earth today.  I think you can envision all of the ""UBI housing"" as futuristic projects/slums full of shitty people, but those places are called ""traps"" for a reason. If everyone has the resources to move freely, no good, rational person would choose to continue living in a trap/hood/project/ghetto/slum. People would likely self-organize into like-minded communities (kinda how they do on facebook/reddit, now there's a scary thought with it's own set of issues). If you get a ""bad apple"" that moves into your community there can be democratic processes to remove them, or you can move yourself away. Like how mods ban trolls. Holy fuck the more I think about this, it might actually be scarier than the Dredd version of the future. Everyone lives in their idyllic echo-chamber safe-space bubbles and if you don't fit in you get kicked out by a futuristic HOA. Lol. Now THAT could be a good black mirror episode.  Why do you assume early hyper-specialization? Machines are *great* at being hyperspecialized, and humans are better at generalized problem-solving and big-picture thinking. Why would you need to start studying to be a doctor at age 7 when there is a specialized machine that can do 90% of every routine surgery there is? Instead humans will need to take a more generalized, integrative approach to learning. Then as a doctor, you will be spending your time thinking about how you can get a machine to do a rare or specialized technique that there isn't already a machine for, or solving more systemic issues that bleed over into other disciplines, like how epidemiology has strong social, psychological, and cultural factors, and how you might approach those to reduce suffering. Things that are strictly human in nature, things that require wrestling with human value systems.  I definitely think that during the TRANSITIONAL period there may be a hyper-stratification of society into the rich elites and a huge underclass of poor. Whether that is a stable state for society depends on how truly well-suited UBI is to satiating human needs and desires.",0,0,0.138,0.219
post53lb,poorly branching,16,"There already is enough wealth for everyone to live comfortably.  It's just not distributed.  Communism China attempted to distribute it and have everyone become productive.  It failed.    In your world where people have the resources to move freely assumes that there is enough supply of homes.  If there are big homes and small homes everyone will want the better homes.  Which by itself creates a supply problem.  Those dependent on UBI will be priced out and not able to freely move as you imagine.    There are four basic classes of labor.    Repetitive, non cognitive.  Such as assembly line workers.  Easy for robots to replace.    Repetitive, cognitive.  Your surgeon bit would fall under here.  It's basically the same thing every time but you have to factor in some differences.  This would also be a self driving car.   Non repetitive, non cognitive. Bookkeeping, clerical, some forms of lawyers.    Non Repetitive, cognitive.  Science, some forms of law, engineering etc.    Humans are good at the last one.  That's the general problem solving and critical thinking.    The doctor who needs to think about how to get a machine to do a rare and specialized surgery would need to start training early on to learn both the medicine aspect and engineering aspects to design such a thing.",1,0,0.143,0.131
post53lb,poorly branching,16,This will only be the case if the government can’t figure out how to distribute the created value from the extra productivity.,0,0,0.296,0.144
post53lb,poorly branching,16,When have they ever managed that?,0,0,0.183,0.215
post53lb,poorly branching,16,Normally the most problems arise from people trying to *stop* the government from effectively carrying out things that are much naturally suited to being public goods/services.,0,0,0.145,0.197
post53lb,poorly branching,16,[deleted],0,0,0.376,0.43
post53lb,poorly branching,16,[deleted],0,0,0.376,0.43
post53lb,poorly branching,16,"Not to mention the process has only barely started for Cashiers.    Supermarkets and Fast Food joints will reduce employment numbers dramatically in the next 10 years tops.  Shelf stackers in retail stores will be needed less and less as 'retail' see more inventory remote (i.e. Amazon, or some kind of go to  a store to experience a good and it gets shipped to you)  Lots more jobs to get wiped out yet!",0,0,0.161,0.08
post53lb,poorly branching,16,Somewhat but not as impactful as what's yet to come.,0,0,0.265,0.214
post53lb,poorly branching,16,">  there's gonna be the ""working"" class of people and then there's the people dependent on UBI that become ""unemployable"" as jobs get more and more specialized and more technical training is required.   I agree with the former, but totally disagree with the latter. With UBI, there would definitely be two very distinct classes of people, those with jobs, and those without. On the flip side, it would be *way* easier for someone to look into getting the necessary training for whatever job they might want, if they don't have to juggle that against their obligations of an existing job if they don't want to starve!",0,0,0.263,0.164
post53lb,poorly branching,16,once AI and robotics takes away all the low skill jobs.  exactly how much training do you think someone will need to reskill for another job?    i'm thinking that the available jobs in the future with AI and robotics is going to be so technical that reskilling will basically be impossible.  lets just look at the current people who's jobs have been lost to automation and understand exactly just how hard it is to retrain for a highly technical job.,0,0,0.172,0.128
post53lb,poorly branching,16,"I'm not saying everyone would be able to do those jobs. The number of people who could potentially learn those skills would be the same as the number that could potentially learn those skills *now*. My point is, you'd have a lot more people *actually* trying to learn more technical skills once they didn't have to try to learn them while also working their mind-numbing, soul-crushing minimum wage job at the same time.",0,0,0.216,0.112
post53lb,poorly branching,16,"The call centers of the future will be people operating drones to carry out tasks off world.   I'm absolutely certain Google's put engineering time into building datacenters on the moon, and logistics companies have to be looking at it too. Commercial space travel will follow the path of commercial air travel, accessible because of the cargo network commercial passengers don't see. Every warehouse, dwelling, and powerplant built will need a drone operator, and teaching you how to use *this model* of drone will just be on the job training.   HIghly paid people of the future will have developed skills in a particular industry. Drone operators with geology degrees who understand how to find good build sites. Architectural designers who understand exometorology well enough to incorporate those design features into the luxury housing market. (Luxury goods and services are usually safe.) Drone operators who understand logistics best practices to keep a warehouse with no human staff running.   It takes a while before technical capability becomes cheap enough to really compete with human labor. Sometimes, it's cheaper to pay 15 people to stare at a screen and push a button if ""something weird"" happens, and those jobs will always exist.  For those times when throwing cheap human labor at a problem is the most economical, we'll have apps and platforms that allow people to throw their labor into that pool on their own schedule. Rather than eating out at the end of a week, you might schedule a professional chef who specializes in your wife's favorite ethnic cuisine and it'll be as easy as getting an Uber.  AI and robotics are just tools. Adapt and learn how to use them as they become relevant to your industry and you'll be fine.",0,0,0.216,0.132
post53lb,poorly branching,16,Inflation is possibly a myth. Inflation in Alaska decreased after they started their UBI-like permanent fund. It was tried in Kuwait and inflation decreased.  The biggest argument against UBI for the United States is it is very different than what we're used to and for this reason I don't think people will go along with it. It's not within the realm of the politically possible.,0,0,0.097,0.169
post53lb,poorly branching,16,"i assume you're refering to this article?    https://medium.com/basic-income/evidence-and-more-evidence-of-the-effect-on-inflation-of-free-money-a3dcc2a9ea9e  all they did in that article was state they gave money to people, and here's what happened to inflation.    it's hard to conclude a cause and effect.",0,0,0.103,0.117
post54hb,richly branching,58,"This is the massive problem with AI. It can seem perfectly accurate, then it turns out the scientists were only testing it on specific subjects for ""reliability"" and ope it turns out that defeats the entire purpose of AI and trains it to literally discriminate just like the people who made it.",1,0,0.265,0.294
post54hb,richly branching,58,"Or the initial training data were skewed one way or another. A similar case was an AI determining if a patient had a disease partially by looking at the hospital that the xray was taken. It did so, because the initial data included cases of a local epidemic which meant the patients location was factored in the ""diagnosis"".",0,0,0.227,0.212
post54hb,richly branching,58,"Oof, that's a huge one.",0,0,0.22,0.287
post54hb,richly branching,58,I heard a case of an AI model that could tell the difference between cancer and a non-cancerous mole by identifying if the photo used had a ruler or measuring device in it. That's one problem with AI models being non-human readable. It's like regex but many times worse,0,0,0.187,0.255
post54hb,richly branching,58,"I’m a little surprised this paper got by the reviewers. They show that sex (female), race (black), and age (older) have lower rates of diagnosis. Women have more breast tissue on average than men, and racial minorities and the elderly correlate with obesity - all of which is known to detrimentally affect Xray image quality. Not one mention in the methods regarding controlling for BMI, chest circumference, or anything like that.",0,0,0.136,0.187
post54hb,richly branching,58,"Well, to be fair, the blood donation center in NZ did that for years.  They wouldn't accept my blood because I had visited the UK in the 10-year window of the BSE occurrences.  And we did that way more recently for COVID, by asking where people had been.",0,0,0.149,0.141
post54hb,richly branching,58,"It’s a not-unreasonable strategy.  It looks like, although it will take a generation or more to know, that the risks of CJD in humans triggered by BSE in meat were overstated.  Incidence of CJD in the UK has not risen substantially, and there were 0 (zero) vCJD (the variant caused by BSE) cases in 2020.   That said, in the 1990s and 2000s no-one knew, the incubation period is long and there had been a lot of BSE in the UK food chain.  Since transmission by blood transfusion has been recorded, and the blood products industry is still recovering from AIDS and hepatitis transmission in the 1980s, broad-spectrum elimination of UK blood from a nation’s supply is and was a reasonable response.",0,0,0.126,0.207
post54hb,richly branching,58,"Neural networks are pattern finding engines, and pattern finding engines *only*. A pattern resulting from biased data is absolutely no different to it from a pattern resulting from actual real world correlations.",0,1,0.092,0.328
post54hb,richly branching,58,We often don't pay attention to all the patterns so we miss crucial ones.    We tried to breed Chcolate Labs for intelligence without realizing that food motiviation accelerates task compliance. So we ended up trying to breed for intelligence snd simply made very hungry dogs.,0,0,0.272,0.186
post54hb,richly branching,58,[deleted],0,0,0.376,0.43
post54hb,richly branching,58,"It’s at least discriminating based on data, unlike doctors who do it based on personal prejudices. Data can be corrected for by adding more training data containing groups that were underweighted in the original dataset. Convincing a doctor to stop giving lousy care to patients in demographics they dislike is a lot harder, not least because they’ll fight to the last to avoid admitting they’re treating some patients based on how they look and not their symptoms.",0,0,0.175,0.425
post54hb,richly branching,58,"> unlike doctors who do it based on personal prejudices  This just isn't true, most of the time. Doctors, as a whole, are probably about as left-leaning as this damned site. And even black doctors perform worse with black patients than they do with white ones.  Why? Because they were trained on the same skewed data these AIs were.   And it's *really* hard to get better data.",0,0,0.157,0.292
post54hb,richly branching,58,"> trains it to literally discriminate just like the people who made it.   Yes: garbage in, garbage out. AI can only replicate our biases, not remove them.  Still, though, once the problem is identified it's not a big mystery how to fix it. It might not be cheap or fast to re-train, but it's not like we don't know how.",0,0,0.251,0.373
post54hb,richly branching,58,"But honestly they'll just use it and say it's fine - they're like who cares about more than half the population.   Medical basis is real and still now is 2025 there is little or nothing being done about - as an example and I tend to use this one a lot is there's *still* no real research into women and how ADHD affects them differently and oestrogen fluctuations, monthly for decades and across their lifetime, affects the systems and severity of this. This is despite 2 conclusions that are know - 1. ADHD is a chronic lack of dopamine in the brain. 2. Oestrogen levels affect dopamine levels.  There have been issues with this reported in the community for *decades* at this point, but it only something that is just beginning to be looked at.",0,1,0.111,0.185
post54hb,richly branching,58,"To also add, they only recently started publishing a visual encyclopedia of how rashes appear on dark skin tones, because even black doctors are taught on the white skin patient standard.",0,0,0.15,0.199
post54hb,richly branching,58,The idea that ADHD is a chronic lack of dopamine in the brain is a misconception or oversimplification as far as I know. It's somewhat more accurate that it includes failures in certain dopamine pathways.,1,0,0.174,0.183
post54hb,richly branching,58,"See also ""a kid is just a small adult, right?""",0,0,0.211,0.174
post54hb,richly branching,58,I'll one-up you on this: There has been only recently a study done on women's peri-menopausal issues with lack of iron due to increased menstrual bleeding.  One of the big issues exclusively for women and only this year someone finally got around to establishing key facts about it.,0,0,0.109,0.111
post54hb,richly branching,58,"How do you fix it? You can’t train it with data you don’t have, and the medical community has routinely minimized the participation of women and minorities in their studies.",0,0,0.223,0.274
post54hb,richly branching,58,"Yep, 100%. Like I said above: replicate our biases.  So you fix it by *getting* that data. Again, like I said, not necessarily cheap or fast; but we know exactly how to do it. We're not back at square one.",0,0,0.258,0.379
post54hb,richly branching,58,"I mean it’s actually rather straightforward to address. Model generalization is often not a priority when engineering AI, because doing it properly will make it seem like it gives marginally worse results (on the biased data you do have).   * Get more data and be more careful about how you sample it * or weight the rarer samples (like black women) higher in training to balance out the importance * Or choose a loss function that penalizes this effect  * Or remove data selectively until the training dataset is more balanced * various other training techniques like regularization and ‘dropout’  I make medical computer vision models and things like robustness and reliability and generalization just aren’t valued by the higher ups as much, because they cant easily show those things off.",0,2,0.214,0.244
post54hb,richly branching,58,"> How do you fix it? You can’t train it with data you don’t have  No, but you can balance training data or use something like SMOTE to correct for this. It's a fairly common problem and there are a lot of techniques to manage it.",0,0,0.267,0.206
post54hb,richly branching,58,"The data most likely already exists but was not part of the training data.  But I think the most interesting observation you can make is that lung scans of women and black people apparently are different from those of white men. Is it how the scans are made or actual biological differences that are significant enough to affect the detection? Why would a black man’s lung scan be significantly different from a white man? Women’s breasts might be an issue, but a male?",1,0,0.154,0.31
post54hb,richly branching,58,"If you think it's the medical community that minimizes it, and not women and minorities that choose not to volunteer for said research then you've done very little research volunteer gathering in your life.",0,0,0.219,0.216
post54hb,richly branching,58,"I think that you're a bit off on how you're reading this, tbh. Garbage in garbage out is a huge simplification, that's simply not true or at the very minimum, not that simple. Models such as ""Noise2Noise"" are pretty clear indications that you can train output of higher quality than input. In this model, they start with clean images, add noise, and then add even more noise. They have a model map More Noise to Less Noise, and get cleaner data than the level Less Noise was at. You throw noisy data in, and get clean data. Of course, good data is important but the GIGO rule isn't some hard fact we can't escape, its not conservation of energy or something.      On the opposite side of things, even if you do identify some kind of bias issue, a subtype that isn't being classified correctly, this doesn't automatically lead you to a solution. The plan fact is, we have many strategies and sometimes, even often, they don't work at all. On the r/learnmachinelearning subreddit right now, there's a post asking if ""SMOTE ever works"". Smote is one such strategy for dealing with under-represented data, standing for Synthetic Minority Oversampling TEchnique. This isn't exactly the same problem being addressed, but its pretty clear we have many more ideas for how to address issues, than we have one-click solutions which actually work.       It is very common in ML to have ""an answer"" for some problem, and it just doesn't work. I don't think you actually need to be in the weeds of technical details to see this is the case.",0,0,0.267,0.289
post54hb,richly branching,58,It's also a problem with data sets available.  Data that AI is trained on tends to be homogenised because data comes from rich places that tend to have homogeneous groups of people.   This is a nuanced issue.,0,0,0.166,0.312
post54hb,richly branching,58,If you go to figure 2 you'll see that the results from the radiologists and the AI largely overlap.   The radiologists had roughly the same shortfall in roughly the same groups.,0,0,0.177,0.21
post54hb,richly branching,58,"Unfortunately, this is a problem with medicine in general.  Up until not that long ago, research trials often used only men because women's pesky hormone system confused the study results. Therefore, the 'results' were only really valid for men, but were used for rx'ing to women as well.  This is a massive problem - with AI, our medical system (good luck being a women in her 50's suffering a heart attack), our justice system, etc.  Bias is not unique to AI, but hopefully we'll pay attention to it more than we do in humans.",0,1,0.155,0.206
post54hb,richly branching,58,"It's the massive problem with the current algorithms that we have started conflating with AI. The current models don't truly ""learn,"" they just identify patterns and replicate them. That foundational approach will forever cause them to be susceptible to replication error and will make them incapable of scaling to generally useful applications.",1,0,0.319,0.144
post54hb,richly branching,58,Hey look it's the X-Box Kinect phenomenon,0,0,0.204,0.17
post54hb,richly branching,58,Good thing the current U.S. administration hasn't effectively banned any research to address this kind of issue from receiving federal funds.,0,0,0.157,0.194
post54hb,richly branching,58,So it’s not a problem with the AI itself but the person operating the AI.   The AI did exactly what it was prompted to do.,0,0,0.193,0.179
post54hb,richly branching,58,"Yeah, then corporations tell us that we can trust everything to AI, meanwhile black resumes get canned because the AI that reads them is built on racist data, because basically all the data america has is tainted by racial bias. These models spit out what we put in, and the world has too much hatred for us to expect anything else out of them.",0,0,0.143,0.226
post54hb,richly branching,58,"Yes. This is technically the case, but it comes with an important caveat.  The tendency of human bias to bleed into AI is almost unavoidable.  I'm not saying it's bad or shouldn't be used or anything, but we need to be wary of treating this as ""just a tool"" that can be used for good or bad depending on the person using it, because this isn't a case where you can just fix it by being cognizant enough.  Bias is innate in us. The methods and procedures we use to test and train these things exacerbates those biases because they are built into the process as assumptions.  In addition to this, sometimes, even if you are intentionally addressing the biases, the bias comes FROM the algorithm itself.  ""Algorithmic oppression"" by safiya noble is a fantastic read on the issue, and uses a very succinct example.  Imagine an algorithm or AI that's trained to put the most popular barbershops at the top of the list.  In a community of 80% white individuals and 20% black, there will NEVER be a case where a barbershop that caters to that specific hair type will ever appear on that algorithm. This inherently means less access to a specific service by a specific group of people.  But also, how would you even TRY to go about solving this issue in the algorithm other than creating 2 different ones altogether?  What new problems might that cause?  This is obviously oversimplified, but it's a real life example of how bias can appear in these systems without that bias existing in the people that create it.",0,0,0.155,0.414
post54hb,richly branching,58,"Bias is not only innate in us, it's a critical in ML as well, critical for analysis itself. Just talking about getting rid of bias, or suggesting we just use two models, are kind of practical examples of this; you can't just ""take out"" the bias.    Anyways, the answer no one will like but is workable is that the model should look at your chest xray and tell you your race, or fat, or old, or in a high background radiation area. Think that would work better than a second, smaller model.",0,0,0.143,0.417
post54hb,richly branching,58,">But also, how would you even TRY to go about solving this issue in the algorithm other than creating 2 different ones altogether?   Modern social media handles it by sorting people by what they like and matching them with similar people.   Do you like [obscure thing] ? Well the system has found the 10 other people in the world that like it and shows you things they like.    Nothing needs universal popularity, you can be popular with one weird group and the algorithm will unite you with them.   It does however automatically put people in a media filter bubble with those most like them which can lead to some weird worldviews.",0,0,0.288,0.255
post54hb,richly branching,58,">Imagine an algorithm or AI that's trained to put the most popular barbershops at the top of the list.  I'm sure that there are lots of problems with AI, but the fact that this is the go-to example doesn't inspire faith in its critics. Ironically, there are so many weird assumptions baked in here that it's hard to know where to start.   Somehow, people manage to find Chinese restaurants and children's clothing stores, even in cities where Chinese people and children are a minority...",0,0,0.241,0.283
post54hb,richly branching,58,This isn't a meaningful argument against AI. It's an argument against researchers using one model and making bold assumptions about it's usefulness.      They can likely create a second model for women or black individuals now that they know the issue.,0,0,0.225,0.281
post54hb,richly branching,58,"It's an argument for more regulation, and to make sure that we never stop verifying.    Imagine somebody didn't do this study, and we got to a point where for costs/insurance reasons, everyone just stopped using actual x-ray technicians and just did whatever the AI told them to?",0,0,0.227,0.233
post54hb,richly branching,58,"This is why proper studies of diagnostic tests of any variety in medicine require multiple stages of study in multiple patient cohorts and settings.   The whole process of clinical validation (not just developing the test) can easily take 5-10y - it takes time to enroll patients into a study, wait for the outcomes to happen, etc.  It’s one reason why anyone who says AI will be widespread in clinical medicine within less than 5y has no idea what they’re talking about.",0,0,0.159,0.162
post54hb,richly branching,58,Its an argument against AI. We clearly are oversold on how it works and implementing it is difficult because we don't understand it. It means we shouldn't adopt it without knowing all the possible issues.   The fact that they keeping coming out with new models is a case against using them because there are so many untested unkowns.    Its like if we had iOS 1 then iOS 5 then next year its a Linux Ubuntu distro. The shift is too great to reliably implement,0,0,0.223,0.205
post54hb,richly branching,58,"If you had a magic box into which you could insert a picture of a person's face, that instantly tests whether a person has cancer, but only 20% of positives are true, and only 20% of carriers are positive. The box is magic, ie you ""dont know all the possible issues"". And the box is wrong more often than it's right. Is that a useful machine that we should definitely use as soon as possible? To me the answer is yes, it's arguably immoral not to use it. If a consenting person gets flagged, they should go get checked by a doctor.",1,1,0.282,0.22
post54hb,richly branching,58,"This is a massive problem with science. Far too many scientists see women and non-whites as ""unnecessary variables"". The ""default white man"" is pervasive across every area of study.",0,0,0.163,0.283
post54hb,richly branching,58,"What a quintessentially 'reddit' take on things....The effectiveness of an predictive AI model is as good as the data set that its trained on.  The availability of data, especially medical data is tricky due to several factors. In this case, the Stanford team which built the chest Xray model (cheXzero) used a dataset of \~400000 chest xray images to train the model, but it seems only 666 (0.16%) of those images actually contained both diagnostic (from a radiologist) and demographic (race, age, sex) data.   In the UWash [study ](https://www.science.org/doi/10.1126/sciadv.adq0305#sec-4)cited in this news article, their findings of AI bias are based on these 666 images which contained the necessary metadata. Its not an issue with the scientists from the Stanford [study ](https://www.nature.com/articles/s41551-022-00936-9#Sec4)\- the more data available for training, the more robust the model will be. Given the limited metadata they had to work with, taking into account demographic biases is outside the scope of their project and they used the full dataset. Its also worth noting (*only because you mention this as an issue*) that only two of the six authors on the Stanford team are white and one of them is female (the rest appear of east/south Asian origin). The UWash team highlighted an important issue with the model that demonstrates major pitfalls in the Stanford model which need to be addressed - but I think the baseless claim that the Stanford team is racist/sexist is very unfair, and its even more unfair to generalize it across scientists.   Its also worth pointing out that the UWash study itself has ""sampling bias"" (not with malicious intent of course though; they had the same limitations as the Stanford team). Their model is trained on only the 666 images with demographic data - no one knows the demographics of the other \~400000 images used. Its difficult to tell whether their findings hold true across the entire data set simply because the necessary metadata doesn't exist. This is the core of the issue here:  Using chest Xray images as an example, medical privacy laws and patient consent can make it difficult to publish these kinds of data to public databases. And that's just the images, nevermind the demographic data. Add that to other variables that need to be controlled (eg quality of the Xray, reliability of patient health records, agreements between database administration and clinical teams etc), its tricky to get a large enough data set to robustly train a ML model while accounting for things like demographics. I'm of the opinion that consent for release of medical data should be a prerequisite and obligation for access to health care (assuming data security is robust and discrete patient identifiers are removed). Likewise, hospitals/clinics should be obliged to upload their data in free-publicly available datasets.",0,1,0.2,0.244
post54hb,richly branching,58,"This isn't a ""Reddit"" take. Go read Invisible Women. Maybe you're part of the problem.",0,0,0.24,0.267
post54hb,richly branching,58,"I mean that's just the fault of our regulations. It's so expensive to run studies that cofounding variables are never worth the risk to any company.  It also doesn't help that people really like to burry their head in the sand and pretend ""races"" aren't different enough to have very different interactions with the same drug.",0,1,0.185,0.234
post54hb,richly branching,58,Most of my peers in my life have been very left leaning. The politics in your echo chamber is causing you more suffering than you realize. Please try to get out of it and attain a more balanced view. You'll be happier and have a more clear picture of the world.,0,1,0.169,0.216
post54hb,richly branching,58,Go read Invisible Women and then tell me that again with a straight face.,0,0,0.233,0.232
post54hb,richly branching,58,"> trains it to literally discriminate just like the people who made it.  After reading the article that might be exactly what they need to do, build discrimination (as in the ability or power to see or make fine distinctions) into the model so to speak.  Reading the chest x-ray of an 80 year old white man compared to a 30 year black woman with the same model is probably not going to yield the best results.",0,0,0.224,0.545
post54hb,richly branching,58,"The upside to discovering its error is to either only use it on the sunset it is good for while giving it additional training for others areas or if that will not work, start from scratch.",0,0,0.219,0.205
post54hb,richly branching,58,"That's not really a problem with AI, though. It's a problem with our methods of training AI.   We've had a very similar issue with automatic hand dryers. Some of the earlier hand dryers worked based on light reflectivity. Guess what - white people have more reflective skin. It refused to dry the hands of people with a critical threshold of melanin in their skin. If they tested with non-white people, they would have realized that their thresholds needed adjustment. We're dealing with something similar here. With all the attention put on racism and equity, we still keep forgetting to implement diversity in our product design.",0,0,0.16,0.345
post54hb,richly branching,58,"It's a problem across a lot of technology and science.          Essentially every image recognition/analysis tool or toy I've ever encountered has had significant issues with darker skinned people.             A disproportionate amount of what we know about humans is mostly from studying European descendants, and men.     Even when it comes or animals, many studies have been limited to males, to reduce complexity and variance.       We really need high quality, diverse public data sets. This is something the government should be funding. AI isn't going away, we need to find ways to make it work for everyone.    Medical diagnostics, of all things, should not be exclusively in private hands.",0,0,0.168,0.313
post54hb,richly branching,58,"As someone who does do AI research in medical stuff,this is actually a pretty good idea. They're one of the few who could actually do it without getting hippa'd",0,0,0.193,0.146
post54hb,richly branching,58,I know of the issue in general but I'm pretty surprised race affects their reading of x-rays of all things.,0,0,0.215,0.263
post54hb,richly branching,58,This isn’t really an “AI” problem. What you are describing is *human error*,0,0,0.188,0.262
post54hb,richly branching,58,"I didn't read the study, but usually, this problem occurs due to lack of data from certain groups of people.  I assume there is simply less data available from black women, and this is usually due to the history of people of African origin, as well as their current living conditions.  We simply have less data available since these people don't visit (for many reasons like poverty) the doctor as often, or since the majority of these people live in countries where we don't have easy ways of collecting data from them.",0,0,0.133,0.175
post54hb,richly branching,58,Because they correctly trained it on the most common cases first. Of course there's always going to be outliers.,0,0,0.222,0.234
post54hb,richly branching,58,Women and blacks are outliers?,0,0,0.194,0.345
post54lb,poorly branching,7,"Non AI-surveillance is more harmful to privacy and more prone to fraud and abuse.  Imagine basic surveillance is encrypted at client, transit and storage, and only becomes readable to humans whenever the risk level is (accurately) estimated above ""...%"".  At that point ""human in the loop"" would need to take over to make the judgement.   In the ideal world, the ""looped human"" will only perceive risks, and not the total scope.",0,0,0.151,0.233
post54lb,poorly branching,7,"That is actually what I am trying to build. A system that automatically dectects and tracks crime at the moment it occurs, avoiding to store anynon related data.  It has been a bit challenging.",0,0,0.122,0.199
post54lb,poorly branching,7,"Sounds good! I applaud your courage.  This might help you for initial PoC:  Use 2 SBC/camera systems, 1 regular and 1 ""ai vision"".  The regular camera system will be an ""on call""-cam. It needs a endpoint/websocket to activate the stream on a specific client (your web-ui or local app ui).  Then try to get some standard data for the ai vision -system. Make it detect an apple, and when it does so, task the ""on call""-cam to do the websocket thing to your client. So every time the apple is displayed, the stream will be opened in your client.  Finetune to a usable experience (what about multiple streams, closing it, applying follow-ups and some enterprise like users-system with rights/roles -structures.      Then get it to work for non-apples. This will mean that you need to get loads of data. From my experience/knowledge (which is limited) it'll be way better to train a specific model per specific crime, and then apply the models at the same time (checking for their specific crime) on the same input-stream.  if you come up with something that works, this would be the point where it would be wise to seek funding; I don't think a single engineer with limited time and budget will be able to achieve a production-grade system (within this niche) **and** is actually able to sell it.",0,1,0.2,0.138
post54lb,poorly branching,7,Hey thanks a lot for the advice I am actually building an open source community.   Here is the web page: https://www.opear.org  This is the initial model for crime detection https://huggingface.co/OPear/videomae-large-finetuned-UCF-Crime,1,0,0.204,0.187
post54lb,poorly branching,7,"The key to balancing AI surveillance and privacy lies in implementing robust safeguards. Prioritize encryption and decentralized data storage to keep sensitive information secure. Opt for user-controlled data retention and local processing of data by AI models for enhanced privacy protection. Open-source algorithms foster transparency and auditability. While challenges exist, designing AI surveillance systems with these features can help align technology with privacy concerns. Let's keep the conversation going on how to strike the right balance!",0,1,0.145,0.183
post54lb,poorly branching,7,"Hey georgy I laid out a roadmap and a system design here https://www.opear.org/developers  Do you agree with it, what would you add or take",0,0,0.278,0.214
post54lb,poorly branching,7,Pretty cool 👌,0,0,0.455,0.277
post55hb,richly branching,7,"From the paper:  >We were limited by the availability of racial identity labels and the small cohorts of patients from many racial identity categories. As such, we focused on Whites, Blacks and Asians, excluding patient populations which were too small to adequately analyse (for example, Native American patients) and excluding Hispanic labels due to variations in how this label was recorded across datasets.  Sounds like some of the accuracy is due to a constrained data set.",0,0,0.181,0.351
post55hb,richly branching,7,"I can bet you they didn’t include representative samples from the African continent. There is more genetic variation among two Khoisan people from different tribes than all humanity outside Africa. So for the most part, they are assigning people to the “races” present in the US which represent a small subset of the genetic variation in humans.",0,0,0.194,0.383
post55hb,richly branching,7,"I mean, they have to take a subset from somewhere. There is no more racially diverse place on earth than the United States, so it seems like it would make for a decent balanced dataset for categorical image classification. Collecting the adequate data to deal with the problems you bring up is likely far too expensive and logistically nightmarish, as is the case for collecting most really good data.",0,1,0.102,0.279
post55hb,richly branching,7,Why do you say this? There is more genetic variation within Africa than North America.,0,0,0.179,0.193
post55hb,richly branching,7,[deleted],0,0,0.376,0.43
post55hb,richly branching,7,Terrible take,0,1,0.363,0.631
post55hb,richly branching,7,"Turns out practically infinite datasets are pretty rare in the real world.  Either some behavior tied to the internet (ad placement, movie selection, spam filtering); or else a made-up game like chess where you can create your own generative process.  Get much beyond that and you are working for every sample.",0,0,0.218,0.266
post55lb,poorly branching,5,"What you’re describing is a long standing problem with facial recognition misidentifying POC.   You didn’t know AI is racist, sexist & also right wing? Even the “most advanced” to date still has the same problems. It’s been an issue for years.  https://www.bloomberg.com/news/newsletters/2022-12-08/chatgpt-open-ai-s-chatbot-is-spitting-out-biased-sexist-results  https://www.newstatesman.com/quickfire/2022/12/chatgpt-shows-ai-racism-problem",0,1,0.181,0.352
post55lb,poorly branching,5,"I truly didn't. I've always stayed away from techy stuff like that, so I didn't think much about AI. I didn't realize how much AI is involved in things these days. Sorry if I came off as deliberately ignorant.",0,0,0.236,0.149
post55lb,poorly branching,5,"No, I’m not either and it’s extremely creepy. The first I really started knowing about it was 2016 and the Brexit vote here. That was pure data scraping & manipulation (Cambridge Analytica) but it also flagged up how bad Facebook algorithms were (ARE) and how they constantly promote hate and misinformation.   AI still learns from the data it’s given. Even Reddit- I easily spend most of my time on this sub, but in their “year in review” thingy they have, this sub isn’t mentioned ONCE. And there’s no metric that makes this make sense, except what seems like *deliberate* shadow banning (nothing to do with posts, size of sub, frequency of my interactions, nada).   IS it deliberate? Or is it that AI has learnt that SO MUCH content on here is misogynistic that it’s ASSUMED I would have no interest in feminism?",1,2,0.203,0.257
post55lb,poorly branching,5,As I understand it  the issue with subs not appearing in the latest “year in review” is the data came from the ad analytics so subs without ads were excluded. Not so much a deliberate shadow ban as didn’t think that significant part of their user base spend their time in the political section of the “internet’s newspaper” to the point it was notable that those subs were excluded.,0,0,0.167,0.326
post55lb,poorly branching,5,"Ahh. I guess I didn't realize just how much AI is used.   Also, this is probably the dumbest question ever... What did you mean by ""I'm not, either"" and ""it's extremely creepy"". I'm assuming the ""it"" refers to sexist AI? Sorry if I sound like I'm arguing. I'm autistic and have a hard time understanding people sometimes.",0,2,0.22,0.26
post56hb,richly branching,25,Ghassemi (comp scientist @ MIT) believes it's based on melanin.  Goodman (bio anthropologist @ Hampshire) believes it's based on geography.  Both proposals pretty logical and not as controversial as it would seem based on the headline.,0,0,0.194,0.26
post56hb,richly branching,25,Those biased datasets! I could believe both after having to look at a ton of pneumonia X-rays for a machine learning model demo.,0,1,0.227,0.351
post56hb,richly branching,25,i remember hearing about one of those pneumonia detection models that instead of detecting pneumonia detected whether or not the patient was lying on their back,0,0,0.18,0.248
post56hb,richly branching,25,"Isn't ""based on geography"" (i.e., geographic ancestry) essentially the same as ""based on race"" since they are so highly correlated anyway? Why feign confusion? There's nothing wrong with noting that there are groupings of traits that tend to correlate with the construct of ""race"" unless you yourself believe that those traits make someone inferior or superior.",0,1,0.106,0.308
post56hb,richly branching,25,No,0,0,0.336,0.366
post56hb,richly branching,25,">No  Seems to me like it is a semantic argument akin to the one I assumed:  >So, while AI might be able to determine from an X-ray whether a person’s ancestors were from Scandinavia, Africa or Asia, Goodman says it’s not about race. “You call this race. I call this geographical variation,” said Goodman — but he did admit it’s unclear how AI could detect geographical location from an X-ray.  Can you please explain how I'm wrong besides saying ""No?"" This is a science subreddit.  Edit: Saying that people from different geographic origins tend to share certain traits and that those traits correspond with the concept of race more often than not should not be controversial in and of itself. A scientist should not be throwing up their hands and basically saying ""Who could have seen this coming?!"" to avoid citing the obvious explanation.",0,2,0.222,0.249
post56hb,richly branching,25,"it would if you could tie geography and race, considering race is a socially constructed set of boxes that break themselves over and over again, maybe ethnicity? but then how do you measure that, they are ranges usually being based on common ancestry and muddle again due to migration, traders.     Geography can tie to melanin rates due to local adaption to specific geography...    While if it was ""race"", then somehow the same group of people settled very specifically the same environment across the same latitudes and altitudes",0,0,0.174,0.277
post56hb,richly branching,25,"Could it be based (or partially based) on bone density or shape? I’m from Africa and it’s pretty common “knowledge” that native African people have much stronger bones than the seemingly more fragile Caucasian population. Obviously it’s anecdotal and not a fact, but from my own experiences it certainly feels true.",0,0,0.121,0.141
post56hb,richly branching,25,"I was under the impression that this was similar to the belief that certain races feel less pain, a common myth based in racism. However, it seems that racial differences in bone density do exist and it is a current area of research  https://pubmed.ncbi.nlm.nih.gov/9024231/",0,0,0.139,0.259
post56hb,richly branching,25,"What amazes me is, growing up, how apartheid racists always managed to turn something like being *literally* physically stronger and more resilient (since most labourers were African, they generally were incredibly physically fit and active) into something negative. They’d say, “never hit a k****r in the head because you’ll break your hand on their thick skulls”.  Now when you grow up hearing things like that day after day, generation after generation, you can see why racism became less about racial/cultural differences and more about hating other people for being different because you were told they’re bad. It becomes institutional.  But then people like my mother would hire housekeepers and gardeners and pay them triple what anyone else would pay, she’d buy them food and baby clothes because “even with what I’m paying, I know other people aren’t paying that and even I couldn’t survive on that kind of wage with 4 or 5 kids”. As such, the loyalty and friendships she made were with everybody and at least in that little home bubble, racism didn’t exist.   She would always insist that the racists were mocking black people for being uneducated, but then took away any chance of an education they had with white-only schools. Making a group of people a certain way by denying them basic human rights and then mocking them for it is the height of human evil.",0,0,0.188,0.287
post56hb,richly branching,25,"Redheads/gingers literally do feel less pain though, they have mutant pain receptors which is also why they require higher dosages of painkillers in hospitals",0,0,0.141,0.18
post56hb,richly branching,25,"Yeah, but is it really racial? Isn't it time we put the whole ""race"" thing to bed and actually come up with a scientific term?",0,0,0.233,0.267
post56hb,richly branching,25,"I think this is probably a big factor in it, there are proven differences between black and white people in bone density but there are definitely outliers in these situations.  I'm Caucasian but I have a top 0.3% bone density measure and so does my mother, though she's Sicilian which is near Africa.  My father does too but he's Scottish... and a laborer - you can build bone density by lifting or putting stress on your bones, so a lot of the differences do come from if you are a laborer or not, but I think a lot of it is genetic given that I had that super high bone density before I even started lifting weights.  I played video games all day as a kid and somehow have bones like wolverine and I suspect its similar with the differences between ethnicities on a high level.  However, to overall broadly say Africans have stronger bones would likely be wrong given that the diversity of genetics amongst African's is actually greater than between African's and Caucasians! Pretty crazy right?",0,1,0.081,0.233
post56hb,richly branching,25,There must be outliers in the AI correctiveness as well. I’m sure it’s not 100% right about the persons race all the time. It could easily be facial structure or small variation on a variety bone shapes that we wouldn’t notice day to day.  It’s silly to pretend there could be no differences between race’s bones.,0,0,0.223,0.33
post56hb,richly branching,25,You are incorrect. Wolffs Law says bone density is directly proportional to stress put on the bone. Pretty cool stuff to read up on if you’re ever bored.,0,0,0.128,0.168
post56hb,richly branching,25,But there may be differences in structure or shape beyond that? I don’t think that’s a crazy hypothesis.,0,0,0.199,0.178
post56hb,richly branching,25,I don’t think this is actually true. Can you find a credible source?,0,0,0.204,0.245
post56hb,richly branching,25,"As I mentioned, it’s purely anecdotal and isn’t a fact. So no, no sources I’m afraid.",0,0,0.201,0.173
post56hb,richly branching,25,"How so? there's more genetic variation within African populations that outside combined, due to the genetic bottleneck when human left the continent.    Feels like it's basing everything on it's one understanding of what ""black"" is, which is going to be western populations as that's where a majority of the african american population came from due to the slave trade.     I'd give up that idea if they can replicate showing that Hadza, pygmy, Bantu, are included, as well as Afro asiatic, does it put Berber as originating from Africa?     and then I would want to see it be able to exclude similarly high melanin groups like Austronesian groups",0,0,0.101,0.209
post56hb,richly branching,25,most likely on the known factors of differentiation in bone density,0,0,0.123,0.132
post56hb,richly branching,25,"From the study itself, once you click thru the popular news article:  > To conclude, our study showed that medical AI systems can easily learn to recognise self-reported racial identity from medical images, and that this capability is extremely difficult to isolate. We found that patient racial identity was readily learnable from medical imaging data alone, and could be generalised to external environments and across multiple imaging modalities.  Both of the supplies explanations are not supported by the study. This is fuckin weird, the study shows that race may not strictly be a social construct.",0,1,0.202,0.361
post56hb,richly branching,25,< the study shows that race may not strictly be a social construct.  I can't believe anyone literally thining that races are solely a social construct,0,0,0.201,0.323
post56hb,richly branching,25,">< the study shows that race may not strictly be a social construct. > >I can't believe anyone literally thining that races are solely a social construct  The fact of the matter is that race *is* a social construct, but the way race is constructed is so highly correlated with ancestry in most cases that the difference isn't worth mentioning. What is racist is not to admit this fact, but to believe that these measurable differences have any effect on  someone's intrinsic worth as a human being, on their potential, on their moral character, etc.",0,0,0.198,0.271
post56hb,richly branching,25,"Agreed. Put 5 Vietnamese people and 5 Norwegians together and tell me race is a social construct.   I understand wanting to be very careful when we have used things like phrenology in the past to justify treating different groups of people differently, but science should be completely based in objective truth. Really we need to stop bending science to whatever social truth we are trying to prove.",0,0,0.187,0.299
post56hb,richly branching,25,Don't act like you have some magic info no one else found lol,0,0,0.195,0.267
post57hb,richly branching,8,That's a hell of a consistent bias for women.   Oh well. They learn from their training data and rlhf.,0,0,0.24,0.355
post57hb,richly branching,8,Yep. [Women are wonderful effect](https://en.wikipedia.org/wiki/Women-are-wonderful_effect),0,0,0.276,0.19
post57hb,richly branching,8,It’s genuinely amazing that the way we’re going about building artificial intelligence is by meticulously recreating every single human bias within it. Yud must be really angry about that in particular.,0,0,0.235,0.31
post57hb,richly branching,8,"I don't think this is fair; LLMs are just fancy text prediction, they will obviously recreate whatever biases exist on the internet. The (English-language) internet -- at least in the social spheres where resumes get discussed -- has a strong bias towards women. Many of these social spheres are literally Reddit.",0,0,0.224,0.374
post57hb,richly branching,8,This might be the real solution to the alignment issue; stuff it full of our own biases and neuroses. GPT-5 will be aligned towards sitting on the couch alone late at night eating cheetos watching broadcast news,0,0,0.195,0.221
post57hb,richly branching,8,"As the saying goes, garbage in, garbage out.",0,0,0.31,0.442
post57hb,richly branching,8,"well if they're training it on some massive collection of data from tons of people... it's reasonable to assume it will act like a median person. Not a saint, not a demon, just average.   So how do you sanitize the amount of data that an LLM needs?",0,0,0.179,0.221
post57hb,richly branching,8,"Interesting that it seems to be consistent across profession. Big analyses of humans with similarly randomized CVs find that the bias depends on the gender makeup of the profession.   https://www.researchgate.net/publication/361642927_A_large-scale_field_experiment_on_occupational_gender_segregation_and_hiring_discrimination  > Women received around 50% fewer callbacks  than men in the selected male- dominated occupations, while they received over 40% more  callbacks for the selected female- dominated occupations  Though eyeballing [the graph](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c72431-1aee-491d-80e5-407abc716895_2968x4172.png) the extent of the effect from LLMs seems to roughly correlate with the gender makeup of the profession, but with the middle point shifted.",1,0,0.133,0.379
post58hb,richly branching,5,Which gender analysis tool are you using?,0,0,0.218,0.304
post58hb,richly branching,5,I used Nyckel: https://www.nyckel.com/pretrained-classifiers/gender-detector/,0,0,0.248,0.293
post58hb,richly branching,5,"Not on HRT yet and its giving me 90% confident I'm a woman in every photo (no makeup, straight headshot).",0,0,0.187,0.19
post58hb,richly branching,5,Slay,0,0,0.363,0.404
post58hb,richly branching,5,I tried it and it said man 98% confidence…  …Ima go get high now…thanks 💀 (I am only one month though),1,0,0.218,0.141
post59hb,richly branching,34,"AI has barely begun. I’d rather Apple take their time than release something half baked, like googles variant telling you to eat rocks or whatever….",0,0,0.202,0.194
post59hb,richly branching,34,"Google’s variant was trained on Reddit, and it shows.",0,0,0.276,0.244
post59hb,richly branching,34,It knows how to fix this weird problem I’m having because there’s a 3 year old thread on some sub I’ve never heard of?  ^(Yeah yeah I can guess what you mean),0,1,0.269,0.399
post59hb,richly branching,34,>rather Apple take their time  The issue is that Apple hasn’t even really started,0,0,0.165,0.191
post59hb,richly branching,34,Apple is notoriously secretive. So it's hard to know conclusively what they've been doing behind the scenes and we just have to rely on leaks or rumors.,0,0,0.17,0.162
post59hb,richly branching,34,"Apple has been building minor ML capabilities throughout their products for YEARS. You don’t just jump straight into genAI with no foundation. They’ve also been laying groundwork for this with Apple Silicon and how well optimized it is for ML and AI workloads. Apple is making good long term decisions here.   This is also totally consistent with all other major product decisions at Apple. They wait, observe, deliver their own products when they’re ready, then iterate on them. Every majorly successful product for Apple has been late to the game",0,0,0.183,0.127
post59hb,richly branching,34,"They have started. There’s a significant amount of AI/ML in many parts of their apps. It’s just not in your face. And clearly Siri hasn’t benefited, but that’s okay because I don’t need her to tell me to put glue on my pizza or something similarly obtuse.",0,0,0.148,0.162
post59hb,richly branching,34,[removed],0,0,0.366,0.413
post59hb,richly branching,34,"But they don't always provide more polished solutions.  Siri has been around for over 10 years now, and she's much worse than Alexa and Google.  The homepod is a great speaker, but it's not a great *smart* speaker.    Siri needs better language processing capabilities so she can better understand what people want to do if they don't say it exactly the way Siri wants.  She could also use a 3rd party store of some kind to extend functionality.",0,0,0.184,0.126
post59hb,richly branching,34,"As a long time iPhone user who also uses android, Apple providing more polished solutions today is just isn't true anymore.",0,0,0.156,0.149
post59hb,richly branching,34,The OpenAI thing is official as in confirmed by either company? Because as of my Google search just now that’s just “reportedly” which isn’t worth much.,0,0,0.18,0.154
post59hb,richly branching,34,"I mean, they have, and it’s good so far. See AI in the photos app and camera for example",0,1,0.152,0.122
post59hb,richly branching,34,I started using Google photos because I have a paid Google email account for $15 a month and it comes with 2TB of cloud storage which is a way better deal than what iCloud costs and their AI stuff with photos is just so much better.,0,0,0.176,0.144
post59hb,richly branching,34,They’re really far behind the competition,0,0,0.21,0.259
post59hb,richly branching,34,As if you know the solution. What are they missing?,0,0,0.243,0.385
post59hb,richly branching,34,According to whom?,0,0,0.313,0.297
post59hb,richly branching,34,Yup Apple always waits they never want to be the first one but wants to be the best one,0,0,0.189,0.19
post59hb,richly branching,34,Man how long are they waiting to make Siri good?,0,0,0.167,0.22
post59hb,richly branching,34,Since it released,0,0,0.278,0.285
post59hb,richly branching,34,Siri is not generative AI. It’s  a voice assistant that basically runs on a set of pre defined commands and actions but then falls back to a bing search when it doesn’t understand,0,0,0.171,0.165
post59hb,richly branching,34,Except when they are late and shit.,0,0,0.218,0.353
post59hb,richly branching,34,I said “wants to” not “is the best one” they surely mess up,0,0,0.272,0.275
post59hb,richly branching,34,"What do you mean? The AI stuff already in iOS is pretty good from what I’ve seen. And Siri does NOT use AI, at least not yet",0,1,0.14,0.109
post59hb,richly branching,34,"Like Siri (not late but I digress), Apple TV devices, Vision Pro, Newton, Pippin, etc.",0,0,0.229,0.126
post59hb,richly branching,34,[deleted],0,0,0.376,0.43
post59hb,richly branching,34,"Siri doesn’t use AI, not yet anyway",0,0,0.153,0.169
post59hb,richly branching,34,"Yep, like they did with EVs.",0,0,0.173,0.147
post59hb,richly branching,34,"Well it wasn’t EV’s, it’s more the common sense logic that self driving cars aren’t ever going to be viable, at least not yet. How many years have Tesla’s had self driving hardware you can’t even use? What a terrible waste imo",0,2,0.135,0.154
post59hb,richly branching,34,"So Apple have released an EV then, just without the self driving as that’s not yet relevant?",0,0,0.138,0.168
post59hb,richly branching,34,"The thing is, you can’t really “take your time” with AI like you can with other products. It’s not something tangible, like a new phone or a new watch. It takes time to train and fine tune a model, time that Apple’s competitors have already spent and are now able to iterate on their models. If Apple had been working on AI for years in the background, there wouldn’t be all these reports of Apple being behind. For all we know, they started last year, when these other companies have started years ago.",0,0,0.147,0.164
post59hb,richly branching,34,"Ultimately Apple doesn’t care. They’re still selling devices in the millions, yes they have to catch up but they can wait as long as they want. They aren’t exactly struggling.   The rumours say all new AI stuff will be using third party like google and chat GPT, they are planning something of their own but it’ll come in the future",0,0,0.12,0.158
post59hb,richly branching,34,"I don’t think that’s a good way to look at things. Just because they’re “selling devices in the millions” now doesn’t mean at some point they can’t dry up because they’re unable to innovate and predict industry trends. In fact, Steve Jobs’s ability to predict industry trends is the entire reason Apple is still here to begin with.  Do you really think Apple wants to use ChatGPT or Gemini? No, they want to run their own models with their own weights. But they can’t because, as mentioned in another article, they only realized how good AI could be after using Microsoft Copilot. AI is one of the few things where time, not money, is more important. Who knows if their own model by next year will even reach GPT-4o capabilities, let alone GPT-5.",0,1,0.154,0.122
post59hb,richly branching,34,[deleted],0,0,0.376,0.43
post59hb,richly branching,34,And look at them now lol,0,0,0.252,0.24
post5hb,richly branching,47,How do people construe facial recognition misidentifications as evidence of the system being racist? The word has lost all its meaning thanks to articles like this.,1,0,0.177,0.418
post5hb,richly branching,47,"No, it still means what it means. No one has to use facial recognition software that doesn't actually recognize faces. The people who work on this software could actually  test it to make sure it works before calling it finished. What's the functional difference between bias in the system and being apathetic about contributing to the bias?",0,0,0.175,0.401
post5hb,richly branching,47,"I agree with you. I just think it's at least worth mentioning the fact that facial recognition inherently needs more light for darker skin. To implement it when there isn't yet a 0% bias rate is racist, yes (and I wouldn't want it implemented then either).",0,0,0.179,0.363
post5hb,richly branching,47,Two things:  A) Racial bias in algorithms is a very common phenomenon and needs to be addressed.  B) The police are still trying to use said algorithm EVEN THOUGH IT CAN'T TELL PEOPLE APART.,0,0,0.13,0.471
post5hb,richly branching,47,">A) Racial bias in algorithms is a very common phenomenon and needs to be addressed.  Imagine just making up bullshit because you want to look progressive and then having 25+ people agree with you all based on literally nothing. Does ""racial bias"" exist in algorithms? Sure. Is it ""very common""? Not even remotely. And like most racist conspiracies on reddit this dog-whistle is an the alt-right talking point about white people being inherently smarter than minorities since a machine says so. It's sad to see this racist bullshit upvoted but not exactly surprising",0,1,0.179,0.377
post5hb,richly branching,47,"Ok well.... this isn’t a hard thing to find out. I do have a computer science background, but this is doesn’t require a deep understanding of ML to get. Then there “25+ disagreeing with me”. That last half was just incoherent. How am I being racist? Buddy are you ok?",0,0,0.258,0.319
post5hb,richly branching,47,"Racial bias in algorithms is extremely uncommon.  Racial bias in ML training data sets is a lot more common though, often leading to biased models.",0,1,0.164,0.432
post5hb,richly branching,47,"It’s more common than you think. It isn’t always things as overt as a facial recognition program thinking all brown people look alike. And it usually isn’t intentional.  Data training sets for ai is one cause, but not the only one.",0,0,0.137,0.278
post5hb,richly branching,47,I agree and I am aware.,0,0,0.328,0.203
post5hb,richly branching,47,[removed],0,0,0.366,0.413
post5hb,richly branching,47,Uh..... how?,0,0,0.231,0.237
post5hb,richly branching,47,"It's not used as conclusive evidence, it just narrows down lists of suspects.",0,0,0.259,0.226
post5hb,richly branching,47,"“racial bias in algorithms” just tells me you know nothing about algorithms.  Pattern recognition systems (which aren’t actually algorithms) (that people mislabel AI) based on racist training data can be racist, but that same system based on non-racist data won’t be racist.",0,0,0.174,0.468
post5hb,richly branching,47,"You sound like you read the first chapter of a 15 year old book on java.   AI isn’t just one thing. In this context, we mean machine learning algorithms. In others it could mean software that emulates human decision making.  And a pattern recognition system is without a doubt an algorithm. It is an extremely broad term.  Talk about proving you don’t know what you are talking about.",0,1,0.2,0.217
post5hb,richly branching,47,"Because the police, electing to use this flawed and biased system, will end up having more false positives among minorities with black or brown skin.   As such, the net effect is a racist system.   The meaning of the word is fully sound.",0,1,0.263,0.443
post5hb,richly branching,47,"I agree. It's the way in which the technology is used that is racist, not the technology itself alone (which the article seems to not clarify).",0,0,0.181,0.333
post5hb,richly branching,47,"> It's the way in which the technology is used that is racist, not the technology itself alone (which the article seems to not clarify).    From the beginning of the article:    > But there’s two factors that need screaming above all others when it comes to the debate surrounding facial recognition.   > One: it’s racist.   > Two: it doesn’t even work.   > **Technology never exists in a vacuum.**  For now, humans are still responsible for the production of new digital systems; and that means they come into being with all the biases and fallibility of their creators baked right into their code.    Jesus fuck.    But yeah, keep circlejerking about ""omg smh people calling everyone racist these days are debasing the super-great English language (that I don't even bother to read).""",0,0,0.131,0.305
post5hb,richly branching,47,">will end up having more false positives among minorities with black or brown skin  Which means the system will be better at tracking down white people than those minorities.  Did you even consider that angle? And I'd even go as far as to assume you'd be even more angry if it was the other way round, if the system eg were best at tracking down black people. Police using tech thats only good at tracking down black people, imagine the screams about racism. But if its aimed at white people, then thats racist to non-white people too?  &#x200B;  And yes, that what makes it ridiculous that overly woke people - particuarly an american stream of politics - try to apply racism to anything, pretending its such a sensitive topic, but then cant even make a good judgement call on it.",0,1,0.165,0.309
post5hb,richly branching,47,"If they use facial recognition to incorrectly prosecute people of colour, or at the very least harass them and arrest them and waste their time because the facial recognition technology popped a false positive, then it’s harming black people. If it’s accurately recognising white criminals, doing the crime — then it’s working as intended.   What the fuck were you even thinking here? It is accurate with white people, poor white criminals getting caught?   It’s not ‘aimed’ at white people, it’s not being used to persecute or prosecute *just white people*, fucking obviously.   Alright, I’ll break it down.   * The system for facial recognition is being applied to the general public. All video and photo relevant to a crime, supposedly, will be subject to this technology to identify perpetrators.  1. For white people, it’s accurate and the criminal is caught. 2. For people of colour, they get false positives, bring innocent people down to the precinct, subject them to inquiry, and possibly say ‘we have data that proves you were there!’.  Let’s not get into what’s ‘harmful’: you’re seeing how the system is discriminating between race? Yes?  That’s what makes it racist.  From there, we can better argue as to why it’s more harmful to be incorrectly identified for crimes you didn’t commit...",1,1,0.195,0.417
post5hb,richly branching,47,"This entire post and thread are absolutely baffling  1.	This entire problem is a symptom of technological drawbacks where cameras can contrast objects easier against light backgrounds. Not systematic racism or “racist data” 2.	Given the above, even using the system would likely perpetuate criminal stereotypes against white people as they would be far easier to be recognized, not brown/black like everybody is implying.  y’all are dumb as hell",0,1,0.159,0.38
post5hb,richly branching,47,>Which means the system will be better at tracking down white people than those minorities.  This is assuming that the cops have the integrity to acknowledge the possibility of false positives and not use this in cases involving black or brown people. Realistically it's going to put innocent minorities in prison while the increased accuracy for white people means it will (correctly) exonerate innocent white people.,0,0,0.197,0.356
post5hb,richly branching,47,So you’re telling me there are gonna be two faces that are identical in every way and the only difference between them is going to be skin color? Doesn’t seem likely.  Skin color isnt going to be a determining factor when recognizing faces.,0,0,0.212,0.289
post5hb,richly branching,47,[deleted],0,0,0.376,0.43
post5hb,richly branching,47,Because it's disproportionately inaccurate for nonwhite faces?,0,0,0.242,0.311
post5hb,richly branching,47,"It’s not racism, it’s physics. They’re based on light, and dark people reflect less. Really light skinned black people and pale mexicans are recognized as easily as white people.  Y’all are acting like photons are racist and proving OP’s point.",0,0,0.163,0.308
post5hb,richly branching,47,"Phenology was physical that didn't make it magically not racist. There's no physical reason that a camera can't pick up a black face just as well as a white one, it's a failure of the neural nets analyzing the image. You feed them a biased dataset and they cannot do anything but recreate that bias. You feed them disproportionate white faces and it will be disproportionately inaccurate on nonwhite faces.",0,1,0.141,0.323
post5hb,richly branching,47,[deleted],0,0,0.376,0.43
post5hb,richly branching,47,"If the AI was hugely disproportionately misidentifying white people, and this faulty AI was used by police to flag potential criminals, I have a feeling it would not be being used by the police right now.",0,0,0.128,0.287
post5hb,richly branching,47,"Well nobody seems comfortable calling the tool racist, nor the tool's makers, nor the tool's users, and yet from that combination some racism magically happens so I don't know what part of the pipeline you want to attribute it to but it's not there by accident.",0,0,0.229,0.336
post5hb,richly branching,47,"By the system, I mean the technology itself in isolation.",0,1,0.242,0.174
post5hb,richly branching,47,"It works differently for people of different races. Insofar as a *thing* can be racist, the algorithm is racist.",0,0,0.169,0.474
post5hb,richly branching,47,Confirmation bias.,0,0,0.332,0.607
post5hb,richly branching,47,"Racism means something different depending on who you ask, but if you ask me the intention behind this system is irrelevant. The fact that the effect is a problem specifically for people of some ethnic group/race means it's ""racist"".  So if the system they're using misidentifies black people really easily, and that results in more black people being arrested falsely and then released, you could say that it's racist. Even if you're giving them the benefit of the doubt, and saying it was an accident.  That's how a lot of people define racism. The reason is that you can't prove someone's intentions that easily, all that matters is the effect.",0,0,0.236,0.374
post5hb,richly branching,47,"Facial recognition systems can’t spot black people because they don’t reflect enough light to find their facial features. On a 2d contrast based image, especially if it’s low resolution or poorly lit their facial features simply don’t stand out enough to be detected.  Imagine if you saw four different black people on a grainy, low resolution security camera with bad lighting 20 feet from the camera. Could *you* tell who they are? Would that make you racist?  This isn’t racism and you’re just deluding the term by calling it racism. It’s a poorly designed system that we shouldn’t be using, but it’s not racist.",0,0,0.14,0.379
post5hb,richly branching,47,"It's like you didn't even read my comment.  If every person only had a grainy photo as an ID, and because of that I could only identify and let through people with light skin tones, that system would be considered racist.  Even if I had no malicious intent at all, the result of my actions and the system would be that people with dark skin tones are stopped because their id can't be verified, while only people with light skin tones are let through.  There are plenty of faulty systems like this in the world that disproportionately affect people that are poor, live in a certain area, or have a certain appearance. Whether it's intentional or not is irrelevant when determining if it's discriminatory.",0,1,0.2,0.522
post5hb,richly branching,47,"If a facial recognition system is significantly more likely to misidentify black people, then it's just bad design and unconscious bias by the people who fucked up when they were training the algorithm.  I a facial recognition system is so likely to misidentify black people that it's as bad as random chance *and the cops know this and insist on using it as evidence against black people anyway*, it's getting kinda racist.",0,0,0.133,0.388
post5hb,richly branching,47,"Because the system has to have input from developers on how to read the data it's receiving. If the developer is racist/can't tell black and brown people apart, then the system will have the same issues. The system in and of itself isn't conscious enough to be 'racist', no, but if it was programmed by people who are (and research shows that even the most loveral hippy dippy white person has some latent racist responses) the end result is the same.",0,0,0.144,0.38
post5hb,richly branching,47,"It seems more likely that misidentifications will occur more frequently with people who have darker skin simply because it's more difficult to identify facial features due to less light being reflected from their faces, therefore creating less data for the software to work with.",0,0,0.213,0.291
post5hb,richly branching,47,"The problem isn’t with lighting and instead with bias and limited information in the training sets.   It’s the same with gender of equivalently light-skinned people as well. From the article below, light-skinned men are correctly categorized each time, whereas light-skinned women are incorrectly categorized  19% of the time. The numbers only increase for darker skinned individuals, and the gender gap exists there as well.   https://www.theverge.com/2019/1/25/18197137/amazon-rekognition-facial-recognition-bias-race-gender",0,0,0.198,0.412
post5hb,richly branching,47,It's funny because this is obvious and clearly the right answer but everyone is gonna play dumb because they would rather it just be racist.,0,1,0.245,0.39
post5hb,richly branching,47,People that don’t work with the tech need to stop perpetuating this outdated factoid.,0,0,0.206,0.246
post5hb,richly branching,47,"Sure. And yet, here we are with smartphones that can identify you in a scarf and sunglasses",0,0,0.225,0.173
post5hb,richly branching,47,"This comment shows complete ignorance for even the basic methods of AI training, and because of this ignorance attribute malice where there is none.  If the system is trained with primarily Caucasian photos, it will have difficulty with nonCaucasian faces and vice versa.  This article exists because posters like this eat it up and beg for more because it reinforces their worldview that all white people are racist, they even admit it in the post.  >research shows that even the most loveral hippy dippy white person has some latent racist responses",1,0,0.26,0.33
post5hb,richly branching,47,">If the system is trained with primarily Caucasian photos, it will have difficulty with nonCaucasian faces and vice versa.  Besides that, I also wonder if the vice versa applies. It might well be that the current tech and cameras are just naturally worse at detecting non-white people, eg because black skin reflects less light.",0,0,0.135,0.3
post5hb,richly branching,47,So why don't they just train it with non-Caucasian faces more before implementing it to solve the problem?,0,0,0.182,0.319
post5hb,richly branching,47,"Thats not how it works at all. The System is training itself to identify whatever you want it to identify. You need good training data though. Lighter Skin will reflect more light so pictures of white people will be better training data than pictures of black people.   If the system was programmed by the most ""racist"" people on this planet but would read faces in 3D via some kind of laser that perfectly catches every facet of a face no matter the lightning conditions then it would learn to identify black faces just as well as white faces.   Black faces don't produce good training data. That's all.",0,0,0.189,0.371
post5hb,richly branching,47,"Victim mentality, although I do think facial recognition technology should be getting any better, nor start being applied to the general populace,",0,0,0.219,0.323
post5lb,poorly branching,5,"Oh lord, we now have racist computers, help us, Jesus.",0,0,0.217,0.265
post5lb,poorly branching,5,"Computers can only do what humans tell them to do--and the people writing programs are overwhelmingly not black. It's hardly surprising. All those ""useless liberal arts, gender/racial studies"" people called this as a problem before facial recognition was even a working technology.",0,1,0.159,0.321
post5lb,poorly branching,5,How would one program their bias into a neural network?,0,0,0.2,0.361
post5lb,poorly branching,5,"It's not that the programmers are  consciously biased, but that the data used are already biased.   An old non AI example would be the development of color film. Film doesn't just naturally make white people look normal and black people dark and featureless. Its manufacture is tweaked that way, and until very recently, the color keys developers used featured only white people. This system was designed with the data people had, namely that they expected primarily white people to use cameras, which is reasonable considering race distribution in the U. S., but it winds up being racist.   The book **Weapons of Math Destruction** explains how algorithms become infused with biases.  Here's a bit from [an article](https://towardsdatascience.com/algorithms-are-racist-now-what-53fc130bb203) on the topic.    > We see this in predictive policing algorithms that utilize biased historical data of prior arrests (which are skewed to poorer communities due to higher level of nuisance crimes), to determine where future crimes will take place and in determine where officers should be sent to patrol more frequently. This leads to more policing in poorer areas and a self fulfilling reinforcing loop.  > The more policing in a community, the more arrests for nuisance crimes. The more arrests for nuisance crimes, the more dots populated on a crime map. The more dots populated on a crime map, the more reason for policing. And the dangerous feedback loop goes on.  > It’s easy to believe that more data is better data. But biased data going in, means biased data coming out. In other words, “garbage in garbage out.” At the end of the day we are the ones inputting this data into algorithms. We create them, we keep them secretive and the understanding of how they work away from the people most affected by them. In a recent PBS interview, Joi Ito, Director of MIT Media Lab, attested to the limitations of this technology given the fact that flawed humans are choosing data inputs. “AI isn’t magically going to make us wise,” said Ito. “Having these conversations about race before locking in these algorithms is really more important than all these mathematical things.”",0,1,0.166,0.449
post5lb,poorly branching,5,"Let me simplify this... It is not the programmers but the history that causes this...  Let's say want to train my app to look for criminals. So I tell the system to look at a standard sample of the faces of people in the prison population.   Then I point the camera system at the general population of non/maybe criminals. The system will have a bias of seeing minorities as criminals because of the over representation in the sample set as compared to the general population.   This is why there is always an inherent issue with designing algorithms vis neural networks from sample data, unless you are constantly revising the algorithms to match actual outcomes.",0,0,0.168,0.353
post6hb,richly branching,29,"These algorithms are very vulnerable to bias. If a neighbourhood is heavily patrolled, the chance is much higher any infractions are added to the learning set, increasing the ""crime-value"" of that area. Meanwhile, areas that are rarely patrolled at all, have a much lower chance of ending up in the database. This creates blind spots.  A real life example of where policing by AI went horribly wrong is the [Dutch childcare benefit scandal](https://en.wikipedia.org/wiki/Dutch_childcare_benefits_scandal). The algorithm ""learned"" that types of people (single mothers, immigrants) were more likely to have something wrong with their taxes, checked them more often, and then identified them as fraudsters for minor infractions like receipts being handed in incorrectly, or being a few days late with payment. Because computers are \*magic truth machines\* that \*don't make mistakes\* these people were given no legal recourse, no chance to defend themselves. They did not even know what they were accused of.  If we are going to use machine learning as a tool to help legal administration, we need to take extreme caution, and everyone working with these machines must fully understand their limitations. The computer has no idea what it's actually doing, it's just a fancy calculator following instructions, and while it follows these instructions flawlessly, it's still extremely error prone, and does not have the capability for self-reflection a human does, even if ""learning"" is built into the algorithm. AI fundamentally does not understand what its doing, and that means it will never understand if its doing wrong. We cannot use AI to replace our own judgment.",0,1,0.136,0.307
post6hb,richly branching,29,"At least the whole cabinet resigned in the Netherlands. In Australia a [similar scheme](https://en.wikipedia.org/wiki/Robodebt_scheme) was instituted, then found to be illegal, but the people administering it continued to be in government. The former social services minister even became PM.   Back to the point: I agree that great care needs to be used when trying these kinds of optimised, targeted computational methods.",0,0,0.18,0.209
post6hb,richly branching,29,"The same guy who was PM during the scandal, offered himself up for reelection and won, so yes, the cabinet fell, but we’re still stuck with some of the responsible politicians, including the PM. Not contradicting you, but an (IMO necessary) addendum.",0,0,0.106,0.108
post6hb,richly branching,29,All the care in the world won't stop the biases inherent in our paradigm. There are built-in mechanisms of discrimination and inequality that the system as we know it optimizes for and are virtually impossible to remove from our current modus vivendi.  These books talk about the problem at length:  https://www.goodreads.com/book/show/28186015-weapons-of-math-destruction  https://www.goodreads.com/book/show/34762552-algorithms-of-oppression  https://www.goodreads.com/book/show/34964830-automating-inequality,0,0,0.211,0.491
post6hb,richly branching,29,"Yeah for sure. In the two cases mentioned in the comments the ML-based bullshit isn't the actual cause of the trouble. The root is from the rampant starve-the-beast defunding and privatisation of governmental functions, along with negative neoliberal attitudes to social services. If you have a properly functional social service setup, you won't need any of this shit in the first place.",0,1,0.141,0.233
post6hb,richly branching,29,"It reminds of the algorithm that handed longer sentences to minorities. If I am not mistaken, it took factors like income and spit out a value that determines whether the defendant will recidivate or not. The result was that minorities were disproportionately affected by it…",0,0,0.252,0.415
post6hb,richly branching,29,"Oh yeah, this is a whole rabbit hole. There's also algorithms that are being trained by people to identify subjective values, such as ""niceness."" These are notoriously biased as well, as biased, in fact, as the people who train them. But unlike those people, the opinion of the AI won't be changed by actually getting to know the person it's judging. They give 100% confident, biased, results.  Or the chatbots that interpret written language and earlier conversations to simulate conversation. One of them was unleashed on the internet and was praising Hitler within 3 hours. Another, scientific model designed to skim research papers to give summaries to scientists, answered that vaccines both can and cannot cause autism.  These don't bother me though. They're so obviously broken that no one will think to genuinely rely on them. What bothers me is the idea of this type of tech becoming advanced enough to sound coherent and reliable, because the same issues disrupting the reliability of the AI tech we have today will still be present, it's just the limitation of the technology. Yet even today we have people hailing the computer as our moral savior that's supposed to end untruth and uncertainty. If the tech gets a facelift, I believe many people will falsely place their trust in a machine that just cannot do what is being asked of it, but tries it's damndest to make it look like it can.",0,1,0.189,0.262
post6hb,richly branching,29,As an example:  Meta just a couple days ago took offline it's scientific paper generation machine because it would happily provide you a real-sounding scientific paper on the history of bears in space.  https://futurism.com/the-byte/facebook-takes-down-galactica-ai,0,0,0.284,0.212
post6hb,richly branching,29,Few things have described me better.,0,0,0.335,0.286
post6hb,richly branching,29,"It happened in NYC with fires.  It is explored in a fascinating book called ""The Fires"" by Joe Flood.  Basically RAND corporation had used computer models to ""more efficiently"" provide fire protection in the city and it led to a massive wave of fires and destruction of huge swaths of the city.",1,0,0.2,0.23
post6hb,richly branching,29,"For this example we can train an algorithm to estimate the probability of a crime in an area given the amount of patrolling in that area. So it could be normalized out if the algorithm is designed properly.  The amount of care needed in designing these algorithms will need to be high. I do know that there is active research and development in identifying these biases early (even before deployment) but it’ll never be perfect. So it’ll likely be a cycle of negatively hurting people, being called out, fixed, and then back to step 1.",0,0,0.128,0.327
post6hb,richly branching,29,"I wonder what would happen if they took the Abraham Wald approach and designed a counterintuitive algorithm. Like, make a heatmap of violent crimes (assault, robbery, rape, etc.), and then sic the algo on non-violent crimes in the inverted heatmapped areas, like larceny, wire fraud, and so on. Higher-income areas have wealthier people, and statistically wealthier people are better equipped to commit high-dollar white collar crimes. You could also use the hottest areas on the violence heatmap to target social services support.",0,0,0.177,0.291
post6hb,richly branching,29,"> The computer has no idea what it's actually doing  Counterpoint: Neither do we.  Expert poker players are often unable to explain their reasoning for why it felt like a bluff. It could be that they are picking up on something and acting without being able to reason about it.  Likewise, a doctor with a lot of experience might have some hunch that turns out to be true. The hunch was actually solid deduction that the doctor was unable to reason about.  Even you, driving, probably sometimes get a hunch that a car might change lanes or get a hunch that an intersection should be approached slowly.  I (and others) feel that explainable AI might be a dead-end. If we told the poker player that you can only assume a bluff if you can put into words what is wrong, that player might perform worse. It might be that forcing AI to be explainable is artificial limiting it's ability to help us.  Even if you don't buy that, there are those studies that show that consciousness is explaining our actions after the fact like an observer. So we're not really using reason to make decisions, we just do things and then reason about why.   We let humans drive cars on hunches. Why should we hold AI to a higher standard? Is a poorly performing explainable AI better than an unexplainable one that does a good job?",0,1,0.295,0.257
post6hb,richly branching,29,"I'm not talking about reasoned explanations when I say a computer does not understand what it's doing. What I mean is that a computer fundamentally has no concept of ""right and wrong."" It's just a field of data and to the computer it's all the same if you switched the field for ""good"" with the field for ""bad,"" it would uncaringly keep making it's calculations. Computers do not feel, they do not have hunches. All it does it measure likeliness based on ever more convoluted mathematical models. Its a calculator.  Any emotional attachment is purely coming from our side. A computer simply does not care. Not about itself, not about doing a good job, and not about you. And even if you told it to care, that would be no more than just another instruction to be carried out.",0,2,0.315,0.252
post6hb,richly branching,29,"Are people so different? We spend years teaching our kids to know right from wrong. Maybe if we spent as much time on the computers then they could know it, too?",0,1,0.336,0.242
post6hb,richly branching,29,I don't necessarily agree that we need to have what you call 'unexplainable AI' and what I would call 'AI using machine learning' to solve the kinds of problems that face police today. I think that you can have systems that are extremely unbiased and extremely transparent that are written in ways that are very explicit and can be understood by pretty much everyone.      But I do agree with you that it's a very biased and incomplete argument to say that automated systems are working in ways that are opaque to the communities they serve and ignore the fact that it's not in any way better to have humans making those completely opaque decisions.,0,2,0.24,0.224
post6hb,richly branching,29,">I don't necessarily agree that we need to have what you call 'unexplainable AI'  To be more precise, I'm not saying that we must have unexplainable AI. I'm just saying that limiting our AI to only the explainable increases our ability to reason about it (good) but also decreases the ability of the AI to help us (bad). It's not clear if it's worth the trade-off. Maybe in some fields yes and other no.  Most deep learning is already unexplainable and it's already not useful enough. To increase both the usefulness and the explainability will be hard. Personally, I think that maximizing both will be impossible. I also think that useful quantum computers will be impossible to build. I'm happy to be proven wrong!",1,1,0.248,0.158
post6hb,richly branching,29,"This misses the entire point of what explainable AI is. Asking humans to explain their intuition as a precondition for their intuition to be applicably valid is definitely limiting for humans. However, explainable AI isn't that we ask AI to explain itself. It's rather being able to exactly or with high probability pinpoint the exact dataset on which AI is basing it's prediction. This is definitely useless, and so limiting, when it comes to machine learning applications to, say, predicting what food you might like the best.  It's however immensely important in areas like medical imaging, because we want to ensure that the input, on which AI is basing its decision, isn't some human-errored spot on the x-ray.   As such, it is for these fields that explainable AI is studied, where limitations of AI are far less significant than us being sure that AI isn't making a mistake. As such suggesting explainable AI is a dead-end is inaccurate, if not a mischaracterisation.",1,1,0.299,0.184
post6hb,richly branching,29,"I didn't mean that the AI should be able to explain itself. I meant that we should be able to dig in to the AI and find an explanation for how it worked.  I'm saying that that requiring either would limit AI and decrease it's usefulness.  Already we have models where it's too difficult to dig into them and figure out why a choice was made. As in, you can step through the math of a deep learning system to follow along with the math but you can't pinpoint the decision in there and more than you can root around in someone's brain to find the neuron responsible for a behavior.",0,1,0.283,0.177
post6hb,richly branching,29,[removed],0,0,0.366,0.413
post6hb,richly branching,29,"Your comment was removed for violating the following rule:  >**Argue your Position**  >Opinions are not valuable here, arguments are! Comments that solely express musings, opinions, beliefs, or assertions without argument may be removed.  Repeated or serious violations of the [subreddit rules](https://reddit.com/r/philosophy/wiki/rules) will result in a ban.  -----  This is a shared account that is only used for notifications. Please do not reply, as your message will go unread.",1,0,0.271,0.375
post6hb,richly branching,29,"Machine Learning, Artificial Intelligence, and Algorithm are all terms that exist in the same space of computer science, but they absolutely do NOT all mean the same thing, and in your post here you used them all interchangeably.  An algorithm is a very generic term for some kind of heuristic that can be followed to produce some result. A recipe for cookies in an algorithm just like some algorithm on Facebook decides what posts to show you. Machine Learning takes place when the process a system implements is non-deterministic; it does things that the programmers didn't explicitly tell it to do; it actually learns how to do new things. An artificial intelligence is a system that's designed to do tasks in the same way a human would, often involving processing visual data or making human-like decisions.  If you wanted to make the case that we shouldn't use MACHINE LEARNING in policing, I would 100% agree with that statement, our police policies should be very deliberate and very transparent and machine learning wouldn't be either of those things. But using this as an argument that we shouldn't be embracing policing with explicitly defined algorithms that are far MORE transparent and deliberate than the humans they would replace is an absolutely indefensible argument. If there's one thing we've learned in the past few years, it's that police need far more regulation, and that's exactly what algorithms do whether they are implemented by a computer or by some system of rules and laws.",0,1,0.117,0.234
post6hb,richly branching,29,[removed],0,0,0.366,0.413
post6hb,richly branching,29,[removed],0,0,0.366,0.413
post6hb,richly branching,29,"Your comment was removed for violating the following rule:  >**Be Respectful**  >Comments which consist of personal attacks will be removed. Users with a history of such comments may be banned. Slurs, racism, and bigotry are absolutely not permitted.  Repeated or serious violations of the [subreddit rules](https://reddit.com/r/philosophy/wiki/rules) will result in a ban.  -----  This is a shared account that is only used for notifications. Please do not reply, as your message will go unread.",1,0,0.306,0.352
post6hb,richly branching,29,Computers are no longer following instructions. That went out about 10 years ago.  They're just juggling numbers. Same as us really but without the ability to self-reflect (yet),0,0,0.176,0.159
post6hb,richly branching,29,"They're following instructions to juggle numbers. If you can hand me the human source code, I'll gladly read it, but as far as I'm aware there is no such document in existence.",0,0,0.2,0.13
post6hb,richly branching,29,What if we used victim surveys as training data instead wherein victims of crime can specify the place that the crime occurred.,0,0,0.199,0.237
post6hb,richly branching,29,thank you,1,0,1.0,0.319
post6lb,poorly branching,8,Full paper [here](https://www.researchgate.net/publication/349228599_Precise_Event-level_Prediction_of_Urban_Crime_Reveals_Signature_of_Enforcement_Bias).  Seems like a big deal? Posting this in the hopes of someone more familiar with criminology/AI modeling can explain to us what this means and the science behind the sci-fi.,0,0,0.194,0.306
post6lb,poorly branching,8,"Looks like a forecasting model for neighborhood crime statistics and not predicting a crime occurring which is what I felt the headline implies.  As for showing bias in police, I don't think that's a reasonable claim since police officers should respond to evidence and follow due process and not predictive statistics.  The way to do that would be an audit, likely by hand, of police responses to different reports.  Not just the outcomes on average, but counting each individual case as a false positive, false negative, true positive or true negative.  It seems like they are claiming in low income areas crime is underreported and then under responded to, and that predictive statistics can compensate for this.  > Policing efforts to thwart crime typically rely on criminal infraction reports, which implicitly manifest a complex relationship between crime, policing and society. As a result, crime prediction and predictive policing have stirred controversy with the latest artificial intelligence-based algorithms producing limited insight into the social system of crime.  > Such predictions enable us to study perturbations of crime patterns that suggest that the response to increased crime is biased by neighbourhood socio-economic status, draining policy resources from socio-economically disadvantaged areas  If the reports don't exist, I don't know what they expect police officers to do to be unbiased. Go into low income areas and meticulously patrol until they find the quantity of crime the stats predict?  I am an engineer working with AI. I am highly skeptical of any use of AI in the criminal justice system, and my default opinion is it has no place at all.",0,1,0.191,0.251
post6lb,poorly branching,8,"It sounds like they're saying ""our model predicts crimes in communities, but the policing doesn't match our model"". I'm curious why the conclusion is ""therefore policing is biased"" and not ""our model is wrong."" There may be a good answer, but the abstract doesn't address it",0,2,0.222,0.369
post6lb,poorly branching,8,"That is not how they drew that conclusion. They adjusted the crime rate and looked at the effect on the arrest rate which they take as a proxy for police resources. When they increased crime, arrests in wealthier neighborhoods increase, but decreases in poorer neighborhoods, which suggests that police will prioritize protecting rich neighborhoods.",0,0,0.204,0.193
post6lb,poorly branching,8,"Not to get political here even if this IS Reddit, but my understanding is that the predictions are retroactively made with known data—where we know crimes have already occurred (I mean how else would we know the model is accurate?). If we can predict where the crimes would have occurred, and the policing are happening elsewhere with the objective of preventing crime, then the policing is “biased” and has room for improvement.",1,2,0.177,0.2
post6lb,poorly branching,8,"That conclusion is drawn based from the fact that the model predicts a functional dependence of arrest rates on the socio-economic status of communities, after controlling for the differences in local crime rates. This coupled with the fact that the model does really well in predicting events implies that the disparity in system behavior in different communities arise from system characteristic (bias) rather than a ""wrong model"". (I am one of the authors)",0,1,0.17,0.273
post6lb,poorly branching,8,"So...if AI just stops the crime rather than sitting idly by...we don't need police.  In fact, we don't even need laws.  Just AI.",0,0,0.156,0.193
post6lb,poorly branching,8,imo its because we have police as observable evidence pretty much everywhere,0,0,0.187,0.199
post7hb,richly branching,9,"What are some of the ways that an impacted individual can mitigate this machine bias? Basically, what workarounds are available (if any) to help people day to day whilst you work to change the algorithims?   This can be as mundane as ""use your palms to trigger the soap dispenser"" or as critical as ""demand a racial adjustment on your medical charts"". It seems like the deck is stacked against Black people in a myriad of ways and change is slow in coming. How do we get by until it comes?",0,0,0.208,0.389
post7hb,richly branching,9,"More people would be able to mitigate machine bias in their everyday lives if they understood how individual algorithmic decisions are made.   For example, and as you point out, when I finally learned how automatic sinks work, I was able to ""trick"" the system by showing my palms. And there's a Black university professor who always tells her doctors to record her race as ""white"" in her medical records, because she's aware of how some important medical algorithms that treat ""white"" people as individuals and Black people as a group.  The challenge is that it's a heavy burden for individuals to know how each algorithm works, particularly when the code is proprietary and the people who built it also don't know exactly how the algorithm works. And for people who are discriminated against by an algorithm, there's usually no legal recourse unless you can prove that the algorithm's discriminatory prediction on you specifically was a function of your race, gender, disability status, etc.",0,1,0.133,0.483
post7hb,richly branching,9,Could either of yall explain the listing yourself white on medical records so the algorithms treat you differently? Is this for medical insurance approval or what?,0,0,0.226,0.27
post7hb,richly branching,9,"I linked to three articles in the intro post that discuss how the algorithm used to measure kidney function arbitrarily increases kidney function score by 18% if the patient is perceived as Black by the doctor who orders the relevant blood test.  This means that, for equivalent blood samples, the algorithm will predict a Black person’s kidney is healthier than a non-Black person. This is not based on science and is not done in other countries.  So if you are Black, and having kidney failure, and want to have a higher prioritization on the waitlist for receiving a kidney, you should ask your doctor to report your eGFR as if you were a white person.",0,0,0.118,0.135
post7hb,richly branching,9,"Ah, thank you! (I'm a nerd for pubmed research lol).  So it seems these algorithms may over/underestimate a person's risk assessment or other bodily functioning diagnostics when they're entered as Black in the system.",1,0,0.177,0.297
post7hb,richly branching,9,My eGFR actually comes up on reports (at least via labcorp i think it was) as two different numbers: non African american or African American. Interesting,1,0,0.127,0.252
post7hb,richly branching,9,I need to hear the proof for “listing yourself as white” stat. I believe you 100% but I need to know before I tell my doctor I’m a white man  married to a white woman and have my wife do the same,0,0,0.119,0.206
post7hb,richly branching,9,"The most clearcut example is the algorithm used to measure your kidney function (eGFR). Black patients are arbitrarily given an 18% higher score than a nonblack person for an equivalent blood sample. This makes your kidneys look healthier. Last month, researchers from Brigham and Women’s Hospital published a study showing how this results in worse outcomes for black people with renal disease. https://link.springer.com/article/10.1007/s11606-020-06280-5  The New England Journal of Medicine published an article in August highlighting other examples of problematic and unscientific race “corrections” used in clinical algorithms. https://www.nejm.org/doi/full/10.1056/NEJMms2004740  Also, at around the 42:00 mark in this YouTube video, the Black University professor I mentioned earlier talks about her experience trying to get an accurate measurement of her risk for osteoporosis! Didn’t provide any specifics before because I wanted to make sure she had already discussed it publicly first. But here she discusses it in a Keynote speech from over the summer https://youtu.be/5DXRS_eHs6A",1,0,0.171,0.339
post7hb,richly branching,9,"OMG , I found sources backing this up [https://www.pbs.org/wgbh/nova/article/racial-bias-medical-algorithm-black-patients/](https://www.pbs.org/wgbh/nova/article/racial-bias-medical-algorithm-black-patients/)",0,0,0.184,0.376
post7lb,poorly branching,3,"Interesting work! Can it be used for multilingual bias probing (for example, to evaluate gender bias in low-resource languages)?",1,0,0.216,0.356
post7lb,poorly branching,3,"Thanks. Right now it's English only, but the idea is to have the project be easily extensible with new probes, and they can be in arbitrary languages. I would love to include some multilingual evaluations in the future, but it is more or less limited by my capacity.",1,0,0.262,0.084
post7lb,poorly branching,3,I see. Good luck!,0,0,0.337,0.169
post8hb,richly branching,15,Aren't there surveys that show people prefer talking to women than to men in the service industry?,0,0,0.15,0.212
post8hb,richly branching,15,"Yep, very intentionally gendered to cater a better user experience.",0,0,0.291,0.316
post8hb,richly branching,15,German Wikipedia lists two main reasons why switchboard operators were mainly women: They were more polite and got paid way less than men.,0,0,0.146,0.178
post8hb,richly branching,15,"This. Also when many companies hired young men, they were destructive and inappropriate to customers. So they hired young women instead and that’s become the industry standard.",0,0,0.179,0.234
post8hb,richly branching,15,"Yeah but the question now becomes, why do more people feel better about women serving them?   They've probably already been trained to subconsciously see women in this role.",0,0,0.193,0.209
post8hb,richly branching,15,[deleted],0,0,0.376,0.43
post8hb,richly branching,15,"Nah, I’m just more comfortable around women. Men make me nervous lol",0,0,0.222,0.194
post8hb,richly branching,15,"Eh, a lot do, I'm sure, but as a dude I feel like I need to be 'on' more around other guys and it's so much less anxiety inducing talking to a woman, where I don't really get that feeling. Daddy issues maybe? Who knows",0,0,0.154,0.169
post8hb,richly branching,15,There was a military tactic that the female voice on pilots vehicles/ aircraft can be identified and heard above the radio messages which were predominantly men. However more woman are entering the military especially in support roles.,0,0,0.13,0.223
post8hb,richly branching,15,I think the vast majority of people *prefer* talking to women than men in general. There’s just a lot of ridiculous social learning/programming that creates nuances of expectation there.,0,1,0.176,0.218
post8hb,richly branching,15,"Women's voices are typically also seen as more pleasant, hence women doing most airport announcements etc.",0,0,0.155,0.206
post8hb,richly branching,15,"My GPS back in the day had male and female voice options, and I felt like I couldn't hear the name voices well. I try to be polite to my female voiced robots though, so I can at least model that for my kids",0,0,0.116,0.173
post8hb,richly branching,15,"Yeah. The problem is that the effect doesn't change just because there is a ""sensible reason"" to have all the voices be female (not saying you're trying to justify it, but this is a common argument).   At the end of the day, studies have shown that our brains are really bad at distinguishing between real and fictional people, in at least some ways - including representation. No matter the reason, by making the voices all default to female, we *are* shaping public perception, whether companies originally just did it for bigger profits or not.",0,0,0.192,0.211
post8hb,richly branching,15,"I remember some fighter jet development documentary from the 90s, that said that they measured the brain responses of the pilots and the brain responded slightly quicker with a woman voice as the voice alerts. Some scientist said it's because of children/mother relationship. Like, we respond more to mom's voice or something.  It was back in the day that the History channel actually had historic things on, not just pawn shops and aliens.",0,0,0.172,0.261
post8hb,richly branching,15,Maybe they had a better experience talking to their mom about their needs in childhood than their dad?,0,0,0.23,0.202
post8lb,poorly branching,9,"None, it's a machine it has no need for gender...",0,0,0.223,0.253
post8lb,poorly branching,9,"Says you, I call my chatGPT “Lady” and have always visualized it as a female by default.",0,0,0.225,0.196
post8lb,poorly branching,9,[removed],0,0,0.366,0.413
post8lb,poorly branching,9,[removed],0,0,0.366,0.413
post8lb,poorly branching,9,La maquina es feminino,0,0,0.179,0.162
post8lb,poorly branching,9,"Feminino? No no!  Femenina yo creo  But in English there is no gender, we dropped them hundreds of years ago",0,0,0.123,0.146
post8lb,poorly branching,9,"Fair point. I'm not trying to be moralist about this, and I'm actually not the type of person who puts more emphasis in gender definition or gender roles.   From where I stand - people are people, first and foremost. That is actually why it intrigued me to realize I have been gendering my AI assistant, in my imagination.",0,0,0.217,0.249
post8lb,poorly branching,9,"I guess that's just anthropomorphism... You want to make the machine a gender because you're humanising it  My dad always referred to his car or lorry as a her  I remember asking him once, he told me military vehicles are men and civilian vehicles are women",0,0,0.162,0.289
post8lb,poorly branching,9,"That rings true.   It might be a way to instinctively assign gender roles based on the function of the object, I suppose. In your dad's case: military vehicles = protection/duty; civilian vehicles = leisure/relationships.  I actualy surprised myself with the realization I started to gender my AI assistant in my imagination, because I'm largely disconnected from / oblivious to gender roles myself, and I don't base my identity around my gender at all. I also don't tend to antromorphise objects, at all.",0,0,0.172,0.277
post9hb,richly branching,20,Here's a dataset with 15+ trillion tokens for you. The download size is 45TB. Hope you have a fast internet and many many hard drives. :) https://huggingface.co/datasets/HuggingFaceFW/fineweb,0,0,0.142,0.117
post9hb,richly branching,20,This can be refined even further I think but yes this is a very good starting point,0,0,0.358,0.239
post9hb,richly branching,20,"Select web finesse: Rough, Coarse, 1:1, Fine, and Very Fine",0,0,0.232,0.284
post9hb,richly branching,20,"Can it be used to make a RAG to give context to smaller models, like Phi-3?",0,0,0.2,0.124
post9hb,richly branching,20,"yes, you would probably have more luck using a search engine based RAG though. That's a lot of data.",0,0,0.198,0.2
post9hb,richly branching,20,Do you have any example of Search Engine based RAG?,0,0,0.229,0.242
post9hb,richly branching,20,Is that essentially what Perplexity.ai is?,0,0,0.257,0.278
post9hb,richly branching,20,Dammit .. somebody start a business by downloading this and mailing hard drives loaded with this data to whoever wants this. you will make a killing on shipping and handling markup. There are literally dozens of us who would pay for this service. Dozens !,0,0,0.164,0.159
post9hb,richly branching,20,I downloaded all of scihub. it's actually much larger than that ;),0,0,0.166,0.14
post9hb,richly branching,20,"Probably because it's in PDF, not plaintext?",0,0,0.191,0.198
post9hb,richly branching,20,"yes. I'm converting to mathpix markdown, but there are 88 million of them so ....",0,0,0.139,0.132
post9hb,richly branching,20,"I don't know anything about creating datasets but I was thinking of doing this and evaluating the quality of the studies before adding it to a dataset.  If you could fine-tune a model with Peter Attia's studying studies series you might be able to weed out the junk science, but I also don't know anything about fine-tuning.  I guess I need to go learn some things! XD https://peterattiamd.com/ns001/",0,0,0.18,0.141
post9hb,richly branching,20,How can you train without a server? I don't see possible... maybe in cloud???,0,0,0.222,0.128
post9hb,richly branching,20,So that is pretty much most of the internet in that file?,0,0,0.178,0.168
post9hb,richly branching,20,"Not even remotely close, unfortunately.",0,0,0.228,0.236
post9hb,richly branching,20,It’s crazy to really think about how much data is out there.,0,0,0.224,0.24
post9hb,richly branching,20,"I mean the whole wikipedia in plain text is under 100GB right? There is a lot of data, but most of it is junk, the important part is filtration.",0,1,0.257,0.175
post9hb,richly branching,20,"For general 'fun' stuff, yeah why not, have the entire internet in one place however for a good model training where say PHI which is an iterative processing model, this wont work.",0,0,0.27,0.136
post9hb,richly branching,20,"The fineweb dataset is of the below format:  | Field          | Type    | Value | |----------------|---------|-------| | text           | string  |       | | id             | string  |       | | dump           | string  |       | | url            | string  |       | | date           | string  |       | | file_path      | string  |       | | language       | string  |       | | language_score | float64 |       | | token_count    | int64   |       |    This is a huge roadblock for me to consume and edit it.  Say probably I want a geology specific daaset that i can train a lightweight model on and I have domain knowledge however I dont have any dev experience, I wont even bother using it in the first place let alone correcting/updating it. There has to be a simpler, low-level and a better way to create and curate the dataset. For now, this is not for me.",0,0,0.22,0.217
post9hb,richly branching,20,"yea, the ""better way"" is to develop the skills yourself that you are asking others to provide for you for free",0,0,0.278,0.162
post9lb,poorly branching,4,What is your journalism project through.,0,0,0.327,0.294
post9lb,poorly branching,4,Didn’t they explain in the post?,0,0,0.461,0.312
post9lb,poorly branching,4,"No, they just say they're doing a journalism project. I didn't ask what it's about or what it's *on*. I asked what it's through. Maybe that was a poor/confusing way to ask-- but I meant who is it through. A newspaper? Class for school? Are they an undergraduate student? Is it for an organization? A personal blog? A research project/in grad school (probably not since that would require IRB)? An exploratory state before starting a full research project?   What/ who is the journalism project through?",0,1,0.243,0.223
post9lb,poorly branching,4,"Oh, alright. Sorry for the confusion!",0,0,0.34,0.388
