{
    "post_title": "Trying to understand Mirostat and Contrastive Search, and their parameters",
    "post_timestamp": "2023-08-22 13:29:36",
    "last_comment_timestamp": "2024-08-27 11:46:33",
    "time_difference": "370 days, 22:16:57",
    "comments": [
        {
            "author": "silenceimpaired",
            "body": "Same here. Seems so alien.",
            "score": 5,
            "depth": 0,
            "timestamp": "2023-08-22 14:32:38",
            "replies": []
        },
        {
            "author": "Open-Opinion-7338",
            "body": "This paper ([Balancing Diversity and Risk in LLM Sampling: How to Select Your Method and Parameter for Open-Ended Text Generation](https://arxiv.org/abs/2408.13586)) and github repo (https://github.com/ZhouYuxuanYX/Benchmarking-and-Guiding-Adaptive-Sampling-Decoding-for-LLMs) delivered the answer to your question.",
            "score": 1,
            "depth": 0,
            "timestamp": "2024-08-27 11:46:33",
            "replies": []
        },
        {
            "author": "UseNew5079",
            "body": "A good blog post on contrastive search is here: [https://huggingface.co/blog/introducing-csearch](https://huggingface.co/blog/introducing-csearch)\n\nI asked GPT-4 to explain and simplify, and it came up with this:\n\n\\--\n\nLet's break down the provided content step by step to understand and simplify the idea of contrastive search as described in the HuggingFace blog post.\n\n## 5.1. Decoding Objective:\n\n## Objective:\n\nWhen the model is trying to predict the next token (`x_t`) given some prefix text (`x_<t`), contrastive search aims to select a token that both:\n\n1. Has high confidence from the language model.\n2. Is not very similar to previously generated tokens (to avoid repetitive outputs).\n\n## Equation Breakdown:\n\n1. `x_t = argmax {...}`: This means that the token `x_t` is chosen such that it maximizes the value of the expression inside the curly braces `{...}`.\n2. `(1 - \ud835\udefc) * p_\ud835\udf3d(v | x_<t)` represents the **model's confidence** in predicting token `v` given the prefix `x_<t`. The `(1 - \ud835\udefc)` is just a weight term.\n3. `(max{ s(h_v,h_(x_j)) : 1 <= j <= t - 1 })` is the **degeneration penalty**. It checks how similar the token `v` is to all previously generated tokens. If `v` is highly similar to any previous token (in terms of their representations), this value will be high. The function `s(\u00b7,\u00b7)` computes the cosine similarity between tokens, a measure of how similar two tokens' representations are.\n4. `\ud835\udefc` is a hyperparameter that determines the balance between model confidence and the degeneration penalty. If `\ud835\udefc` is 0, the method becomes just like the regular greedy search.\n\n## Intuition:\n\nThe whole idea is to balance two objectives:\n\n1. Trust the language model's prediction.\n2. Avoid picking tokens too similar to what's already been said.\n\n## 5.3. Visual Demonstration of Contrastive Search:\n\n## Visualization:\n\nTwo figures are provided, showing token similarity matrices for both greedy and contrastive search methods. Darker shades represent higher similarity scores.\n\n* **Figure 1 (Greedy Search)**:\n   * There are high similarity scores in off-diagonal entries. This indicates that greedy search is often selecting repetitive tokens, leading to high similarities between different tokens in the sequence.\n* **Figure 2 (Contrastive Search)**:\n   * Most high similarity scores are on the diagonal, indicating that each token is mostly just similar to itself. Repetitions are reduced compared to the greedy search, thus addressing the degeneration problem.\n\n## Summary:\n\nContrastive search is a method that improves the quality of text generated by models by reducing the repetition of tokens. It does this by adding a penalty for selecting tokens that are similar to previously generated tokens, while still factoring in the model's original confidence in its predictions. The visualizations provided in the blog post seem to verify the effectiveness of this approach.\n\nI hope this breakdown helps in understanding the core idea and mechanics of the contrastive search method!",
            "score": 1,
            "depth": 0,
            "timestamp": "2023-08-22 17:20:04",
            "replies": [
                {
                    "author": "ReMeDyIII",
                    "body": ">Contrastive search is a method that improves the quality of text generated by models by reducing the repetition of tokens. It does this by adding a penalty for selecting tokens that are similar to previously generated tokens\n\nIs that why Miro is so much slower? Does it have to scan the entire in-context of the conversation?",
                    "score": 2,
                    "depth": 1,
                    "timestamp": "2023-08-22 19:37:47",
                    "replies": []
                }
            ]
        },
        {
            "author": "ambient_temp_xeno",
            "body": "https://arxiv.org/abs/2007.14966\n\nhttps://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md#mirostat-sampling\n\nThe effects you get by changing the values seems to depend on the model/finetune (especially model size) and what you're looking for. Tweaking it.\n\nI copied the following from somewhere so I'm not 100% about the accuracy of it:\n\nWhen mirostat is enabled, llama.cpp will sample new tokens in the following order:\n1) Repetition penalties are applied\n2) Frequency and presence penalties are applied\n3) Temperature is sampled\n4) mirostat is sampled\nEverything else is ignored.\ntau, eta, repeat-last-n, repeat-penalty, presence-penalty, and frequency-penalty parameters will affect generation.",
            "score": 1,
            "depth": 0,
            "timestamp": "2023-08-22 15:24:55",
            "replies": []
        }
    ]
}