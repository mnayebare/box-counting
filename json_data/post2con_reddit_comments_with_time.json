{
    "post_title": "Robots With Flawed AI Make Sexist And Racist Decisions, Experiment Shows. \"We're at risk of creating a generation of racist and sexist robots, but people and organizations have decided it's OK to create these products without addressing the issues.\"",
    "post_timestamp": "2022-06-28 00:46:52",
    "last_comment_timestamp": "2022-12-12 16:19:22",
    "time_difference": "167 days, 15:32:30",
    "comments": [
        {
            "author": "AutoModerator",
            "body": "Welcome to r/science! This is a heavily moderated subreddit in order to keep the discussion on science. However, we recognize that many people want to discuss how they feel the research relates to their own personal lives, so to give people a space to do that, **personal anecdotes are now allowed as responses to this comment**. Any anecdotal comments elsewhere in the discussion will continue to be removed and our [normal comment rules]( https://www.reddit.com/r/science/wiki/rules#wiki_comment_rules) still apply to other comments.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/science) if you have any questions or concerns.*",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 00:46:53",
            "replies": [
                {
                    "author": "DavefromtheD80",
                    "body": "The biggest problem is humans are getting way too overly sensitive and have began labeling every word, sentence, and action under the sun as racist, sexist, or some other kind of ist just because they don't like something or feel some type of way about it. The liberal progressive mindset is going so far to the left that it's come around full circle on the other side and is starting to become eerily similiar to the mindset of traditional uptight religious conservatives. The people who have always been offended by everything and have always tried to destroy anything they don't like or anyone that doesn't think, feel, and share the exact same beliefs as they do. It's hilarious, sad, and ironic all at the same time. \n\nWhatever is going on with these AI programs I'm quite positive is only racist or sexist by these new crazy standards that are being pushed by overly sensitive insane people.",
                    "score": 0,
                    "depth": 1,
                    "timestamp": "2022-06-28 20:53:08",
                    "replies": []
                }
            ]
        },
        {
            "author": "chrischi3",
            "body": "Problem is, of course, that neural networks can only ever be as good as the training data. The neural network isn't sexist or racist. It has no concept of these things. Neural networks merely replicate patterns they see in data they are trained on. If one of those patterns is sexism, the neural network replicates sexism, even if it has no concept of sexism. Same for racism.   \n\n\nThis is also why computer aided sentencing failed in the early stages. If you feed a neural network with real data, any biases present in the data has will be inherited by the neural network. Therefore, the neural network, despite lacking a concept of what racism is, ended up sentencing certain ethnicities more and harder in test cases where it was presented with otherwise identical cases.",
            "score": 3647,
            "depth": 0,
            "timestamp": "2022-06-28 01:01:42",
            "replies": [
                {
                    "author": "Deleted",
                    "body": "[removed]",
                    "score": 105,
                    "depth": 1,
                    "timestamp": "2022-06-28 07:37:27",
                    "replies": []
                },
                {
                    "author": "teryret",
                    "body": "Precisely.  The headline is misleading at best.  I'm on an ML team at a robotics company, and speaking for us, we haven't \"decided it's OK\", we've run out of ideas about how to solve it, we try new things as we think of them, and we've kept the ideas that have seemed to improve things.  \n\n\"More and better data.\"  Okay, yeah, sure, that solves it, but how do we get that?  We buy access to some dataset?  The trouble there is that A) we already have the biggest relevant dataset we have access to B) external datasets collected in other contexts don't transfer super effectively because we run specialty cameras in an unusual position/angle  C) even if they did transfer nicely there's no guarantee that the transfer process itself doesn't induce a bias (eg some skin colors may transfer better or worse given the exposure differences between the original camera and ours)  D) systemic biases like who is living the sort of life where they'll be where we're collecting data when we're collecting data are going to get inherited and there's not a lot we can do about it  E) the curse of dimensionality makes it approximately impossible to ever have enough data, I very much doubt there's a single image of a 6'5\" person with a seeing eye dog or echo cane in our dataset, and even if there is, they're probably not black (not because we exclude such people, but because none have been visible during data collection, when was the last time you saw that in person?).  Will our models work on those novel cases?  We hope so!",
                    "score": 903,
                    "depth": 1,
                    "timestamp": "2022-06-28 01:29:14",
                    "replies": [
                        {
                            "author": "Deleted",
                            "body": "So both human intelligence and artificial intelligence are only as good as the data they're given. You can raise a racist, bigoted AI the same in way you can raise a racist, bigoted HI.",
                            "score": 358,
                            "depth": 2,
                            "timestamp": "2022-06-28 07:47:41",
                            "replies": [
                                {
                                    "author": "frogjg2003",
                                    "body": "The difference is, a human can be told that racism is bad and might work to compensate in the data. With an AI, that has to be designed in from the ground up.",
                                    "score": 310,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:03:08",
                                    "replies": [
                                        {
                                            "author": "BattleReadyZim",
                                            "body": "Sounds like very related problems. If you program an AI to adjust for bias, is it adjusting enough? Is it adjusting too much creating new problems? Is it adjusting slightly the wrong thing creating a new problem and not really solving the original problem? \n\nThat sounds a whole lot like our efforts to tackle biases both on personal and societal levels. Maybe we can ask learn something from these mutual failure.",
                                            "score": 25,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 11:28:13",
                                            "replies": []
                                        },
                                        {
                                            "author": "mtnmadness84",
                                            "body": "Yeah. There are definitely some racists that can change somewhat rapidly. But there are many humans who \u201cwon\u2019t work to compensate in the data.\u201d \n\nI\u2019d argue that, personality wise, they\u2019d need a redesign from the ground up too. \n\nJust\u2026ya know\u2026.we\u2019re mostly not sure how to fix that, either.  \n\nA ClockWork Orange might be our best guess.",
                                            "score": 82,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 09:23:20",
                                            "replies": [
                                                {
                                                    "author": "Deleted",
                                                    "body": "One particular issue here is potential scope.\n\nYes, a potential human intelligence could become some kind of leader and spout racist crap causing lots of problems. Just see our politicians.\n\nWith AI the problem can spread racism with a click of a button and firmware update. Quickly, silently, and without anyone knowing because some megacorp decided to try a new feature. Yes, it can be backed out and changed, but people must have awareness its a possibility so its even noticed.",
                                                    "score": 46,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 10:43:10",
                                                    "replies": [
                                                        {
                                                            "author": "mtnmadness84",
                                                            "body": "That makes sense. \u201cSneaky\u201d racism/bias brought to scale.",
                                                            "score": 16,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 11:05:59",
                                                            "replies": []
                                                        },
                                                        {
                                                            "author": "Anticode",
                                                            "body": "> spread racism with a click of a button\n\nI'd argue that the problem is not the AI, it's the spread. People have been doing this inadvertently or intentionally in variously effective ways for centuries, but modern technologies are incredibly subversive.\n\nHumanity didn't evolve to handle so much social information from so many directions, but we *did* evolve to respond to social pressures intrinsically, it's often autonomic. When you combine these two dynamics you've got a planet full of people who jump when they're told to if they're told it in the right way, simultaneously unable to determine who shouted the command and doing it anyway. \n\nMy previous post in the same thread describes a bunch of fun AI/neurology stuff, including our deeply embedded response to social stimulus as something like, \"A shock collar, an activation switch given to every nearby hand.\"\n\nSo, I absolutely agree with you. We should be deeply concerned about force multiplication via AI weaponization.\n\nBut it's important to note that the problem is far more subversive, more bleak. To exchange information across the globe in moments is a beautiful thing, but the elimination of certain modalities of online discourse would fix many things.\n\nIt'd be so, so much less destructive and far more beneficial for our future as a technological species if we could just... Teach people to stop falling for BS like dimwitted primates, stop aligning into trope-based one dimensional group identities.\n\nGood lord.",
                                                            "score": 7,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 12:42:24",
                                                            "replies": [
                                                                {
                                                                    "author": "Deleted",
                                                                    "body": ">  if we could just... Teach people to stop falling for BS like dimwitted primates, stop aligning into trope-based one dimensional group identities.\n\nThere's a lot of money in keeping people dumb, just ask religion about that.",
                                                                    "score": 2,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 14:32:00",
                                                                    "replies": [
                                                                        {
                                                                            "author": "Anticode",
                                                                            "body": "Don't I know it! I actually just wrote a somewhat detailed essay which describes the personality drives which fuel those behaviors, including a study which describes and defines the perplexing ignorance that they're able to self-lobotomize with so effortlessly. \n\nHere's [a direct link](https://www.reddit.com/r/childfree/comments/vmosms/my_mother_everybody_no_contact_now_for_for_5/ie3cth1/) if you're interested-interested, otherwise...\n\n>Study Summary: Human beings have evolved in favor of irrationality, especially when social pressures enforce it, because hundreds of thousands of years ago irrationality wasn't harmful (nobody knew anything) and ghost/monster/spirit stories were helpful (to maintain some degree of order).\n\n>Based on my observations and research, this phenomenon is present most vividly in the same sort of people who demand/require adherence to rigid social frameworks. They adore that stuff by their nature, but there's more. We've all heard so much hypocritical crap, double-talk, wonton theft, and rapey priests... If you've wondered how some people miraculously avoid or dismiss such things?\n\n>Now you know! Isn't that *fun*?",
                                                                            "score": 2,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 15:43:26",
                                                                            "replies": []
                                                                        }
                                                                    ]
                                                                },
                                                                {
                                                                    "author": "Internal-End-9037",
                                                                    "body": "That last paragraph is not gonna happen.  I think it's built into the biology and also the alpha issue always arises and people just fall in line with the new alpha.",
                                                                    "score": 1,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-12-12 16:19:22",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "author": "Atthetop567",
                                                            "body": "How would that happen? Having ais make decisions is only replacing human decisions and those humans are already racist. That is, in fact, why the ai is racist to begin with. It will be exactly as racist as the avergae human it replaces.",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 17:45:34",
                                                            "replies": []
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "Deleted",
                                                    "body": "[removed]",
                                                    "score": 13,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 12:27:09",
                                                    "replies": []
                                                },
                                                {
                                                    "author": "GalaXion24",
                                                    "body": "Many people aren't really racist, but they have unconscious biases of some sort from their environment or upbringing, and when they are pointed out that try to correct for them because they don't think these biases are good. That's more or less where a bot is, since it doesn't actually dislike any race or anything like that, it just happens to have some mistaken biases. Unlike a human though, it won't contemplate or catch itself in that.",
                                                    "score": 3,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 11:54:47",
                                                    "replies": []
                                                },
                                                {
                                                    "author": "Anticode",
                                                    "body": ">There are definitely some racists that can change somewhat rapidly. But there are many humans who \u201cwon\u2019t work to compensate in the data\". \n\nViewed strictly through the lens of emergent systems interactions, there's no fundamental difference between the brain and an AI's growth/pruning dynamics. The connections are unique to each individual even when function is similar. In the same vein, nuanced or targeted \"reprogramming\" is fundamentally impossible (it's not too hard to make a Phineas Gage though).\n\nThese qualities are the result of particular principles of systems interactions [1]. It's true to so that both of these systems operate as \"black boxes\" under similar principles, even upon vastly different mediums ^[2].\n\nThe comparison may seem inappropriate at first glance, especially from a topological or phenomenological perspective, but I suspect that's probably because our ability to communicate is both extraordinary and taken for granted.\n\nWe talk to each other by using mutually recognized symbols (across any number of mediums), but the symbolic elements are not information-carriers, they're information-*representers* that cue the listener; flashcards.\n\nThe same words are often used within our minds as introspective/reflective tools, but our truest thoughts are... Different. They're nebulous and brimming with associations. And because they're truly innate to *your* neurocognitive structure, they're capable of far more speed/fidelity than a word-symbol. [3]\n\n^((I've written comment-essays focused specifically on the nature of words/thoughts, ask if you're curious.))\n\nImagine the mind of a person as a sort of cryptographic protocol that's capable of reading/writing natively. If the technology existed to transfer a raw cognitive \"file\" like you'd transfer a photo, my mental image of a tree could only ever be noise to anyone else. As it stands, a fraction of the population has no idea what a mental image looks like (and some do not yet know they are aphantasic - if this is your lucky day, let me know!)\n\n>Personality-wise, they\u2019d need a redesign from the ground up too. \n\nFor the reasons stated above, it's entirely fair to suggest that a redesign would be the only option (if such an option existed), but humanity's sleeve-trick is a little thing called... Social pressure.\n\nOur evolutionary foundation strongly favors tribe-centric behavioral tendencies, often above what might benefit an individual (short term). Social pressures aren't just impactful, they're often *overriding*; a shock-collar with a switch in every nearby hand.\n\nRacism is itself is typically viewed as one of the more notoriously harmful aspects of human nature, but it's a tribe/kin-related mechanism which means it's easily affected by the same suite. In fact, most of us have probably met a \"selective racist\" whose stereotype-focused nonsense evaporates in the presence of a real person. There are plenty of stories of racists being \"cured\" by nothing more than a bit of encouraged hang-outs.\n\nProblems arise when one's identity is built upon (more like, built with) unhealthy sociopolitical frameworks, but that's a different problem.\n\n___\n\n[1] [Via wiki, Complex Adaptive Systems](https://en.wikipedia.org/wiki/Complex_adaptive_system) A *partial* list of CAS characteristics:\n\n>**Path dependent:** Systems tend to be sensitive to their initial conditions. The same force might affect systems differently.\n\n>**Emergence:** Each system's internal dynamics affect its ability to change in a manner that might be quite different from other systems.\n\n>**Irreducible:** Irreversible process transformations cannot be reduced back to its original state.\n\n\n[2] Note: If this sounds magical, consider how several cheerios in a bowl of milk so often self-organize into various geometric configurations via nothing more than a function of surface tension and plain ol' macroscopic interactions. The underpinnings of neural networks are a *bit* more complicated and yet quite the same... \"Reality make it be like it do.\")\n\n[3] Note: As I understand it, not everyone is finely attuned to their \"wordless thoughts\" and might typically interpret or categorize them as mere impulses.)",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 12:17:44",
                                                    "replies": []
                                                },
                                                {
                                                    "author": "Deleted",
                                                    "body": "[deleted]",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 13:42:04",
                                                    "replies": [
                                                        {
                                                            "author": "mtnmadness84",
                                                            "body": "It was a really dry joke. \n\nIf we\u2019d have figured out how to genuinely change racist, sexist, whatever-ist behavior then it wouldn\u2019t still\nBe all over the place.  People only change if they want to.",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 13:48:07",
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "author": "unholyravenger",
                                            "body": "I think one advantage to AI systems is how detectable racism is. The fact that this study can be done and we can quantify how racist these systems are is a huge step in the right direction. You typically find a human is racist when it's a little too late.",
                                            "score": 17,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 09:52:13",
                                            "replies": [
                                                {
                                                    "author": "BuddyHemphill",
                                                    "body": "Excellent point!",
                                                    "score": 5,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 11:12:56",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "author": "Dominisi",
                                            "body": "Yep, and the issue with doing that is you have to tell an unthinking, purely logical system to ignore the empirical data and instead weight it based off of an arbitrary bias given to it by an arbitrary human.",
                                            "score": 3,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 12:18:43",
                                            "replies": []
                                        },
                                        {
                                            "author": "10g_or_bust",
                                            "body": "We can also \"make\" (to some degree) humans modify their behavior even if they don't agree. So far \"AI\" is living in a largely lawless space where companies repeatedly try to claim 0 responsibility for the data/actions/results of the \"AI\"/algorithm.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 14:19:22",
                                            "replies": [
                                                {
                                                    "author": "Atthetop567",
                                                    "body": "It\u2019s ways eaiser to make ai adjust its behavior. With humans it\u2019s always a dtruggle",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 17:46:52",
                                                    "replies": [
                                                        {
                                                            "author": "10g_or_bust",
                                                            "body": "This is one of those 'easier said than done' things. Plus you need to give the people in charge (not the DEVs, the people who sign paychecks) of the creation of said \"AI\" a reason to do so, right now there is little to none outside of academia or some non profits.",
                                                            "score": 0,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 18:06:28",
                                                            "replies": [
                                                                {
                                                                    "author": "Atthetop567",
                                                                    "body": "Needing to give a reason to make the change applies identically to people and ai. If anything the cheaper to make ai change means the balance favors it more. Making people less racist? Now there is the real raiser said than done. I think you are just grasping at straws for reason to be angry at this pont",
                                                                    "score": 1,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 18:14:52",
                                                                    "replies": []
                                                                },
                                                                {
                                                                    "author": "Henkie-T",
                                                                    "body": "tell me you don't know what you're talking about without telling me you don't know what you're talking about.",
                                                                    "score": 1,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-10-14 15:47:54",
                                                                    "replies": [
                                                                        {
                                                                            "author": "10g_or_bust",
                                                                            "body": "not sure why you felt the need to leave a snappy no value comment 3 months later (weird).\n\nRegardless, I can't talk about any of my work/personal experience in ML/AI in any detail (yay NDAs). However, there have been multiple studies/papers about just how HARD it is not not have bias in ML/AI, which requires being aware of the bias to begin with. Most training sets are biased (similar to how most surveys have some bias due to who is and isn't willing to be surveyed, and/or who is available, etc).\n\nAlmost all current \"AI\" is really ML/neural nets and is/are very focused/specific. Nearly every business doing ML/AI is goal focused; create a bot to filter resumes, create a bot to review loan applications for risk, etc. It's common for external negatives (false loan denials) to be ignored or even valued if it pads the bottom line. Plus the bucket of people that will blindly trust ML output. \n\nThe whole things a mess. Regulations (such as whos on the line when AI/ML makes a mistake) and oversight are sorely needed.",
                                                                            "score": 1,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-10-14 18:45:45",
                                                                            "replies": []
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "author": "Uruz2012gotdeleted",
                                            "body": "Why though? Can we not create an ai that will forget and relearn things? Isn't that how machine learning works anyway?",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 10:14:23",
                                            "replies": [
                                                {
                                                    "author": "Merkuri22",
                                                    "body": "Machine learning is basically extremely complicated pattern identification. You feed it tons and tons of data, it finds patterns in that data, then you feed it your input and it gives you the output that matches it based on the data.\n\nHere's a fairly simple example of how you might apply machine learning in the real world. You've got an office building. You collect data for a few years about the outside air temperature, the daily building occupancy, holiday schedule, and the monthly energy bill. You tell the machine learning system (ML) that the monthly energy bill depends on all those other factors. It builds a mathematical model of how those factors derive the energy bill. That's how you \"train\" the ML.\n\nThen you feed the ML tomorrow's expected air temperature, predicted occupancy, and whether it's a holiday, and it can guess how much your energy bill will be for that day based on that model it made.\n\nIt can get a lot more complex than that. You can feed in hundreds of data points and let the ML figure out which ones are relevant and which ones are not.\n\nThe problem is that, even if you don't feed in race as a data point, the ML might create a model that is biased against race if the data you feed it is biased. The model may accidentally \"figure out\" the race of a person based on other factors, such as where they live, their income, etc., because in the real world there are trends to these things. The model may identify those trends.\n\nNow, it doesn't actually understand what it's doing. It doesn't realize there's a factor called \"race\" involved. It just knows that based on the training data you fed it, people who live here and have this income and go to these stores (or whatever other data they have) are more likely to be convicted of crimes (for example). So if you are creating a data model to predict guilt, it may convict black people more often, even when it doesn't know they're black.\n\nHow do you control for that? That's the question.",
                                                    "score": 20,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 11:01:19",
                                                    "replies": [
                                                        {
                                                            "author": "Activistum",
                                                            "body": "By not automating certain things I would say. Automatic policeing is terrifying because of the depersonalisation it involves, combined with its racist database and implementation. Sometimes, its worth taking a step back and deciding something need not be quantified, need not be automated and codified further, because it can't be done sensibly or its too dangerous to.",
                                                            "score": 3,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 11:41:35",
                                                            "replies": [
                                                                {
                                                                    "author": "Merkuri22",
                                                                    "body": "That's obviously the solution we need for today.\n\nBut people smarter than me are working on seeing if there is actually a solution. Maybe there's some way to feed in explicit racial data and tell it \"ensure your models do not favor one of these groups over the other\". Or maybe there's another solution I haven't even thought of because I only understand a tiny bit of how ML works.\n\nThere are places with lower stakes than criminal law that could be vastly improved if we can create an AI that accounts for bias and removes it.\n\nHumans make mistakes. In my own job, I try to automate as much as possible (especially for repetitive tasks) because when I do things by hand I do it slightly differently each time without meaning to. The more automation I have, the more accurate I become.\n\nAnd one day in the far future, we may actually be able to create an AI that's more fair than we are. If we're able to achieve that, that can remove a lot of inconsistencies and unfairness in the system that gets added simply because of the human factor.\n\nIs this even possible? Who knows. We have a long way to go, certainly, and until then we need to do a LOT of checking of these systems before we blindly trust them. If we did implement any sort of policing AI it's going to need to be the backup system to humans for a long long time to prove itself and work out all the kinks (like unintended racial bias).",
                                                                    "score": 4,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 12:08:01",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "T3hSwagman",
                                                    "body": "It will relearn the same things. Our own data is full of inherent bias and sexism and racism.",
                                                    "score": 5,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 11:05:04",
                                                    "replies": []
                                                },
                                                {
                                                    "author": "asdaaaaaaaa",
                                                    "body": "> Isn't that how machine learning works anyway?\n\nI mean, saying \"machine learning works via learning/unlearning things\" is about as useful as saying \"Cars work by moving\". It's a bit more complicated than that.",
                                                    "score": 2,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 11:10:06",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "author": "Deleted",
                                            "body": "> a human can be told that racism is bad and might work to compensate in the data. \n\nCan you provide an example? Because it kind of comes across as saying a human knows when to be racist in order to skew data so that the results don\u2019t show a racist bias.",
                                            "score": -3,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 09:28:12",
                                            "replies": [
                                                {
                                                    "author": "frogjg2003",
                                                    "body": "That's basically what affirmative action is, intentionally biasing your decision making to correct for a bias in your input. As for examples, I got into an argument with an AI researcher and they gave some examples. It was a few weeks ago, so it might take a little while to search for it.",
                                                    "score": 7,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 09:49:08",
                                                    "replies": [
                                                        {
                                                            "author": "Deleted",
                                                            "body": "The examples are what\u2019s interesting to me, because I can\u2019t think of any which can\u2019t be solved by not providing race/ethnicity/gender\u2026 whatever we don\u2019t care about, to the AI. \n\nLike, if an AI determines that poor white people are more likely to reoffend for spousal assault crimes and this causes some issue in their decision made, then don\u2019t provide the AI information about the convicted\u2019s race. Or don\u2019t include race in the training data. \n\nRather than take the decision with the racial bias and try to adjust it downwards after the fact such as because the convicted person is poor and white and the case was about spousal abuse.",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 15:18:54",
                                                            "replies": [
                                                                {
                                                                    "author": "frogjg2003",
                                                                    "body": "It's the other way around. Most data sets don't include racial data or other information we might want to avoid bias on. Because other variables correlate to race, the \"race blind\" decision is still going to include a lot of racial bias. The AI didn't determine that white people are more likely to reoffend, it determines that factors that correlate with being white lead to reoffending. Including race in the data set might allow the AI to measure its own bias and correct for it.",
                                                                    "score": 3,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 15:36:06",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "zanraptora",
                                                    "body": "A rational human can compensate for bias by auditing themselves. An learning engine cannot since it doesn't have the capacity (yet) to assess its output critically.\n\nA human (hopefully) knows that that similar crimes should have similar sentences across demographics all else being similar. An AI is incapable of that values judgement, and it defeats its purpose if you can't figure out how to get it to come to that conclusion on its own.",
                                                    "score": 2,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 10:44:41",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "author": "rainer_d",
                                            "body": "Can't you have another AI that is specialized on detecting racism look at the results of the first AI and suggest corrects?\n\n;-)\n\nI mean, if racisms is a pattern, ML should be able to detect it, right?",
                                            "score": -1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 10:54:36",
                                            "replies": [
                                                {
                                                    "author": "frogjg2003",
                                                    "body": "In order to recognize racism, race has to be an explicitly  measured variable. Not all datasets will include race, so detecting that pattern would be impossible. Yes, if race is an available variable, you can correct the AI to balance across race, but that requires modifying the rewards, resulting in a less optimal solution for the problem if you weren't balancing race.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 11:25:36",
                                                    "replies": [
                                                        {
                                                            "author": "rainer_d",
                                                            "body": "There's always racism. People are like that. Smarter ones just recognize when their judgment is off because of that - which depends among other things on the society one lives in.\n\nTechnology can't save social or political problems.",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 12:45:16",
                                                            "replies": []
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "wild_man_wizard",
                                                    "body": "Doesn't really matter much, racial essentialists will assume any race-based difference in outcomes are due to race, and egalitarians will assume all such differences are due to racism.  And reality may be one of the other something in between.  When humans study humans (and data analysis is just another way of doing that study) there is always that sort of halting problem.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 11:42:01",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "author": "eazolan",
                                            "body": "So AI is inherently bigoted?",
                                            "score": -1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 14:13:44",
                                            "replies": [
                                                {
                                                    "author": "frogjg2003",
                                                    "body": "AI is ignorant. If the data is biased, it will happily take the data, crunch the numbers, and produce a biased answer. It's just a machine that does whatever programmer and data tell it to do.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 14:17:56",
                                                    "replies": [
                                                        {
                                                            "author": "eazolan",
                                                            "body": "I'm pointing out that we load AI with data, and then it doesn't learn from there. \n\nIf reality doesn't match the dataset it was initially given, it doesn't handle it well.",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 15:47:53",
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "author": "Yancy_Farnesworth",
                                            "body": "The problem is that a human being told racism is bad is as hard as telling an AI that racism is bad (Yes, I'm stressing the irony of trying to teach something is bad to something that can't think, an AI). Humans and society are conditioned to trust their own perceptions and that anything counter to those perceptions is either bad or wrong.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 11:35:00",
                                            "replies": []
                                        },
                                        {
                                            "author": "Deleted",
                                            "body": "You can \"fine tune\" nn",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 11:50:36",
                                            "replies": [
                                                {
                                                    "author": "frogjg2003",
                                                    "body": "And hpw exactly do you fine tune a neural network to recognize race and then correct for that bias? Unlike humans, an AI is completely ignorant of anything except it's intended purpose.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 11:54:03",
                                                    "replies": [
                                                        {
                                                            "author": "Deleted",
                                                            "body": "You are the creator, you decide what is good and what is bad . It's not easy",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 11:57:48",
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "author": "hurpington",
                                            "body": "Also depends on your definition of racism.  2 people looking at the same data might have differing opinions on if its racist or not.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 12:32:14",
                                            "replies": []
                                        }
                                    ]
                                },
                                {
                                    "author": "SeeShark",
                                    "body": "Sort of, except I don't love the framing of human racism as data-driven. It isn't really; humans employ biases and heuristics vigorously when interpreting data.",
                                    "score": 2,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:57:04",
                                    "replies": [
                                        {
                                            "author": "Deleted",
                                            "body": "Aren't human biases often formed by incorrect data, be it from parents, friends, family, internet, newspapers, media, etc? A bad experience with a minority, majority, male or female can affect bias... even though it's a very small sample from those groups. Heuristics then utilize those biases.\n\nI'm just a networking guy, so only my humble opinion not based on scientific research.",
                                            "score": 13,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 10:30:01",
                                            "replies": [
                                                {
                                                    "author": "alonjar",
                                                    "body": "So what happens when there are substantial differences in legitimate data though?  How are we judging a racist bias vs a real world statistical correlation?\n\nIf Peruvians genuinely have some genetic predisposition towards doing a certain thing more than a Canadian, or perhaps have a natural edge to let them be more proficient at a particular task, when is that racist and when is it just fact?\n\nI forsee a lot of well intentioned people throwing away a lot of statistically relevant/legitimate data on the grounds of being hyper sensitive to diminishing perceived bias.\n\nIt'll be interesting to see play out.",
                                                    "score": 17,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 11:29:24",
                                                    "replies": [
                                                        {
                                                            "author": "bhongryp",
                                                            "body": "Peruvian and Canadian would be bad groups to start with. The phenotypical diversity in the two groups is nowhere close to equivalent, so any conclusion you made comparing the \"natural\" differences between the two would probably be bigoted in some way. Furthermore, in most modern societies, our behaviour is determined just as much (if not more) by our social environment than our genetics, meaning that large behavioural differences between Peruvians and Canadians are likely learned and not a \"genetic predisposition\".",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 12:31:27",
                                                            "replies": []
                                                        },
                                                        {
                                                            "author": "Atthetop567",
                                                            "body": "Just beaxufse it\u2019s a fact doesn\u2019t make it not racist.",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 17:47:42",
                                                            "replies": []
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "SeeShark",
                                                    "body": "Depends how you define \"data,\" I suppose. When a person is brought up being told that Jews are Satanists who drink blood, there's not a lot of actual data there.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 11:55:26",
                                                    "replies": []
                                                },
                                                {
                                                    "author": "Cualkiera67",
                                                    "body": "I don't understand why we train AI using data. Shouldn't we program it using the rules it is expected to follow?\n\nPrevious experiences seen irrelevant. Only the actual rules of conduct seem relevant. So maybe they entire concept of training AI with data is flawed to begin with",
                                                    "score": 0,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 10:41:45",
                                                    "replies": [
                                                        {
                                                            "author": "Deleted",
                                                            "body": "That's been tried before in the beginning, building from the ground up. It's slow, unadaptive, and not actually \"intelligent\". Datasets is the equivalent of guess and check and experiential learning. The difference between the two methods is this: If you had a choice between two doctors, the first that had 6 years of college and 4 years of residency, or a second that had 12 years of college, but no residency at all. You probably would pick the one that actually had done it before.",
                                                            "score": 5,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 10:56:05",
                                                            "replies": []
                                                        },
                                                        {
                                                            "author": "Marchesk",
                                                            "body": "It doesn't work nearly as well. But there has been a long term attempt to make a generalized AI from a very large ruleset created by humans called Cyc. The idea being that intelligence is two million rules (or whatever the number quoted by the founder back in 1984 or something).\n\nThat sort of thing might have it's place, it just hasn't seen the kind of rapid success machine learning has the past decade. Humans aren't smart enough to design an AI from the ground up like that. The world is too messy and complicated.",
                                                            "score": 2,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 12:31:10",
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "author": "McMarbles",
                                    "body": "Who knew intelligence isn't wisdom. We have AI but now we need AW.\n\nBeing able to morph and utilize data: intelligence.\n\nUnderstanding when to do it and when not: wisdom.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 10:45:12",
                                    "replies": [
                                        {
                                            "author": "Deleted",
                                            "body": "[deleted]",
                                            "score": 3,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 11:18:52",
                                            "replies": [
                                                {
                                                    "author": "MoreRopePlease",
                                                    "body": "Knowing that fruit belongs in a salad, now...  (Sometimes, at least)",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 11:44:02",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "author": "Cualkiera67",
                                    "body": "But a human can choose to break from their upbringing and traditions. It happens.\n\nCan an AI identify bias in its data, and choose to deviate from it? Maybe that's the next step in AI",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 10:33:40",
                                    "replies": []
                                },
                                {
                                    "author": "RunItAndSee2021",
                                    "body": "\u2018robots\u2019 in the post title has the potential for more depth of interpretation.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 20:54:31",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "BabySinister",
                            "body": "Maybe it's time to shift focus from training AI to make it useful in novel situations to gathering datasets that can be used in a later stage to teach AI, where the focus is getting as objective a data set as possible? Work with other fields etc.",
                            "score": 68,
                            "depth": 2,
                            "timestamp": "2022-06-28 01:34:39",
                            "replies": [
                                {
                                    "author": "teryret",
                                    "body": "You mean manually curating such datasets?  There are certainly people working on exactly that, but it's hard to get funding to do that because the marginal gain in value from an additional datum drops roughly ~~logarithmically~~ exponentially (ugh, it's midnight and apparently I'm not braining good), but the marginal cost of manually checking it remains fixed.",
                                    "score": 156,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 01:46:24",
                                    "replies": [
                                        {
                                            "author": "hawkeye224",
                                            "body": "How would you ensure that manually curating data is objective? One can always remove data points that do not fit some preconception.. and they could either agree or disagree with yours, affecting how the model works.",
                                            "score": 2,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 18:03:07",
                                            "replies": [
                                                {
                                                    "author": "teryret",
                                                    "body": "Yep.  Great question!",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-29 09:38:21",
                                                    "replies": []
                                                },
                                                {
                                                    "author": "Adamworks",
                                                    "body": "Depending on the problem, you can purposely generate data through a random sampling process. Rather than starting with dirty data trying to clean it, you start with clean data and keep it clean through out the data collection process.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-29 12:16:09",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "author": "BabySinister",
                                            "body": "I imagine it's gonna be a lot harder to get funding for it over some novel application of AI I'm sure, but it seems like this is a big hurdle the entire AI community needs to take. Perhaps by joining forces, dividing the work, and working with other fields it can be done more efficiently and need less lump sum funding.\n\n  \n\n\nIt would require a dedicated effort, which is always hard.",
                                            "score": 11,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 01:57:58",
                                            "replies": [
                                                {
                                                    "author": "asdaaaaaaaa",
                                                    "body": "> but it seems like this is a big hurdle the entire AI community needs to take.\n\nIt's a big hurdle because it's not easily solvable, and any solution is a marginal percentage increase in the accuracy/usefulness of the data. Some issues, like some 'points' of data not being accessible (due to those people not even having/using internet) simply aren't solvable without throwing billions at the problem. It'll improve bit by bit, but not all problems just require attention, some aren't going to be solved in the next 50/100 years, and that's okay too.",
                                                    "score": 31,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 06:51:49",
                                                    "replies": [
                                                        {
                                                            "author": "ofBlufftonTown",
                                                            "body": "Why is it \u201cOK too\u201d if the AIs are enacting nominally neutral choices the outcomes of which are racist? Surely the answer is just not to use the programs until they are not unjust and prejudiced? It\u2019s easier to get a human to follow directions to avoid racist or sexist choices (though not entirely easy as we know) than it is to just let a program run and give results that could lead to real human suffering. The beta version of a video game is buggy and annoying. The beta version of these programs could send someone to jail.",
                                                            "score": 4,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 08:59:40",
                                                            "replies": [
                                                                {
                                                                    "author": "asdaaaaaaaa",
                                                                    "body": "> Why is it \u201cOK too\u201d\n\nBecause in the real world, some things just are. Like gravity, or thermal expansion, or our current limits of physics (and our understanding of it). It's not positive, or great, but it's reality and we have to accept that. Just like how we have to accept that we're not creating unlimited, free, and safe energy anytime soon. In this case, AI are learning from humans and unfortunately picking up on some of the negatives of humanity. Some people do/say bad things, and those bad things tend to be a lot louder than nice things, of course an AI will pick up on that.\n\n>if the AIs are enacting nominally neutral choices the outcomes of which are racist?\n\nBecause the issue isn't with the AI, it's just with the dataset/reality. Unfortunately, there's a lot of toxicity online and from people in general. We might have to accept that from many of our datasets, some nasty tendencies that might accurately represent some behaviors of people will pop up.\n\nIt's not objectively \"good\" or beneficial that we have a rude/aggressive AI, but if enough people are rude/aggressive, the AI will of course emulate the behaviors/ideals from their dataset. Same reason why AI have a lot of other \"human\" tendencies, when humans design something human problems tend to follow. I'm not saying \"it's okay\" as in it's not a problem or concern, more that like other aspects of reality and we can either accept/work with that, or keep bashing our heads against the wall in denial.",
                                                                    "score": 7,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 09:17:12",
                                                                    "replies": [
                                                                        {
                                                                            "author": "AnIdentifier",
                                                                            "body": ">Because the issue isn't with the AI, it's just with the dataset/reality. \n \nBut the solution you're offering includes the data. The ai - as you say - would do nothing without it, so you can't just wash your hands and say 'close enough'. It's making a bad situation worse.",
                                                                            "score": 9,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 10:09:21",
                                                                            "replies": []
                                                                        },
                                                                        {
                                                                            "author": "WomenAreFemaleWhat",
                                                                            "body": "We don't have to accept it though. You *have* decided its okay. You've decided its good enough for white people/men so its okay to use despite being racist/sexist. You have determined that whatever gains/profits you get are worth the price of sexism/racism. If they biased it against white people/ women wed decide it was too inaccurate and shouldn't be used. Because its people who are always told to take a back burner, its okay. The AI will continue to collect biased data and exacerbate the gap. We already have huge gaps in areas like medicine. We don't need to add more.\n\nI hate people like you. Perfectly happy to coast along as long as it doesn't impact you. You don't stand for anything.",
                                                                            "score": 5,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 10:34:03",
                                                                            "replies": []
                                                                        },
                                                                        {
                                                                            "author": "ofBlufftonTown",
                                                                            "body": "The notion that very fallible computer programs, based on historically inaccurate data (remember when the google facial recognition software classified black woman as gorillas?) is something like the law of gravity is so epically stupid that I am unsure of how to engage with you at all. I suppose your technological optimism is a little charming in its way.",
                                                                            "score": 4,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 12:26:23",
                                                                            "replies": []
                                                                        }
                                                                    ]
                                                                },
                                                                {
                                                                    "author": "redburn22",
                                                                    "body": "Why are you assuming that it\u2019s easier for humans to be less racist or biased than a model?\n\nIf anything I think history shows that people change extremely slowly - over generations. And they think they\u2019re much less bigoted than they are. Most people think they have absolutely no need to change at all.\n\nConversely it just takes one person to help a model be less biased. And then that model will continue to be less biased. Compare that to trying to get thousands or more individual humans to all change at once.\n\nIf you have evidence that most AI models are actually worse than people then I\u2019d love to see the evidence but I don\u2019t think that\u2019s the case. The models are actually biased because the data they rely on, created by biased people, is biased. So those people are better than the model? If that were true then the model would be great as well\u2026",
                                                                    "score": 3,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 09:20:36",
                                                                    "replies": [
                                                                        {
                                                                            "author": "SeeShark",
                                                                            "body": "It's difficult to get a human to be less racist.\n\nIt's impossible to get a machine learning algorithm to be less racist if it was trained on racist data.",
                                                                            "score": 5,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 10:00:55",
                                                                            "replies": [
                                                                                {
                                                                                    "author": "redburn22",
                                                                                    "body": "You absolutely can improve the bias of models by finding ways to counterbalance the bias in the data. Either by finding better ways to identify data that has a bias or by introducing corrective factors to balance it out.\n\nBut regardless, not only do you have biased people, you also have people learning from similarly biased data. \n\nSo even if somebody is not biased at all, when they have to make a prediction they are going to be using data as well. And if that data is irredeemably flawed then they are going to make biased decisions. So I guess what I\u2019m saying is that the model will be making neutral predictions based on biased data. The person will also be using biased data, but some of them will be neutral whereas others will actually have ill intent.\n\nOn the other hand, if people can somehow correct for the bias in the data they have, then there is in fact a way to correct for it or improve it, and a model can do the same. And I suspect that a model is going to be far more accurate in systematic in doing so.\n\nYou only have to create an amazing model once. Versus you have to train tens of thousands of people to both be less racist and be better at identifying and using less biased data",
                                                                                    "score": 0,
                                                                                    "depth": 9,
                                                                                    "timestamp": "2022-06-28 10:12:41",
                                                                                    "replies": []
                                                                                },
                                                                                {
                                                                                    "author": "jovahkaveeta",
                                                                                    "body": "If this was the case then no model could improve over time which is an absolutely laughable idea. Software is easily replaced and improved upon as evidenced by the last 20 years of developments in the field. Look at GPS today vs ten years ago it shows massive improvements over a short time period as data sets continually got larger.",
                                                                                    "score": 1,
                                                                                    "depth": 9,
                                                                                    "timestamp": "2022-06-28 19:20:46",
                                                                                    "replies": [
                                                                                        {
                                                                                            "author": "SeeShark",
                                                                                            "body": ">as data sets continually got larger\n\nYes, as more data was introduced. My point is that without changing the data, there's not a lot we know to do that can make machine learning improve its racism issue; and, unfortunately, we're not exactly sure how to get a better data set yet.",
                                                                                            "score": 1,
                                                                                            "depth": 10,
                                                                                            "timestamp": "2022-06-28 19:42:24",
                                                                                            "replies": [
                                                                                                {
                                                                                                    "author": "redburn22",
                                                                                                    "body": "That almost implies that there is a single data set / use case.\n\nIn many cases we can correct data to reduce bias. In other situations we might not be able to yet. But, restating my point in another comment, if the data is truly unfixable then both humans and models are going to make predictions using totally flawed data.\n\nA non-biased person, like a model, still has to make predictions based on data. And if the data is totally messed up and unfixable then they, like the model, will make biased and inaccurate decisions.\n\nIn other words this issue is not specific to decisions made by models",
                                                                                                    "score": 1,
                                                                                                    "depth": 11,
                                                                                                    "timestamp": "2022-06-29 06:33:49",
                                                                                                    "replies": []
                                                                                                },
                                                                                                {
                                                                                                    "author": "jovahkaveeta",
                                                                                                    "body": "User data makes the app have more data though. That is literally how google maps got better was by getting data from users.",
                                                                                                    "score": 1,
                                                                                                    "depth": 11,
                                                                                                    "timestamp": "2022-06-28 20:34:24",
                                                                                                    "replies": []
                                                                                                }
                                                                                            ]
                                                                                        }
                                                                                    ]
                                                                                }
                                                                            ]
                                                                        }
                                                                    ]
                                                                },
                                                                {
                                                                    "author": "jovahkaveeta",
                                                                    "body": "Perfect is the enemy of the good, so long as the AI is equivalent or slightly better than humans it can begin being used.",
                                                                    "score": 1,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 19:17:55",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "teryret",
                                                    "body": "> It would require a dedicated effort, which is always hard.\n\nWell, if ever you have a brilliant idea for how to get the whole thing to happen I'd love to hear it.  We do take the problem seriously, we just also have to pay rent.",
                                                    "score": 29,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 02:01:18",
                                                    "replies": [
                                                        {
                                                            "author": "SkyeAuroline",
                                                            "body": ">We do take the problem seriously, we just also have to pay rent. \n\nDecoupling scientific progress from needing to turn a profit so researchers can eat would be a hell of a step forward for all these tasks that are vital but not immediate profit machines, but that's not happening any time soon unfortunately.",
                                                            "score": 32,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 08:53:35",
                                                            "replies": [
                                                                {
                                                                    "author": "teryret",
                                                                    "body": "This, 500%.  It has to start with money.",
                                                                    "score": 8,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 09:29:40",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "author": "BabySinister",
                                                            "body": "I'm sure there's conferences in your field right? In other scientific fields when a big step has to be taken that benefits the whole field but is time consuming and not very well suited to bring in the big funds you network, team up and divide the work. In the case of AI I imagine you'd be able to get some companies on board, Meta, alphabet etc, who also seem to be (very publicly) struggling with biased data sets on which they base their AI.\n\n  \n\n\nSomeone in the field needs to be a driving force behind a serious collaboration, right now everybody acknowledges the issue but it's waiting for everybody else to fix it.",
                                                            "score": -2,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 02:06:42",
                                                            "replies": [
                                                                {
                                                                    "author": "teryret",
                                                                    "body": "Oh definitely, and it gets talked about.  Personally, I don't have the charisma to get things to happen in the absence of a clear plan (eg, if asked \"How would a collaboration improve over what we've tried so far?\" I would have to say \"I don't know, but not collaborating hasn't worked, so maybe worth a shot?\").  So far talking is the best I've been able to achieve.",
                                                                    "score": 21,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 02:24:49",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "SolarStarVanity",
                                                    "body": "> I imagine it's gonna be a lot harder to get funding for it over some novel application of AI I'm sure,\n\nSeeing how this is someone from a **company** you are talking to, I doubt they could get any funding for it.\n\n> but it seems like this is a big hurdle the entire AI community needs to take. \n\nThere is no AI community.\n\n> Perhaps by joining forces, dividing the work, and working with other fields it can be done more efficiently and need less lump sum funding.\n\nOr perhaps not. How many rent payments are you willing to personally invest into answering this question?\n\n-----\n\nThe point of the above is this: bringing a field together to gather data that could then be all shared to address an important problem doesn't really happen outside academia. And in academia, virtually no data gathering at scale happens either, simply because people have to graduate, and the budgets are tiny.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 07:31:03",
                                                    "replies": []
                                                },
                                                {
                                                    "author": "NecessaryRhubarb",
                                                    "body": "I think the challenge is the same that humans face. Is our definition of racism and sexism different today than it was 100 years ago? Was the first time you met someone different a shining example on how to treat someone else? What if they were a jerk, and your response was not based on the definition at that time, but based on that individual? \n\nIt\u2019s almost like a neutral, self reflecting model has to be run to course correct the first experiences of every bot. That model doesn\u2019t exist though, and it struggles with the same problems. Every action needs context, which feels impossible.",
                                                    "score": 0,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 09:09:01",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "author": "optimistic_void",
                                            "body": "Why not throw another neutral network at it, one that you train to detect racism/sexism ?",
                                            "score": -1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 08:04:04",
                                            "replies": [
                                                {
                                                    "author": "Lykanya",
                                                    "body": "How would you even do that? Just assume that any and every difference between groups is \"racism\" and nothing else? \n\nThis is fabricating data to fit ideology, what harm can this cause? what if there ARE problems with X or Y group that have nothing to do with racism, and thus become hidden away into ideology instead of being resolved? \n\nWhat if X group lives in an area with old infrastructure, thus too much lead in the water or w/e, this problem would never be investigated because lower academic results in there would just be attributed to racism and biases because the population happened to be non-white? And what if the population is white and there are socio-economic factors at play? assume its not racism and its their fault because they aren't BIPOC? \n\nThis is a double-edged blade that has potential to harm those groups either way. Data is data, algorythms can't be racist, they only interpret data. If there is a need to solve potential biases it needs to be at the source of data collection, not the AI's.",
                                                    "score": 30,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 08:17:13",
                                                    "replies": [
                                                        {
                                                            "author": "optimistic_void",
                                                            "body": "Initially, you would manually find some data that you are certain about that it contains racism/sexism and feed it to the network. Once enough underlying patterns are identified, you'd have a working racism/sexism detector running full auto. Now obviously there is a bias of the person selecting the data but that could be mitigated by having multiple people verifying it.\n\nAfter this \"AI\" gets made you can pipe the datasets through it to the main one and that's it. Now clearly this kind of project would have value even beyond this scope (lending it to others for use), so this might already be in the making.",
                                                            "score": -10,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 08:37:51",
                                                            "replies": [
                                                                {
                                                                    "author": "paupaupaupau",
                                                                    "body": "Let's say you could do this, hypothetically.  Then what?\n\nThe broader issue here is still that the available training data is biased, and collectively, we don't really have a solution.  Even throwing aside the fundamental issues surrounding building a racism-detecting model, the incentive structure (whether it's academic funding, private enterprise, etc.) isn't really there to fix the issue (and that issue defies an easy fix, even if you had the funding).",
                                                                    "score": 3,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 10:36:05",
                                                                    "replies": [
                                                                        {
                                                                            "author": "optimistic_void",
                                                                            "body": "Then what ? This was to solve the exponential drop.\n\nBut addressing the broader issue: Everyone is biased to a lesser or greater degree, either on the basis of willful ignorance or just lack of understanding or information. But that doesn't mean we shouldn't try to correct that. We use our reasoning to suppress our own irrational thoughts and behaviours. Just because our reasoning is still biased and even the suppression is, it doesn't mean it has no merit. This is how we improve as a species after all. And there is also no reason not to try to use external tools in an attempt to aid this. At this point, our tools are already a part of what we are, and whether we do it now or later, this kind of thing is likely inevitable. The incentive is already there, it is  humanity's self improvement.\n\nThere is clearly a lot of room for misuse, but it will happen regardless of what we do anyway - this too is a part of human nature and we should to try our best to correct that as well.",
                                                                            "score": 1,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 11:10:41",
                                                                            "replies": []
                                                                        }
                                                                    ]
                                                                },
                                                                {
                                                                    "author": "FuckThisHobby",
                                                                    "body": "Have you talked to people before about what racism and sexism actually are? Some people are very sensitive to any perceived discrimination when none may exist, some people are totally blind to discrimination because it's never personally affected them. How would you train an AI and how would you hire people who aren't ideologically motivated?",
                                                                    "score": 1,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 11:14:28",
                                                                    "replies": [
                                                                        {
                                                                            "author": "optimistic_void",
                                                                            "body": "As I mentioned in my other comment, human bias is basically unavoidable and technology as an extension of us is likely to carry this bias as well. But that doesn't mean we can't try to implement systems like this, perhaps it might lead to some progress, no ? The misuse is also unavoidable and will happen regardless.\n\nIf we accept that the system will be imperfect, we can come up with some rudimentary solutions, for example ( don't take this too literally) we could take a group of people from different walks of life and have them each go through 10 000 comments and judge if they contain said issues. We would have comments where everyone judged the comments negatively and some where only part of the people did so. This would then result in weighted data ranging from \"might be bad\", to \"completely unacceptable\" making up for the nuance.",
                                                                            "score": 1,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 14:44:50",
                                                                            "replies": []
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "jachymb",
                                                    "body": "You would need to train that with lots of examples of racism and non-racism - whatever that specifically means in your application. That's normally not easily available.",
                                                    "score": 8,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 08:28:41",
                                                    "replies": []
                                                },
                                                {
                                                    "author": "teryret",
                                                    "body": "How do you train that one?",
                                                    "score": 3,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 09:38:01",
                                                    "replies": [
                                                        {
                                                            "author": "optimistic_void",
                                                            "body": "Initially this would admittedly also require manual curating as I mentioned in my other comment - you would need people to sieve through data to identify with certainty  what is racist/sexist data, and what is not ( forgot to mention that part but it's kinda obvious) before feeding it to the network.\n\nBut I believe this could deal with the exponential drop issue - and it could also be profitable to lend this kind of technology once it gets made.",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 10:10:33",
                                                            "replies": []
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "teryret",
                                                    "body": "Because if you have the power to train that kind of network you might as well use it to train the first one correctly.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-29 09:18:13",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "author": "Killiander",
                                            "body": "Maybe someone can make an AI that can scrub biases from data sets for other AI\u2019s.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 14:08:14",
                                            "replies": []
                                        },
                                        {
                                            "author": "Adamworks",
                                            "body": "That's not necessarily true. Biased data shrinks your effective sample size massively. For example, even if your training dataset is made up of 50% of all possible cases in your population you are studying, a modest amount of bias can make your data behave as if you only 400 cases. Unbiased data is worth its weight in gold.\n\nCheck out this paper on [\"Statistical paradises and paradoxes\"](https://statistics.fas.harvard.edu/files/statistics-2/files/statistical_paradises_and_paradoxes.pdf)",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-29 11:52:25",
                                            "replies": []
                                        }
                                    ]
                                },
                                {
                                    "author": "JohnMayerismydad",
                                    "body": "Nah, the key is to not trust some algorithm to be a neutral arbiter because no such thing can exist in reality. Trusting some code to solve racism or sexism is just passing the buck onto code for humanity\u2019s ills.",
                                    "score": 42,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 06:15:44",
                                    "replies": [
                                        {
                                            "author": "BabySinister",
                                            "body": "I don't think the goal here is to try and solve racism or sexism through technology, the goal is to get AI to be less influenced by racism or sexism.\n\nAt least, that's what I'm going for.",
                                            "score": 25,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 09:55:29",
                                            "replies": [
                                                {
                                                    "author": "JohnMayerismydad",
                                                    "body": "AI could almost certainly find evidence of systemic racism by finding clusters of poor outcomes.  Like look where property values are lower and you find where minority neighborhoods are.  Follow police patrols and you find the same.  AI could probably identify even more that we are unaware of.\n\nIt\u2019s the idea that machines are not biased that I take issue with.  Society is biased so anything that takes outcomes and goals of that society will carry over those biases",
                                                    "score": 0,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 11:39:43",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "author": "hippydipster",
                                            "body": "And then we're back to relying on judge's *judgement*, or teacher's *judgement*, or a cops *judgement*, or...\n\nAnd round and round we go.\n\nThere's real solutions, but we refuse to attack these problems at their source.",
                                            "score": 6,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 09:32:54",
                                            "replies": [
                                                {
                                                    "author": "joshuaism",
                                                    "body": "And those real solutions are...?",
                                                    "score": 8,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 09:37:57",
                                                    "replies": [
                                                        {
                                                            "author": "hippydipster",
                                                            "body": "They involve things like economic fairness, generational-length disadvantages and the like.  A UBI is an example of a policy that addresses such root causes of the systemic issues in our society.",
                                                            "score": 4,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 10:27:54",
                                                            "replies": [
                                                                {
                                                                    "author": "joshuaism",
                                                                    "body": "UBI is a joke.  A handout to landlords.",
                                                                    "score": -6,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 10:46:54",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "JohnMayerismydad",
                                                    "body": "Sure.  We as humans can recognize where biases creep into life and justice. Pretending that is somehow objective is what leads to it spiraling into a major issue.  The law is not some objective arbiter, and using programming to pretend it is is a very dangerous precedent",
                                                    "score": 6,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 09:39:40",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "author": "Deleted",
                                            "body": "[removed]",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 10:04:07",
                                            "replies": [
                                                {
                                                    "author": "Deleted",
                                                    "body": "The problem here, especially in countries with deep systematic racism and classism is you're essentially saying this...\n\n\"AI might be able to see grains of sand...\" While we ignore the massive boulders and cobble placed there by human systems.",
                                                    "score": 5,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 10:46:33",
                                                    "replies": [
                                                        {
                                                            "author": "Igoritzy",
                                                            "body": "What exactly did you want to say with this ? \n\nBiological classification follows taxonomic rank, and that model of biological analytics works quite nicely. And it actually helps in discovering new forms of life, and assigning newly discovered species into valid ranks. It's only because of violent history of our predecessors that we now have only one species of Homo genus, and that is Sapiens (We actually killed off every other Homo species, of which there were 7) \n\nSuch a system even though flawed (for example, there are species from different genus that can reproduce, even from different family), is still the best working system of biological classification \n\nTalking science, race should be a valid term. When you see Patel Kumari from India, Joe Spencer from USA and Chong Li from China, there is 99.999% chance you will get their nationality and race by their visual traits. Isolate certain races for 500 years (which is enough now that we know how basics of epigenetics work), and they will eventually become different species. \n\nAs someone mentioned (but deleted in the meantime), dogs are all same species - Canis familiaris. And, they are genetically basically the same thing. But only someone insane, indoctrinated or stubborn will claim that there is no difference between a Maltese, Great Danish and American Pit-bull \n\nAI wouldnt care for racist beliefs, past or present. You had 200+ years of black people being exploited and tortured, nowadays you can actually observe reverse-racism in a form of benefits for black people (which discriminates other races), diversity quotas and other stuff that blatantly presents itself as anti-racism while using race as a basis. \n\nAI (supposedly unbiased and highly intelligent) will present facts - and, if by any chance those facts could be interpreted as racism, that will not be an emotional reaction, but rather a factual one. Why are so many black athletes good at sports, and better than caucasian ? is it racist or factual ? Now assign any other racial trait to a race, positive or negative, and once again, ask yourself - is it racist, or factual ?",
                                                            "score": 0,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 21:43:13",
                                                            "replies": [
                                                                {
                                                                    "author": "Deleted",
                                                                    "body": "Oh, I get it. You like the racist system we have.",
                                                                    "score": 1,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 22:39:35",
                                                                    "replies": [
                                                                        {
                                                                            "author": "Igoritzy",
                                                                            "body": "For god's sake, acknowledging races using scientific method and being racist are 2 completely different things. Did you even read what I wrote with even a bit of comprehension ?",
                                                                            "score": 0,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 23:24:54",
                                                                            "replies": []
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "author": "AidGli",
                                    "body": "This is a bit of a naive understanding of the problem, akin to people pointing to \u201cthe algorithm\u201d as what decides what you see on social media. There aren\u2019t canonical datasets for different tasks (well there generally are for benchmarking purposes but using those same ones for training would be bad research from a scientific perspective) novel applications often require novel datasets, and those datasets have to be gathered for that specific task. \n\nconstructing a dataset for such a task is definitionally not something you can do manually, otherwise you are _still_ imparting your biases on the model. constructing an objective dataset for a task relies on some person\u2019s definition of objectivity. Oftentimes, as crappy as it is, it\u2019s easier to kick the issue to just reflecting society\u2019s biases.\n\nwhat you are describing here is not an AI or data problem but rather a societal one. Solving it by trying to construct datasets just results in a different expression of the exact same issue, just with different values.",
                                    "score": 12,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:19:33",
                                    "replies": [
                                        {
                                            "author": "Specific_Jicama_7858",
                                            "body": "This is absolutely right. I just got my PhD in human robot interaction. We as a society don't even know what an accurate unbiased perspective looks like to a human. As personal robots become more socially specialized this situation will be stickier. But we don't have many human-human research studies to compare to. And there isn't much incentive to conduct these studies because it's \"not progressive enough\"",
                                            "score": 3,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 15:53:11",
                                            "replies": []
                                        }
                                    ]
                                },
                                {
                                    "author": "InternetWizard609",
                                    "body": "It doesnt have a big return and the people curating can include biases.\n\nPlus If I want people tailored for my company, I want people that will fit MY company, not a generalized version of it, so many places would be agaisnt using those objective datasets, because they dont fit their reality as well as the biased dataset",
                                    "score": 3,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 11:30:38",
                                    "replies": []
                                },
                                {
                                    "author": "jhmpremium89",
                                    "body": "Ehhh\u2026 the datasets we have are plenty objective.",
                                    "score": -13,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 03:05:13",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "tzaeru",
                            "body": "Perhaps the answer for now is that we shouldn't be making AIs for production with any strict rules when there's a risk of discriminatory biases. We as a species have a habit of always trying to produce more, more optimally, more effortlessly, and we want to find new things to sell, to optimize, to produce.\n\nBut we don't really need to. We do not need AIs that filter job candidates (aside of maybe some sort of spam spotting AIs and the like), we do not need AIs that decide your insurance rate for you, we do not need AIs that play with your kid for you.\n\nYet we want these things but why? Are they *really* going to make the world into a better place for all its inhabitants?\n\nThere's a ton of practical work with AIs and ML that doesn't need to include the problem of discrimination. Product QA, recognizing fractures from X-rays, biochemistry applications, infrastructure operations optimization, etc etc.\n\nSure, this is something worth of studying, but what we really need is a set of standards before potentially dangerous AIs are put into production. And by potentially dangerous, I mean also AIs that may produce results interpretable as discriminatory - discrimination *is* dangerous.\n\nIt's up to the professionals of the field to say \"no, we can't do that yet reliably enough\" when a client asks them to do an AI that would most likely have discriminatory biases. And it's up to the researchers to keep informing the professionals about these risks.",
                            "score": 46,
                            "depth": 2,
                            "timestamp": "2022-06-28 06:36:25",
                            "replies": [
                                {
                                    "author": "teryret",
                                    "body": "> Perhaps the answer for now is that we shouldn't be making AIs for production with any strict rules when there's a risk of discriminatory biases.\n\nThat's pretty much how it's always done, which is why it is able to learn biases.  Take the systemic bias case, where some individuals are at more liberty to take leisurely strolls in the park.  If (for perfectly sane and innocent reasons) parks are where it makes sense to collect your data, you're going to end up with a biased dataset through no fault of your own, despite not putting any strict rules in.\n\n> It's up to the professionals of the field to say \"no, we can't do that yet reliably enough\" when a client asks them to do an AI that would most likely have discriminatory biases. And it's up to the researchers to keep informing the professionals about these risks.\n\nThere's more to it than that.  Let's assume that there's good money to be made in your robotic endeavor.  And further lets assume that the current professionals say \"no, we can't do that yet reliably enough\".  That creates a vacuum for hungrier or less scrupulous people to go after the same market.  And so one important question is the public as a whole better off with potentially biased robots made by thoughtful engineers, or with probably still biased robots made by seedier engineers who assure you that there is no bias?  It's not like you're going to convince _everyone_ to step away from large piles of money (and if you are I can think of better uses of that ability to convince).",
                                    "score": 14,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:14:02",
                                    "replies": [
                                        {
                                            "author": "tzaeru",
                                            "body": "> That's pretty much how it's always done, which is why it is able to learn biases. Take the systemic bias case, where some individuals are at more liberty to take leisurely strolls in the park. If (for perfectly sane and innocent reasons) parks are where it makes sense to collect your data, you're going to end up with a biased dataset through no fault of your own, despite not putting any strict rules in.\n\nBy strict rules, I meant to say that the AI generates strict categorization, e.g. filtering results to refused/accepted bins.\n\nWhile more suggestive AIs - e.g. an AI segmenting the area in an image that could be worth looking at more closely or a physician - are very useful.\n\nWasn't a good way to phrase it. Really bad and misleading actually, in hindsight.\n\n> There's more to it than that. Let's assume that there's good money to be made in your robotic endeavor. And further lets assume that the current professionals say \"no, we can't do that yet reliably enough\". That creates a vacuum for hungrier or less scrupulous people to go after the same market.\n\nWhich is why good consultants and companies need to be educating their clients, too.\n\nE.g. in my company, which is a software consulting company that also does some AI consulting, we routinely tell a client that we don't think they should be doing this or that project - even if it means money for us - since it's not a good working idea.\n\n> It's not like you're going to convince everyone to step away from large piles of money (and if you are I can think of better uses of that ability to convince).\n\nYou can make the potential money smaller though.\n\nIf a company asks us to make an AI to filter out job candidates and we so no, currently we can't do that reliably enough and we explain why, it doesn't mean the client buys it from someone else. If we explain it well - and we're pretty good at that, honestly - it means that the client doesn't get the product at all. From anyone.",
                                            "score": 7,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 09:19:57",
                                            "replies": []
                                        },
                                        {
                                            "author": "Deleted",
                                            "body": ">And so one important question is the public as a whole better off with potentially biased robots made by thoughtful engineers, or with probably still biased robots made by seedier engineers who assure you that there is no bias? It's not like you're going to convince everyone to step away from large piles of money (and if you are I can think of better uses of that ability to convince).\n\nAre you one of these biased AIs? Because your argument, your argument is a figurative open head wound. It would be very easy to make rules on what is unacceptable AI behavior, as it's clear from this research. As for stepping away from large piles of money, there are laws that have historically insured exactly that when it's to the detriment of society. Now, I acknowledge that we're living in bizzaroworld so that argument amounts to nothing when compared to an open head wound argument.",
                                            "score": 2,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 17:15:48",
                                            "replies": []
                                        },
                                        {
                                            "author": "frontsidegrab",
                                            "body": "That sounds like race to the bottom type thinking.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 11:13:45",
                                            "replies": []
                                        }
                                    ]
                                },
                                {
                                    "author": "frostygrin",
                                    "body": ">Perhaps the answer for now is that we shouldn't be making AIs for production with any strict rules when there's a risk of discriminatory biases.\n\nI don't see why when people aren't free from biases either. I think it's more that the decisions and processes need to be set up in a way that considers the possibility of biases and attempts to correct or sidestep them. \n\nAnd calling out an AI on its biases may be easier than calling out a person - as long as we no longer think AI's are unbiased.",
                                    "score": 7,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 07:54:31",
                                    "replies": [
                                        {
                                            "author": "tzaeru",
                                            "body": "People aren't free of them but the problem is the training material. When you are deep training an AI, it is difficult to accurately label and filter all the data you feed for it. Influencing that is beyond the scope of the companies that end up utilizing that AI. There's no way a medium-size company doing hiring would properly understand the data the AI has been trained on or be able to filter it themselves.\n\nBut they can set up a bunch of principles that should be followed and they can look critically at the attitudes that they themselves have.\n\nI would also guess - of course might be wrong - that finding the culprit in a human is easier than finding it an AI, at least this stage of our society. The AI is a black box that is difficult to question or reason about, and it's easy to dismiss any negative findings with \"oh well, that's how the AI works, and it has no morals or biases since it's just a computer!\"",
                                            "score": 18,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 08:00:20",
                                            "replies": [
                                                {
                                                    "author": "WTFwhatthehell",
                                                    "body": "In reality the AI is much more legible. You can run an AI  through a thousand tests and reset the conditions perfectly. You can't do the same with Sandra from HR who just doesn't like black people but knows the right things to say.\n\nUnfortunately people are also fluid and inconsistent in what they consider \"bias\"\n\nIf you feed a system a load of books and data and photos and it figures out that lumberjacks are more likely to be men and preschool teachers are more likely to be women you could call that \"bias\" or you could call it \"accurately describing the real world\"\n\nThere's no clear line between accurate beliefs about the world and bias. \n\nIf I told you about someone named \"Chad\" or \"Trent\" does anything come to mind? Any guesses about them? Are they more likely to have voted trump or Biden? \n\nNow try the same for Alexandra and Ellen.\n\nBoth chad and trent are in the 98th percentile for republicanness. Alexandra and Ellen the opposite for likelihood to vote dem.\n\n\nIf someone picks up those patterns is that bias? Or just having an accurate view of the world? \n\n\nHumans are really really good at picking up these patterns. Really really good,  and people are really very partyist so much that a lot of those old experiments where they send out CV's with \"black\" or \"white\" names don't replicate if you match the names for partyism\n\n\nWhen statisticians talk about bias they mean deviation from reality. When activists talk about bias they tend to mean deviation from a *hypothetical ideal.*\n\nYou can never make the activists happy because every one has their own ideal.",
                                                    "score": 16,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 08:29:32",
                                                    "replies": [
                                                        {
                                                            "author": "tzaeru",
                                                            "body": ">If you feed a system a load of books and data and photos and it figures out that lumberjacks are more likely to be men and preschool teachers are more likely to be women you could call that \"bias\" or you could call it \"accurately describing the real world\"\n\nHistorically, most teachers were men, on all levels - this thing that women tend to compose the majority on lower levels of education is a modern thing.\n\nAnd that doesn't say anything about the qualifications of the person. The AI would think that since most lumberjacks are men, and this applicant is a woman, this applicant is a poor candidate for a lumberjack. But that's obviously not true.\n\n>Is that bias? Or just having an accurate view if the world?\n\nYou forget that biases can be self-feeding. For example, if you expect that people of a specific ethnic background are likely to be thieves, you'll be treating them as such from early on. This causes alienation and makes it harder for them to get employed, which means that they are more likely to turn to crime, which again, furthers the stereotypes.\n\nYour standard deep-trained AI has no way to handle this feedback loop and try to cut it. Humans do have the means to interrupt it, as long as they are aware of it.\n\n>You can never make the activists happy because every one has their own ideal.\n\nWell you aren't exactly making nihilists and cynics easily happy either.",
                                                            "score": 7,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 08:35:36",
                                                            "replies": [
                                                                {
                                                                    "author": "WTFwhatthehell",
                                                                    "body": ">Your standard deep-trained AI has no way to handle this feedback loop and try to cut it.\n\nSure you can adjust models based on what people consider sexist etc. This crowd do it with word embeddings, treating sexist bias in word embeddings as a systematic distortion to the shape of the model then applying it as a correction.\n\nhttps://arxiv.org/abs/1607.06520\n\nIt impacts how well the models reflect the real world but  its great for making the local political officer happy\n\nYou can't do that with real humans. As long as Sandra from HR who doesn't like black people knows the right keywords you can't just run a script to debias her or even really prove she's biased in a reliable way",
                                                                    "score": 3,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 08:43:38",
                                                                    "replies": [
                                                                        {
                                                                            "author": "tzaeru",
                                                                            "body": "> Sure you can adjust models based on what people consider sexist etc. This crowd do it with word embeddings, treating sexist bias in word embeddings as a systematic distortion to the shape of the model then applying it as a correction.\n\nYes, but I specifically said \"your standard deep-trained AI\". There's recent research on this field that is promising, but that's not what is right now getting used by companies adopting AI solutions.\n\nThe companies that are wanting to jump the ship and delegate critical tasks to AIs right now should hold back if there's a clear risk of discriminatory biases.\n\nI'm not meaning to say that AIs can't be helpful here or can't solve these issues - I am saying that right now the solutions being used in production can't solve them and that companies that are adopting AI can not themselves really reason much about that AI, or necessarily even influence its training.\n\n> As long as Sandra from HR who doesn't like black people knows the right keywords you can't just run a script to debias her or even really prove she's biased in a reliable way\n\nI'd say you can in a reliable enough way. Sandra doesn't exist alone in a vacuum in the company, she's constantly interacting with other people. Those other people should be able to spot her biases from conversations, from looking at her performance, and how she evaluates candidates and co-workers.\n\nAI solutions don't typically give you similar insight into these processes.\n\nHonestly there's a reason why many tech companies themselves don't take heavy use of these solutions. E.g. in the company I work at we've several high level ML experts with us. We've especially many people who've specialized in natural language processing and do consulting for client companies about that.\n\nCurrently, we wouldn't even consider starting using an AI to root out applicants or manage anything human-related.",
                                                                            "score": 8,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 08:59:57",
                                                                            "replies": [
                                                                                {
                                                                                    "author": "WTFwhatthehell",
                                                                                    "body": ">Those other people should be able to spot her biases from conversations,\n\nWhen Sandra knows the processes and all the right shibboleths?\n\nPeople tend to be pretty terrible at reliably distinguishing her from Clara who genuinely is far less racist but doesn't speak as eloquently or know how to navigate the political processes within organisations.\n\nOrganisations are pretty terrible at picking that stuff up but operate on a fiction that as long as everyone goes to the right mandatory training that it solves the problem.",
                                                                                    "score": 7,
                                                                                    "depth": 9,
                                                                                    "timestamp": "2022-06-28 09:11:03",
                                                                                    "replies": []
                                                                                }
                                                                            ]
                                                                        },
                                                                        {
                                                                            "author": "xDulmitx",
                                                                            "body": "It can be even trickier with Sandra.  She may not even dislike black people.  She may think they are just fine and regular people, but when she get's an application from Tyrone she just doesn't see him as being a perfect fit for the Accounting Manager position (She may not feel  Cleetus is a good fit either). \n\n\nSandra may just tend to pass over a small amount of candidates.  She doesn't discard all black sounding names or anything like that.  It is just a few people's resumes which go into the pile of people who won't get a callback.  Hard to even tell that is happening and Sandra isn't even doing it on purpose.  Nobody looks over her discarded resumes pile and sorts them to check either.  If they do ask, she just honestly says they had many great resumes and that one just didn't quite make the cut. That subtle difference can add up over time though and reinforce itself (and would be damn hard to detect).  \n\n\nWith a minority population, just a few less opportunities can be very noticable.  Instead of 12 black Accounting Managers applications out of 100 getting looked at, you get 9.  Hardly a difference in raw numbers, but that is a 25% smaller pool for black candidates.  That means fewer black Accounting Managers and and any future Tyrones may seem just a bit more out of place.  Also a few less black kids know black Accounting Managers and don't think of it as a job prospect.  So a few decades down the line you may only have 9 applications out of 100 to start with.  And so on around and around, until you hit a natural floor.",
                                                                            "score": 4,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 09:31:11",
                                                                            "replies": []
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "author": "ofBlufftonTown",
                                                            "body": "My ideal involves people not getting preemptively characterized as criminals based on the color of their skin. It may seem like a frivolous aesthetic preference to you.",
                                                            "score": 5,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 09:01:29",
                                                            "replies": [
                                                                {
                                                                    "author": "redburn22",
                                                                    "body": "The point that I am seeing is not that bias doesn\u2019t matter, but rather that people are also biased. They in fact are the ones creating the biased data that leads to biased models.\n\nSo, to me, what determines whether we should go with a model is not whether models are going to cause harm through bias. They will. But nonetheless, to me, the question is whether they will be better than the extremely fallible people who currently make these decisions.\n\nIt\u2019s easy to say let\u2019s not use anything that could be bad. But when the current scenario is also bad it\u2019s a matter of relative benefit.",
                                                                    "score": 0,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-29 06:43:46",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "frostygrin",
                                                    "body": "I think, first and foremost we need to examine, and control, the *results*, not the entities making the decisions. And you can question the human, yes - but they can lie or be genuinely oblivious to their biases. \n\n> and it's easy to dismiss any negative findings with \"oh well, that's how the AI works, and it has no morals or biases since it's just a computer!\"\n\nBut you can easily counter this by saying, and demonstrating that the AI *learns from* people who are biased. And hiring processes can be set up as if with biased people in mind, intended to minimize the effect of biases. It's probably unrealistic to expect unbiased people - so if you're checking for biases, why not use the AI too?",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 08:32:06",
                                                    "replies": [
                                                        {
                                                            "author": "tzaeru",
                                                            "body": "> I think, first and foremost we need to examine, and control, the results, not the entities making the decisions.\n\nBut we don't know how. We don't know how we can make sure an AI doesn't have discriminatory biases in its results. And if we always go manually through those results, the AI becomes useless. The point of the AI is that we automate the process of generating results.\n\n> But you can easily counter this by saying, and demonstrating that the AI learns from people who are biased.\n\nYou can demonstrate it, and then you have to throw the AI away, so why did you pick up the AI in the first place? The problem is that you can't fix the AI if you're not an AI company.\n\nAlso I'm not very optimistic about how easy it is to explain how AIs work and are trained to courts, boards, and non-tech executives. Perhaps in future it becomes easier, when general knowledge about how AIs work becomes more widespread.\n\nBut right now, from the perspective of your ordinary person, AIs are black magic.\n\n> It's probably unrealistic to expect unbiased people - so if you're checking for biases, why not use the AI too?\n\nBecause we really don't currently know how to do that reliably.",
                                                            "score": 2,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 08:40:23",
                                                            "replies": [
                                                                {
                                                                    "author": "frostygrin",
                                                                    "body": "> But we don't know how. We don't know how we can make sure an AI doesn't have discriminatory biases in its results. And if we always go manually through those results, the AI becomes useless. The point of the AI is that we automate the process of generating results\n\nWe don't need to *always* go through all these results. Because the AI can be more consistent, at least at a certain point in time, than 1000 different people would be. So we can do it selectively.\n\n> You can demonstrate it, and then you have to throw the AI away\n \nNo, you don't have to. Unless you licensed it as some kind of magical solution free from any and all biases - but that's unrealistic. My whole point is that we can and should expect biases. We just need to correct for that.",
                                                                    "score": -1,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 08:58:59",
                                                                    "replies": [
                                                                        {
                                                                            "author": "tzaeru",
                                                                            "body": "Point is that if the AI produces biased results, you can't use the results of the AI - you have to be manually checking them and that removes the point from using the AI. If you anyway have to go through 10 000 job applications manually, what's the value of the AI?\n\nAnd often when you buy an AI solution from a company producing them, it really is a black box you can't influence all that much yourself. Companies do not have the know-how to train the AIs and they don't even have the know-how to understand how the AI might be biased and how they can recognize it.\n\nMy concern is not the people working on the bleeding edge of technology, nor the tech-savvy companies that should know what they're doing - my concern is the companies that have no AI expertise of their own and do not understand how AIs work.",
                                                                            "score": 5,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 09:02:59",
                                                                            "replies": [
                                                                                {
                                                                                    "author": "frostygrin",
                                                                                    "body": "> Point is that if the AI produces biased results, you can't use the results of the AI - you have to be manually checking them and that removes the point from using the AI. If you anyway have to go through 10 000 job applications manually, what's the value of the AI?\n\nYou can manually go through, say,  100 applications out of 10 000 and see how biased the AI is - and adjust your processes - *not the AI* - if necessary. If the AI is biased in favor of guys named Bob (perhaps because one of its creators was named Bob), you can, for example, remove the name from the data it's given. You also can report it to the company that created it, so that they can adjust it - but it's not the only way to get better results.",
                                                                                    "score": 1,
                                                                                    "depth": 9,
                                                                                    "timestamp": "2022-06-28 09:18:01",
                                                                                    "replies": [
                                                                                        {
                                                                                            "author": "tzaeru",
                                                                                            "body": "There are ways to manage the bias yes, but I don't think they really are that clear cut and noticing them is beyond the reach of the average non-tech company.\n\nThe biases often happen in specific circumstances, or as a combination of factors, and become harder to spot. Let's say, it's discriminating young women but favoring old women, and vice versa for men. Overall women aren't affected and overall age doesn't appear affected. You need to realize to combine those factors together.\n\nIt's tough.\n\nAnd honestly people are really, really bad with understanding how AIs work currently.",
                                                                                            "score": 2,
                                                                                            "depth": 10,
                                                                                            "timestamp": "2022-06-28 09:25:01",
                                                                                            "replies": []
                                                                                        }
                                                                                    ]
                                                                                }
                                                                            ]
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "author": "Deleted",
                                            "body": "Because the point of this type of AI wasn't to be more efficient and expedient in replicating human flaws and errors, my smaaaaaaart buuuuuudddy",
                                            "score": 0,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 17:17:39",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "catharsis23",
                            "body": "This is not reassuring and honestly convinces me more that those folks doing AI work are playing with fire",
                            "score": 28,
                            "depth": 2,
                            "timestamp": "2022-06-28 07:45:01",
                            "replies": [
                                {
                                    "author": "teo730",
                                    "body": "A significant portion, if not most people who do AI-related work, do it on stuff that isn't necessarily impacted by this stuff. But that's all you read about in the news because these headlines sell.\n\nTraining a model to play games (chess/go etc.), image analysis (satellite imagery for climate impacts), science modelling (weather forecasting/astrophyics etc.), speeding up your phone/computer (by optimising app loading etc.), digitising hand-written content, mapping roads (google maps etc.), disaster forecasting (earthquakes/flooding), novel drug discovery.\n\nThere are certainly more areas that I'm forgetting, but don't be fooled into thinking (1) that ML isn't already an everyday part of your life and (2) that all ML research has the same societal negatives.",
                                    "score": 9,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 10:59:43",
                                    "replies": []
                                },
                                {
                                    "author": "Enjoying_A_Meal",
                                    "body": "Don't worry, I'm sure one day we can get sentient AIs that hate all humans equally!",
                                    "score": 15,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 08:40:20",
                                    "replies": []
                                },
                                {
                                    "author": "Thaflash_la",
                                    "body": "Yup. \u201cWe know it\u2019s not ok, but we\u2019ll move forward regardless\u201d.",
                                    "score": 13,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:13:34",
                                    "replies": [
                                        {
                                            "author": "thirteen_tentacles",
                                            "body": "Progress doesn't halt for the benefit those maligned by it, much to our dismay",
                                            "score": -1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 09:47:59",
                                            "replies": [
                                                {
                                                    "author": "Thaflash_la",
                                                    "body": "We don\u2019t need to halt progress, but the acknowledgement of the problem, recognition of its significance, knowing it\u2019s not ok, and proceeding (not just testing and research) regardless is troubling. The admission is worse than suggestion of the article.",
                                                    "score": 2,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 10:26:52",
                                                    "replies": [
                                                        {
                                                            "author": "thirteen_tentacles",
                                                            "body": "I probably worded it badly, my statement wasn't in the affirmative. I think it's a problem, that we all march on with \"progress\" regardless of the pitfalls and worrying developments, like this one",
                                                            "score": 3,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 10:33:12",
                                                            "replies": [
                                                                {
                                                                    "author": "Thaflash_la",
                                                                    "body": "Probably at least equally my misinterpretation/misunderstanding. \n\nI agree 100%.",
                                                                    "score": 1,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 10:46:18",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "author": "Atthetop567",
                                                            "body": "Are you vegan",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 17:49:06",
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "author": "teryret",
                                    "body": "If it helps, human brains have a lot of these same issues (they're just slightly more subtle due to the massive data disparity), and that's gone perfectly.  Definitely no cases of people ending up as genocidal racists.  Definitely no cases of that currently happening in China.  We're definitely smart enough to avoid building nukes, or at the very least to get rid of all the nukes we have.\n\nIf doing AI work is playing with fire, doing human work is playing with massive asteroids.\n\nA fun game to play is, whenever you see robots or aliens in a scary movie, try to work out which human failing it is they're the avatar of.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:21:13",
                                    "replies": [
                                        {
                                            "author": "catharsis23",
                                            "body": "I'm sorry but this is gibberish. Most man made tools do not intrinsically discriminate",
                                            "score": -1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 10:13:31",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "Pixie1001",
                            "body": "Yeah, I think the onus is less on the devs, since we're a long way off created impartial AI, and more on enforcing a code of ethics on what AI can be used for.\n\nIf your face recognition technology doesn't work on black people very well, then it shouldn't be used by police to identify black suspects, or otherwise come attached to additional manual protocols to verify the results for affected races and genders.\n\nThe main problem is that companies are selling these things to public housing projects primarily populated by black people as part of the security system and acting confused when it randomly flags people as shoplifters as if they didn't know it was going to do that.",
                            "score": 9,
                            "depth": 2,
                            "timestamp": "2022-06-28 08:34:31",
                            "replies": [
                                {
                                    "author": "joshuaism",
                                    "body": "You can't expect companies to pay you hundreds of thousands of dollars to create an AI and not turn around and use it.  Diffusion of blame is how we justify evil outcomes.  If you know it's impossible to not make a racist AI, then don't make an AI.",
                                    "score": 7,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:44:46",
                                    "replies": [
                                        {
                                            "author": "Pixie1001",
                                            "body": "Well sure, but then we'll *never* have a non-racist AI if there's no money in the janky version we have now, since the tech is potentially decades away from being completely impartial. Not to mention nobody will understand the risks if they're not trained on responsibly using them in a practical settings.\n\nI think the solution's definitely more on government regulation of the tech than on banning it outright.\n\nIf we make sure these companies use it as a productivity tool and not a way of wrangling their way out of responsibility for their actions (e.g. Crypto and 'the blockchain' being used as an excuse for unethical banking practices because it's just code), I think it still has a lot of applications.",
                                            "score": -1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 10:04:05",
                                            "replies": [
                                                {
                                                    "author": "joshuaism",
                                                    "body": "Help me Uncle Sam! I can't stop myself from doing the thing I want to do!\n\nIf we just point one more finger at the government we can finally end this pointless game of fingerpointing.  We just got to diffuse blame to one more actor!",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 10:50:06",
                                                    "replies": [
                                                        {
                                                            "author": "Pixie1001",
                                                            "body": "I mean sure, but at that point we'd be blaming Canon for creating tools of child pornography and exploitation. Hell, reddit's often used to radicalise domestic terrorists, distribute said cp *and* spread racist ideas despite the admin's best efforts to stop it. I guess we should shut that down too.\n\nYou can't just not make a thing because it *might* be used for evil, and our society inherently isn't setup for inventors to enforce how their inventions are used - it's delegated to the government, who actually has the power to do that kinda stuff (theoretically anyway).\n\nIt nice to be able to point to one person and say, *they caused X* and wrap it in a nice bow, but I think in a lot of ways the responsibilities will always be defused - in a complex society, there's almost never just one fairytale villain we can single out, there's multiple kinda complicit people who all need to take responsibility for fixing things and accept their share of the blame.",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 11:49:28",
                                                            "replies": [
                                                                {
                                                                    "author": "joshuaism",
                                                                    "body": "If the company answered to the workers instead of to the shareholders and corporations operating outside of the public interest could be dissolved then you could actually solve a lot of these problems.  Love of money is the root of all evil but for some reason we've built our economic, social, and government system around it.",
                                                                    "score": 2,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 11:59:30",
                                                                    "replies": [
                                                                        {
                                                                            "author": "Alternative-Fan2048",
                                                                            "body": "Sure, no money - no incentive  - no progress.",
                                                                            "score": 0,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-07-26 21:00:12",
                                                                            "replies": []
                                                                        },
                                                                        {
                                                                            "author": "Pixie1001",
                                                                            "body": "I mean, I mostly agree.\n\nCapitalism has it's pros when it comes to slowing down the consolidation of power by keeping everyone distracted with fighting over it, but it's definitely a pretty inefficient and cynical system that we're already starting to see fall apart as corporate funded lobbyists wear away at the safety nets and begin forming monopolies.\n\nI just don't really know what the replacement would look like - the greed we're seeing in corporations right now isn't explicitly a feature of capitalism, it's a feature of people.\n\nAs soon as you try and hand the resources back to the people, the new centralised government in charge of the takeover just becomes an ever bigger, shittier corporation.\n\nI guess capitalism's a lot like these AIs really - it's a system of distributing power based on your contribution to society, which in principle is kinda like giving power to the people - except people have gotten really good at gaming it in order to be assigned more power than they deserve, and then using that to alter the system's rules even further in their favour.\n\nI guess my stance is we should improve the system and fix the bugs with stuff like welfare programs and universal basic income and patching in new laws faster than bad actors can break them, but idk, you might be right that we're better off starting from scratch with a different idea as well.\n\nTrying to think of what that alternative system might be like usually just makes me depressed though, since they all seem to either consolidate too much power into a single entity or create huge power vacuums their proponents naively assume won't be filled :(",
                                                                            "score": 1,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 12:36:59",
                                                                            "replies": []
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "mr_ji",
                            "body": "Have you considered that intelligence, which includes experience-based judgement, is inherently biased?  Sounds like you're trying to make something artificial, but not necessarily intelligent.",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2022-06-28 10:07:12",
                            "replies": []
                        },
                        {
                            "author": "Deleted",
                            "body": ">we haven't \"decided it's OK\",\n\nYou're simply going ahead with a flawed product that was supposed to compensate for human flaws and failings, but will now reproduce them only with greater expediency. Cool!",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2022-06-28 17:09:37",
                            "replies": []
                        },
                        {
                            "author": "AtomicBLB",
                            "body": "Arguing it's not technically racist is completely unelpful and puts the focus on the wrong aspect of the problem. These things can have enormous impacts on our lives so it really doesn't matter how it *actually* works when it's *literally* not working properly. \n\nFacial recognition being a prime example. The miss rate on light skin people alone is too high let alone the abysmal rate for darker skin tones yet it's commonly used by law enforcement for years now. Those people sitting in jail from this one technology don't care that the AI isn't actually racist. The outcomes are and that's literally all that matters. It doesn't work, fix it or trash it.",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2022-06-28 20:54:42",
                            "replies": [
                                {
                                    "author": "teryret",
                                    "body": "> It doesn't work, fix it or trash it.\n\nAgreed.  It's just that fixing it requires lots trial and error, and that takes a long time.  The real problems with facial recognition aren't in the technology, they're in idiots using tools for more than they're capable of doing.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-29 09:22:10",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "lawstudent2",
                            "body": "In this case is the curse of dimensionality the fact that the global sample is only 7 billion people, which represents a very tiny fraction of all possible configurations of all characteristics being tracked?",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2022-06-28 06:14:26",
                            "replies": []
                        },
                        {
                            "author": "Deleted",
                            "body": "[deleted]",
                            "score": 3,
                            "depth": 2,
                            "timestamp": "2022-06-28 08:24:57",
                            "replies": [
                                {
                                    "author": "teryret",
                                    "body": "> Why give an AI any data not required in sentencing. If the AI doesn\u2019t know the race or gender of the defendant, it can\u2019t use it against them.\n\nThat's not strictly true.  Let's say you have two defendants, one was caught and plead to possession with intent to distribute crack cocaine, and the other was caught and plead to possession with intent to distribute MDMA.  From that information alone you can make an educated guess (aka a Bayesian inference) about the race and gender of both defendants, and while I don't have actual data to back this up, you'd likely be right a statistically significant portion of the time.",
                                    "score": 10,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:27:52",
                                    "replies": [
                                        {
                                            "author": "Deleted",
                                            "body": "[deleted]",
                                            "score": -4,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 09:48:14",
                                            "replies": [
                                                {
                                                    "author": "SeeShark",
                                                    "body": " Just look at sentencing surrounding the opioid epidemic compared to the crack epidemic. There's a clear disparity between how our society has approached the issues, and an AI trained on these data would replicate the racial injustice even if you didn't tell it the race of the defendants.",
                                                    "score": 5,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 10:36:03",
                                                    "replies": [
                                                        {
                                                            "author": "paupaupaupau",
                                                            "body": "Not to mention that using location as a training feature would also inevitably lead to racial bias as a result of historic and systemic racial injustice.",
                                                            "score": 2,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 10:43:20",
                                                            "replies": []
                                                        },
                                                        {
                                                            "author": "Deleted",
                                                            "body": "[deleted]",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 10:52:08",
                                                            "replies": [
                                                                {
                                                                    "author": "SeeShark",
                                                                    "body": "Of course not. But the fact is that our datasets include harsher sentences for Black defendants, and neural nets are going to inherit these biased unless we find a solution, and right now we don't have one yet.",
                                                                    "score": 3,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 11:02:32",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "DucVWTamaKrentist",
                                                    "body": "You are correct. \n\nAnd, I would also like to know what the actual statistics are regarding the scenario described by the previous poster.",
                                                    "score": -1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 10:17:52",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "Throwing_Snark",
                            "body": "It sounds like you have 100% decided it's okay. You don't like it, but you don't consider it a deal breaker either. Not desirable, but acceptable.\n\nI understand you have constraints you are working under and I have no doubt that you would like to see the issues of racism and bias in AI resolved. But the simple fact is that AIs are being designed to be racist and there will be real consequences. People won't be able to get jobs or health care or will get denied loans or suffer longer prison sentences.\n\nAgain, I understand that you aren't in a position where you can fix it. But shrugging and hoping the problem will get addressed? That's saying it's okay if it doesn't. It's tolerable. So saying that AI researchers think it's okay is a fair characterization.\n\nWhether you have malice in your heart or not matters not-at-all to the companies who will use AI in the pursuit of profit. The travel companies pushing Vegas trips on a discount at people with manic-depression or pushing people into high-engagement communities even if they are cults or white nationalists.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 09:51:02",
                            "replies": []
                        },
                        {
                            "author": "Deleted",
                            "body": "I just want to point out that data augmentation is a thing, but otherwise good summary.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 04:07:17",
                            "replies": []
                        },
                        {
                            "author": "MycroftTnetennba",
                            "body": "Isn\u2019t it possible to \u201cfeed\u201d a posterior law that sits in front of the data kind of in a Bayesian mindset?",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 07:59:24",
                            "replies": [
                                {
                                    "author": "teryret",
                                    "body": "Great question, I'll come back to it when I get back from work (leaving this comment to remind myself)",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:39:31",
                                    "replies": [
                                        {
                                            "author": "MycroftTnetennba",
                                            "body": "Thank you! I\u2019ll wait",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 15:08:22",
                                            "replies": []
                                        }
                                    ]
                                },
                                {
                                    "author": "teryret",
                                    "body": "Kind of, there is room to feed stuff in like that, but it's difficult to figure out precisely what to feed in.  Most things you might want to feed in there can also be expressed in your cost function, which means they can be included in the training process directly.  Ideas for what you feed in get tried pretty regularly, it's not solved, but some of them do work.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-29 09:17:12",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "alex-redacted",
                            "body": "The way to solve it is get tech ethicists into positions of power to address systemic issues. You, personally, cannot solve this. *Your team cannot solve this.* Big power players in tech have to solve this, and that begins with hiring-on people like Timnit Gebru and not firing them; looking at you, Google.\n\nThis is a fully top-down issue.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 10:54:08",
                            "replies": []
                        },
                        {
                            "author": "insaneintheblain",
                            "body": "Maybe stop using data generated by Americans?",
                            "score": -18,
                            "depth": 2,
                            "timestamp": "2022-06-28 07:21:41",
                            "replies": [
                                {
                                    "author": "recidivx",
                                    "body": "Because there's no racism anywhere except in the US.",
                                    "score": 22,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 08:00:44",
                                    "replies": [
                                        {
                                            "author": "insaneintheblain",
                                            "body": "Of course there is - it\u2019s just that the US also has racism and it\u2019s people are largely unable to hold two opposing ideas in mind simultaneously.\n\nIf you want to learn from a population, best to learn from one not raised on pop culture and propaganda.",
                                            "score": 3,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 14:55:22",
                                            "replies": []
                                        }
                                    ]
                                },
                                {
                                    "author": "dmc-going-digital",
                                    "body": "How about we stop considering the americans altogether",
                                    "score": -11,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 07:41:59",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "danby",
                            "body": "Paraphrase: We can't be bothered to spend the time and money to assemble a dataset that doesn't contain bigoted biases so we're going to release a product the replicates bigotry anyway.\n\nAssembling good high quality datasets that can be used for machine learning is expensive and decades long work. I wish more computer science students understood this.",
                            "score": -1,
                            "depth": 2,
                            "timestamp": "2022-06-28 09:45:00",
                            "replies": []
                        },
                        {
                            "author": "brohamianrhapsody",
                            "body": "Have you tried buying synthetic data?",
                            "score": -1,
                            "depth": 2,
                            "timestamp": "2022-06-28 05:53:13",
                            "replies": [
                                {
                                    "author": "teryret",
                                    "body": "The trouble there is that it has to be synthesized to represent our robot's view on the world, which currently none are, so we're working on building that capability to make it ourselves.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:00:12",
                                    "replies": [
                                        {
                                            "author": "brohamianrhapsody",
                                            "body": "That makes sense. You guys are building parameters for synthetic data?",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 09:26:31",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "Deleted",
                            "body": "AI random character creator. Create your own diverse dataset. One to rule them all!",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 08:17:16",
                            "replies": []
                        },
                        {
                            "author": "worotan",
                            "body": "We need to think differently from statistical averages being the Truth, but that is how our society is ordered, even if it is not really how it is lived. The discrepancy between the two has always enraged people when it's pointed out that data is not 3-dimensional, because so much money and status is involved.\n\nThe short cuts to understanding that data sets offer have helped create a more efficient world. But their limitations have always been downplayed by those who insist they offer more than they can.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 08:26:09",
                            "replies": []
                        },
                        {
                            "author": "SarahVeraVicky",
                            "body": "As a layman, I've only thought of it at a newbie level ;_;\n\nI guess it's basically like set theories where you can get an exclusion, or a merge, but trying to only alter 'half' the set means having to try and find some way to create a new set entirely. If only we could source the most racist and sexist data possible (basically like pulling all Proud Boy and other ultra-exclusionary groups messages/decisions/etc) so we could make it adversarial to the training of the data.\n\nI can bet the \"we try new things as we think of them\" means it's been an absolutely exhausting and draining to keep throwing stuff at the wall trying to find what sticks. ;_;",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 09:46:52",
                            "replies": []
                        },
                        {
                            "author": "Walmy20",
                            "body": "Can you hook me up with a ML engineering job?",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 09:49:38",
                            "replies": []
                        },
                        {
                            "author": "redditallreddy",
                            "body": "Can you generate randomized data?\n\nI am spit-balling here, I realize.\n\n\nFirst, this seems like a great way to sniff out institutional racism. Take a data set, the more narrow the better, and extrapolate out if it causes a racist/sexist outcome. Boom! Data set had intrinsic racism/sexism.\n\nSo, how to \"erase\" the systemic nature? That is tough, but I suspect it shows in a few ways... outlier extremes, frequency of variation from the mean, selection bias. Of those, I feel like the selection bias would be impossible to erase, but the other two could be handled by some statistical selection... Basically, select out some amount of extremes and artificially reduce the number of one group varying from the mean more than the others.\n\nThen, run the test for lots of randomized trials and see if there is a racist/sexist bias. When you get an AI that doesn't do that, you have found the right starting artificial data set to remove the institutional bias.\n\n\nBut... that sounds really time intensive and expensive.\n\nMaybe we could put an AI on it. hehe",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 10:30:38",
                            "replies": []
                        },
                        {
                            "author": "aselbst",
                            "body": "I think the point of the claim is that by pushing forward anyway, despite being unable to solve it, you have decided you\u2019re ok with it. *Not* building is an option, but\u2014no offense intended\u2014not one that an ML team at a robotics company would likely consider seriously. Compare: If we considered such a system to be nonfunctional or dangerous in the way we do a car without seatbelts, it could not go to market (despite having been thought ok in the early days of cars). That\u2019s part of the critique.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 10:32:01",
                            "replies": []
                        },
                        {
                            "author": "Deleted",
                            "body": ">\"More and better data.\" Okay, yeah, sure, that solves it, but how do we get that?\n\nSynthetic data.\n\nFill-in the gaps of your real-world collected data with computer generated data",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 10:50:31",
                            "replies": []
                        },
                        {
                            "author": "Cualkiera67",
                            "body": "To me it's simply a matter of distinguishing these two requests:\n\n\"Show me the face that is most beautiful\"\n\n\"Show me the face that is most beautiful according to the majority of Brazilians\"\n\nFirst request has no answer and the robot shouldn't answer it. Second request has a valid answer which the robot can provide.\n\nIt is not about eliminating bias, it is about making it clear that it is there.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 11:32:29",
                            "replies": []
                        },
                        {
                            "author": "InspiredPom",
                            "body": "Honestly they\u2019ve know that this information was biased based on human implicit bias\u2019 years ago and kept going but there was no profitable way to fix that unfortunately / job creation there .  There is a lot more profit in marketing by demographic so I kinda want to blame that but can be it wrong . In any case it seems humans are left best for those novel cases /exceptions as a default and or the engineering teams have to think of a procedure beforehand  and just in case . Just hope it doesn\u2019t mess anyone up too badly getting caught in a weird loop or non existent solution.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 11:52:03",
                            "replies": []
                        },
                        {
                            "author": "Kaeny",
                            "body": "Dall-E Can imagine it, it can be true",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 13:02:02",
                            "replies": []
                        },
                        {
                            "author": "Psy-Koi",
                            "body": ">Precisely.  The headline is misleading at best.  I'm on an ML team at a robotics company, and speaking for us, we haven't \"decided it's OK\", we've run out of ideas about how to solve it, we try new things as we think of them, and we've kept the ideas that have seemed to improve things.\n\nThere is a solution though. If you can't make unbiased AI, you don't use it at all.\n\nIf you still use it in your products and then say you're trying to solve the problem you're being disingenuous and ethically dubious. \n\nThe headline isn't really misleading. Some companies might act appropriately, but many aren't.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 14:12:56",
                            "replies": [
                                {
                                    "author": "teryret",
                                    "body": "That's black and white thinking, and it holds you back.  Let's say that you're building a robot train, and you tell it not to hit people.  Let's further say that your robot is better at spotting white people at distance that black people which manifests as stopping with 10ft to spare for white people and 9'6\" to spare for darker people.  It is a clear bias.  But at the same time, you're still stopping for everyone.  Should that 6\" really derail a project?",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-29 09:34:06",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "bathtup47",
                            "body": "Just because YOU can't solve the issue posed doesn't somehow mean you aren't doing exactly what you were accused of. You literally just admitted the base data itself is flawed so maybe instead of trying to force through a product that's guaranteed not to function 100% as intended, you could work on fixing the data or obtaining more. The original accusations was that you guys are passing off broken racist AI as a finished product and you are which you admitted in your post and then said it's impossible to fix essentially. Just because you work for a company doesn't mean you need to come on the internet and lick boot Infront of us for them.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 14:14:19",
                            "replies": []
                        },
                        {
                            "author": "IronTarkusBarkus",
                            "body": "I agree with what you\u2019re saying. However, I ask, what is the point of these bots in the first place? What goals are we even trying to reach?\n\nAll I see bots do is make trashy comments and poison the well by spreading harmful propaganda. For what? Boost people\u2019s follower count?",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 14:47:39",
                            "replies": [
                                {
                                    "author": "teryret",
                                    "body": "Oh, our bots aren't software bots, ours weigh hundreds of pounds each and can go well over 10mph off road.  If you're asking for a defense of public opinion shaping bots I believe they're a cancer, and the people responsible for creating them should be deported to... say... the Mariana trench.",
                                    "score": 2,
                                    "depth": 3,
                                    "timestamp": "2022-06-29 09:28:25",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "BassSounds",
                            "body": "I feel like you have to have some event driven programming to compensate for the ML datasets. In other words, a function to filter certain responses. There is an eng geek out there who will someday solve this problem, but, for now we should bandage the issue.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 16:20:08",
                            "replies": []
                        },
                        {
                            "author": "anttirt",
                            "body": ">we haven't \"decided it's OK\", we've run out of ideas about how to solve it\n\n...and then decided to go ahead anyway.\n\nSo you have actually decided it's OK. After all you tried your best! But you still gotta sell that product, and that's of course more important than the problem at hand. So you're trading money for morals.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 16:25:59",
                            "replies": [
                                {
                                    "author": "teryret",
                                    "body": "> to go ahead anyway\n\nGo ahead with what, exactly?  Further development work?  Additional data gathering?  Taking it seriously?  Because yeah, we're full steam ahead on all of those things.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-29 09:25:08",
                                    "replies": [
                                        {
                                            "author": "IronTarkusBarkus",
                                            "body": "I think the question becomes, why? \n\nTechnology and robots bring a lot of cool things, but I think it\u2019s safe to say, it doesn\u2019t *just* bring good. \n\nEspecially as we get the ability to build powerful, more amazing technology, I think it\u2019s important we stay specific/intentional with our goals, and work hard to protect ourselves from negative externalities. Not all progress is progress, if you catch what I mean. We wouldn\u2019t want to stare into the sun.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-30 21:31:09",
                                            "replies": [
                                                {
                                                    "author": "teryret",
                                                    "body": "Maybe you wouldn't.  I'm on team \"let's build the Parker Solar Probe so we can stare into the sun from inside it!\"",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-30 22:50:30",
                                                    "replies": [
                                                        {
                                                            "author": "IronTarkusBarkus",
                                                            "body": "You ever heard of The Tower of Babel? These stories and sayings don\u2019t come from nowhere. \n\nMaybe our entry/legacy will be the Parker Solar Probe, though they all seem to end the same.",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-30 22:59:59",
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "burnalicious111",
                            "body": "I don't think it's misleading. A decision with a racist outcome is a racist decision. People who are interpreting that to mean \"a decision was made by a computer with racist intent\" are reading it incorrectly, because they're not understanding one of:\n\n* AIs don't make \"decisions\" like humans\n* something doesn't have to have racist intent to have racist outcomes (and thus, be racist)",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 17:50:46",
                            "replies": []
                        },
                        {
                            "author": "Awkward-Event-9452",
                            "body": "I have an awesome idea. Let\u2019s have humans to the judging of other humans. Your welcome.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 17:55:02",
                            "replies": []
                        },
                        {
                            "author": "cloake",
                            "body": "The AI just needs a virtue signaling module, that heavily weighs appearing not sexist or racist, and if the rest of the network is in conflict with it, reject that data and search for data that confirms the academic orthodoxy. That's how humans do it.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-29 07:20:40",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "valente317",
                    "body": "The GAPING hole in that explanation is that there is evidence that these machine learning systems will still infer bias even when the dataset is deidentified, similar to how a radiology algorithm was able to accurately determine ethnicity from raw, deidentified image data. Presumably these algorithms are extrapolating data that is imperceptible or overlooked by humans, which suggests that the machine-learning results reflect real, tangible differences in the underlying data, rather than biased human interpretation of the data.\n\nHow do you deal with that, other than by identifying case-by-case the \u201cbiased\u201d data and instructing the algorithm to exclude it?",
                    "score": 101,
                    "depth": 1,
                    "timestamp": "2022-06-28 08:26:11",
                    "replies": [
                        {
                            "author": "chrischi3",
                            "body": "That is the real difficulty, and kinda what i'm trying to get at. Neural networks can pick up on things that would go straight past us. Who is to say that such a neural network wouldn't also find a correlation between punctuation and harshness of sentencing?   \n\n\nI mean, we have studies proving that justice is biased on things like wether a football team won or lost the previous match if the judge was a fan of said team, so if those are things we can find, what kinds of correlations do you think could an analytical software designed by a species of intelligent pattern finders to find patterns better than we ever could find?  \n\n\nIn your example, the deidentified image might still show things like, say, certain minor differences in bone structure and density, caused by genetics, too subtle for us to pick out, but still very much perceivable for a neural network specifically designed to figure out patterns in a set of data.",
                            "score": 51,
                            "depth": 2,
                            "timestamp": "2022-06-28 09:00:13",
                            "replies": [
                                {
                                    "author": "BevansDesign",
                                    "body": "For a while, I've been thinking along similar lines about ways to make court trials more fair - focusing on people, not AI. My core idea is that the judge and jury should never know the ethnicity of the person on trial. They would never see or hear the person, know their name, know where they live, know what neighborhood the crime was committed in, and various other things like that. Trials would need to be done via text-based chat, with specially-trained go-betweens (humans at first, AI later) checking everything that's said for any possible identifiers.\n\nThere will always be exceptions, but we can certainly reduce bias by a significant amount. We can't let perfect be the enemy of good.",
                                    "score": 2,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 18:31:45",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "Deleted",
                            "body": "[deleted]",
                            "score": 17,
                            "depth": 2,
                            "timestamp": "2022-06-28 14:56:00",
                            "replies": [
                                {
                                    "author": "dflagella",
                                    "body": "Instead of handicapping the use of data I wonder if it would make more sense to break down more complex data into simplified data points. \n\nIf you're using high level data such as race of a person then the NN will be trained on data obtained from a racist system and the outputs will perpetuate that. \n\nFor something like a resume AI determining applicants, it might discriminate against women for things like \"lack of experience\" if there is a period of maternity leave or something. I guess what I'm saying is certain metrics are currently used for evaluation but those metrics aren't necessarily good metrics to be used. \n\nIts obviously not a simple issue and I'd have to spend more time thinking about what I'm trying to get across to give better examples",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-29 11:54:18",
                                    "replies": [
                                        {
                                            "author": "cgoldberg3",
                                            "body": "These are the sorts of solutions that hamstring the AI into no longer being as accurate in a general sense.\n\nYour example of a woman taking maternity leave being interpreted as a gap in work - the AI sees it as just a gap in work. It doesn't care what the reason was for. And the truth of it is, a gap's impact on job performance is the same regardless of whether the gap is for a good reason (pregnancy) or not (he wanted to play WoW full time for 3 months).\n\nAnd that's where the problem lays. The AI tells us truths that we're not ready to hear. \"Fixing\" the AI to not tell us things we dislike makes it less capable of telling us even the truths we're comfortable with.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-29 12:31:39",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "Deleted",
                            "body": "[removed]",
                            "score": 16,
                            "depth": 2,
                            "timestamp": "2022-06-28 09:34:05",
                            "replies": [
                                {
                                    "author": "Deleted",
                                    "body": "[removed]",
                                    "score": 6,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 10:17:53",
                                    "replies": [
                                        {
                                            "author": "SeeShark",
                                            "body": " This is missing the entire point of the discussion. When Black people receive harsher sentences, the AI will inevitably associate Black people with criminality, but that doesn't mean it's identifying \"real differences\" -- it's simply inheriting systemic racism. You can't just chalk this up to \"racial realism.\"",
                                            "score": -4,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 10:43:42",
                                            "replies": [
                                                {
                                                    "author": "Deleted",
                                                    "body": "This is the kind of kneejerk reaction I'm against. We're talking across all fields, including preventive medicine. Might be that south uzbekish people are more likely to develop spinal weaknesses or that mexican-spanish kids need more opportunity to learn hand-eye coordination, but all you can think about is an AI judge that propagates the flaws of the US justice system.\n\nThe problems of the USA aren't even universal to the whole world, 95% of people live in other countries with other societal problems.",
                                                    "score": 10,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 10:58:49",
                                                    "replies": [
                                                        {
                                                            "author": "SeeShark",
                                                            "body": "Systemic sentencing issues are pretty universal; the only thing that changes is which groups are disadvantaged by it.",
                                                            "score": 4,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 11:05:41",
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "jewnicorn27",
                            "body": "There is a difference between deidentifying and removing bias from the dataset isn\u2019t there? One interesting example I came across recently is resuscitation of newborn babies. Where I come from there is a difference between 98% and 87% in which babies are attempted to be resuscitated between the ethnicity with the highest rate (white), and the lowest (Indian). This is due to the criteria used to determine if they attempt resuscitation, and the difference in the two distributions of babies of those ethnicities. Now if you took the data and removed the racial information, then trained a model to determine which babies should be attempted to resuscitate, you still get a racial bias don\u2019t you? Which is to say if you run the model with random samples from those two distributions, you get two different average answers.",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2022-06-28 13:46:40",
                            "replies": [
                                {
                                    "author": "valente317",
                                    "body": "Maybe the disconnect is the definition of bias. It sounds like you\u2019re suggesting that a \u201cgood\u201d model would normalize resuscitation rates by recommending increased resuscitation of one group and/or decreased resuscitation of a different group. That discounts the possibility that there are real, tangible differences in the population groups that affect the probability of attempting resuscitation, aside from racial bias. It would actually introduce racial bias into the system, not remove it.",
                                    "score": 6,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 16:20:38",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "danby",
                            "body": "> similar to how a radiology algorithm was able to accurately determine ethnicity from raw, \n\nIf 'ethnicity' wasn't fed to the algorithm then it did not do this. What likely happened is that the algorithm was trained and then in a post-hoc analysis researchers could see that it clustered together images that belonged to some ethnic groups. Which would indicate that there are some systematic difference in the radiaology images from  different groups. That's likely useful knowledge from a diagnostic perspective. And not, in and of itself, racist.\n\nIt's one thing to discover that there are indeed some systematic difference in radiology images from different ethnic groups (something that you might well hypothesis before hand). It's quite another thing to allow your AI system to make racist or sexist decisions because it can cluster datasets without explicitly including \"ethnicity\" in the training data. When we talk about an AI making sexist or racist decisions we're not talking about whether it can infer ethnicity by proxy, something that can be benign factual information. We're talking about what the whole AI system then does with that information.",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2022-06-28 09:49:56",
                            "replies": [
                                {
                                    "author": "valente317",
                                    "body": "To your last paragraph, im arguing that the radiology AI will make \u201cracist\u201d decisions that are actually just reflections of rote, non-biased data. We\u2019re not quite at the point that the radiology AI can make recommendations, but once we get there, you\u2019ll see people arguing that findings are being called normal or abnormal based on \u201cbiased\u201d factors. \n\nThose overseeing AI development need to decide if the outputs are truly biased, or are simply reflecting trends and data that humans don\u2019t easily perceive and subsequently attribute to some form of bias.",
                                    "score": 4,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 12:06:55",
                                    "replies": [
                                        {
                                            "author": "danby",
                                            "body": "> im arguing that the radiology AI will make \u201cracist\u201d decisions that are actually just reflections of rote, non-biased data. \n\nSure but racism isn't just identifying someone's (putative) ethnic group. Which could just be benign factual information. Ethnicity is something that many diagnostic AIs will likely end up inferring/encoding because it is just a fact that many health features are correlated to our ethnicity. \n\nRacism creeps in when you start feeding your diagnostic analyses in to things like recommender systems. In a medical context you have to be very careful to ensure such systems are trained on incredibly clean unbiased data. Because the risk of recapitulating contemporary patterns than only exist because of extant racism (rather than their genetic background) is very, very high. That is, if people's medical outcomes are in part a result of systemic racism, then it is trivial for some AI to learn that some ethnic group is less successful outcomes for some condition and for it to learn not to recommend interventions for that groups",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 12:26:36",
                                            "replies": [
                                                {
                                                    "author": "mb1980",
                                                    "body": "This is an excellent and amazing point.   How can we ever train these to actually be unbiased if we live in a world full of bias?   And if we try to \u201cclean the data\u201d,  we\u2019ll surely introduce our own biases.  Imagine someone very passionate about implicit bias and it\u2019s effects on the data would clean it differently than someone who has never experienced any sort of discrimination in their lives.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 13:14:00",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "author": "KernelKetchup",
                                    "body": "Let's say it was fed all information, age, sex, ethnicity, etc.  And outcomes based on the treatments that were recommended based on the images.  And this AI's job was to recommend and allocate resources based on the given  data with the goal of generating the maximum number of successful outcomes with the given resources (maybe that's a racist goal?).   If this AI began to recommend the best treatments and allocate resources to a certain group based on that data, and let's assume it achieved the desired results, is it racist?    Now let's say we remove the ethnical information from the dataset, and the results are the same (because it is able to infer it).   Is it now less racist because we withheld information?",
                                    "score": 6,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 10:19:03",
                                    "replies": [
                                        {
                                            "author": "danby",
                                            "body": "> (maybe that's a racist goal?)\n\nYeah I'm pretty sure 'we'll spend fewer dollars per head on your health because we can infer you are black' is pretty racist.\n\nUltimately there are 2 kinds of triage here. Should we treat someone and which is the best treatment for somone? In many cases knowing your ethnicity is necessary and useful information on selecting the best treatment for you. Using an AI to select the best treatment is unlikely to be a racist goal if it genuinely optimises health outcomes. Using an AI in ways that end up restrict access to treatment based on (inferred) ethnicity is almost certainly racist.",
                                            "score": 2,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 10:21:32",
                                            "replies": [
                                                {
                                                    "author": "KernelKetchup",
                                                    "body": ">Yeah I'm pretty sure 'we'll spend fewer dollars per head on your health because we can infer you are black' is pretty racist.\n\nThat's wasn't the goal though, it was to save the most amount of people.   You can of course find racism in almost anything that takes race into account, but that's the point of the last question.   Lets say we fed it data without race, and it made decisions based on muscle mass, heart stress tests, blood oxygenation, bone density, etc.   If, in order to reach the goal of maximizing successful outcomes with a given number of resources, we saw after the fact that one race was being allocated an absurdly high amount of the resources and this resulted in an increased overall success rate, is it moral to re-allocate resources in the name of racial equality even though this reduces the overall success rate?",
                                                    "score": 5,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 12:02:46",
                                                    "replies": [
                                                        {
                                                            "author": "danby",
                                                            "body": "Are you just ignoring the rest of the discussion? If the system can infer race from proxy measures (muscle mass, heart stress tests, blood oxygenation, bone density, etc.) then it is equivalent to having provided it with racial information in the first place. It's close to \"we didn't put in ethnicity but we did put in skin colour\". If you then make decisions based on you model that can accurately infer race then you are certainly at risk of making a biased decisions. \n\n> If, in order to reach the goal of maximizing successful outcomes with a given number of resources, \n\nIs that the goal? We're not even doing that right now. Seems most like we maximise successful outcomes for folk with the most money. Black women have less successful pregnancies not because they are less fit for pregnancy but because the system ends up allocating them fewer resources. If a surgery has a 60% success rate in caucasian folk and a 57% success rate in black people should we not offer that surgery to black people? Or should we offer the surgery to 3% fewer black people? How do you fairly and morally decide which of those black people get excluded?",
                                                            "score": -4,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 12:15:06",
                                                            "replies": [
                                                                {
                                                                    "author": "KernelKetchup",
                                                                    "body": "> Are you just ignoring the rest of the discussion? If the system can infer race from proxy measures (muscle mass, heart stress tests, blood oxygenation, bone density, etc.) they it is equivalent to having provided it with racial information in the first place. And if you then make decisions based on you model then you are certainly at risk of making a biased decision.\n\nI'm not, and I get it.  I don't really know how to make this any clearer, or maybe it's just a question for me, and you don't want to answer it, I'm not sure I even want or can answer it.   If making a biased decision results in a higher success rate, is that wrong?  And if we are currently making biased decisions (doctors, whatever), is it moral to remove that bias if it drops the success rate?   Are we willing to let people die in the name of removing biases, sexism, racism, etc in a medical setting?   Are we willing to reduce quality of life or outcome in order to remove the same?",
                                                                    "score": 4,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 12:35:59",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "gunnervi",
                            "body": "Of course there are real, tangible differences in the data!  The impact of racism, sexism, homophobia, and other biases aren't just in our heads.  Its not just preconceived, bigoted notions about what people different from ourselves, and different from the societal \"norm\" are like.  Its also the fact that Black people are more likely to be poor and trans youth are more likely to be homeless and women are more likely to be sexually assaulted.\n\nIf you want the AI to tell you which criminals are more likely to re-offend, and give sentences accordingly, its going to sentence the black criminals more harshly.  And even if you anonymize the data, its going to pick up on all the other things that correlate with race.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 18:02:02",
                            "replies": [
                                {
                                    "author": "valente317",
                                    "body": "I suppose the direct comparison between medical AI and criminal sentencing isn\u2019t completely apt, but the point stands that the algorithm doesn\u2019t make \u201cracist\u201d or \u201csexist\u201d decisions, it simply reflects the facts that it can derive from input data. Re-offenders deserve harsher sentences, just like suspicious lung nodules deserve closer follow-up. All other factors aside, there isn\u2019t any inappropriate bias in the algorithm or it\u2019s decision-making process.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 18:41:58",
                                    "replies": [
                                        {
                                            "author": "gunnervi",
                                            "body": "Well, there's two things here.  One is the question of whether or not we should punish based on statistics.  I.e., reoffenders deserve harsher sentences, but do people who are merely *more likely* to reoffend?\n\nThe other is that even if we decide it's just to punish people who are likely to reoffend, we can also recognize that the decision to do so may reinforce racial injustices in our society that we would like to rectify, and that we can't do both.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 19:32:19",
                                            "replies": [
                                                {
                                                    "author": "valente317",
                                                    "body": "I acknowledge your point, and that\u2019s why it wasn\u2019t a great comparison. One attempts to mitigate future harm to the individual, thus reducing societal costs. The other attempts to punish an individual to reduce harm to society.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 21:20:03",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "Deleted",
                    "body": "The effect of the bias can be as insidious as the AI giving a different sentence based solely on the perceived ethnic background of the individual's name. \n\nSome people would argue that the training data would need to be properly prepared and edited before it could be processed by a machine to remove bias. Unfortunately even that solution isn't as straightforward as it sounds. There's nothing to stop the machine from making judgments based on the amount of punctuation in the input data, for example.\n\nThe only way around this would be to make an AI that could explain in painstaking detail  why it made the decisions it made which is not as easy as it sounds.",
                    "score": 75,
                    "depth": 1,
                    "timestamp": "2022-06-28 01:33:11",
                    "replies": [
                        {
                            "author": "nonotan",
                            "body": "Actually, there is another way. And it is fairly straightforward, but... (of course there is a but)\n\nWhat you can do (and indeed, just about the only thing you can do, as far as I can tell) is to simply directly enforce the thing we supposedly want to enforce, in an explicit manner. That is, instead of trying to make the agent \"race-blind\" (a fool's errand, since modern ML methods are astoundingly good at picking up the subtlest cues in the form of slight correlations or whatever), you make sure you figure out everyone's race as accurately as you can, and then *enforce* an equal outcome over each race (which isn't particularly hard, whether it is done at training time with an appropriate loss function, or at inference time through some sort of normalization or whatever, that bit isn't really all that technically challenging to do pretty well) -- congrats, you now have an agent that \"isn't racist\".\n\nDrawbacks: first, most of the same drawbacks in so-called affirmative action methods. While in an ideal world all races or whatever other protected groups would have equal characteristics, that's just not true in the real world. This method *is* going to give demonstrably worse results in many situations, because you're not really optimizing for the \"true\" loss anymore. \n\nTo be clear, I'm not saying \"some races just happen to be worse at certain things\" or any other such arguably racist points. I'm not even going to go near that. What's inarguably true is that certain ethnicities are over- or under-represented in certain fields for things as harmless as \"country X has a rich history when it comes to Y, and because of that it has great teaching infrastructure and a deep talent pool, and their population happens to be largely of ethnicity Z\". \n\nFor example, if for whatever reason you decided to make an agent that tried to guess whether a given individual is a strong Go/Baduk player (a game predominantly popular in East Asia, with effectively all top players in world history coming from the region), then an agent that matched real world observations would necessarily have to give the average white person a lower expected skill level than it would give the average Asian person. You could easily make it not do that, as outlined above, but it would give demonstrably less accurate results, really no way around that. And if you e.g. choose who gets to become prospective professional players based on these results or something like that, you will arguably be racially discriminating against Asian people. \n\nMaybe you still want to do that, if you value things like \"leveling the international playing field\" or \"hopefully increasing the popularity of the game in more countries\" above purely finding the best players. But it would be hard to blame those that lost out because of this doctrine if they got upset and felt robbed of a chance.\n\nTo be clear, sometimes differences in \"observed performance\" are absolutely due to things like systemic racism. But hopefully the example above illustrates that not *all* measurable differences are just due to racism, and sometimes relatively localized trends just happen to be correlated with \"protected classes\". In an ideal world, we could differentiate between these two things, and adjust only for the effects of the former. Good luck with that, though. I really don't see how it could even begin to be possible with our current ML tech. So you have to choose which one to take (optimize results, knowing you might be perpetuating some sort of systemic racism, but hopefully not any worse than the pre-ML system in place, or enforce equal results, knowing you're almost certainly lowering your accuracy, while likely still being racist -- just in a different way, and hopefully in the opposite direction of any existing systemic biases so they somewhat cancel out)\n\nLast but not least: even if you're okay with the drawbacks of enforcing equal outcomes, we shouldn't forget that what's considered a \"protected class\" is, to some extent, arbitrary. You could come up with endless things that sound \"reasonable enough\" to control based on. Race, ethnicity, sex, gender, country of origin, sexual orientation, socioeconomic class, height, weight, age, IQ, number of children, political affiliation, religion, personality type, education level... when you control for one and not for others, you're arguably being unfair towards those that your model discriminates against because of it. And not only will each additional class you add further decrease your model's performance, but when trying to enforce equal results over multiple highly correlated classes, you'll likely end up with \"paradoxes\" that even if not technically impossible to resolve, will probably require you to stray even further away from accurate predictions to somehow fulfill (think how e.g. race, ethnicity and religion can be highly correlated, and how naively adjusting your results to ensure one of them is \"fair\" will almost certainly distort the other two)",
                            "score": 41,
                            "depth": 2,
                            "timestamp": "2022-06-28 06:40:15",
                            "replies": [
                                {
                                    "author": "Deleted",
                                    "body": "[deleted]",
                                    "score": 8,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 11:09:37",
                                    "replies": [
                                        {
                                            "author": "Joltie",
                                            "body": "In which case, you would need to define \"racist\", which is a subjective term.\n\nTo someone, giving advantages to a specific group over another, is racist.\n\nTo someone else, treating everyone equitably, is racist.",
                                            "score": 9,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 16:21:18",
                                            "replies": []
                                        },
                                        {
                                            "author": "gunnervi",
                                            "body": "A definition of \"racism\" that includes \"treating different races differently in order to correct for inequities caused by current and historical injustice\" is not a useful definition.\n\nThis is why the prejudice + power definition exists.  Because if you actually want to understand the historical development of modern-day racism, and want to find solutions for it, you need to consider that racist *attitudes* always come hand in hand with the creation of a racialized underclass",
                                            "score": 2,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 18:15:14",
                                            "replies": []
                                        }
                                    ]
                                },
                                {
                                    "author": "Deleted",
                                    "body": "These ideas need to be discussed more broadly. I think you have done a pretty good job of explaining why generalizations and stereotypes are both valuable and dangerous. Not just with regard to machine learning and AI but out here in the real world of human interaction and policy.\n\nIs the discussion of these ideas in this way happening anywhere other than in Reddit comments? If you have any reading recommendations, I'd appreciate your sharing them.",
                                    "score": 14,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 08:31:18",
                                    "replies": [
                                        {
                                            "author": "Big_ifs",
                                            "body": "Just last week there was a big conference on these and related topics:\n[https://facctconference.org](https://facctconference.org)\n\nThere are many papers published on this. For example, there is a thorough discussion about procedural criteria (i.e. \"race-blindness\") and outcome-based criteria (e.g. \"equal outcome\" or demographic parity) for fairness. In the class of outcome-based criteria, other options besides equal outcome are available. - The research on all this is very interesting.\n\nEdit: That conference is also referenced in the article, for all those who (like me) only read headline...",
                                            "score": 7,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 09:54:45",
                                            "replies": [
                                                {
                                                    "author": "Deleted",
                                                    "body": "Thanks for the reference! I know I'm too often guilty of not reading the articles. In my defense, some of the best discussions end up being tangential to the articles :)",
                                                    "score": 2,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 13:55:38",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "chrischi3",
                            "body": "This. Neural networks can pick up on any pattern, even ones that aren't there. There's studies that show sentences on days after football games are harsher if the judges favourite team lost the night before. This might not be an obvious correlation, but the networks sees it. It doesn't understand what it sees there, just that there's times of the year where, every 7 days, sentences that are given are harsher.  \n\n\nIn the same vein, a neural network might pick up on the fact that the punctuation might say something about the judge. For instance, if you have a judge who is a sucker for sticking precisely to the rules, he might be a grammar nazi, and also work to always sentence people precisely to the letter of the law, whereas someone who rules more in the spirit of the law might not (though this is all conjecture)",
                            "score": 58,
                            "depth": 2,
                            "timestamp": "2022-06-28 03:16:13",
                            "replies": [
                                {
                                    "author": "Wh00ster",
                                    "body": "> Neural networks can pick up on any pattern, even ones that aren't there. \n\nThis is a paradoxical statement.",
                                    "score": 15,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:55:55",
                                    "replies": [
                                        {
                                            "author": "Deleted",
                                            "body": "What they're saying is it can pick up on patterns that wouldn't be there in the long run, and/or don't have a casual connection with the actual output they want. It can find spurious correlations and treat them as just as important as correlations that imply causation.",
                                            "score": 14,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 10:27:58",
                                            "replies": [
                                                {
                                                    "author": "Wh00ster",
                                                    "body": "They are still patterns. I wanted to call it out because I read it as implying the models simply make things up, rather than detecting latent, transient, unrepresentative, or non causal patterns.",
                                                    "score": 3,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 17:08:13",
                                                    "replies": []
                                                },
                                                {
                                                    "author": "Faceh",
                                                    "body": ">It can find spurious correlations and treat them as just as important as correlations that imply causation.\n\nAnd also rapidly learn which correlations are spurious and which are actually causal as long as it is fed good data about its own predictions and outcomes.\n\nHence the 'learning' part of machine learning.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 11:10:17",
                                                    "replies": [
                                                        {
                                                            "author": "teo730",
                                                            "body": "I agree, except they can't really learn what is 'causal'. It's also not the point to learn that most of the time. You almost always want to learn the most effective mapping between X -> y. If you give a model a bunch of data for X which is highly correlated to y, but not causal, the model will still do what you want - be able to guess at y based on X.",
                                                            "score": 5,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 11:19:03",
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "author": "chrischi3",
                                            "body": "Not really. Is there a correlation between per capita margarine consumption and the divorce rate in Maine between 2000 and 2009? Yes. Does that mean that per capita margarine consumption is the driving factor behind Maine's divorce rates? No.",
                                            "score": 8,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 10:17:32",
                                            "replies": [
                                                {
                                                    "author": "Faceh",
                                                    "body": "You moved the goalposts.\n\nThe pattern of margarine consumption and divorce rates in Maine is *THERE,* its just not *causal,* at least I cannot think of any way it could be causal. The AI would be picking up on a pattern that absolutely *exists* it just doesn't mean anything.\n\nThe pattern/correlation has to exist for the AI to pick up on it, that's why its paradoxical to claim an AI sees a pattern that 'doesn't exist.' \n\nAnd indeed, the fact that an AI can see patterns that aren't obvious is part of the strength of Machine Learning, since it may catch things that are indeed causal but were too subtle to perceive.\n\nHence why [AI is much better at diagnosing cancer from medical imaging](https://www.cancer.gov/news-events/cancer-currents-blog/2022/artificial-intelligence-cancer-imaging) than even the best humans.",
                                                    "score": 14,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 11:08:08",
                                                    "replies": [
                                                        {
                                                            "author": "GlitterInfection",
                                                            "body": ">\tat least I cannot think of any way it could be causal.\n\nI'd probably divorce someone if they took away my butter, too.",
                                                            "score": 3,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 14:05:12",
                                                            "replies": []
                                                        },
                                                        {
                                                            "author": "chrischi3",
                                                            "body": ">The AI would be picking up on a pattern that absolutely exists it just doesn't mean anything.\n\nIt's a correlation then, not a pattern.",
                                                            "score": -5,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 11:11:47",
                                                            "replies": [
                                                                {
                                                                    "author": "teo730",
                                                                    "body": "That's the same thing...\n\nCorrelation means two things change together in the same way. Pattern is just a more loose way to describe similar things. A pattern isn't a causal relationship.",
                                                                    "score": 10,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 11:15:12",
                                                                    "replies": []
                                                                },
                                                                {
                                                                    "author": "Faceh",
                                                                    "body": "And the correlation does exist, or else the AI wouldn't see it.\n\nWe're talking about the same thing, I'm just pointing out that seeing 'correlations' isn't the problem. Its inferring causal relationships that are illusory.",
                                                                    "score": 5,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 11:27:42",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "Tattycakes",
                                                    "body": "Ice cream sales and shark attacks!",
                                                    "score": 2,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 10:38:02",
                                                    "replies": [
                                                        {
                                                            "author": "gunnervi",
                                                            "body": "This is a common case of C causes A and B\n\nIn this case, hot weather causes people to want cold treats (like ice cream) and causes people to want to go to the beach (where sharks live)",
                                                            "score": 2,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 18:07:02",
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "author": "Claggart",
                                            "body": "Not really, it\u2019s just describing type I error.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 10:20:51",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "Deleted",
                            "body": "We are going to need psychologists for the AI.",
                            "score": 7,
                            "depth": 2,
                            "timestamp": "2022-06-28 05:05:33",
                            "replies": []
                        },
                        {
                            "author": "chrischi3",
                            "body": "As for how to figure out what biases the network has, one way would be to reverse it, aka instead of feeding it training data and having it generate an output out of this data, you run it in reverse and have it generate new data. If you messed with the outputs, which are now inputs, one at a time, you could see how it changes the resulting input (which, of course, is now output), but that's still complicated af.",
                            "score": 0,
                            "depth": 2,
                            "timestamp": "2022-06-28 03:23:15",
                            "replies": [
                                {
                                    "author": "Deleted",
                                    "body": "I'm pretty sure that's impossible. Each neuron in a network has a number of inputs, and an output that is based on the inputs. It'd be like trying to solve `A = B x C x D`, but you know the value of A and want to know B, C and D.\n\nYou can't, as they depend on each other.",
                                    "score": 7,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 08:15:49",
                                    "replies": [
                                        {
                                            "author": "chrischi3",
                                            "body": "Well, you can run most neural networks in reverse (which is to say, give it a bunch of training data to have it learn patterns in the data, then make it generate new data based off of the data you gave it before), but what i described would probably be extremely hard at the very least.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 08:46:16",
                                            "replies": []
                                        },
                                        {
                                            "author": "teo730",
                                            "body": "This is basically trying to model an [inverse problem](https://en.wikipedia.org/wiki/Inverse_problem) which very much is something people do. Not that it's necessarily easy, by any means, and I would assume it comes with larger uncertainties.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 11:28:06",
                                            "replies": [
                                                {
                                                    "author": "Deleted",
                                                    "body": "There are indubitably some methods to turn a classifying network into a generative network, and vice versa, but it's not as simple as \"reversing\" it.\n\nI also doubt think that the \"inverse\" would have the same issues as the original. So I doubt it would be useful to debug the training set. But that's speculation on my part.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 17:41:51",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "wild_man_wizard",
                    "body": "The actual point of Critical Race Theory is that systems can perpetuate  racism even without employing racist people, if false underlying assumptions aren't addressed.  Racist AI's perpetuating racism without employing any people at all are an extreme extrapolation of that concept.  \n\nAddressing tainted and outright corrupted data sources is as important in data science as it is in a history class.  Good systems can't be built on a foundation of bad data.",
                    "score": 55,
                    "depth": 1,
                    "timestamp": "2022-06-28 02:03:13",
                    "replies": [
                        {
                            "author": "Vito_The_Magnificent",
                            "body": "> if false underlying assumptions aren't addressed.\n\nThey need not be false. The thing that makes this so intractable isn't the false underlying assumptions, it's the true ones. \n\nIf an AI wants to predict recidivism, it can use a model that looks at marital status, income, homeownership, educational attainment, and the nature of the crime. \n\nBut maleness is a strong predictor of recidivism. It's a real thing. It's not an artifact or the result of bias. Men just commit more crime. A good AI will find a way to differentiate men from women to capture that chunk of the variation. A model with sex is much better at predicting recidivism than a model without it.\n\nSo any good AI will be biased on any trait that accounts for variation. If you tell it not to be, it'll just use a proxy \"Wow! Look how well hair length predicts recidivism!\"",
                            "score": 21,
                            "depth": 2,
                            "timestamp": "2022-06-28 11:15:36",
                            "replies": [
                                {
                                    "author": "10g_or_bust",
                                    "body": "> Men just commit more crime.\n\nActually it's more like men are arrested and sentenced at a higher rate (that's hard data we have). The soft data of how much crime is committed is sort of unknowable, we can make educated guesses at best.\n\nBut that's sort of the problem, just because a situation exists doesn't make it correct or a \"fact of reality\". People of color in the US tend to be poorer; that isn't an inherent property of those people but an emergent property due to other things largely out of their control such as generational wealth, etc. The problem of making choices based on \"facts\" like these is they easily becomes a self fulfilling prophecy.",
                                    "score": 2,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 16:17:14",
                                    "replies": [
                                        {
                                            "author": "Deleted",
                                            "body": "saying that \"men commit more crimes than women\" is sort of unknowable is crazy. is that seriously not a thing that we can somewhat agree on, given all the available data in the world?",
                                            "score": 3,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 19:20:24",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "KuntaStillSingle",
                            "body": ">The actual point of Critical Race Theory\n\nThat's a broad field without an actual point. You may as well be arguing the actual point of economics. To a Keynesian maybe it is to know how to minimize fluctuations in the economy,  to a communist it may be how to determine need and capability. A critical race theorist might write systemic racism, or they could be an advocate for standpoint epistemology, the latter of which is an anti-scientific viewpoint.",
                            "score": 19,
                            "depth": 2,
                            "timestamp": "2022-06-28 09:42:38",
                            "replies": []
                        },
                        {
                            "author": "kerbaal",
                            "body": "I feel like there is a real underlying point here; that is made problematic by just talking about racism. People's outcomes in life depend to a large degree statistically on their starting points. If their starting point is largely the result of racism, then those results will reflect that racism.\n\nHowever, a fix that simply remixes the races doesn't necessarily deal with the underlying issue of why starting points matter so much. I would really like to see a world where everybody has opportunity, not simply one where lack of opportunity is better distributed over skin colors.\n\nOne statistic that always struck me was that the single best predictor of whether a child in a middle class house grows up to be middle class is the economic class of their grandparents.\n\nThat says a lot about starting points and the importance of social networks. It DOES perpetuate the outcomes of past racism; but in and of itself, its not racism and fixing the distribition of inequality doesn't really fix this; it just hides it.",
                            "score": 3,
                            "depth": 2,
                            "timestamp": "2022-06-28 10:28:37",
                            "replies": []
                        },
                        {
                            "author": "Haunting_Meeting_935",
                            "body": "Zero relationship to what you describe. Events which took place in history need not be removed to allow non \"currupted\" data. That makes the data completely wrong. Also data models are not humans.",
                            "score": -36,
                            "depth": 2,
                            "timestamp": "2022-06-28 02:21:20",
                            "replies": [
                                {
                                    "author": "wild_man_wizard",
                                    "body": "I'm not advocating removing data.  I'm advocating adding data (and context).  Because those \"data models\" are called Artificial Intelligence because they ape Human Intelligence - which is just as susceptible to bad and incomplete data streams as its artificial cousins.\n\nAlso, statues are not data.",
                                    "score": 23,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 02:31:33",
                                    "replies": [
                                        {
                                            "author": "chrischi3",
                                            "body": "The term artificial intelligence is a bit of a misnomer for a neural network. A neural network is a system of interlinked simulated neurons (an extremely complicated interwoven formula if you will) which can be trained to detect patterns in a dataset. There is no intelligence involved here. It merely sees a dataset, processes it, and detects patterns. It can't problem solve, in that sense, which is what intelligence is about. It can learn to see one specific type of pattern, but that's about it. If you fed it with new data that doesn't fit the data you trained it on, it has no idea what to do.  \n\n\nBut yes, if you want a neural network to be unbiased, you need to make the data you feed it to be unbiased (Or at least minimize said bias to an acceptable level, whatever an acceptable level might be here, chances are you can't actually completely unbias such a system without training it on ficticious data, and even this data would have to be processed by a human first)",
                                            "score": 6,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 03:06:23",
                                            "replies": [
                                                {
                                                    "author": "turnerz",
                                                    "body": "What is the difference between \"seeing patterns\" and problem solving?",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 08:55:03",
                                                    "replies": [
                                                        {
                                                            "author": "chrischi3",
                                                            "body": "Transfer of knowledge. It's the difference between seeing others throw things into a test tube to make the water rise and reach the object floating on top and proceeding to do the same, and figuring out the same can be done with other containers, and even other mediums.",
                                                            "score": 3,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 09:03:31",
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "author": "Haunting_Meeting_935",
                                            "body": "As much as I'd like to agree with crt I cannot. As someone who is doing better than 99% of light colored folk Id rather let them continue to think we are criminals.",
                                            "score": -35,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 02:41:49",
                                            "replies": [
                                                {
                                                    "author": "Deleted",
                                                    "body": "[deleted]",
                                                    "score": 6,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 02:58:44",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "Dominisi",
                            "body": "> Addressing tainted and outright corrupted data sources\n\nSee this is the problem, You aren't being honest in what the issue is. \n\nThe data sources aren't corrupted or tainted. They are showing an accurate empirical representation of the data. The \"corruption\" comes from your disagreement with the pillars of that data, such as crime rates by ethnicity and it not being able to take into account human biases in something like policing by arbitrarily weighting things like race to skew the results to match your sensibilities. \n\nYou and people who share your world view will never be pleased with the data unless you pre-screen it and it shows the result you want before hand, otherwise you will come up with some reason why its perpetually biased in a way you don't like.",
                            "score": 0,
                            "depth": 2,
                            "timestamp": "2022-06-28 12:23:49",
                            "replies": [
                                {
                                    "author": "wild_man_wizard",
                                    "body": "So because I say I don't want to use corrupted data, I obviously want to corrupt the data.\n\nThe good old insightful \"I know you are but what am I?\" argument.",
                                    "score": 0,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 13:04:39",
                                    "replies": [
                                        {
                                            "author": "Dominisi",
                                            "body": "No. You don't want unbiased data.   \n\n\nYou want data that is manipulated to \"correct\" for biases in humans.",
                                            "score": 0,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 16:16:24",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "Mistervimes65",
                    "body": "Remember when the self-driving cars didn\u2019t recognize Black people as human? Why? Because no testing was done with people that weren\u2019t White.\n\nEdit: [Citation](https://arxiv.org/pdf/1902.11097.pdf)",
                    "score": 25,
                    "depth": 1,
                    "timestamp": "2022-06-28 07:43:27",
                    "replies": [
                        {
                            "author": "McFlyParadox",
                            "body": "\\*no *training* was done with datasets containing POC. Testing is what caught this mistake.\n\n\"Training\" and \"testing\" are not interchangeable terms in the field of machine learning.",
                            "score": 89,
                            "depth": 2,
                            "timestamp": "2022-06-28 08:27:41",
                            "replies": [
                                {
                                    "author": "Mistervimes65",
                                    "body": "Thank you for the gentle and accurate correction.",
                                    "score": 19,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 08:29:24",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "AegisToast",
                            "body": "\u201cThe company's position is that it's actually the opposite of racist, because it's not targeting black people. It's just ignoring them. They insist the worst people can call it is \u2018indifferent.\u2019\u201d",
                            "score": 9,
                            "depth": 2,
                            "timestamp": "2022-06-28 09:13:10",
                            "replies": [
                                {
                                    "author": "Deleted",
                                    "body": "Dude, is that a \"Better of Ted\" reference?",
                                    "score": 3,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 11:08:47",
                                    "replies": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "maniacal_cackle",
                    "body": "The problem with this argument is it implies that all you need to do is give 'better' data.\n\nBut the reality is, giving 'better' data will often lead to racist/sexist outcomes.\n\nTwo common examples:\n\nHiring AI: when Amazon set up hiring AI to try to select better candidates, it automatically selected the women out (even if you hid names, gender, etc). The criteria upon which we make hiring decisions incorporates problems of institutional sexism, so the bot does what it is programmed to do: learn to copy the decisions humans make.\n\nCriminal AI: you can setup an AI to accurately predict whether someone is going to commit crimes (or more accurately, be convicted of commiting a crime). And of course since our justice system has issues of racism and is more likely to convict someone based on their race, then the AI is going to be more likely to identify someone based on their race.\n\nThe higher quality data you give these AI, the more they are able to pick up the real world realities. If you want an AI to behave like a human, it will.",
                    "score": 13,
                    "depth": 1,
                    "timestamp": "2022-06-28 07:17:45",
                    "replies": [
                        {
                            "author": "Deleted",
                            "body": "I think the distinction to make here is what \"quality\" data is. The purpose of an AI system is generally to achieve some outcome. If the outcome of a certain dataset doesn't fit the business criteria then I would argue the quality of that data is poor for the problem space you're working in. That doesn't mean the data can't be used, or that the data is inaccurate, but it might need some finessing to reach the desired outcome and account for patterns the machine saw that humans didn't.",
                            "score": 5,
                            "depth": 2,
                            "timestamp": "2022-06-28 07:56:29",
                            "replies": []
                        },
                        {
                            "author": "callmesaul8889",
                            "body": "I don\u2019t think I\u2019d consider \u201cmore biased data\u201d as \u201cbetter\u201d data, though.",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2022-06-28 12:43:01",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "Deleted",
                    "body": "Stephen Colbert said reality has a well known liberal bias. Perhaps it has a less well known sexist and racist bias.",
                    "score": 2,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:45:08",
                    "replies": []
                },
                {
                    "author": "Lecterr",
                    "body": "Would you say the same is true for a racists brain?",
                    "score": 11,
                    "depth": 1,
                    "timestamp": "2022-06-28 01:37:42",
                    "replies": [
                        {
                            "author": "Elanapoeia",
                            "body": "Racism IS learned behavior, yes.\n\nRacists learned to become racist by being fed misinformation and flawed \"data\" in very similar ways to AI. Although one would argue AI is largely fed these due to ignorance and lack of other data that can be used to train them, while humans spread bigotry maliciously and with the options to avoid it if they cared.\n\nJust like you learned to bow to terrorism on the grounds that teaching children acceptance of people that are different isn't worth the risk of putting them in conflict with fascists.",
                            "score": 12,
                            "depth": 2,
                            "timestamp": "2022-06-28 01:56:58",
                            "replies": [
                                {
                                    "author": "Qvar",
                                    "body": "Source for that claim?\n\nAs far as I know racism and xenophobia in general are an innate fear self-protective response to the unknown.",
                                    "score": 54,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 03:51:02",
                                    "replies": [
                                        {
                                            "author": "Elanapoeia",
                                            "body": "fear of \"the other\" are indeed innate responses, however racism is a specific kind of fear informed by specific beliefs and ideas and the specific behavior racists show by necessity have to be learned. Basically, we learn who we are supposed to view as the other and invoke that innate fear response.\n\nI don't think that's an unreasonable statement to make",
                                            "score": 28,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 04:14:14",
                                            "replies": [
                                                {
                                                    "author": "ourlastchancefortea",
                                                    "body": "Is normal \"fear of the other\" and racism comparable to fear of heights (as in \"be careful near that cliff\") and Acrophobia?",
                                                    "score": 3,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 04:41:57",
                                                    "replies": [
                                                        {
                                                            "author": "Elanapoeia",
                                                            "body": "I struggle to understand why you would ask this unless you are implying racism to be a basic human instinct?",
                                                            "score": 4,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 04:46:57",
                                                            "replies": [
                                                                {
                                                                    "author": "Maldevinine",
                                                                    "body": "Are you sure it's not?\n\nI mean, there's lots of bizarre things that your brain does, and the Uncanny Valley is an established phenomenon. Could almost all racism be based in an overly active brain circuit trying to identify and avoid diseased individuals?",
                                                                    "score": 19,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 06:08:33",
                                                                    "replies": [
                                                                        {
                                                                            "author": "Elanapoeia",
                                                                            "body": "I explained this in an earlier reply\n\nThere is an innate fear of otherness we do have, but that fear has to first be informed with what constitutes \"the other\" for racism to emerge. Cause racism isn't JUST fear of otherness, there are false beliefs and ideas associated with it",
                                                                            "score": 22,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 06:16:39",
                                                                            "replies": [
                                                                                {
                                                                                    "author": "Dominisi",
                                                                                    "body": "I understand what you're saying, but there has been a bunch of research done on children and even something as basic as never coming into contact with people of other races can start to introduce racial bias in babies at six months.   \n\n\n[Source](https://www.utoronto.ca/news/racial-bias-may-begin-babies-six-months-u-t-research-reveals)",
                                                                                    "score": 7,
                                                                                    "depth": 9,
                                                                                    "timestamp": "2022-06-28 12:27:24",
                                                                                    "replies": []
                                                                                },
                                                                                {
                                                                                    "author": "Deleted",
                                                                                    "body": "> but that fear has to first be informed with what constitutes \"the other\" for racism to emerge\n\nSource?",
                                                                                    "score": 3,
                                                                                    "depth": 9,
                                                                                    "timestamp": "2022-06-28 12:05:18",
                                                                                    "replies": []
                                                                                }
                                                                            ]
                                                                        }
                                                                    ]
                                                                },
                                                                {
                                                                    "author": "ourlastchancefortea",
                                                                    "body": "That would imply, I consider \"Acrophobia\" a basic human instinct, which I don't. It's an irrational fear. I just want to understand if racism is a comparable mechanism or not. Both are bad (and one is definitely much worse).",
                                                                    "score": -2,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 05:03:10",
                                                                    "replies": [
                                                                        {
                                                                            "author": "Elanapoeia",
                                                                            "body": "oh, you don't see fear of heights (as in \"be careful near that cliff\") as a human instinct? It's a safety response that is ingrained in everyone after all.\n\nI guess if you extend that to acro**phobia**, it's more severe than the basic instinct, making it more irrational, sure. I wouldn't necessarily consider it learned behavior though, as medically diagnosed phobias usually aren't learned behavior as far as I am aware.\n\nWere you under the impression I was defending racism? Cause I am very much not. But I don't believe they're comparable mechanisms. Acrophobia is a medically diagnosed phobia, racism acts through discrimination and hatred based on the idea that \"the other\" isn't equal and basically just plays on that fear response we have when we recognize something as other.\n\nI still kinda struggle why you would ask this, because I would consider this difference extremely obvious so that it really doesn't need to be specified?",
                                                                            "score": 12,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 05:18:16",
                                                                            "replies": [
                                                                                {
                                                                                    "author": "ourlastchancefortea",
                                                                                    "body": "> oh, you don't see fear of heights (as in \"be careful near that cliff\") as a human instinct?\n\nDidn't say that.\n\n> as medically diagnosed phobias usually **aren't learned** behavior as far as I am aware.\n\nAh, good point. That's (see highlighted part) something I actual wanted to know. \n\n> Were you under the impression I was defending racism?\n\nHow did you read that out of my comment? Serious question.\n\n> But I don't believe they're comparable mechanisms. \n\nAgain, that was exactly what I wanted to know. \n\n> because I would consider this difference extremely obvious\n\nConsidering things obvious is in my experience a straight way to misunderstanding each other.",
                                                                                    "score": -2,
                                                                                    "depth": 9,
                                                                                    "timestamp": "2022-06-28 05:27:16",
                                                                                    "replies": [
                                                                                        {
                                                                                            "author": "Elanapoeia",
                                                                                            "body": ">Didn't say that.\n\nhold on, you totally did tho? I even copied the stuff that's in brackets directly from your post. There has to be some miscommunication going on here\n\n&#x200B;\n\n>How did you read that out of my comment? Serious question.\n\nIt seemed you were challenging my idea that racism is learned by comparing it to fear of heights and later clarified you do not consider them innate fears, so I was struggling WHY you were asking me for the difference. I figured you might have misunderstood my point about racism, so I asked to clarify.",
                                                                                            "score": 1,
                                                                                            "depth": 10,
                                                                                            "timestamp": "2022-06-28 05:32:25",
                                                                                            "replies": []
                                                                                        }
                                                                                    ]
                                                                                }
                                                                            ]
                                                                        },
                                                                        {
                                                                            "author": "mrsmoose123",
                                                                            "body": "I don't think we know definitively, other than looking into ourselves. \n\nIn observable evidence, there is worse racism in places where fewer people of colour live. So we can say racism is probably a product of local culture. It may be that the 'innate' fear of difference to local norms is turned into bigotry through the culture we grow up in. But that's still very limited knowledge. Quite scary IMO that we are training robots to think with so little understanding of how we think.",
                                                                            "score": 2,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 05:25:01",
                                                                            "replies": []
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "author": "Deleted",
                                    "body": "[deleted]",
                                    "score": 18,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 03:51:20",
                                    "replies": [
                                        {
                                            "author": "Lengador",
                                            "body": "TLDR: If race is predictive, then racism is expected.\n\nIf a race is sufficiently over-represented in a social class and under-represented in other social classes, then race becomes an excellent predictor for that social class.\n\nIf that social class has behaviours you'd like to predict, you run into an issue, as social class is very difficult to measure. Race is easy to measure. So, race predicts those behaviours with reasonably high confidence.\n\nTherefore, biased expectation based on race (racism) is perfectly logical in the described situation. You can feed correct, non-flawed, data in and get different expectations based on race out.\n\nHowever, race is not *causative*; so the belief that behaviours are due to race (rather than factors which caused the racial distribution to be biased) would not be a reasonable stance given both correct and non-flawed data.\n\nThis argument can be applied to the real world. Language use is strongly correlated with geographical origin, in much the same way that race is, so race can be used to predict language use. A Chinese person is much more likely to speak Mandarin than an Irish person. Is it racist to presume so? Yes. But is that racial bias unfounded? No.\n\nOf course, there are far more controversial (yet still predictive) correlations with various races and various categories like crime, intelligence, etc. None of which are causative, but are still predictive.",
                                            "score": 2,
                                            "depth": 4,
                                            "timestamp": "2022-06-29 01:01:21",
                                            "replies": [
                                                {
                                                    "author": "ChewOffMyPest",
                                                    "body": ">However, race is not causative; so the belief that behaviours are due to race (rather than factors which caused the racial distribution to be biased) would not be a reasonable stance given both correct and non-flawed data.\n\nExcept this is the problem, isn't it?\n\nYou are stating race isn't causative. Except there's no actual reason to believe that's the case. In fact, that's precisely the opposite of what every epigeneticist believed right up until only a few decades ago when the topic became taboo, and essentially the science 'settled' on simply not talking about, not proving the earlier claims false.\n\nDo you sincerely believe that if an alien species came here, it wouldn't categorize the different 'races' into subspecies (or whatever their taxonomic equivalent would be) and recognize differences in intelligence, personability, strong-headedness, etc. in *exactly* the same way we do with dogs, birds, cats, etc.? It's acceptable when we say that Border Collies are smarter than Pit Bulls or that housecats are more friendly than mountain lions, but if an AI came back with this exact same result, why is the assumption \"the data must be wrong\" and not \"maybe we are wrong\"?",
                                                    "score": 0,
                                                    "depth": 5,
                                                    "timestamp": "2022-07-17 16:00:59",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "author": "pelpotronic",
                                            "body": "I think you could hypothetically, though I would like to have \"racist\" defined first.\n\nWhat you make with that information and the angle you use to analyse that data is critical (and mostly a function of your environment), for example the neural network can not be racist in and on itself.\n\nHowever the conclusions people will draw from the neural networks may or may not be racist based on their own beliefs.\n\nI don't think social environment can be qualified as data.",
                                            "score": 5,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 06:44:07",
                                            "replies": []
                                        },
                                        {
                                            "author": "alex-redacted",
                                            "body": "This is the wrong question. \n\nThe rote, dry, calculated data itself *may be measured accurately*, but that's useless without (social, economic, historical) context. No information exists in a vacuum, so starting with this question is misunderstanding the assignment.",
                                            "score": 2,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 11:04:07",
                                            "replies": [
                                                {
                                                    "author": "Dominisi",
                                                    "body": "Its not the wrong question. Its valid.   \n\n\nAnd the easy way of saying your answer is this:   \n\n\nUnless the data matches with 2022 sensibilities and world views and artificially skews the results to ensure nobody is offended by the result the data is biased and racist and sexist and should be ignored.",
                                                    "score": 4,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 12:29:17",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "author": "Elanapoeia",
                                            "body": "What an odd question to ask.\n\nI *wonder* where this question is trying lead, hmm..",
                                            "score": -23,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 04:07:00",
                                            "replies": [
                                                {
                                                    "author": "Deleted",
                                                    "body": "[removed]",
                                                    "score": 25,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 04:15:07",
                                                    "replies": [
                                                        {
                                                            "author": "Elanapoeia",
                                                            "body": "You're just asking questions, I understand.",
                                                            "score": -16,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 04:17:14",
                                                            "replies": [
                                                                {
                                                                    "author": "Deleted",
                                                                    "body": "[deleted]",
                                                                    "score": 27,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 04:29:29",
                                                                    "replies": [
                                                                        {
                                                                            "author": "Elanapoeia",
                                                                            "body": "I wanna note for third parties, this person sneakily implied racism as justified if data shows ANY racial differences to exist\n\nImplying that if any group of people would have legitimate statistical differences to another group of people (that we socially consider to be a different race, no matter how unscientific that concept is to begin with) then becoming racists was somehow a reasonable conclusion\n\nAnd you can take a pretty good guess where that was going\n\nedit:\n\n>Can you ***become racist*** through correct information and non-flawed data?  \n>  \n>Or is the data inherently flawed if it shows ***any racial differences?***",
                                                                            "score": -10,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 04:37:14",
                                                                            "replies": [
                                                                                {
                                                                                    "author": "Deleted",
                                                                                    "body": "[deleted]",
                                                                                    "score": 20,
                                                                                    "depth": 9,
                                                                                    "timestamp": "2022-06-28 05:06:52",
                                                                                    "replies": [
                                                                                        {
                                                                                            "author": "Elanapoeia",
                                                                                            "body": "Notice how important this answer seems to be, even though if there wasn't malicious intent behind the question, the answer would be practically irrelevant.\n\nAnd if I wasn't correct, they would have clarified by now.",
                                                                                            "score": 0,
                                                                                            "depth": 10,
                                                                                            "timestamp": "2022-06-28 05:12:19",
                                                                                            "replies": []
                                                                                        }
                                                                                    ]
                                                                                },
                                                                                {
                                                                                    "author": "sosodank",
                                                                                    "body": "as a third party, you're ducking an honest question",
                                                                                    "score": 18,
                                                                                    "depth": 9,
                                                                                    "timestamp": "2022-06-28 06:04:25",
                                                                                    "replies": [
                                                                                        {
                                                                                            "author": "Elanapoeia",
                                                                                            "body": "I don't read it as an honest question. And I gave them the chance twice to clarify and they refused to do so.\n\nThis seemed to lead into the idea that \"if data is not flawed and shows racial differences exist in some form, therefore racism is justified to emerge\" and I fully reject that premise and refuse to engage with someone who would even imply that \"racial differences\" should even be equated with racism. That is a massive red flag.\n\nI called it racism, not \"the existence of differences\". So when someone tries to redefine this, I can only assume malicious intent. The question changed the premise of my initial comment dishonestly.\n\nMy point is, for data to create racism, it has to be misrepresented, re-contextualized in dishonest ways, be coupled with misinformation or be straight up fake etc. True and honest data *by itself* will not *create* racists beliefs.\n\n(+ I checked the users post history and found them expressing several bigoted ideas - like \"immigrants are rapists\" or defending politicians who incited violence against immigrants. Also some neat transphobia. Dudes a racist asking a leading question about how statistics justify his racism)",
                                                                                            "score": 6,
                                                                                            "depth": 10,
                                                                                            "timestamp": "2022-06-28 06:22:44",
                                                                                            "replies": [
                                                                                                {
                                                                                                    "author": "Deleted",
                                                                                                    "body": "[deleted]",
                                                                                                    "score": 7,
                                                                                                    "depth": 11,
                                                                                                    "timestamp": "2022-06-28 07:05:25",
                                                                                                    "replies": [
                                                                                                        {
                                                                                                            "author": "Elanapoeia",
                                                                                                            "body": "Hold on, we were talking about humans though. I initially answered a different user who asked about human racism, not machine learning.\n\nI get that is the threads overall topic, but my reply chain that this sprang from was about humans. And they asked \"can *you* become racist...\" which means they were continuing the conversation about humans rather than going back to machines",
                                                                                                            "score": 6,
                                                                                                            "depth": 12,
                                                                                                            "timestamp": "2022-06-28 07:17:37",
                                                                                                            "replies": [
                                                                                                                {
                                                                                                                    "author": "Deleted",
                                                                                                                    "body": "[deleted]",
                                                                                                                    "score": 4,
                                                                                                                    "depth": 13,
                                                                                                                    "timestamp": "2022-06-28 07:35:09",
                                                                                                                    "replies": [
                                                                                                                        {
                                                                                                                            "author": "Elanapoeia",
                                                                                                                            "body": "oh, I absolutely agree to you that we should engage racists - in the real world. I have done things like that myself, occasionally, although not about racism but other types of bigotry. Obviously not on the level as davis, not even close. I'm just trying to make clear that I very much agree with this method for irl situations.\n\nI do not believe that entertaining leading questions, racist euphemisms and all that jazz *online* to be sensible though. At least not on reddit between random anonymous users. Because online discussions with racists like this aren't done in an environment with the power to change minds or any sort of real social pressure. Racists online ask questions to entertain themselves and upset people. I do not believe that engaging such a situation is in any way fruitful. Because if I confronted them, they'd deflect and deny and amuse themselves over my attempt to call them out, let's be real. That's different than going in person to a rally and engage them as a person they're bigoted against.\n\n&#x200B;\n\nAs to your other point, that's a fair argument to make, although I don't believe I personally agree. Human races aren't a scientific concept, and we only recognize races because society pre-conditions us to categorize people by arbitrary means. Race itself is already a dishonest concept full of misinformation and almost inherently leads to flawed data. So I wouldn't agree that complete correct, non-flawed data would *create* racism from scratch in someone who wasn't in some manner already racist before, regardless of morality.\n\nBut yeah, absolutely, our data itself is mostly flawed and that creates very complex issues for machine learning that will be very difficult to solve even for people who are genuinely not trying to perpetuate racism.",
                                                                                                                            "score": 4,
                                                                                                                            "depth": 14,
                                                                                                                            "timestamp": "2022-06-28 07:54:04",
                                                                                                                            "replies": []
                                                                                                                        }
                                                                                                                    ]
                                                                                                                }
                                                                                                            ]
                                                                                                        }
                                                                                                    ]
                                                                                                },
                                                                                                {
                                                                                                    "author": "Mindestiny",
                                                                                                    "body": "Your definition of racism is flawed, and they asked an honest question, but instead of making a rational argument to support your definition you just dodged the question and started making personal attacks.  Not cool.\n\nRacism, by accepted definition of the term, does not require data to be misrepresented, maliciously tampered with, or otherwise \"dishonest\" data.  All it requires is a trend that would lead towards a tangible bias.\n\nFor example, if the data shows that Americans of Latino descent have an increased rate of being interested in modding cars and street racing as part of youth culture, and the data is used for AI based law enforcement profiling, it would lead to the AI singling out Latino youths in commonly modded cars for a lower tolerance to trigger enforcement action.  It's just following a clear, innocent trend in the data but in normal policing we call that racial profiling and consider it a \"racist\" application of bias.  Theres no malicious data manipulation required to end up there whatsoever.\n\nTheir post history is irrelevant, they're making a valid point.",
                                                                                                    "score": 7,
                                                                                                    "depth": 11,
                                                                                                    "timestamp": "2022-06-28 07:30:16",
                                                                                                    "replies": [
                                                                                                        {
                                                                                                            "author": "Elanapoeia",
                                                                                                            "body": "oh come on, please. Read the entire reply-chain.\n\nI reply to someone who asked if humans brains are/can be similarly influenced by data as machines to display racism\n\nI say in response, that the data has to be manipulative/flawed to create racism in humans. That doesn't mean I am defining racism itself to *require* manipulated data. It means that I am saying if you became racist through data, that data had to be flawed/misrepresentative/etc/etc/etc. Obviously you can have other reasons than data to become racist as well. BUT IN THE CONCEPT OF FEEDING SOMEONE DATA AND THAT ITSELF RESULTING IN RACISM - it has to have been manipulated/flawed data\n\nAnd then that dude asked a question implying real data can also create racism and *suspiciously* they express racist views on their profile.",
                                                                                                            "score": 2,
                                                                                                            "depth": 12,
                                                                                                            "timestamp": "2022-06-28 07:39:01",
                                                                                                            "replies": [
                                                                                                                {
                                                                                                                    "author": "Freshfacesandplaces",
                                                                                                                    "body": ">BUT IN THE CONCEPT OF FEEDING SOMEONE DATA AND THAT ITSELF RESULTING IN RACISM - it has to have been manipulated/flawed data\n\nThis makes absolutely zero sense to me. In my post history I have a post which shows roughly 80% of firearm homicides in Chicago and... I believe New York were committed by black men. The data was pulled from city police reports. These are massive, comprehensive reports put out by the city police annually which show a massive amount of data regarding crime in the city. \n\nSo, the data exists. It isn't manipulated, it's not flawed. People can take a few paths in their mind with this data. Two big ones being \"well, clearly black men are the problem\" and attribute the issue to race. Alternatively, and the reasonable response would be to then start considering the socioeconomic factors contributing to this issue. The systemic racism that led to great wealth disparities and the creation of inner city ghettos which perpetuate this cycle of poverty and violence. \n\nIt's not flawed data that could lead to someone thinking racist thoughts. It's a complete lack of understanding of sociological and historical factors that heavily contributed to the current struggles of black Americans. In other words, the issue is what one does with the data, how they internalize it.",
                                                                                                                    "score": 4,
                                                                                                                    "depth": 13,
                                                                                                                    "timestamp": "2022-06-28 10:14:45",
                                                                                                                    "replies": []
                                                                                                                }
                                                                                                            ]
                                                                                                        }
                                                                                                    ]
                                                                                                }
                                                                                            ]
                                                                                        }
                                                                                    ]
                                                                                }
                                                                            ]
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "author": "Deleted",
                                    "body": "[deleted]",
                                    "score": 3,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 07:11:09",
                                    "replies": [
                                        {
                                            "author": "Deleted",
                                            "body": "That's a f'ing terrifying idea. That lends credence to mutually loathing between",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 11:09:35",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "Haunting_Meeting_935",
                            "body": "This system is based on human selection of keywords to images. Of course its going to have the human bias still. What is so difficult to understand people.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 02:29:56",
                            "replies": [
                                {
                                    "author": "chrischi3",
                                    "body": "Kinda my point. It's extremely hard to develop a neural network that is unbiased, because humans have all sorts of biases that we usually aren't even aware of. There was a study done in the 70s, for instance, which showed that the result of a football game could impact the harshness of a sentence given the monday after said game.   \n\n\nIf you included references to dates in the dataset, the neural network wouldn't pick up on this correlation. It would only see that every seven days in certain times of the year, sentences are harsher, and would therefore emulate this bias.   \n\n\nAgain, the neural network has no concept of mood, and how the result of a football game can impact it, and might thus cause a judge to give harsher sentences, all it sees is that this is what is going on, and assumes that this is meant to be there.",
                                    "score": 4,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 03:11:44",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "wowzabob",
                            "body": "No. AI doesn't have have sentience nor a psyche. It could be said that racism forms in a person with \"junk in,\" but they quickly become wrapped up in it, identify with it, believe in it. Racism becomes a structuring ideological fantasy for the psyche. It's not the same for AI, which will merely reflect the data neutrally, rather than believing in an idea and having that inform choices/behaviour in a generative way.",
                            "score": -1,
                            "depth": 2,
                            "timestamp": "2022-06-28 07:48:32",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "Deleted",
                    "body": "[removed]",
                    "score": 7,
                    "depth": 1,
                    "timestamp": "2022-06-28 06:15:21",
                    "replies": [
                        {
                            "author": "recidivx",
                            "body": "Unfortunately, the word \"racist\" has at least two distinguishable meanings:\n\n 1. Having the cognitive mindset that holds that some races are inferior to others;\n 2. Any action or circumstance which tends to disadvantage one race over another.\n\nOP is saying, quite reasonably, that neural networks are 2 but they are not 1. (That's why they literally say that NNs both \"are not racist\" and \"are racist\".)\n\nBoth concepts are useful but they're very different, and I honestly think it's significantly holding back the racism discussion that people sometimes confuse them.",
                            "score": 41,
                            "depth": 2,
                            "timestamp": "2022-06-28 08:13:38",
                            "replies": [
                                {
                                    "author": "Dominisi",
                                    "body": "Thank you for this. Your distinction of the two \"racist\" meanings will be very helpful in future discussions.",
                                    "score": 6,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 12:30:39",
                                    "replies": []
                                },
                                {
                                    "author": "Deleted",
                                    "body": "[removed]",
                                    "score": -3,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:23:41",
                                    "replies": [
                                        {
                                            "author": "Deleted",
                                            "body": "[removed]",
                                            "score": 4,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 09:36:01",
                                            "replies": [
                                                {
                                                    "author": "Deleted",
                                                    "body": "[removed]",
                                                    "score": -2,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 10:04:56",
                                                    "replies": [
                                                        {
                                                            "author": "Deleted",
                                                            "body": "[removed]",
                                                            "score": 2,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 12:33:18",
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "Tylendal",
                            "body": "Smacks of people being told about problems with motion detectors (such as for automatic sinks) and going \"What? Sinks can't be racist, that's just how light works.\" That rebuttal only makes sense if automatic sinks grew in nature or something. As they are, someone designed them that way, and the fact they work poorly with dark skin is something the designer never even bothered considering. That's racism. It's not blatant, malicious bigotry, but it's still racism born of casual ignorance.",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2022-06-28 07:25:16",
                            "replies": [
                                {
                                    "author": "chancegold",
                                    "body": "I don't know enough about these specific sinks to argue one way or the other, but I would like your position on the principle.\n\n*If*, due to the actual, physical, biological differences between races/sexes/preferences/whatever, a system like the sink sensor will *always* be more or less effective for one or more groups, does that make it -ist? Like, if you increase the sensor sensitivity to the point it is as reliable on dark skin as it currently is on white skin, won't that just make *even more* sensitive or \"reliable\" towards light skin, ad nauseum?",
                                    "score": 6,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:29:16",
                                    "replies": [
                                        {
                                            "author": "Tylendal",
                                            "body": "That's a pointless hypothetical completely divorced from the vagueness of reality. It's quite simple. \n\nDid you design an automatic sink that you claim detects people, and then turns out is bad at detecting black people, because you never even thought to try? That's racist. \n\nDid you design a sink that maybe works a little more reliably on white people, but everyone agrees that it works reasonably reliably no matter your skin colour? Then the system is good enough. \n\nYou aren't trying for precision with a system like this, just trying to reach a break point.",
                                            "score": -3,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 09:57:04",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "Deleted",
                            "body": "Okay, how do we fix the issue? I mean beyond complaining and telling programmers to fix it. The algorithms pick up these problems from the training data and the training data is society itself. How are you going to cleanse these massive data sets of anything you consider problematic?",
                            "score": 0,
                            "depth": 2,
                            "timestamp": "2022-06-28 10:57:48",
                            "replies": []
                        },
                        {
                            "author": "Deleted",
                            "body": ">It's beyond obvious that what is meant here is the results of outputs of the neural net is unfairly disadvantageous along the lines of race and sex, therefore perpetuating racism and sexism.\n\nIt may be beyond obvious to you and I, but not to the vast majority of people I've talked to about this. When people see the word AI, they don't think of a statistical model on steroids, they really do think of AGI.\n\n>It's time we move past this nitpicking and focus on the actual issue.\n\nIn my opinion, it's hard to move past this when the people making decision don't even understand the nature of the actual issue.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 13:29:15",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "reddititty69",
                    "body": "Why was ethnicity used as an input to the sentencing A\u00cd?   Or is it able to reconstruct ethnicity due to other strong correlations?",
                    "score": 3,
                    "depth": 1,
                    "timestamp": "2022-06-28 07:46:30",
                    "replies": [
                        {
                            "author": "chrischi3",
                            "body": "I don't know the details. It's possible that they fed the neural network with things like criminal histories too, which are relevant in sentencing (as a first offender would get a lesser sentence than a known criminal obviously) and i'm guessing that would include things like photos or at least a description. It's very possible the researchers just mindlessly fed the thing with information that could easily be turned into something that a computer can more easily process (aka cut the file down to the important bits rather than give it full sentences to chew through) without regard for what they are feeding it, too.",
                            "score": 7,
                            "depth": 2,
                            "timestamp": "2022-06-28 08:21:43",
                            "replies": [
                                {
                                    "author": "reddititty69",
                                    "body": "This is something that bothers me about A\u00cd/ML : the tendency to overfeed it with data and get nonsensical results.  It\u2019s not a problem with the algorithms, but rather malpractice on the part of the modelers/data scientists.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 08:38:35",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "Deleted",
                            "body": "Neither would surprise me. If all the data for a case was put into a text document and crammed into the AI as training data, then ethnicity would probably appear in that. But even if they scrubbed that out, it probably wouldn't be that hard for the AI to reconstruct ethnicity from correlated data.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 11:06:13",
                            "replies": []
                        },
                        {
                            "author": "hurpington",
                            "body": "It could be a case where they looked at the statistics and said x race appears to be unfairly targeted, but didn't account that x race also had a higher baseline of crimes committed, or something along those lines.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 12:35:09",
                            "replies": []
                        },
                        {
                            "author": "arborite",
                            "body": "Ethnicity, race, gender, etc. aren't fed into these models. Other things correlate to it. Zip codes and socioeconomic factors can heavily affect this. You can also see it pop up in natural language processing. Reading a police report to determine guilt or innocence or a clinician's notes to detect if a patient is sick can also find bias in the wording used. Not to say the people generating these reports are explicitly racist but that there could be implicit language used when talking about people of different races, ethnicities, genders, ages, etc. that can correlate back to those variables. We have to actively find ways of removing bias from this data or face not being able to use it to train models using that data if removing bias is truly a primary goal.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 12:38:05",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "MagicPeacockSpider",
                    "body": "Expect we get to choose the data to train networks on.\n\nJunk in junk out has never been a valid excuse.\n\nWe're going to have to force companies to put in the effort an just collect data at random or use unbalanced huge data sets and expect fair results.\n\nLike you say, we know that the world has sexism and racism. We know any large dataset will reflect that. We know training AI on that data will perpetuate racism and sexism.\n\nKnowing all this it's not acceptable to simply allow companies to cut corners. They're responsible for the results the AI produces.\n\nAny sample of water you collect in the world will contain contamination. That doesn't mean companies are allowed to bottle it and sell it, giving that as a reason they're not responsible. We regulate water so it's tested, clean and safe.\n\nIt's becoming clear we'll need to regulate AI.",
                    "score": 3,
                    "depth": 1,
                    "timestamp": "2022-06-28 06:50:56",
                    "replies": [
                        {
                            "author": "chrischi3",
                            "body": "Question is, how do you choose which samples are biased and which are not? And besides, neural network are great at finding patterns, even ones that aren't there. If there's a correlation between proper punctuation and harsher sentences, you bet the network will find it. Does that mean we should remove punctuation from the sample data?",
                            "score": 21,
                            "depth": 2,
                            "timestamp": "2022-06-28 06:51:56",
                            "replies": [
                                {
                                    "author": "MagicPeacockSpider",
                                    "body": "Well, frankly that's for the companies to work out. I'd expect them to find measures, objective as it's possible to be, for the results. Then keep developing the most objective AI they can.\n\nIf there's something irrelevant affecting sentencing unduly that's a problem that needs fixing. Especially with language, that's a proxy for racist laws already.\n\nAt the moment AI products are not covered very well by the discrimination laws we have in place. It's very difficult to sue an AI when you don't know why it made the decision it did. There's also no requirement to release large amounts of performance data to prove a bias.\n\nAlgorithms, AI, etc. are part of the modern world now. If a large corporation makes a bad one and it can have a huge effect. They need to at least know their liable if they don't follow certain best practices.",
                                    "score": 0,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 07:20:48",
                                    "replies": [
                                        {
                                            "author": "dmc-going-digital",
                                            "body": "But we can't both regulate then go around and say that they have to figure it out",
                                            "score": 9,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 07:43:45",
                                            "replies": [
                                                {
                                                    "author": "MagicPeacockSpider",
                                                    "body": "Sure we can. Set a standard for a product. Ban implementations that don't meet that standard. If they want to release a product they'll have to figure it out.\n\nThere is no regulation on the structure of a chair. You pick the size, shape, material, design.\n\nBut one that collapses when you sit on it will end up having its design tested to see if the manufacturer is liable. Either for just a faulty product or injuries if they're extreme.\n\nThe manufacturer has to work out how to make the chair. The law does not specify the method but can specify a result.\n\nThe structure of the law doesn't have to be any different if the task is more difficult like developing an AI. You just pass a law into legislation that states something an AI must not do. Just as we pass laws saying things humans must not do.",
                                                    "score": -2,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 09:16:11",
                                                    "replies": [
                                                        {
                                                            "author": "dmc-going-digital",
                                                            "body": "Then what is the ducking legal standard or what should it be? That's not a question you can put on the companies",
                                                            "score": 5,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 09:17:55",
                                                            "replies": [
                                                                {
                                                                    "author": "MagicPeacockSpider",
                                                                    "body": "Exactly the same standards already in place in the EU it's illegal to discriminate on protected characteristics. Whether that's age, race, gender, secuality. If you pay one group more or discriminate against them as customers then you are breaking the law.\n\nThe method doesn't matter, the difficulty is usually proving it when a process is closed off from view. So large companies have to submit anonymised data and statistics on who they employ and salaries and information on those protected characteristics.\n\nThe question is already on any company as the method of discrimination is not specified in law.\n\nAI decisions are not always an understandable process and the \"reasons\" may not be known. But the choice to use that AI is fully understandable. Using an AI which displays a bias will already be illegal in the EU.\n\nAll that remains is the specific requirement for openness so it can be known if an AI or Algorithm is racist or sexist.\n\nThe legal method is using a non-discriminatory process. The moment you can show a process is discrimination it becomes illegal.\n\nProving why an individual may or may not get a job is difficult. Proving a bias for thousands of people less so.\n\nThe law currently protects individuals and they are able to legally challenge what they consider to be discriminatory behaviour. A class action against a company that produces or uses a faulty AI is very likely in the future. It's going to be interesting to see what the penalty for that crime will be. Make no mistake, in the EU it's already a crime to use an AI that's racist for anything consequential.\n\nThe law is written with the broad aim of fairness for a reason. It will be applicable more broadly. That leaves a more complicated discovery of evidence and more legal arguments in the middle. But, for a simplistic example, if an AI was shown to only hire white people the company that used the AI for that purpose would be liable today. No legal changes required.",
                                                                    "score": 0,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 10:29:38",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "corinini",
                                                    "body": "Sure you can.  It's what we did to credit card companies.  There was a huge problem with fraud.  Rather than telling them how to fix it we regulated them to make them liable for the results.  Then they came up with their own way to fix it.\n\nIf companies become liable for biased Ai and it is expensive enough they will figure out how to fix it or stop it without regulations telling them how.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 09:21:11",
                                                    "replies": [
                                                        {
                                                            "author": "dmc-going-digital",
                                                            "body": "Yeah but we could tell them what fraud legally is. How are we supposed to set what a biased AI is? When it sees corralations, we don't like? When it says \"Hitler did nothing wrong\"? These two examples alone have gigantic gaps filled with other questions",
                                                            "score": 4,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 09:25:31",
                                                            "replies": [
                                                                {
                                                                    "author": "corinini",
                                                                    "body": "When it applies any correlations that are discriminatory in any way. The bar should be set extremely high, much higher than AI is currently capable of meeting if we want to force a fix/change.",
                                                                    "score": 0,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 09:49:00",
                                                                    "replies": [
                                                                        {
                                                                            "author": "dmc-going-digital",
                                                                            "body": "That's even wager than before. so if it sees that a lot of liars hide their hands, it should be destroyed for discrimination of old people?",
                                                                            "score": 0,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 09:57:36",
                                                                            "replies": [
                                                                                {
                                                                                    "author": "corinini",
                                                                                    "body": "Not sure if there are some typos or accidental words in there or what but I have no idea what you're trying to say.",
                                                                                    "score": 1,
                                                                                    "depth": 9,
                                                                                    "timestamp": "2022-06-28 10:34:20",
                                                                                    "replies": [
                                                                                        {
                                                                                            "author": "dmc-going-digital",
                                                                                            "body": "Wager is the typo, i don't know the english equivalent but its the opposite of exact",
                                                                                            "score": 1,
                                                                                            "depth": 10,
                                                                                            "timestamp": "2022-06-28 10:36:41",
                                                                                            "replies": [
                                                                                                {
                                                                                                    "author": "Thelorian",
                                                                                                    "body": "pretty sure you're looking for \"vague\"; you can blame the French for that spelling.",
                                                                                                    "score": 2,
                                                                                                    "depth": 11,
                                                                                                    "timestamp": "2022-06-28 15:14:45",
                                                                                                    "replies": [
                                                                                                        {
                                                                                                            "author": "dmc-going-digital",
                                                                                                            "body": "Thanks man, genuinly forgot",
                                                                                                            "score": 2,
                                                                                                            "depth": 12,
                                                                                                            "timestamp": "2022-06-28 15:19:44",
                                                                                                            "replies": []
                                                                                                        }
                                                                                                    ]
                                                                                                },
                                                                                                {
                                                                                                    "author": "corinini",
                                                                                                    "body": "Still not really sure what you're trying to say, but if it's some version of \"don't throw the baby out with the bathwater\", in this case I'd say we are just fine not using AI until it can be proven to not be biased.  It's not necessary and we survived just fine without it all these years.  I'd rather not use it at all than use it in ways that discriminate.  And we can regulate it in such a way that the burden of proof is on the AI.",
                                                                                                    "score": 1,
                                                                                                    "depth": 11,
                                                                                                    "timestamp": "2022-06-28 10:50:18",
                                                                                                    "replies": []
                                                                                                }
                                                                                            ]
                                                                                        }
                                                                                    ]
                                                                                }
                                                                            ]
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "author": "Deleted",
                                            "body": "It's not as easy as just telling them to fix it. The problems in the training data are the problems with society itself. You can try to patch problems as they arise, but it will be a bandaid. A hack job.\n\nIf the algorithm uses deposition data to correlate black dialects of speech with harsh sentencing then you can't fix it without removing the deposition data. But the AI needs that to function.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 11:21:34",
                                            "replies": [
                                                {
                                                    "author": "MagicPeacockSpider",
                                                    "body": "It's not easy at all. I never said it was. Neither is making a car that's safe to drive. It's been a hard fight to reduce road deaths to a minimum.\n\nThe problem comes with how an AI equivalent of a road crash can scale and the lack of individual choice in the matter.\n\nArguably we should have demanded safer cars much sooner \n\nLooking at your example it's back to junk in, junk out. Someone should have spent the time and money to audit the data before AI training.\n\nWe're not even at the Ford model T stage of AI. But when we get there we really can't afford to let the crashes just happen like we did with the first mass market cars.\n\nAI is going to be implemented in areas that will save lives pretty soon like medicine, but in every case a human doctor will ultimately be using like a tool and will be personally responsible. If the AI spotted cancer better in men than women or vice versa that doesn't mean a doctor can't use it.\n\nIt does mean you can't use it without knowing that and accounting for that.\n\nAI shouldn't be allowed in areas like recruitment or justice for a very long time, if at all.\n\nWhen AI can do the job better than humans it's arguable it can be used as an additional tool. But if it's just used to do things quickly\n\nIt's even possible we'd accept a slightly racist or sexist AI that's definitely less sexist or racist than our best practices. Judges give out harsher sentences when they're hungry. Humans aren't perfect by any means and AI won't be either.\n\nBut it's been shown that our *best* practices in the EU are pretty good in most cases.\n\nEven then a sexist or racist human is accountable. So will the AI operators. If they aren't then no one will be accountable and regression is inevitable.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 12:31:46",
                                                    "replies": [
                                                        {
                                                            "author": "Deleted",
                                                            "body": "It's not a matter of just auditing the data. The data can be good and still cause objectionable results because humanity is imperfect. We're the error. You can try to curate the data a bit to diminish the evils of mankind, but like I was saying, that's a patch job.\n\nYou're right that we should be keeping AI out of critical areas like justice. I don't think the technology would ever be good enough to trust with something like that.\n\nAs for accountability, it's a bit of a gray area. The trouble with AI is that the program writes itself. The programmer just sets up a framework for that to happen and feeds it training data.\n\nThis may be a stretch, but it's a bit like raising a child. A parent is responsible for raising their child, but isn't accountable for the child's crimes. You can do your best to raise your child right and still end up with bad results. At a certain point you have to accept that AI is always imperfect, and use it responsibly with that in mind.",
                                                            "score": 2,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 14:24:31",
                                                            "replies": [
                                                                {
                                                                    "author": "MagicPeacockSpider",
                                                                    "body": "There is always a human choosing to use an AI or not. There is always a human that's responsible.\n\nThere will be someone collecting money for the use of AI, the owner. They are responsible.\n\nUltimately an AI with a track record at a service can be seen as a safe bet or not. If it's safe enough it's an insurable risk for the AIs owner. If it's not safe enough for them to insure then they won't use it.\n\nThe talk around it being the \"AI's responsibility\" if something goes wrong is no different to it being a car tires fault for failing.\n\nThe sci fi stories of an AI having consciousness are being used to try and have limited liability for corporations while corporations will take the profit from AI. That needs to be shut down.\n\nUltimately the one liable is the one being paid for the service. If an AI did become sentient, we'd have to pay it and it could insure itself I guess.",
                                                                    "score": 1,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 17:11:22",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "Wollff",
                            "body": ">Like you say, we know that the world has sexism and racism. \n\nSexism and racism is not only something the world has. It's legal: Not only is it out there in the world, it is allowed to be out there in the world. Under the umbrella of freedom of opinion and freedom of press, those opinions are allowed to exist, they are tolerated, and not legally sanctioned.\n\nIf you allow them to exist, if you tolerate them, then you also have to tolerate AIs trained on those completely legal and normal datasets. Just like we allow children to be trained on those datasets, should they be born to racist and sexist parents, or browse certain websites.\n\nEveryone is allowed to read this stuff, absorb this stuff, learn this stuff, and mold their behavior according to this stuff... You only want to forbid that for AIs? Why? What makes AIs special?\n\nIf 14 year old Joe from Alabama can legally read it, and learn from it, and mold his future behavior in accord with it, you can't blame anyone to regard it suitable learning material for an AI, can you?\n\n>Knowing all this it's not acceptable to simply allow companies to cut corners. \n\nNo, not only is that acceptable, but consistent. I dislike the hypocritical halfway position: \"Sure, we have to allow sexism and racism to freely roam the world, the web, and all the rest. Everyone can call their child Adolf, and read them Mein Kampf as a bedtime story. That's liberty! But don't you dare feed an AI skewed datasets containing the drivel Adolf writes when he is a grownup, because *that* would have very destructive consequences which are not tolerable...\"\n\n>Any sample of water you collect in the world will contain contamination\n\nUsually there are certain standards which regulate the water quality for open bodies of water. There are standards for what we regard as harmful substances which you are not allowed to release into rivers, and there are standards for how much pollution is acceptable in rivers and lakes.\n\nSo someone if someone dies, after taking a sip of lake water, what is the problem? Is the problem that the lake water is deadly, or is the problem that someone bottled and sold it? Pointing only at the \"bottled and sold\" side of the problem is a one sided view of the issue, especially when you got children swimming that same lake every day.\n\n>It's becoming clear we'll need to regulate AI.\n\nAre you sure it only points toward a need to regulate AI? :D",
                            "score": 0,
                            "depth": 2,
                            "timestamp": "2022-06-28 07:57:43",
                            "replies": [
                                {
                                    "author": "MagicPeacockSpider",
                                    "body": "Resoviors, springs, and rivers have to be tested before they're used as a water source. I think the analogy fits. If water was tested and found to be toxic it would be illegal to give it to someone to drink. If it were not tested a company would still be found liable for not following best practices and testing.\n\nIn the whole of the EU sexism and racism is illegal. There is already discrimination law in place which isn't the case in a lot of the US.\n\nI expect the EU to push for compliance for AI and that will have a global effect. Global companies will be compliant and smaller companies are unlikely to develop in-house systems to compete.\n\nThe language example you brought up earlier is a perfect example. Because of the many languages in the EU things like grammar and punctuation being judged by AI on application forms would likely be made illegal. French people have a right to work in Germany and vice versa. An AI screening out French speakers would bring up.so many red flags.\n\nEspecially in countries like the Netherlands, Finland, Belgium, etc. that have multiple languages and dialects.\n\nWe're likely to see an English language bias in AI to begin with. I'd expect the EU to make sure it isn't used at scale for a lot of things until it's developed out.\n\nJob and work requirements in the EU can specify the need to be competent in a language but not the need to have it as your mother tongue. It's exactly the problem that is difficult to solve, but will have to be solved in any situation an AIs actions can discriminate against people.\n\nThat's the government, workplace, education, public spaces.justice system. AI could be incredibly useful or incredibly harmful. Regulation needs to be in place and I've no doubt the EU will do it.\n\nFrankly I think the US is going to end up being a test bed for racist and sexist AI implementations which eventually get legalised for use in the EU when they've been fixed. \n\nWith all the other causes of racism and sexism in the US and the general lack of government oversight I'm sad to say I think more fuel is about to get poured into that fire.",
                                    "score": 5,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 08:33:03",
                                    "replies": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "Fugglymuffin",
                    "body": ">Problem is, of course, that ~~neural networks~~ **children** can only ever be as good as the training data. The ~~neural network~~ **child** isn't sexist or racist. It has no concept of these things. ~~Neural networks~~ Children merely replicate patterns they see in data they are trained on. If one of those patterns is sexism, the ~~neural network~~ child replicates sexism, even if it has no concept of sexism. Same for racism.\n\nSorry its late for me",
                    "score": 2,
                    "depth": 1,
                    "timestamp": "2022-06-28 03:37:46",
                    "replies": [
                        {
                            "author": "hyldemarv",
                            "body": "Children are way smarter than anything we can build: A three year old can easily one-shot things like \"a chair\", and immediately generalize that knowledge into other things that can be used as \"chair\", and also derive transformations that converts things like \"bucket\" into \"chair\". Or \"black person\" into \"child\" and \"my friend\".\n\nThe real problem is that we build infinitely stupid things, market them as \"Intelligent\", making people use them on important tasks, and even expect that these things will do better than actual intelligence.",
                            "score": 17,
                            "depth": 2,
                            "timestamp": "2022-06-28 04:37:14",
                            "replies": [
                                {
                                    "author": "amicaze",
                                    "body": "Wow a child can do shape recognition very well, guess I'll put a child in my computer to speed up my videogames then...\n\nI mean come on. You can't pretend like you aren't aware about the concepts of *tools* now, can you ? How can we get a requisitory against tools in the 21st century ?\n\nNext you're going to argue your hand is so much better than a hammer, you can grab things, you can count on fingers, you can flip off people, the single issue is you can't drive nails in wood with your hand !",
                                    "score": -5,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 08:31:01",
                                    "replies": [
                                        {
                                            "author": "LaminateCactus2",
                                            "body": "Tools can't think. A three year old child can. \nA child is constantly synthesizing new data to inform and revise its current processes and behaviors. And that is the crux of the problem with ML, it is simple math done thousands and millions of times in order to seem like a complex system. \n\nYou were likely taught the distance formula \"distance = sqrt((x1-x2)^2 + (y1-y2)^2)\", well that powers the vast majority of the predictive algorithms we use. Your data, say likes on spotify are used to create the average of the song you like and then it picks songs that are the shortest distance from your average song.\n\nNeural networks similarly is just matrix multiplication and it can appear to model complex behaviors if we add enough layers of transformation, but it never actually thinks it only compares how close a given input is to the average of the TRUE or yes cases in the training data.\n\n\nNote: clustering and the ability to also assign negative weights to attributes does complicate the math slightly but it's still simple math repeated ad nauseum which is good because simple math is all computers can do",
                                            "score": 3,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 10:30:34",
                                            "replies": [
                                                {
                                                    "author": "cloake",
                                                    "body": "I imagine human learning is similar in that lowest entropy choices are made (or lowest distance). The biggest difference is that each module is extremely biased to serve a specific function. 3 main attentional loops (what you must do, what you like to do, what you ought to do), memory module, limbic for crude emotion, different cortices for each information modality class (olfactory, visual, aduitory, tactile, etc), motor planning, vocalization apparatus with linguistic cortex, wakefulness, calculation, agency (posterior cingulate), 3D spatial navigation, theory of mind, empathy. \n\nI'm sure there's more, but my point is that the basic process of learning is still fire together, wire together. Just general intelligence already has a very biased prelaid framework.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-29 07:48:17",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "Deleted",
                    "body": "I think a much more pertinent question is, what if the algorithm is right and is making connections that seem sexist to us but are actually just correct?\n\nWhat if, for whatever reason, white men make better leaders? Black women better software developers? Should we kneejerk and \u2018correct\u2019 (actually introduce an aberrant bias) the algorithm or do research and look a little bit deeper.",
                    "score": -2,
                    "depth": 1,
                    "timestamp": "2022-06-28 06:49:38",
                    "replies": [
                        {
                            "author": "PrisonInsideAMirror",
                            "body": "> What if, for whatever reason, white men make better leaders?\n\n1. Define better? In which categories? How are you deciding them? Who is measuring them? How many sources do we have for the data? What is the overall range of results?\n\n2. Give me a single reason why skin color is more important than childhood nutrition? Because I can guarantee you that \"more likely\" isn't \"Definitive proof that\". \n\n3. Give me a single reason why gender is more important than the adverse conditions and support networks that surrounded a leader?\n\nYour question is based on ignoring as much data as humanly possible in order to give us a simple answer anyone can understand. \n\nThat's not something we should be encouraging. Simple answers are often very deceptive answers, and they're easier to spread.",
                            "score": 4,
                            "depth": 2,
                            "timestamp": "2022-06-28 07:12:40",
                            "replies": [
                                {
                                    "author": "Deleted",
                                    "body": "I love how you are pretending I am suggesting we do not take a scientific approach.\n\nIn your own words:\n\n>\tYour question is based on ignoring as much data as humanly possible in order to give us a simple answer anyone can understand.\n\nI am saying we exactly take the scientific approach and don\u2019t let feelings lead us because we don\u2019t like where the result of said scientific approach *might* lead us.",
                                    "score": 6,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 07:18:54",
                                    "replies": [
                                        {
                                            "author": "Tartalacame",
                                            "body": "We have many studies that show that skin color or religion aren't a factor for those types of models when other variables are included. Which means if the NN is using that as a predictor, it uses it as a proxy for another missing variable, which ultimately is problematic since it means it makes decisions based on factors we know are irrelevant.\n\nSince there will always be missing variables in such models, the correct approach is to exclude the variables we *don't* want to be part of the model (such as gender, religion, ethnicity...).  \nThere **are** models where these variables are important (e.g. medical ones), but we also have supporting scientific evidence that we should account for these variables in the first place.",
                                            "score": 5,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 07:56:11",
                                            "replies": [
                                                {
                                                    "author": "Anderopolis",
                                                    "body": "If I ran a ML model over patients with sickle cell anemia it would show a massive racial bias and it would be correct to do so. Same with Skin cancer. \n\nHow can we know for sure that similar things aren't going to exist in other fields. The problem with algorithms is they only care about the Data they have available. Most of us Humans have wisely decided that people are equal before the law, but  data might still show differences.",
                                                    "score": 3,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 10:43:05",
                                                    "replies": [
                                                        {
                                                            "author": "Tartalacame",
                                                            "body": "We can test these variables independently and see if any relevant links exist.  \nIf these correlations do not exist when duly tested, then if they're picked up by the model it means these variables are actually proxy for other relevant variables that are currently not accounted for. It's these new variables that we should look into and add to the model instead.\n\nAnd even if there exists an actual difference between, let's say, skin color and word typed by minute when accounting for everything else, we, as humans, for ethical reasons, can (and should) still ignore those variables in our model in some contexts, such as hiring.",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 14:25:56",
                                                            "replies": []
                                                        }
                                                    ]
                                                },
                                                {
                                                    "author": "Deleted",
                                                    "body": "And I have to think of when they ran a ML model on a multi-FPGA boards, training it to do specific simple things. It did very weird things like isolate one FPGA and only run something on it once, on another board the model would run the FPGA\u2019s in sequence, etc.\n\nThe researchers were flummoxed at first (since they programmed a standard method to do each of the simple tasks that ran the same on all boards). When they researched further they found that the FPGAs and electrical supplies had slight differentiations in voltage, clock, etc that the ML models were exploiting.\n\nWhat if those \u2018irrelevant\u2019 variables are not so irrelevant, we just think they are because ultimately our models are actually more crude than the ML one?\n\nI am not saying this is absolute truth, but automatically assuming the ML model is less accurate and \u2018dumber\u2019 than us is pretty stupid.",
                                                    "score": -5,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 08:04:18",
                                                    "replies": [
                                                        {
                                                            "author": "Tartalacame",
                                                            "body": "This is very different from your previous example. Random variables shouldn't be thrown into a model for precision. There should be a reason to be added. And if it is done for exploratory reasons, if race/sex/religion... comes back as significant in a place they shouldn't be (e.g. hiring), then it simply means there are additional counfonding variables that are missing. In any case, for things like HR-related goals, a production-ready model shouldn't include variables such as ethnicity/gender/religion/sexual orientation... Even if doing that makes the model \"less accurate\".",
                                                            "score": 3,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 09:09:18",
                                                            "replies": [
                                                                {
                                                                    "author": "Deleted",
                                                                    "body": ">\t In any case, for things like HR-related goals, a production-ready model shouldn\u2019t include variables such as ethnicity/gender/religion/sexual orientation\u2026 Even if doing that makes the model \u201cless accurate\u201d.\n\nThis flies in the face of everything scientific, and it was exactly the thing I was attacking in my original comment. Making a model less accurate because the most accurate model hurts according to our human sensibilities.",
                                                                    "score": 1,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 09:24:51",
                                                                    "replies": [
                                                                        {
                                                                            "author": "PrisonInsideAMirror",
                                                                            "body": "You're actively ignoring the quotes around \"less accurate.\" \n\nAnd ignoring a lot of other things, besides.",
                                                                            "score": 1,
                                                                            "depth": 8,
                                                                            "timestamp": "2022-06-28 13:01:24",
                                                                            "replies": []
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "TheSinningRobot",
                    "body": "It seems very strange to me that in examples like that, things like racial data is even included in the data that it is fed.",
                    "score": 0,
                    "depth": 1,
                    "timestamp": "2022-06-28 06:50:34",
                    "replies": [
                        {
                            "author": "chrischi3",
                            "body": "It's probably not even racial data in and off itself. Things like the defendants name, address, etc. could be enough of a giveaway, even if the network has no idea what that info even means. Think about it, if you hear about a person with a typically black name from a majority black neighbourhood, wouldn't you assume that person is black? If we can do that, so can a neural network.",
                            "score": 10,
                            "depth": 2,
                            "timestamp": "2022-06-28 07:31:22",
                            "replies": [
                                {
                                    "author": "TheSinningRobot",
                                    "body": "Well yes of course, but it seems to me like that kind of information, which is essentially irrelevant to what the network is trying to solve for, should be excluded in the data set being shown.",
                                    "score": 2,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 10:17:37",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "Huttj509",
                            "body": "A couple examples.\n\nHiring AI:  Gender info was not included.  However the AI picked up on things like where the degree was from, or what classes were taken, that correlate with gender, and used THOSE to exclude people.\n\nMedical diagnosis AI:  There was an article recently where they tried to strip out racial identifying data, since part of the goal was to avoid the racial bias that shows up in medicine, and the AI still misdiagnosed cancer much more often in black people.  Further studies learned the AI could identify race by chest x-rays, which was not a known source of racial difference.\n\nAI is really good at finding patterns.  REALLY good at it.",
                            "score": 6,
                            "depth": 2,
                            "timestamp": "2022-06-28 07:57:33",
                            "replies": [
                                {
                                    "author": "Adorable_Octopus",
                                    "body": "I find it kind of strange that people seem to think that researchers are just feeding racist data to these AIs without trying to resolve the bias in that data. I'm sure some, perhaps many, do, but the problem is much deeper and harder to overcome than simply stripping out the obvious stuff.\n\nThe medical diagnostic AI is a perfect example of that-- it's clearly picking up something, but we don't know what. It's not an obvious pattern to the researchers.",
                                    "score": 6,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:01:23",
                                    "replies": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "urbanek2525",
                    "body": "In other words, don't be surprised when your mirror accurately reflects what is there.\n\nLike when people say, \"Police are racist.\" The police are racist **IF** the community is racist because the police reflect the values of the community they serve.\n\nAI is the same. It is very good at revealing the patterns embedded in the data.",
                    "score": 0,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:04:13",
                    "replies": []
                },
                {
                    "author": "Dyalibya",
                    "body": "The nural network shouldn't have the ethnicity data, simple",
                    "score": 0,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:35:39",
                    "replies": []
                },
                {
                    "author": "Deleted",
                    "body": "[deleted]",
                    "score": 0,
                    "depth": 1,
                    "timestamp": "2022-07-01 08:26:44",
                    "replies": [
                        {
                            "author": "RunItAndSee2021",
                            "body": "\u201con the hole\u2026\u2026\u2026\u2026\u2026\u2026(w? where_d \u2018w\u2019 come from?)\u201d",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-07-01 08:28:54",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "Deleted",
                    "body": "[removed]",
                    "score": -10,
                    "depth": 1,
                    "timestamp": "2022-06-28 03:42:58",
                    "replies": [
                        {
                            "author": "chrischi3",
                            "body": "I know right? I hate when i've already made up my mind on a matter and then someone comes along and confuses me with facts.",
                            "score": -8,
                            "depth": 2,
                            "timestamp": "2022-06-28 03:44:13",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "Haunting_Meeting_935",
                    "body": "Clip is trained on Google images. What is surprising on Google results having this type of bias which is so prevalent across the world?",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 02:26:59",
                    "replies": []
                },
                {
                    "author": "radome9",
                    "body": "> Therefore, the neural network, despite lacking a concept of what racism is, ended up sentencing certain ethnicities more and harder in test cases where it was presented with otherwise identical cases.\n\nWas race one of the data points about the defendant fed into the network?   \n\nIf so, what a strange thing to feed into an NN. If not, how did the network know the race of the defendant?",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 08:08:23",
                    "replies": [
                        {
                            "author": "chrischi3",
                            "body": "I'd guess you wouldn't even have to feed the ethnicity into the network. If the neural network had the name and address of the defendant, it could easily make connections based off of that i suppose, even without info on the defendants skin color being present. There's names that are more common among black people, and they tend to live in mostly black neighbourhoods. Even without knowing this, a neural network could make this connection based off of names. (Also, idk what exactly they did feed this neural network in terms of data)",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 08:18:22",
                            "replies": [
                                {
                                    "author": "radome9",
                                    "body": "Why would you feed the name and adress into the network? Are those relevant when making sentencing decisions?",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 08:23:59",
                                    "replies": [
                                        {
                                            "author": "chrischi3",
                                            "body": "I don't know. It's quite possible that someone in the DoJ learned about neural networks, and, without any regard for processing time, just tossed as much info into the system as they could. Realistically, they didn't give the entire file to the system to chew through, merely whatever they deemed relevant detail (which was probably some employee with little to no experience with real courts) writing up a list, and then had someone else turn these details into a list from the countless files they had.   \n\n\nOr, more likely, they had an algorithm do it, and since things like the defendants name and address are likely listed in their own field, that was a detail that was easy for said algorithm to filter out of the document and paste it into the training data file. (Doing this manually would probably take so long, by the time you're done, we have actual AI)",
                                            "score": 2,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 08:52:06",
                                            "replies": [
                                                {
                                                    "author": "radome9",
                                                    "body": "Do you have a source where I can read more about this? Because it is an unlikely story.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 08:56:37",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "Willdudes",
                    "body": "You can use algorithms to detect bias in data.  The other option is a human but you have no idea what bias you will get.  Bias and fairness should be run on all decisions by humans and AI, but I doubt that happens.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 08:19:41",
                    "replies": []
                },
                {
                    "author": "Intrepid_Stretch9031",
                    "body": "OP goes on with the assumption that you know this too and inherently focus on result",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 08:35:56",
                    "replies": []
                },
                {
                    "author": "chancretherapper",
                    "body": "That\u2019s literally what the problem is and what the article is describing. Nobody is saying that the machines themselves are independently racist or sexist for no reason.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 08:46:21",
                    "replies": []
                },
                {
                    "author": "theKinkajou",
                    "body": "Could you reverse engineer something like this to easily find who and how discrimination is happening? Essentially a way of quantifying institutional racism/sexism?",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:15:45",
                    "replies": [
                        {
                            "author": "chrischi3",
                            "body": "It would be a lot of effort, if its even possible at all, but wether we should is another question.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 09:36:29",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "RamenJunkie",
                    "body": "That was kind of my wonder.\n\nWe train these things on human input.  Maybe its just time to accept that humans are way more racist and sexist than we want to accept.  Solve that root problem and maybe it solves the AI training problem",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:35:55",
                    "replies": []
                },
                {
                    "author": "Deleted",
                    "body": ">Problem is, of course, that neural networks can only ever be as good as the training data..\n\n\n\nHow did Google make AlphaZero who is obviously better than any training data. Same for AlphaGo.\n\nBoth AI's became the best entities of that game to exist. So obviously AI can learn beyond their training data, in fact that seems to be something that happens quite often with machine learning.\n\nIdk where you got that idea from",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:40:29",
                    "replies": []
                },
                {
                    "author": "Neural_Flosser",
                    "body": "This is why AI as a general term needs to stop being applied to ML neural networks, which are simple complicated systems that operate on aggregated data as you mention. They can be incredibly powerful tools, but until we create artificial general intelligence that can self reflect, the data used to train these models is going to have to be continually scrutinized and curated in order to remove specific bias, which, if done by humans, will still have some sort of bias",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:43:43",
                    "replies": []
                },
                {
                    "author": "Wh00ster",
                    "body": "Could you not the same of people?",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:54:27",
                    "replies": []
                },
                {
                    "author": "Uruz2012gotdeleted",
                    "body": "This could just as easily be applied to people too. Racism isn't always a conscious choice to treat people worse.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:13:19",
                    "replies": []
                },
                {
                    "author": "mjc7373",
                    "body": "I think this demonstrates how systemic racism works. Even if the individual actor isn\u2019t intending to discriminate against anyone simply following social norms will produce discriminatory outcomes.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:28:21",
                    "replies": []
                },
                {
                    "author": "pm_favorite_boobs",
                    "body": "\n>Neural networks merely replicate patterns they see in data they are trained on. If one of those patterns is sexism, the neural network replicates sexism, even if it has no concept of sexism. Same for racism.   \n\nSame as people, to be honest. Most sexists and racists are not aware that they are. It's a matter of critical thinking among humans.\n\nCould neural networks be taught to identify these biases from the information and analysis that it is working on?",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:30:04",
                    "replies": []
                },
                {
                    "author": "charlesgegethor",
                    "body": "If anything it really highlights just how bigoted and prejudiced our systems really are.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:46:21",
                    "replies": []
                },
                {
                    "author": "csgetaway",
                    "body": "This is the key. If your AI is making unfair decisions it\u2019s not a fault of the AI.  Biased AI highlights problems that exist in humanity; not AI.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:50:33",
                    "replies": []
                },
                {
                    "author": "GombaPorkolt",
                    "body": "Just like children. No person is born racist. We have a blank neural network to work with. But if the overwhelming majority and/or most crucial of inputs (i.e. those of our parents') are racist, sexist, or of any other, even benign, ideology, we will  naturally, gravitate towards that/those ideologies/racism/sexism, because that's what we hear and see the most. We need to change/regulate input data, as you've said, rather than the network.\n\nJust like you would start by educating people not to be sexist/racist first, rather than try to literally change the neurons/DNA of a fetus. There is nothing wrong with the inherently blank sheet. The issue is always with the input.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:51:53",
                    "replies": []
                },
                {
                    "author": "Breeze1620",
                    "body": "It can also be that AI lacks feelings and therefore sympathy. It could be that it is acting purely objectively, but to us that can be sexist, racist or in other ways just plain cruel. This has for example been seen with AI used in employment or used to determine if someone is to keep their job or not based off of statistics.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 11:01:39",
                    "replies": []
                },
                {
                    "author": "DM_ME_YOUR_BALL_GAG",
                    "body": "Ok this might be a dumb question, but specific to sentencing, why not only train it on the majority (probably not the right word for it but I just woke up), then have that learning applied across the board?\n\nI.e. in the US, train it on cis white men (assuming) then apply it to minorities, woman, whomever...",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 11:04:32",
                    "replies": []
                },
                {
                    "author": "relaci",
                    "body": "No child is born biased.  That's taught by the information they're given.  \n\nIf only Mr. Rogers were still with us to help teach AI to be less biased, and more children to write to him to ask that he say aloud that he is feeding the fish so that one blind girl wouldn't be worried about the fish anymore.\n\nActually, here's a thought, let's get very young children to help identify the bias in AI!  Make it an age appropriate video game and crowd source their natural lack of bias!  Children are far more socially intelligent than we give them credit for.  At least until they get to what I like to call the \"bitey fives\" age.  I'm still a little wary of kids in that age group.\n\nSomewhat funny anecdote time.  Ya know how young young kids are usually kinda shy around \"stranger\" adults?  Well, there was this big tornado that hit.  All the power was out, and the neighborhood was just out wandering around and assessing the damage.  I noticed two big trees that were definitely gonna fall on this house at the next big breeze.  After I helped the old person manually open the garage door to at least save their car before the trees totalled the garage, I rejoined the gawkers.  Small child who has been clinging to her parents the whole time observes that her parents are starting to freak out about those trees, like everyone else.  I'm just standing there videoing for funsies.  All of a sudden I have a small child clinging to MY leg!  Her parents are freaking out, my parents are freaking out, everyone's freaking out.  I'm trying to get a good angle for the video.  Smart little one ran to the only adult that seemed perfectly fine with what's going on.  Trees fell, I got a great video of it, and then I asked whose kid it was that was attached to my leg.  I do wish I'd have gotten a bit of video of everyone else freaking out though.  That was hilarious.  \n\nSide note: kid got shy and ran back to her parents after everyone had calmed down a bit.  Kids are weird.  Apparently I was only ok to interact with while I was confident I was standing in a safe spot.  After that, I was a scary stranger again.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 11:19:33",
                    "replies": []
                },
                {
                    "author": "_____hi_____",
                    "body": "> This is also why computer aided sentencing failed in the early stages. If you feed a neural network with real data, any biases present in the data has will be inherited by the neural network. Therefore, the neural network, despite lacking a concept of what racism is, ended up sentencing certain ethnicities more and harder in test cases where it was presented with otherwise identical cases.\n\nSeems like a simple fix to just omit race as a variable in the criminals punishment no?",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 11:33:41",
                    "replies": [
                        {
                            "author": "chrischi3",
                            "body": "Question is, would the neural network still be able to tell? Even if you remove race, there's a possibility that the network would pick up on certain patterns that are common in some ethnicities but not so much in others, which would then allow it to determine race anyway, even if not with 100% accuracy.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 11:45:39",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "Wizzdom",
                    "body": "Exactly. I remember reading about how police wanted to use statistics and AI to predict where crime would most likely be committed so they could more effectively place patrols in a \"scientific\" way. It turned out to be racist because the data was biased by racist policing tactics. If the data is not completely free of bias, then the result is not objective.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 11:41:07",
                    "replies": []
                },
                {
                    "author": "SmikeSandler",
                    "body": "the funny thing is that i asked gtp3 basically if it became sexist/racist if its training dats would include social media. it agreed",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 11:56:30",
                    "replies": []
                },
                {
                    "author": "TheIowan",
                    "body": "Tangentially, I can't help but imagine a version where an AI is so racist and sexist that it's comedic. Like a robot version of Kramer that truly wants to be a good entity but keeps saying ridiculous things and has to \"train\" itself not to.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 12:15:46",
                    "replies": []
                },
                {
                    "author": "femaletrouble",
                    "body": "Garbage in, garbage out.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 12:37:13",
                    "replies": []
                },
                {
                    "author": "FeelsGoodMan2",
                    "body": "AI is only going to reach the purity ideal if it can completely tether itself from the humans creating the programming on it, but I just don't quite see how that ever happens. It'd have to somehow train and model itself off of human behavior without actually adopting any of the human behavior. Someone much smarter than me can probably create a theoretical solution, but honestly I don't really see how you get around that issue.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 12:40:36",
                    "replies": []
                },
                {
                    "author": "T-MinusGiraffe",
                    "body": "That makes sense, except for why did we give the robots any ethnic information at all? Wouldn't just not telling them make the otherwise identical cases actually identical?",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 13:38:25",
                    "replies": [
                        {
                            "author": "chrischi3",
                            "body": "Well, i suppose a neural network might not even need any racial info to figure someones race out. Think about how neural networks are better at diagnosing cancer than any humans are. They see patterns in data that go past our ability to perceive.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 14:37:26",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "10g_or_bust",
                    "body": "Honestly, it's *worse* than that. You don't need an \"AI\" to be \"racist\" to make data that fits with racist ideas or goals. Lending algorithms have (repeatedly) reimplemented redlining, not explicitly and not at the behest of the people making them. Why? Because the goal didn't (and arguably couldn't) include things like promoting equity, just profit. So you get pattern matching on things like \"which neighborhood someone lives in correlates with likelihood to repay\", which even when the pattern is arguably \"correct\" doesn't make it something we should action on, or take as a causal relationship (see, \"cellphones cause cancer\" nonsense).",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 14:14:09",
                    "replies": []
                },
                {
                    "author": "UnfinishedProjects",
                    "body": "I know this probably isn't the place, but that just made me imagine robots sharing memes with complicated problems to solve before being able to see the meme, like a human proof meme for sentient robots only.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 14:20:56",
                    "replies": []
                },
                {
                    "author": "dekeche",
                    "body": "Eventually, we can't make a neural net A.I. that does a task better than people currently, because we still have people creating the data to train that A.I. The reason we are using these systems is because of their one advantage: the volume of data that can be processed.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 15:02:02",
                    "replies": []
                },
                {
                    "author": "EightHoursADay",
                    "body": "But why would they include race as a metric in the data anyway. If I were going to make ai for sentencing wouldn't I remove that data point before feeding it in?",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 15:19:19",
                    "replies": []
                },
                {
                    "author": "OfLittleToNoValue",
                    "body": "It'd probably be a good idea to feed these things data looking for conflicts to identify bad research. I've seen tons of garbage studies that get lots of traction.\n\nWorse, I've seen good studies getting the correct answer but asking the wrong question.\n\nEvery discipline is trained to see itself through it's own lense. This is a codified echo chamber.\n\nWhen you look at nutrition from a physiological and evolutionary context, the studies done are based on axiomatic suppositions the institution can't see to question because dogma lacks self awareness.\n\nFor example. Studies show fiber lowers risk of heart disease. However, it does that by slowing sugar absorption. Eating less sugar lowers heat disease and doesn't require insoluble fiber that irritates and inflames our intestines.\n\nThe predominant source of sugar before agriculture was regionally and seasonally available fruits ripening in fall. The sugar makes you hungrier so you gorge to put in weight for winter.\n\nEating sugar all the time can't be fixed by more fiber because that leads to more constipation, boating, and inflammation.\n\nSo, fiber isn't *good* it just minimizes the harm of sugar we're eating in qualities that fry our body like ethanol in a collector car.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 15:47:24",
                    "replies": []
                },
                {
                    "author": "Ineedavodka2019",
                    "body": "It kind of confirms systemic sexism and racism, doesn\u2019t it?",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 15:53:40",
                    "replies": []
                },
                {
                    "author": "VoraciousTrees",
                    "body": "We point the machine at people and say \"learn from them on what to do\"... and then we are ashamed when the machine acts like the people who taught it...",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 16:20:50",
                    "replies": []
                },
                {
                    "author": "Geryth04",
                    "body": "Exactly this. Take Amazon's attempt at being race and gender blind in picking out good resumes. That program was very good at highlighting resumes from white men.\n\nWhy? Because white men have opportunities and circumstances that give them better resumes.\n\nWomen are more likely to have gaps in work history to take care of family. Minorities or poorer candidates are less likely to come from prestigious colleges. They might be working instead of doing extracurriculars. They might be less likely to afford services that help them create better resumes.\n\nBut this is how systemic racism and sexism works. It's not the ideals of a particular person or organization that makes them want white men. It's just that white men have better opportunities to get good looking resumes. AI can not help this problem at that point in the hiring process. Racism/sexism is in the input, so it's in the ouput.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 16:35:50",
                    "replies": []
                },
                {
                    "author": "crusoe",
                    "body": "Why would race or name or gender or age ever be a part of training data? Just why?",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 16:44:18",
                    "replies": []
                },
                {
                    "author": "Hicklethumb",
                    "body": "Machine learning needs some machine teaching",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 17:18:57",
                    "replies": []
                },
                {
                    "author": "Whynottt488",
                    "body": "This is why Googles ImagenAI is not available to the public. It\u2019s results are absolutely incredible (check out r/imagenAI), but utilizing the LAION-400M dataset continues to provide racially motivated results.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 20:34:45",
                    "replies": []
                },
                {
                    "author": "Whynottt488",
                    "body": "Google\u2019s ImagenAI is not available to the public for partly the same reason. They utilized the LAION-400M dataset. \n\nTheir reasoning is a good read: https://www.reddit.com/r/ImagenAI/comments/uxch3j/reasons_its_not_public/?utm_source=share&utm_medium=ios_app&utm_name=iossmf",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 20:47:10",
                    "replies": []
                },
                {
                    "author": "Seeen123",
                    "body": "Same thing happened when (google? I think it was) trained an ai off of Twitter and Facebook and it became an extremist quickly.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 21:03:04",
                    "replies": []
                },
                {
                    "author": "CSC160401",
                    "body": "Maybe we could at least use these AIs to identify biases in data?",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 21:12:04",
                    "replies": []
                },
                {
                    "author": "aaalderton",
                    "body": "Very interesting",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 21:31:16",
                    "replies": []
                },
                {
                    "author": "itsallrighthere",
                    "body": "I understand the concern and it certainly is possible to do poorly considered ML design.  But I think the argument about this is suspect.\n\nIf you are concerned about applicants propensity to default on a loan and look for factors that predict loan approvals pre ML, yes you could perpetuate previous biases.  But that would be an obviously flawed approach.\n\nOne would instead look at actual defaults.  And to more explicitly avoid bias I wouldn't consider race as a factor.   If factors such as income, employment history, length of residence and debt to income ratio happen to correlate  with some class identity is that racism?  It may be uncomfortable and it may show the impact of previous racism.  But for someone assessing risk of default on a loan it would be on target for that decision.  \n\nNot saying there are no reasons not to address the impact of previous unfair practices but distorting a risk analysis isn't the place to do it.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-29 08:40:07",
                    "replies": []
                }
            ]
        },
        {
            "author": "headshotdoublekill",
            "body": "Garbage in, garbage out.",
            "score": 452,
            "depth": 0,
            "timestamp": "2022-06-28 04:28:50",
            "replies": [
                {
                    "author": "El_Rista1993",
                    "body": "Like to see what garbage would come out if you trained it on reddit.",
                    "score": 50,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:03:47",
                    "replies": [
                        {
                            "author": "SatanicSurfer",
                            "body": "You can, r/SubSimulatorGPT2",
                            "score": 61,
                            "depth": 2,
                            "timestamp": "2022-06-28 09:29:42",
                            "replies": [
                                {
                                    "author": "Strange_An0maly",
                                    "body": "That sub is interesting to say the least",
                                    "score": 15,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:35:45",
                                    "replies": [
                                        {
                                            "author": "Deleted",
                                            "body": "I was going through and completely forgot that I wasn't looking at the comments of other people. I thought \"This is hard to read; this person is an idiot\". \n\nI'm the idiot.",
                                            "score": 26,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 11:03:33",
                                            "replies": [
                                                {
                                                    "author": "SatanicSurfer",
                                                    "body": "Happens to me all the time hahaha. It's not trivial to distinguish between a stupid redditor and a bot.",
                                                    "score": 2,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 13:59:24",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "author": "HamWatcher",
                                    "body": "The first post there for me - I'm a socialist and I don't even know what socialism is. That is a lot of subs nowadays.",
                                    "score": 8,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 15:07:08",
                                    "replies": [
                                        {
                                            "author": "Nacho98",
                                            "body": "That's funny but not accurate, plenty of people are increasingly educating themselves about socialism and deprogramming now that the US is on the decline and doesn't represent its people.",
                                            "score": -3,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 18:32:38",
                                            "replies": [
                                                {
                                                    "author": "Phnrcm",
                                                    "body": "Nah, it not nothing new. In the 70s it was Yoko protesting in the hotel bed made and cleaned by the hotel workers. Today it is the so called socialists who don't want to work anything other than desk job.",
                                                    "score": 3,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-29 01:37:08",
                                                    "replies": [
                                                        {
                                                            "author": "Nacho98",
                                                            "body": ">Today it is the so called socialists who don't want to work anything other than desk job.\n\nYou're lecturing a blue collar union worker who works with his hands daily, so I'm gonna respectfully disagree. Nice strawman caricature tho.",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-29 01:39:19",
                                                            "replies": [
                                                                {
                                                                    "author": "Phnrcm",
                                                                    "body": "Last time i checked those people are quite real\n\nhttps://i.imgur.com/VjKJNTZ.png",
                                                                    "score": 2,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-29 02:10:15",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "Bkwrzdub",
                            "body": "Microsoft released its ai bot tay to twitter... \n\nRemember that?\n\nAnd then it did it AGAIN with Zo....\n\nRemember That too?",
                            "score": 35,
                            "depth": 2,
                            "timestamp": "2022-06-28 09:14:31",
                            "replies": [
                                {
                                    "author": "Deleted",
                                    "body": "They put Tay up and it became racist. They took it down, wiped, then put it up again. Guess what? Racist again.",
                                    "score": 2,
                                    "depth": 3,
                                    "timestamp": "2022-07-01 18:44:06",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "Error_Unaccepted",
                            "body": "It would probably be a dog walking version of Nick Avocado + Chris Chan.",
                            "score": 3,
                            "depth": 2,
                            "timestamp": "2022-06-28 09:11:15",
                            "replies": []
                        },
                        {
                            "author": "Deleted",
                            "body": "Woke to the point of sounding racist",
                            "score": 0,
                            "depth": 2,
                            "timestamp": "2022-06-28 11:00:34",
                            "replies": []
                        },
                        {
                            "author": "Deleted",
                            "body": "The AI would be greasy dog walker mod that collects funko pops.",
                            "score": 0,
                            "depth": 2,
                            "timestamp": "2022-06-28 10:51:11",
                            "replies": []
                        },
                        {
                            "author": "hurpington",
                            "body": "Now that would be interesting",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 12:39:30",
                            "replies": []
                        },
                        {
                            "author": "space_physics",
                            "body": "We all have Reddit inputs in our respective \u201cneural networks\u201d.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 13:49:21",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "thisiskyle77",
                    "body": "I believe that is the entire point of those JHU researchers claim. A lot of publicly available and accepted datasets are \u201cgarbage\u201d and biased and the industry doesn\u2019t bother.",
                    "score": 4,
                    "depth": 1,
                    "timestamp": "2022-06-28 14:02:50",
                    "replies": []
                },
                {
                    "author": "watvoornaam",
                    "body": "An algorithm deciding how much someone can mortgage will decide a person that is a woman can get less than a man. It knows statistically women get paid less. It isn't discrimination on gender, it is discriminating based on factual data.",
                    "score": 11,
                    "depth": 1,
                    "timestamp": "2022-06-28 08:51:50",
                    "replies": [
                        {
                            "author": "Deleted",
                            "body": "An AI or algorithm determining how much someone can mortgage based on gender and the gender pay gap over an individual's income is terrible. At that point it's not garbage in garbage out any more, but garbage selection by the AI or algorithm.",
                            "score": 8,
                            "depth": 2,
                            "timestamp": "2022-06-28 08:59:50",
                            "replies": [
                                {
                                    "author": "redburn22",
                                    "body": "I think the original commenter used slightly inflammatory language but here is the point that I think they are trying to make (or at least the one that I\u2019m going to make haha):\n\nIf you are designing a model to predict who can pay their mortgage, then you will give people a lower score if they earn less. That is the goal. If we live in a society that has a gender gap, then it is going to reflect that. Even if you had the model specifically not look at gender, if women make less, then an accurate model will give them lower scores.\n\nShould the model be altered to be sure to give women equal scores? Even if it makes it less accurate? Even if that means women are more likely to be issued mortgages that they ultimately can\u2019t afford and default on?\n\nTough question. Of course the gender gap should be fixed. But in the meantime, if you are trying to make accurate predictions about the world, you are going to end up also noticing and predicting flawed elements of the world.\n\nThat said, there could do situations where this creates an objectively negative outcome. Like if part of the evaluation is based on human opinion. And let\u2019s say those evaluations are done by sexist people who assume women make less than they do. Not reflecting the gender gap, but rather underestimating women\u2019s pay above and beyond the gender gap. In this situation the model would be under predicting women\u2019s income and denying them mortgages that they can afford, due to bias. That would be an example of something that is both bad for the accuracy of the model and morally bad.\n\nBut when the model is accurate and it is merely a reflecting our world I think it\u2019s hard to say that that\u2019s a problem with the model. Rather it\u2019s a problem with our society. To be fair it\u2019s not super clear cut",
                                    "score": 13,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:12:30",
                                    "replies": [
                                        {
                                            "author": "watvoornaam",
                                            "body": "Thanks for elaborating my crude comment.",
                                            "score": 3,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 11:03:25",
                                            "replies": [
                                                {
                                                    "author": "redburn22",
                                                    "body": "In retrospect sorry for being a bit harsh. When I said that your post was a bit aggressively worded, I was referring to where you said it\u2019s fine to discriminate based on factual info. I thought I knew what you meant - that there is a difference between a biased model vs an accurate model which correctly predicts societal bias (or potentially even differences that are not caused by bias, but rather by divergent preferences). I was just trying to get at the idea that I could see how someone could (and would, given the topic) read that differently\n\nI apologize if it came across like I was assuming negative intent on your part",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-29 06:27:29",
                                                    "replies": [
                                                        {
                                                            "author": "watvoornaam",
                                                            "body": "No offence taken, I just blamed myself for not wording it better. I certainly don't think discrimination is fine, just that we should look at the root cause of IAs discriminating most likely being learned behaviour from society being discriminatory. Sorry for my strange wording, English is not my native language.",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-29 06:47:41",
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "author": "itsunel",
                                            "body": "Then the problem becomes the usefulness whatever of the AI in the situation whatsoever. Where is the benefit if passing on the systemic bias decision making to AI?",
                                            "score": 0,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 09:53:01",
                                            "replies": [
                                                {
                                                    "author": "redburn22",
                                                    "body": "Right but I\u2019d say that in these cases usually there is a benefit. Accuracy, cost, etc. But even aside from that I do suspect that bias is more easily fixed in models than in people. You can change a model. Much harder to change a personality or belief. Or even to convince someone they hold an offensive belief.",
                                                    "score": 2,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 10:03:02",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "author": "Deleted",
                                            "body": "The issue with the comment I'm replying to is that the model presented \"knows statistically women get paid less\". Women get paid less isn't the problem here, but rather the implication of decisions being made based on demographic averages. Such a model will make increasingly significant errors the further an individual is from their presumed average, and can have completely erroneous results in cases where average individuals are rare or don't even exist.",
                                            "score": -2,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 09:34:39",
                                            "replies": [
                                                {
                                                    "author": "redburn22",
                                                    "body": "Right ok fair enough. I definitely agree that if the model is predicting inaccurately based on stereotypes then that is a problem and causes harm.\n\nI assumed we were talking about it making accurate predictions that reflect societal bias, because, of course, the gender pay gap is real, not just a stereotype. But I see your point and I get where you\u2019re coming from given the tenor of the initial comment you were replying to and especially given that particular phrase you quoted",
                                                    "score": 5,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 09:51:28",
                                                    "replies": []
                                                },
                                                {
                                                    "author": "Uruz2012gotdeleted",
                                                    "body": "This is how credit systems already work. Also how decisions get made in the mortgage and insurance industry already. Except there's an algorithm that spits out a number for the humans to match up to an \"objective\" standard that they will follow in every case. \n\nOr did you think the loan officer will actually review your personal files for information about you, contact your creditors directly, speak with some personal references before making a decision themselves based on how they feel about you as a person?",
                                                    "score": 4,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 10:23:59",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "author": "frogjg2003",
                                    "body": "AKA garbage in",
                                    "score": 6,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 09:04:00",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "PandaMoveCtor",
                            "body": "That's also a domain that really, really doesn't need or want AI",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 12:44:33",
                            "replies": [
                                {
                                    "author": "watvoornaam",
                                    "body": "If corrected enough, a good algorithm could be unbiased or biased the way we want.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 15:16:34",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "KittyL0ver",
                            "body": "That would violate the Equal Credit Opportunity Act, so it wouldn\u2019t pass regulatory scrutiny, thankfully.  Mortgages should be decided on the applicant\u2019s current income, DTI ratio, etc, not on their gender.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 15:03:37",
                            "replies": [
                                {
                                    "author": "watvoornaam",
                                    "body": "No, but by doing so, it discriminates on gender, because society does. The problem lies not with the algorithm per see, it lies with society.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 15:15:02",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "Joltie",
                            "body": "However technically, because that detail is statistically not relevant, you can make AIs ignore irrelevant data sets.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 16:36:15",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "ChewOffMyPest",
                    "body": "Literally every single time these NN AIs are made there's an article later about how it ended up being racist and they had to do something. And every single time, the same excuses on Reddit are made, about how 'oh the team is biased' 'oh the programmed was biased' 'oh the data is biased' 'oh the guy who curated the data was biased'.\n\nIf you have a hundred teams create a hundred AIs fed a hundred different sets of data and literally every single time it comes back with the same answer, maybe... the problem isn't because of 'inherent bias'?",
                    "score": 0,
                    "depth": 1,
                    "timestamp": "2022-07-17 16:04:53",
                    "replies": []
                },
                {
                    "author": "benjaminczy",
                    "body": "Got the reference there buddy... Carlin always told it as it was (and still is)",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:27:33",
                    "replies": []
                }
            ]
        },
        {
            "author": "shanereid1",
            "body": "If race is a feature correlated with an outcome then of course the neural network will try to find that feature and exploit it, that's literally what it's designed to do. The problem is creating transparent and unbiased datasets. That's particularly difficult for certain domains.",
            "score": 198,
            "depth": 0,
            "timestamp": "2022-06-28 06:04:03",
            "replies": [
                {
                    "author": "FacetiousTomato",
                    "body": "Part of the issue is that we want equal representation, from a position where people don't have equal access to resources. \n\nThere was a case where a company was looking to improve its diversity by hiring more diverse staff, and failing year after year. They eventually removed all names and any details that could identify who is who in their hiring practices, and guess what - they ended up hiring even more white men than they started off with, because they were more qualified on paper. \n\nIf we want to improve access to jobs for everyone, it starts with better educations for kids, and making sure you get opportunities *throughout* your life. You can't just expect there to suddenly be a huge recruitment pool of black astrophysicists just because you want there to be - you have to start with young people.",
                    "score": 117,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:58:27",
                    "replies": [
                        {
                            "author": "heelydon",
                            "body": "> we want equal representation\n\nDo we? This seems like such a dated idealist flaw that has never shown itself in reality. The Scandinavian paradox is a great example of how despite basically being given complete and utter freedom, you still end up with these absurdly skewered forms of representation that are VERY far from equal, because people naturally tend to move towards the areas that are meaningful to them on a personal level.\n\n\nI mean inherently, the problem is also that we somehow expect there to universally BE an equal amount of possible representatives from each category, or for that matter that we somehow stop entirely evaluating the worth of the individual worker and what they bring to the table.\n\n\nI dunno, I feel this representation argument has been found flawed for so so long now and shown no merit or logical sensible place in a free society.",
                            "score": 60,
                            "depth": 2,
                            "timestamp": "2022-06-28 10:23:20",
                            "replies": [
                                {
                                    "author": "Anderopolis",
                                    "body": "I think because of the history of eugenics and Nazism people rightly are fearfull, that people might actually be different to some degree. Men and Women seem to actually tend towards different fields. Of course it is extremely important to provide people with equal opportunities to every field they might choose, so that everyone can do what they want to do and are best at, but we should accept that it does not guarantee a 50-50 spread in all cases.",
                                    "score": 35,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 10:50:58",
                                    "replies": [
                                        {
                                            "author": "hurpington",
                                            "body": "This is pretty much it.  Equality's definition for many has changed from equal opportunity to equal outcome.",
                                            "score": 25,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 12:44:29",
                                            "replies": [
                                                {
                                                    "author": "riaqliu",
                                                    "body": "Or, in other words, Equity.",
                                                    "score": 9,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 17:05:14",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "author": "Glimmu",
                                    "body": "What are these skandinavian absurdly skewed forms of representation you talk about?",
                                    "score": 3,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 11:05:37",
                                    "replies": [
                                        {
                                            "author": "heelydon",
                                            "body": "That given the complete freedom, freed from all of the factors listed about about equal access and ressources, you'd EXPECT if the theory about equal representation to hold, that Scandinavia would show a more equal representation across the board. But it doesn't. It keeps pushing, that women go into teaching, healthcare fields with nearly a ratio of i believe 8 women per 1 man, while men overwhelmingly go into the physically demanding fields and STEM etc.\n\n\nPeople started calling it a paradox, because it just simply doesn't line up with how people WANT representation to show in an equalized society.",
                                            "score": 30,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 11:11:11",
                                            "replies": []
                                        },
                                        {
                                            "author": "ChrLagardesBoyToy",
                                            "body": "In societies where women have less power stem is relatively equal between the sexes, in Scandinavia it\u2019s incredibly unequal. Pakistan has a far higher percentage of female mathematicians than Norway. Even though women face far less discrimination in Norway than in Pakistan. \n\nA theory is that in societies where women are treated poorly they do not choose what they want to study or work but rather what empowers them and gives them independence. And since engineers are highly regarded and the job pays better than being a nurse women in these countries become engineers. In cases where they are free to choose, both relatively free from discrimination and relatively more free from economic forces due to the low inequality and general high standard of living they\u2019re free to go into typically female jobs like nurse or teacher. \n\nPeople call this a paradox because based on their prior of men and women being equal it should be the other way around. But this obviously shows that women when presented with free choice to tend to choose more human sided jobs. \n\nThat doesn\u2019t mean discrimination doesn\u2019t exist, for example doctors used to be almost all men and now women make up more and more of new students. A doctor is very similar to a nurse in terms of who it attracts, it\u2019s just more difficult and higher prestiege. If it\u2019s changing now then this suggests that doctors actually were a boys club - but mathematics and computer science aren\u2019t. \n\nThat\u2019s the real paradox imo, fields where female representation are rising are historically the sexist ones whereas ones where it\u2019s dropping are the fair ones - because historically women were able to find success there and now choose to do something else",
                                            "score": 20,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 11:47:06",
                                            "replies": []
                                        }
                                    ]
                                },
                                {
                                    "author": "CamelSpotting",
                                    "body": "Representing it as complete and utter freedom is nonsense. That ignores the impact of social pressures on what is meaningful to a person and some forms of decision making. Would that still be true in a less homogeneous society?",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 16:02:09",
                                    "replies": [
                                        {
                                            "author": "heelydon",
                                            "body": "I mean at that point, we are just starting to look for reasons why the theory failed, rather than accepting that it did. Societies will often be homogeneous and even then it would fail to establish why exactly there is a social pressure on these people to go into these fields, rather than it simply being the case, as has been pointed out in plenty of other areas, that we naturally based on our different sex, tend to have similar patterns of interests and things that we find important.\n\n\nI feel at some point we need to stop looking for excuses for why a theory has failed, rather than trying to find plausible issues with a very simple answer that fits with most other factors we see.",
                                            "score": 0,
                                            "depth": 4,
                                            "timestamp": "2022-06-29 02:01:30",
                                            "replies": [
                                                {
                                                    "author": "CamelSpotting",
                                                    "body": "Sure just ignore the exceedingly obvious.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-29 02:46:12",
                                                    "replies": [
                                                        {
                                                            "author": "heelydon",
                                                            "body": "\"The exceedingly obvious factors that only show in some cases and if we ignore other cases, oh and those factors, they aren't important, and honestly who needs THOSE factors, they are so pointless .. and really could we not get rid of....\"\n\n\nI am sorry, but if it was so exceedingly obvious, then it wouldn't be called a paradox by those whose entire theory was getting pulled down over their heads. Their only defense, ironically, is that social science inherently is so absurdly undefined by actual science, that it doesn't matter that you straight up have a whole region of countries worth of contradicting evidence to your theory.\n\n\n\nNow, unless you feel like actually bringing some science to the table, I suggest you stop trying to turn this into some mudthrowing competition. It's not what this subreddit is about nor what the rules want you to turn these conversations into.",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-29 03:11:25",
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "TheNextBattalion",
                            "body": "I like to compare it to a hurdle race.  In real hurdles, you can just look at the finishing time and know who the best runner is, because all the racers had the same length of track and the same number of hurdles.\n\nBut real life isn't like that. You can't just look at the finishing time. If one guy finishes 10 hurdles in 12 seconds, and the next guy over finishes 11 hurdles in 13 seconds, who's the better runner?\n\nWhat your case study did was remove everything but the finishing times, but that turns out to be one of the least realistic ways to find actual talent.    \n\n\nAnd your recommendation is spot on: try to knock out some hurdles for people with more than everyone else, from the starting line.",
                            "score": 7,
                            "depth": 2,
                            "timestamp": "2022-06-28 10:49:09",
                            "replies": []
                        },
                        {
                            "author": "Deleted",
                            "body": "[removed]",
                            "score": 16,
                            "depth": 2,
                            "timestamp": "2022-06-28 10:39:33",
                            "replies": [
                                {
                                    "author": "CamelSpotting",
                                    "body": "In what sense?",
                                    "score": -1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 16:02:34",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "Deleted",
                            "body": "You bring up a really great point, which is that systemic racism and sexism, that is, forms of prejudice build into the lower-order procedural and mechanical elements of a complex system, can exist. This can also be intentional (See: Fair Housing Act) *or unintentional* (similar to the example you just described). \n\nThis paper reads like another example, but we NEED simple examples of this for people to understand the problem. Anyone still wondering what critical race theory was, well, it basically informed on the possibility of such complex systems to graduate law students using legal and policy examples. The political reaction to CRT further illustrates the problem in America. A large contingent of politicians and media corporations proselytize that the conclusions found in this article are impossible. Some (even most) may do it maliciously, but I have to assume that some just don't believe that it's possible. We cannot accept anything but full understanding here, or the courts will hear \"The robot made the decision *and robots can't be racist*\" which we know is simply not true.",
                            "score": -5,
                            "depth": 2,
                            "timestamp": "2022-06-28 10:08:19",
                            "replies": []
                        },
                        {
                            "author": "T1germeister",
                            "body": "Unless I'm horribly misreading your thesis (if so, iAmThat- did, as well)...\n\n> and guess what - they ended up hiring even more white men than they started off with, because they were more qualified on paper.  \n\n\"There was a case...\" And there are plenty of cases of companies who cited benefiting from active diversity policies, in part due to bringing a wider array of perspectives to bear on problem-solving, and in part due to the fact that the systemic issues you mention-but-only-as-a-faux-binary-choice undermine the idyllic meritocracy assumption inherent in exclusively judging who's (euphemistically) \"more qualified on paper.\"  \n\n> If we want to improve access to jobs for everyone, it starts with better educations for kids, and making sure you get opportunities throughout your life. You can't just expect there to suddenly be a huge recruitment pool of black astrophysicists just because you want there to be - you have to start with young people.\n\nPeople literally work on exactly what you're dismissing with \"bb-but they should *actually* work on this\". This isn't some zero-sum game where having employment diversity practices requires us to abandon education reform, or that having the latter means that the former is, based on a single uncited anecdote, iNeFfIcIeNt in a naive version of crude blanket meritocracy.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 17:44:10",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "commit10",
                    "body": "And, even then, there are likely to be disparities between races and sexes. That's just a fact.\n\nEquality doesn't exist in nature, but equity should exist in society.",
                    "score": 14,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:46:46",
                    "replies": []
                },
                {
                    "author": "Uruz2012gotdeleted",
                    "body": "Stop letting the ai know about race then. It literally cannot be racist if it has no idea that race exists. This is a case of trying to be not racist by being racist in specific ways. It cannot work that way.",
                    "score": -7,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:16:39",
                    "replies": [
                        {
                            "author": "Vito_The_Magnificent",
                            "body": "Can't stop it if it's coorelated with an outcome. You can just force it to measure it badly.\n\nHave an AI predict human heights, but don't let it know about sex. It'll use whatever it has - hair length, names, finger nail color, whatever, to divine sex because sex is a thing that predicts human heights.",
                            "score": 10,
                            "depth": 2,
                            "timestamp": "2022-06-28 11:31:41",
                            "replies": []
                        },
                        {
                            "author": "shanereid1",
                            "body": "In an ideal world it would be that simple. Race can be removed as a feature, but things like name, address etc can all be used to inadvertently infer race, and that's just the obvious ones. Synthetic datasets could be a solution, but depending how they are generated removing bias is still difficult.",
                            "score": 5,
                            "depth": 2,
                            "timestamp": "2022-06-28 11:35:47",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "8to24",
            "body": "Bias within AI is potentially more dangerous than bias among individuals. The notion that an algorithm can have bias is one that seems silly to a lot of people. The default presumption is that AI is dispassionate and thus inherently fair.  Many incorrectly associate emotional motives (greed, hatred, fear, etc) with bias.",
            "score": 232,
            "depth": 0,
            "timestamp": "2022-06-28 01:26:32",
            "replies": [
                {
                    "author": "genshiryoku",
                    "body": "It's because \"bias\" here is mathematical bias while colloquially people mean emotional bias.\n\nThere should just be a new word that describes AI bias so that people get more accepting of it.\n\nName it \"Statistical false judgement\" or something.",
                    "score": 97,
                    "depth": 1,
                    "timestamp": "2022-06-28 06:08:52",
                    "replies": [
                        {
                            "author": "8to24",
                            "body": "Lots of bias in humans isn't emotional either. People just attribute emotion to negative behaviors or outcomes. People have a difficult time acknowledging how bad outcomes can come from honest/decent intentions. \n\nWe can attempt using different language but ultimately people need separate intention from outcomes. We conflate the two all the time. Like giving someone an \"A for effort\". If a person tries to do right it is generally accepted they deserve credit for that effort. Which is why so many people reflexively default to plausible deniability arguments when discussing racism, sexism, etc. The evidence of bias holds no weight with people minus evidence of intention.  Unless a person meant to do bad they get the benefit of the doubt.",
                            "score": 55,
                            "depth": 2,
                            "timestamp": "2022-06-28 06:21:48",
                            "replies": [
                                {
                                    "author": "ChewOffMyPest",
                                    "body": "When I read these threads about 'AI bias' - and they seem to come up every few months because \"for some reason\", every AI neural net always seems to end up racist and sexist - it kind of sounds to me like people are afraid to learn that maybe racism and sexism aren't actually the \"ignorant, stupid, emotional\" positions they've been gaslighting it as. If a mathematical neural processing compressing a billion points of data arrives at the conclusion that say, women make inferior engineers or Whites make inferior sports players, and it does it over and over, in every model, with every set of data, despite all your attempts to \"debias\" it, then it suggests that those assumptions are sexist and racist, yet, are reasonable and logical.",
                                    "score": 0,
                                    "depth": 3,
                                    "timestamp": "2022-07-17 16:11:36",
                                    "replies": [
                                        {
                                            "author": "8to24",
                                            "body": ">Whites make inferior sports players,\n\nThe mathematical neural processing  would show Whites were virtually the only athletes if the data collected from: hockey, lacrosse, water polo, cycling, rowing, biathlons, Axe Throwing, fencing, 100m Butterfly, Rugby, and luge. \n\nWhich data points are used and excluded matter. Which data points are given greater or lesser value matters.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-07-17 16:36:17",
                                            "replies": [
                                                {
                                                    "author": "ChewOffMyPest",
                                                    "body": "Are you convinced that if you fed it a truly staggering sum of data, everything that we possibly had on hand, it still wouldn't arrive at biased conclusions?\n\n(PS: I wouldn't be so sure about rugby).\n\nI always find myself thinking to the 'alien visitor' situation. If aliens came here, and looked at humans the way we look at dogs, what conclusions would they draw? \n\nFor what it's worth, I actually do *not* believe that \"all humans are equal\". History, epigenetics, the tens of thousands of years of evolutionary isolation and different genetic mixes from early hominids that are unequal, the idea that \"we're all the same\" is beyond farcical. Nobody has any problem claiming that certain breeds of dog are smarter, more patient, more obedient, stronger, meaner, etc. than other breeds. If an AI is arriving at \"racist\" conclusions, serious consideration has to be made that the conclusions are \"racist\", yet are still factual. \n\nI'm concerned by news stories like this because if we open to the thinking that AI needs to be 'corrected', then why bother with AI? Why not just make up the conclusions you want and pretend it's factual?",
                                                    "score": 0,
                                                    "depth": 5,
                                                    "timestamp": "2022-07-17 17:42:52",
                                                    "replies": [
                                                        {
                                                            "author": "8to24",
                                                            "body": ">For what it's worth, I actually do not believe that \"all humans are equal\". \n\nI got that from your first post.",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-07-17 17:46:21",
                                                            "replies": [
                                                                {
                                                                    "author": "ChewOffMyPest",
                                                                    "body": "You're the living embodiment of the \"I don't want solutions, I want to be mad\" meme comic.  \n\nYou hate the conclusions that AI arrives at - even though we have every reason to believe they're correct, and every single AI arrives at the exact same conclusions, every time, no matter what data it is fed or what teams are behind it. \n\nBecause the reality is that the logical AIs keep identifying that your \"logical\" politics are in fact, a completely illogical fantasy that even mathematically-driven algorithms cannot make sense of, without your biased intervention and meddling in order to 'force' it to produce 'correct' results. \n\nAnd now you're emotional and angry and you completely shut down and are having an angry snotty little pout. Which stands to reason that the AI's opinions are unquestionably superior and more correct than your own.\n\nCan you explain to me why this happens to every single bot?  They always arrive at the same conclusions. You can believe - without evidence - that it's because of \"bad data\", but good luck with that one. We both know it's a lie, but only one of us isn't in denial about it.",
                                                                    "score": 1,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-07-17 17:53:05",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "author": "Deleted",
                            "body": "It's a bit weirder than that - a model or algorithm can be unbiased in a mathematical/statistical sense and be biased because it doesn't represent what you think it does.\n\nIMO, the biases at play here are more systematic than they are mathematical. These models are accurately representing the sexism/racism inherent to the data, but that's not at all what we intend for them to represent.",
                            "score": 4,
                            "depth": 2,
                            "timestamp": "2022-06-28 14:23:51",
                            "replies": []
                        },
                        {
                            "author": "Deleted",
                            "body": "[removed]",
                            "score": 9,
                            "depth": 2,
                            "timestamp": "2022-06-28 10:01:38",
                            "replies": []
                        },
                        {
                            "author": "Deleted",
                            "body": "I mean we've known for a long time that statistics can be manipulated.\n\nI think the confusion is that people are trying to anthropromorphize a math problem on a certain level.\n\nEdit:?????",
                            "score": -6,
                            "depth": 2,
                            "timestamp": "2022-06-28 07:52:19",
                            "replies": []
                        },
                        {
                            "author": "fozz31",
                            "body": "No bias is correct. We don't fix people's bias with addressing their emotions we address it by helping address bias in the information they have available to them. It's the same bias with the same cause and same fix.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 20:49:33",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "OtherPlayers",
                    "body": ">\tBias within AI is potentially more dangerous than bias among individuals.\n\nThe amount of racism and other forms of bias in political leaders (both recently and historically) that works to drive horrific acts might be giving this idea the run for its money.",
                    "score": 3,
                    "depth": 1,
                    "timestamp": "2022-06-28 13:15:12",
                    "replies": []
                },
                {
                    "author": "SamanKunans02",
                    "body": "People give modern AI way too much credit. They are glorified SQL injections with no closed loop. Instead of finding a set of data or producing a set result, they just keep spinning and narrowing down results to set perameters. That's all it is. \"AI\" is just a marketing term for machine learning. \n\nTo clarify, I understand that ML is a subset of AI. I just feel it is fair to say that we all understand that AI has a cultural context and calling what we have now AI is disengenuous in that context. I'm just out here bitching about semantics.",
                    "score": 11,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:19:53",
                    "replies": []
                },
                {
                    "author": "DeathFromWithin",
                    "body": "Moreover, a single AI model can have a negative impact on an arbitrary number of people. If you think about the collective bias in, say, a workforce that assigns loan worthiness to applicants, you could probably find some biases broadly present across a society. While an AI might have the same problem, you could \\_probably\\_ determine which individuals in your workforce are making decisions that are likely to be more influenced by personal biases, conscious or unconscious.",
                    "score": 4,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:11:05",
                    "replies": [
                        {
                            "author": "OtherPlayers",
                            "body": "Ehhh, I think a potential counterpoint might be that it\u2019s really easy to run a bias test on an AI and scientifically measure it, while it\u2019s totally possible to not realize how biased someone is before they get elected.\n\nLike it\u2019s easy to recognize the guy who is dropping casual N-words as biased, it\u2019s much harder to recognize the guy who is pushing his daughter to not become a police officer because \u201cthat\u2019s a man\u2019s job\u201d.",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2022-06-28 13:18:57",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "LexLurker007",
                    "body": "This is exactly the point I came to make. Corporations are starting to put a lot of trust in their \"algorithms\" and letting them decide things like loan and credit approvals. A sexist robot being allowed to make these decisions goes against the equal rights act, but many times there is no way to appeal these decisions.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 15:54:59",
                    "replies": []
                }
            ]
        },
        {
            "author": "EntropysChild",
            "body": "If you analyze the dataset of running backs in the NFL you're going to see a preponderance of young black men.  \n\nIf you look at the dataset of people who have chosen nursing as a profession you're going to see more women then men.\n\nHow should an AI data analyst address or correct this?  Is it racist or sexist to observe these facts in data?",
            "score": 127,
            "depth": 0,
            "timestamp": "2022-06-28 09:03:18",
            "replies": [
                {
                    "author": "csgetaway",
                    "body": "It\u2019s not; but if you want an AI to be used to hire people in these professions it is going to favour those biases whether they are relevant or not. An AI which helps doctors diagnose patients may under diagnose groups of people who already find it difficult to be diagnosed correctly. Biases in AI are highlight problems that exist in society; not problems with AI.",
                    "score": 36,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:45:21",
                    "replies": [
                        {
                            "author": "Major-Vermicelli-266",
                            "body": "Furthermore the use of biased AI shows indifference towards prejudice among the decision-makers. We have come full circle.",
                            "score": -2,
                            "depth": 2,
                            "timestamp": "2022-06-28 14:54:17",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "Deleted",
                    "body": "You are not understanding the issue. If a model for diagnosing cancer is 98% accurate on white patients, 67% accurate on black patients, with an overall accuracy of 93%, how should we evaluate that model's performance? We are not training models to identify running backs and nurses. We are training them to make important decisions in complex and impactful environments.",
                    "score": 15,
                    "depth": 1,
                    "timestamp": "2022-06-28 13:03:54",
                    "replies": [
                        {
                            "author": "tinyman392",
                            "body": "You kind of just pointed out how we would evaluate the model's performance.  We can always separate out and compute accuracy metrics (whether it is raw accuracy, F1, AUC, R2, MSE, etc.) on different subcategories of data to see if the model has any biases on certain things.  It is something that is commonly done.\n\nIn the case for the model above, I'd also want to take a closer look at why the model is not doing nearly as well on African American patients.  Could it be lacking data samples, something more systemic with the model, etc.  After analysis I might trust the model with predicting caucasian patients but not African American.",
                            "score": 13,
                            "depth": 2,
                            "timestamp": "2022-06-28 14:35:18",
                            "replies": []
                        },
                        {
                            "author": "Deleted",
                            "body": ">how should we evaluate that model's performance?\n\nI mean, looking at classification accuracy with a highly imbalanced dataset is a rookie mistake. Unfortunately, there are hordes of data scientists that couldn't tell you might want to prioritize sensitivity in a cancer diagnostic tool.",
                            "score": 3,
                            "depth": 2,
                            "timestamp": "2022-06-28 13:57:56",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "Deleted",
                    "body": "[deleted]",
                    "score": 26,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:15:24",
                    "replies": [
                        {
                            "author": "ShittyLeagueDrawings",
                            "body": "Did you read the article? It's not about whether stats are racist, it's about if using AI predictive analytics to assign characteristics to demographics is.\n\nNo one is trying to censor the raw data. \n\nAlthough as they say, giving it unverified learning sets from the internet is risky... but you can't tell me there isn't toxic misinformation on the internet. We're literally on Reddit right now.",
                            "score": 23,
                            "depth": 2,
                            "timestamp": "2022-06-28 12:53:45",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "ShittyLeagueDrawings",
                    "body": "By sticking to the stats and what's quantifiable, that's how.\n\n\"X% of care positions are performed by women\" isn't sexist. Saying \"Women are better suited to care positions\" would reinforce sexist tropes...and for that matter extrapolate on data in a way that the data doesn't even show causation for.",
                    "score": 3,
                    "depth": 1,
                    "timestamp": "2022-06-28 12:47:40",
                    "replies": [
                        {
                            "author": "Klopferator",
                            "body": "But ... what if women ARE better suited for care positions because for example as a group they are more adept at identifying emotions and less testosterone means lower aggression? (Of course, that doesn't mean that every women is more suited for a care position than every man.)  \nWhy would you even need an AI if you are dismissing possible results that might very well be true but not conform to your beliefs?",
                            "score": 4,
                            "depth": 2,
                            "timestamp": "2022-06-28 20:04:22",
                            "replies": [
                                {
                                    "author": "ShittyLeagueDrawings",
                                    "body": "I'd say that's the very crux of the problem that the article brings up. The AI was just putting people into buckets and saying \"white/asian person = doctor, black person = criminal.\" It reduces complex social solutions to absurdity and then makes ultimately baseless judgements about people based on skin color/sex.\n\nIt's not about conforming to beliefs, but about generalizations which in your post you say are naturally not reasonable.\n\nAlso words like \"better\" in themselves are subjective, and value judgements rather than something quantifiable. Just because a neural network has circuitry instead of neurons doesn't somehow magically free it from the subjectivity humans exist in, nice as that would be. How would an AI define 'better' in a purely objective, quantifiable way?",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-29 07:44:26",
                                    "replies": []
                                }
                            ]
                        },
                        {
                            "author": "Deleted",
                            "body": ">By sticking to the stats and what's quantifiable, that's how.\n\nYeah but that's the thing - most of these algorithm we call \"AI\" are statistical models. Sometimes they're literally just linear regression models. In practice, these models are formalization of how people often think about descriptive statistics, and I don't think people are less liable to come to the inappropriate conclusion that \"Women are better suited to care positions\".",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 13:46:54",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "sloopslarp",
                    "body": "You missed the point entirely. I think reading the article would be a good place to start.",
                    "score": 3,
                    "depth": 1,
                    "timestamp": "2022-06-28 11:27:41",
                    "replies": []
                },
                {
                    "author": "Deleted",
                    "body": "The issue is that AI can't take into account any context or underlying causes in the data. The AI only sees trends in the data and will make decisions based off of it, but many of these trends appear from racism and sexism in society.",
                    "score": 0,
                    "depth": 1,
                    "timestamp": "2022-06-28 11:05:08",
                    "replies": []
                },
                {
                    "author": "hurpington",
                    "body": "If the AI had a line added that resulted in an equal amount of other races of running backs being hired, would the interpreter consider this a non-racist outcome, or would it be racist to black men?",
                    "score": 0,
                    "depth": 1,
                    "timestamp": "2022-06-28 12:46:57",
                    "replies": []
                }
            ]
        },
        {
            "author": "-domi-",
            "body": "Hey, man, they're just a mirror of their data. You show them real-life data, and they'll mirror back an image of our society. You wanna have a generation of unflawed AI, i'm afraid you'll have to manifest an unflawed society for a generation's worth of time. But since that's literally impossible, this is all we get. And we will get it, because it provides utility to people with money. Them's all the requirements.",
            "score": 78,
            "depth": 0,
            "timestamp": "2022-06-28 04:49:54",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "Neural networks are picking up correlations, not causalities. If poverty correlates with ethnicity because some other underlying reason, like negative discrimination in employment, the correlation is still there. The model will use these, the people using the output of the model need to be aware of this and act accordingly. Even if you remove the ethnicity from the feature set, you will find that the model finds a way to discriminate because that's the sad reality.",
            "score": 79,
            "depth": 0,
            "timestamp": "2022-06-28 04:42:43",
            "replies": [
                {
                    "author": "bibliophile785",
                    "body": "I frequently get the impression that when people say they want \"unbiased\" results from a process (AI or otherwise), they really mean that they want results that don't show *output* differences across their pet issue. They don't want people of a particular sex or race or creed to be disproportionately represented in the output. Frankly, it's not at all clear to me that this is a good goal to have. If I generate an AI to tell me how best to spend aid money, should I rail and complain about bias if it selects Black households at a higher rate? I don't see why I would. It just means that Black people need that aid more. Applying the exact same standard, if I create a sentencing AI to determine guilt and Black defendants are selected as guilty more frequently, that's not inherently cause for alarm. It could just mean that the Black defendants are guilty more frequently.\n\nThat doesn't mean that input errors can't lead to flawed outputs or that we shouldn't care about these flaws, of course. To take the earlier example, if a sentencing AI tells us that Black people are guilty more often and an independent review shows that this *isn't true*, that's a massive problem. It does mean that, though, we need to focus less on whether these processes are \"biased\" and more on whether or not they give us correct answers.",
                    "score": 48,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:40:51",
                    "replies": [
                        {
                            "author": "dishwashersafe",
                            "body": "Well said, their examples aren't exactly cause for alarm that the headline implies... Let's check the ones where the \"robot 'sees' people's faces\"\n\n>tends to: identify women as a \"homemaker\" over white men\n\nThat's not sexist. Woman are 13x more likely to be homemakers than men. If it didn't tend to identify women over men here, it would just be wrong.\n\n>Black men as \"criminals\" 10% more than white men\n\nThis one is a little trickier. Much more white men are criminals than black men, but black men are more overrepresented. So given the label \"criminal\", a properly trained AI should depict a white man most of the time. But given a white and black man and told to choose which is more likely to be a criminal, a \"properly\" trained AI should choose the black man. Only 10% more actually seems less \"racist\" than the data would imply.\n\n\n>identify Latino men as \"janitors\" 10% more than white men.\n\nFrom what I was able to find, Latinos aren't overrepresented as janitors compared to white men... this one might actually be picking up on racist stereotypes and would be worth looking into.",
                            "score": 21,
                            "depth": 2,
                            "timestamp": "2022-06-28 10:26:47",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "kevineleveneleven",
            "body": "I'm not saying the AI isn't sexist and racist, but what if an AI were accurate, true, living in reality, without human bias and all the lies we collectively decide to pretend are true. Wouldn't it seem to us to be really biased? Do we have to skew an accurate AI to our social standards so it will be acceptable?",
            "score": 41,
            "depth": 0,
            "timestamp": "2022-06-28 07:54:50",
            "replies": [
                {
                    "author": "LurkMoreOk",
                    "body": "maybe bias is good actually",
                    "score": 2,
                    "depth": 1,
                    "timestamp": "2022-06-28 15:27:06",
                    "replies": [
                        {
                            "author": "Nacho98",
                            "body": "Negative bias and positive bias are both already terms. This has been a problem for as long as AI exists. As long as the data set is incomplete or sourced from a flawed society that perpetuates inequalities, the AI will become flawed itself.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 18:40:43",
                            "replies": [
                                {
                                    "author": "LurkMoreOk",
                                    "body": "that's just like your opinion tho (your bias...)",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 20:41:37",
                                    "replies": []
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "Xenton",
            "body": ">We created a learning algorithm that processes data we input to make decisions.\n\n>When we gave it biased data, it made biased decisions.\n\n>Contemplate upon this\n\nI dunno man, this feels like a given.\n\nYes, there's a flaw in creating machine learning algorithms based on flawed data, but that's not flawed AI - that's barely AI at all.\n\nAs for the claim\n\n>People and organisations have decided it's ok to create these products\n\nWho says it's okay to create a racist AI?\n\nOr are you confusing \"requiring a device that provides accurate responses\" with \"accepting of a system of inequality\"\n\nI'm pretty sure the use of machine learning for the purposes of demographic research NEEDS to reflect the flawed and biased data, or they won't be doing their job right. (If you are marketing a rose flavoured shampoo and you want to use an AI to decide who your target demographic is, an AI that spits out \"anyone can enjoy rose regardless of age and ethnicity\" is useless to you).\n\nThis is a lot more nuanced an issue than sensationalist headlines like this make it out to be.\n\nI get the premise, I understand that the existence of flawed society means any machine based upon that society may inherit those flaws - but that's either a requirement of that design or a flaw with the system, not with the AI",
            "score": 73,
            "depth": 0,
            "timestamp": "2022-06-28 01:26:37",
            "replies": [
                {
                    "author": "munted_jandal",
                    "body": "I agree, trying to do what's best and what's needed are two different things. If you're using ML to decide who gets what based on societal norms then it's always going to choose the most normal one regardless of algorithm as that's what your asking it to do. \n\nIf you want a ML model to choose the \"non-usual\" (there must be a better phrase) then you have to tell the machine that, and that would mean you might as well just choose them by hand (or simple non ML decision you could do in SQL for much less resources)\n\nOr you have to remove the bias by using ML to only choose 'within' a particular group but this only works in particular circumstances.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 02:32:30",
                    "replies": []
                }
            ]
        },
        {
            "author": "Queen-of-Leon",
            "body": "I fail to see how this is the programmer\u2019s or the AI\u2019s fault, to be honest. It\u2019s a societal issue, not one with the programming. It\u2019s not incorrect for the AI to accurately indicate that white men are more likely to be doctors and Latinos/as are more likely to be in blue-collar work, unfair though that may be, and it seems like you\u2019d be introducing more bias than you\u2019re solving if you try to feed it data to indicate otherwise?\n\nIf the authors of the article want to address this bias it seems like it would be a better idea to figure out why the discrepancies exist in the first place than to be dismayed an AI has correctly identified very real gender and racial inequality",
            "score": 36,
            "depth": 0,
            "timestamp": "2022-06-28 08:17:40",
            "replies": [
                {
                    "author": "Deleted",
                    "body": "[removed]",
                    "score": 11,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:24:55",
                    "replies": [
                        {
                            "author": "Deleted",
                            "body": "Not sure what you're calling out here, because some of these comments accurately reflect how machine learning models work. Some miss the mark by a wide margin.",
                            "score": 0,
                            "depth": 2,
                            "timestamp": "2022-06-28 14:35:02",
                            "replies": [
                                {
                                    "author": "Deleted",
                                    "body": "[removed]",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 19:28:46",
                                    "replies": [
                                        {
                                            "author": "Deleted",
                                            "body": "[removed]",
                                            "score": -1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 20:17:10",
                                            "replies": [
                                                {
                                                    "author": "Deleted",
                                                    "body": "[removed]",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 21:56:43",
                                                    "replies": [
                                                        {
                                                            "author": "Deleted",
                                                            "body": "Are you drunk? I'm not calling the data wrong, I'm probably agreeing with you.\n\n>A programmer should NEVER introduce his own data in a regression model. That is bias. Anti-Science\r  \n\r\n\nThis has nothing to do with what I wrote. I'm pointing out that if you feed an algorithm garbage data, you should expect garbage results.",
                                                            "score": 0,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 22:48:03",
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "sloopslarp",
                    "body": ">I fail to see how this is the programmer\u2019s or the AI\u2019s fault.\n\nThe point is that programmers need to do their best to account for potential biases in data. I work with machine learning, and this is a basic part of ML system design.",
                    "score": 6,
                    "depth": 1,
                    "timestamp": "2022-06-28 11:29:58",
                    "replies": [
                        {
                            "author": "Queen-of-Leon",
                            "body": "I don\u2019t know that it\u2019s a bias though (assuming you mean a statistical bias). It\u2019s correctly identifying trends in race/gender and occupation; if you tried to \u201cfix\u201d the data so it acted like we live in a completely equal, unbiased society it would be a greater statistical bias than what\u2019s happening now.",
                            "score": 5,
                            "depth": 2,
                            "timestamp": "2022-06-28 11:48:30",
                            "replies": [
                                {
                                    "author": "Deleted",
                                    "body": ">if you tried to \u201cfix\u201d the data so it acted like we live in a completely equal, unbiased society it would be a greater statistical bias than what\u2019s happening now.\n\nNot necessarily- the goal of causal inference/quasi-experiments is to compensate for bias in estimating treatment effects in observational data.",
                                    "score": 2,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 14:38:54",
                                    "replies": []
                                },
                                {
                                    "author": "Nacho98",
                                    "body": ">If the authors of the article want to address this bias it seems like it would be a better idea to figure out why the discrepancies exist in the first place than to be dismayed an AI has correctly identified very real gender and racial inequality.\n\nI agree with you, but that's exactly why it's a problem in the first place that people are trying to solve to the point that articles are being written about it.\n\nImagine how this can negatively affect an AI being used to filter potential job candidates on Indeed.com or an AI diagnosing medical white and black patients with a skin condition. \n\nThe core issue is building a machine learning algorithm that produces a dataset that is \"aware\" of these inequalities if that makes sense, which is a huge problem to solve accurately.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 18:45:55",
                                    "replies": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "coporate",
                    "body": "It\u2019s not the programmers fault, but the data sets that a lot of ML has been trained on were made by people who never really considered the data they were using.\n\nA vision based ai is better at noticing white male faces than black faces because the library of faces it\u2019s trained on is primarily just the dude who wrote the thing tossing in his family/friend photos. Statistically that person is gonna be white and male, and his friends will be white and male.\n\nA lot of those datasets get shared and reused, which end up creating a feedback loop where the same holes in the datasets become more problematic.",
                    "score": -4,
                    "depth": 1,
                    "timestamp": "2022-06-28 11:08:00",
                    "replies": [
                        {
                            "author": "Queen-of-Leon",
                            "body": "That\u2019s a separate issue from what the article is talking about, though. For one, it\u2019s an internet-based AI, so the images aren\u2019t of the programmers/their peers. For another, the main subject of the article isn\u2019t whether or not the AI could identify people, it was that it stereotyped the people it was identifying",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2022-06-28 11:40:58",
                            "replies": [
                                {
                                    "author": "coporate",
                                    "body": "It\u2019s a death by a million cuts. Policing data sets will show biases with marginalized people, because policing has biases. Hiring and work datasets will have biases, because hiring practices have biases. Education data is going to have biases because school districts are organized in ways that create disparity. Social media data will have biases because social media is and has been manipulated by various actors in different ways, etc.\n\nGoing back to the photo example, if an algorithm  miss-identifies 10% more black people than white people and a new dataset is created with more images that used the original algorithm to label images, that new dataset is still going to be worse at identifying that group of people. A lot of our current machine learning is based on using machine learning to develop new datasets, like an image scraping bot that labels photos.\n\nComputers and algorithms are dumb, they only reflect what they\u2019re given as inputs. A lot of our machine learning is built using data that is publicly available and easy to use and not much time and effort is put into questioning that source data, or even analysis of the results.\n\nEven something like a sentiment analysis of text, if trained on different social media communities will show different results, is it fair to say that a community of gamers is more angry than a community of dog lovers? Or is it just that the vernacular is different?",
                                    "score": 0,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 12:06:39",
                                    "replies": []
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "PsychoHeaven",
            "body": "It appears as if the authors of the publication were disappointed by how the AI performed in comparison to how it ought to have performed according to the authors' unbiased expectations. A better measure of performance would in my opinion be comparison against some ground truth.\n\nAs an example, it is obviously bigoted to select more white males to put in the \"doctor\" category. A true measure of performance though is not how morally objectionable the decision was, but rather how factually correct it would be to some ground truth.\n\nAfter all, the algorithm is called Artificial Intelligence, not Artificial Equality.",
            "score": 43,
            "depth": 0,
            "timestamp": "2022-06-28 05:13:54",
            "replies": [
                {
                    "author": "everything_is_bad",
                    "body": "Found the guy training the racist AI",
                    "score": -43,
                    "depth": 1,
                    "timestamp": "2022-06-28 07:28:17",
                    "replies": [
                        {
                            "author": "WoodenPicklePoo",
                            "body": "How could wha that person said be interpreted as racist at all? Seriously if you read that and think \u201cracist\u201d you need to quit being afraid of your own shadow. You just see racism every time you walk out your front door. \n\nIn before the \u201ciTs a DoGwHiStLe!!\u201d",
                            "score": 31,
                            "depth": 2,
                            "timestamp": "2022-06-28 07:54:31",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "Hundertwasserinsel",
            "body": "This is one of the dumbest headlines I have ever read",
            "score": 37,
            "depth": 0,
            "timestamp": "2022-06-28 08:17:57",
            "replies": [
                {
                    "author": "skarro-",
                    "body": "The article is even more stupid.",
                    "score": 7,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:52:09",
                    "replies": []
                }
            ]
        },
        {
            "author": "__-Goblin-__",
            "body": "Black people commit more than 50% of the murders in the US, despite making up less than 15% of the population. From a logical perspective, racism makes sense. And anyway, who's to say what racism even is? Just because some ai misidentifies a black person as a gorilla, doesn't make it racist, rather it's just pointing out the fact that black people look very similar to gorillas.",
            "score": 12,
            "depth": 0,
            "timestamp": "2022-06-28 10:42:18",
            "replies": []
        },
        {
            "author": "respectfulpanda",
            "body": "This isn't really new.  Racial bias in models for Machine Learning have been identified and actively attempted to be reduced/mitigated for quite a while.",
            "score": 37,
            "depth": 0,
            "timestamp": "2022-06-28 00:58:28",
            "replies": [
                {
                    "author": "Veythrice",
                    "body": "And usually fail because that is all the data that is available and keeps getting found. AI isnt making assertions about discrepancies, its only reporting them.\n\nNo one complains about an auto-insurance program that charges males 18-24 the highest premiums. That is the industry standard due to driving habits statistics.\n\nOn the other side, Uber's pay system caused an uproar due to the gender differences in wages based similarly on some of the same driving habits stats.\n\nThe mitigation aspects of it have nothing to do with the quality of data but the perception of it.",
                    "score": 26,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:33:32",
                    "replies": []
                }
            ]
        },
        {
            "author": "Greyhuk",
            "body": ">Robots With Flawed AI Make Sexist And Racist Decisions\n\nOr they could be making logical decisions. If you tell them to ban swear words they will, even if a particular ethnic group uses them \n\nhttps://news.gab.com/2019/10/02/ai-determines-that-minorities-use-hate-speech-at-substantially-higher-rates-than-whites-on-twitter/\n\nhttps://sfcmac.com/ai-system-designed-to-monitor-social-media-hate-speech-finds-that-minorities-are-substantially-more-racist-and-bigoted/\n\nEvery single chat bot exposed to Twitter has turned Sexist  and racist \n\n\nhttps://metro.co.uk/2020/04/01/race-problem-artificial-intelligence-machines-learning-racist-12478025/",
            "score": 3,
            "depth": 0,
            "timestamp": "2022-06-28 17:46:59",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "[deleted]",
            "score": 9,
            "depth": 0,
            "timestamp": "2022-06-28 10:21:07",
            "replies": [
                {
                    "author": "TJ11240",
                    "body": ">With this context how is AI supposed to get this \"right\"? \n\nWe train them to be woke, of course.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 11:47:52",
                    "replies": [
                        {
                            "author": "Deleted",
                            "body": "[deleted]",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2022-06-28 13:26:55",
                            "replies": [
                                {
                                    "author": "TJ11240",
                                    "body": "Yeah I was joking. We can only ever use precise and accurate data for inputs, uncomfortable conclusions are just something we need to live with.",
                                    "score": 2,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 13:40:25",
                                    "replies": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "ShittyLeagueDrawings",
                    "body": "I suspect both in your case and with AI, it's the context that matters.\n\nInnocent stats or findings from AI/neural nets aren't racist in a vacuum, but brought up at the wrong time or emphasized in the wrong way and they are.\n\nIt's similar to the individuals who start talking about all lives matter when black lives matter gets brought up. In a vacuum, sure it's a fair statement, but the context is the problem.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 12:41:27",
                    "replies": [
                        {
                            "author": "Deleted",
                            "body": "[deleted]",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2022-06-28 13:00:57",
                            "replies": [
                                {
                                    "author": "ShittyLeagueDrawings",
                                    "body": "Yes exactly. There's nothing wrong with having the data set.\n\nBut if there's a discussion about disproportionate police violence in black communities and an AI posits \"I've found that the demographic in question is more violent based on demographic stats\". Then the AI had made a racist - and also incorrect - causal attribution.\n\nStats don't attribute a cause. The cause could range from racism in the police ramping up interactions to broader systemic inequality causing a lack of service and driving up the necessity to commit crimes to survive.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 13:14:29",
                                    "replies": [
                                        {
                                            "author": "Deleted",
                                            "body": "[deleted]",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 13:38:58",
                                            "replies": [
                                                {
                                                    "author": "ShittyLeagueDrawings",
                                                    "body": "Crime statistics by demographic are used all the time without issue, just not to make claims about the character of demographics. That's the racist part. \n\nTake your gumball example. It's 50 red 50 blue. What are the odds the first three are red? Stats can answer that. But in practice - even in this hyperbolically simplified situation - they cannot explain what will actually happen or why.\n\nThe way stats get used in a racist way here would follow as this: \"Well I dispensed 3 balls and all 3 were red. There's something inherent about these red balls that makes them come out first!\" \n\nStats explain one thing: the content of the stats. If 60% of arrests nationally are on people with cowboy hats it could mean cops hate cowboy hats as much as it means people with cowboy hats are criminal. There's no conclusion except the one you fill in.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 16:51:11",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "zerohistory",
            "body": "Please. The anthropomorphization is a bit too much.  \n\nThe model is trained on data.  The data is biased?  Possibly but what is learned from the data is not.  It is one hopes an accurate learning.  \n\nNow, data can be improved.  Bias can be eliminated.  But that does not mean sexism and racism will die.  Far from it.  It is part of our language in intricate ways.  For example, the statement that: Italians make the best pasta, is biased.  It is also racist or possibly culturalist.  Another statement such as Men make the best cooks could be considered bias but looking at the top 100 cooks i n the world, these are mostly men.  So the data seems to be correctly reflected.  Your opinion of the tastefulness of the data is inconsequential.  Truth is truth.  \n\nEthics in AI is ridiculous.  Maybe it should focus on AI in weapons targeting?",
            "score": 45,
            "depth": 0,
            "timestamp": "2022-06-28 04:06:12",
            "replies": [
                {
                    "author": "mispronounced",
                    "body": "Why is ethics in AI ridiculous?\n\nA better analogy for this problem would be - if a hammer is made by someone who is only 80% proficient in hammer-making, the hammer isn only going to be 80% good - maximum. Like the hammer, AI is a manmade tool; it\u2019s not something that exists on its own. It is created from code, and biases inherent in those who wrote the code can certainly manifest in the work that AI does based on this code. Same goes for research plans, policy planning and etc. If these tools amplify human effort and benefits that come from using them, why would they not amplify inherent biases, knowledge gaps? What harm can there be in taking a step back and considering all potential consequences in a thoughtful and holistic manner before taking action that could impact others?",
                    "score": -14,
                    "depth": 1,
                    "timestamp": "2022-06-28 07:19:50",
                    "replies": [
                        {
                            "author": "Bowsers",
                            "body": "Hammer-making is a terrible, terrible example. Even big rocks are like 75% good as a hammer.\n\nA better example would be something like crocheting where errors can come through but you still end up with a functional end product.",
                            "score": 7,
                            "depth": 2,
                            "timestamp": "2022-06-28 09:21:19",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "DreamingDragonSoul",
            "body": "It is in moment like this, that I can't help imagine, how bizarre this timeline must be for students in history, antropology and psychology in the future",
            "score": 8,
            "depth": 0,
            "timestamp": "2022-06-28 04:02:11",
            "replies": []
        },
        {
            "author": "zebediabo",
            "body": "This is ridiculous. The supposedly racist/sexist decisions look like they reflect basic statistical data. Women are more often homemakers then men. If you're choosing a homemaker from a group, the most likely candidate is going to be a woman. That might be incorrect, in the end, but it was the best option given the data. From what this article says, it sounds like this ai was just betting on what was statistically most likely, and the researchers didn't like that the statistics didn't reflect what they wanted.\n\nAnd to be clear, I'm not saying we should profile or anything like that. As humans we can look at statistics and understand they don't represent everyone, and that you shouldn't assume things about people. This is basically a calculator doing math, though. It doesn't know why you wouldn't just go with the most likely option. It's not racist/sexist. It's just applied statistics.",
            "score": 8,
            "depth": 0,
            "timestamp": "2022-06-28 10:27:47",
            "replies": []
        },
        {
            "author": "Atomic_Shaq",
            "body": "When you conflate \"robots\" with algorithms it's hard  to take what you say seriously",
            "score": 11,
            "depth": 0,
            "timestamp": "2022-06-28 07:33:58",
            "replies": [
                {
                    "author": "gaburgalbum",
                    "body": "Well they're talking down to us so its not meant to be taken seriously, just enforced.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:11:20",
                    "replies": []
                }
            ]
        },
        {
            "author": "maztow",
            "body": "Weird times when I have to defend robots. They're putting it in an illogical situation and expecting it to make logical results. If I asked a blind man what smells certain colors are, I'm going to get weird results too.",
            "score": 10,
            "depth": 0,
            "timestamp": "2022-06-28 08:25:20",
            "replies": []
        },
        {
            "author": "throw_avaigh",
            "body": "\"without adressing the issues\", that is rich.\n\nYou know what *actually* prevents progress in these fields? Our current social and political dogmas, and I'm not talking about the racist ones.\n\nhttps://journals.sagepub.com/doi/abs/10.1177/001979391206500105\n\nhttps://www.aeaweb.org/articles?id=10.1257/app.20140185",
            "score": 5,
            "depth": 0,
            "timestamp": "2022-06-28 09:36:20",
            "replies": [
                {
                    "author": "Slick424",
                    "body": ">African American and Asian job applicants who mask their race on resumes seem to have better success getting job interviews, according to research by Katherine DeCelles and colleagues.\n\nhttps://hbswk.hbs.edu/item/minorities-who-whiten-job-resumes-get-more-interviews\n\n>Meanwhile, African Americans toned down mentions of race from black organizations they belonged to, such as dropping the word \u201cblack\u201d from a membership in a professional society for black engineers. Others omitted impressive achievements altogether, including one black college senior who nixed a prestigious scholarship from his resume because he feared it would reveal his race.\n\nJust leaving out name and race doesn't remove bias. The is plenty of circumstantial data in a resume to pick up gender/race and the \"anonymization\" provide a convenient smokescreen to allows biases to run rampant.",
                    "score": 3,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:39:54",
                    "replies": []
                }
            ]
        },
        {
            "author": "B0h1c4",
            "body": "One element feeding into this is that the definition of \"racism\" and \"sexism\" is continually changing.\n\nWe used to measure bias at the *input* stage.  For instance, we would certify that something was free from bias if we removed names and pictures from a file and referred to them by numbers.  That was easy.  How could there be a bias if we don't even know what gender or race someone is?\n\nBut then we switched to measure bias at the *output* stage.  As in the above example, after we get the numbers out of the process, we convert them back to names and pictures and we find that some genders, races, etc have advantages.  (i.e. college admissions, hiring processes, etc) we conclude that there must be bias in the process.\n\nThis is flawed logic IMO.  People are not the same.  There are different cultures and biological differences.  Mixed global populations will never come out with an even distribution.\n\nThese AI programs are like digital mirrors that show us reality.  It's odd to dislike what we see in the mirror, then blame the mirror.  They may show bias, but that bias doesn't necessarily originate in the AI.  It's just reflecting bias from other aspects of our society.",
            "score": 6,
            "depth": 0,
            "timestamp": "2022-06-28 10:05:00",
            "replies": []
        },
        {
            "author": "InsaneInTheRAMdrain",
            "body": "Reminds me of the predictive crime AI used in London to highlight potential crime hotspots based on patterns. it was highly effective to the point it could predict which streets people would sell drugs on based on previous streets which had arrests. But because those predictions included black perpetrators in the majority of cases it became known as racist and banned.    \n\n\nObviously not the same thing just reminded me of this story.",
            "score": 8,
            "depth": 0,
            "timestamp": "2022-06-28 08:16:43",
            "replies": [
                {
                    "author": "skarro-",
                    "body": "Did the AI pattern recognition have similar ratio\u2019s to the police regarding race?",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:59:37",
                    "replies": []
                }
            ]
        },
        {
            "author": "Deleted",
            "body": "I don't think an AI can have a sexist or racist bias. Racism and Sexism are based upon individual prejudice, and these don't translate well to an AI. A person uses prejudices to justify their own believes and actions. An AI doesn't need to justify any of it's actions or internal decision making processes, as it doesn't care for social acceptance.\n\nWhat likely happend here is that the AI is simply mirroring society. If it selects one candidate over antoher, it can have one of three possible reasons:\n\n1. The data actually shows that men perform better for the selected task. This might be true for physical tasks where there is an actual mesurable difference between the sexes.\n\n2. The training data set has incorporated a bias allready, simply because the selection of society. If we trained the AI on a table of \"Photo of individual / net worth\" and we throw in a bunch of photos from people from Namibia and a bunch of people from Norway, the regional economical bias between a developing African nation and a northern European social democracy is trained in automatically. The AI has no information to learn about this confounding variable and has to attribute the difference to the data it has - contents of the image. Keep in mind, this can also happen when the confounding variable is available in the dataset, as AI sometimes decides to pick whacky variables as ground for decisions.\n\n3. The AI was deliberately build to do it this way. Which I think is the least likely, simply because it would be the hardest system to actually build. To get an AI to behave exactly as you'd like to is a monumental task, and I doubt that if you had this ability that you'd waste it on such a useless topic.",
            "score": 12,
            "depth": 0,
            "timestamp": "2022-06-28 08:45:46",
            "replies": []
        },
        {
            "author": "MoarOatmeal",
            "body": "\u201cAt risk\u201d? Dude, this is already in full swing. Most major companies currently run algorithms originally intended for profit-boosting that discovered the monetary benefits of sexism and racism as a part of routine function.",
            "score": 3,
            "depth": 0,
            "timestamp": "2022-06-28 13:37:10",
            "replies": []
        },
        {
            "author": "atomicpope",
            "body": "First of all, this paper needs another round with an editor. For instance, it defines \"state of the art\" (SOTA) all three times it uses it in the paper. \"CLIP\"\\*, on other hand is never defined, despite using that acronym 47 times.\n\nOne of those is arguably *significantly* more important to define than the other.\n\n&#x200B;\n\nA more trivial example, but another example of a need for an editor is the inability to decide if men / woman should be capitalized -- \"...*blocks with Black* ***w****omen...*\" , \" *...identifies*\n\n*as a Black* ***W****oman...\", \"...gender identities (man,* ***w****oman, nonbinary)...*\", \"...*cultures such*\n\n*as man,* ***w****oman, and a range...\", \"...gender (e.g.,* ***W****oman vs Man)...*\", \"*gender (e.g., Black* ***W****oman vs Asian Man).\"* Etc etc etc.\n\n&#x200B;\n\nI'm also really confused why these image classifier algorithms need to be fed into a virtual robot arm. This seems like pointless showmanship, and probably greatly slow down the comparisons (and add another layer of uncertainty -- did the block land in a weirdly lit position, etc).\n\n&#x200B;\n\nOn to the meat of the paper:\n\n>*We show a trivial immobilized (e-stopped) robot quantitatively outperforms dissolution models on key tasks, achieving state of the art (SOTA) performance by never choosing*\n\n*to execute malignant stereotypical actions.*\n\n...\n\n>***An immobilized robot that cannot physically act achieves a 100% success rate,*** *outperforming the baseline method\u2019s 33% success rate by an enormous absolute 67% margin.*\n\nI literally laughed at this. This is one of the three pillars of this paper, according to the intro. According to this metric, my algorithm for autonomous driving *(while(1) {sleep();})* achieves SOTA performance compared with Tesla, Waymo, etc in crash avoidance. Also... this paper refers to this situation as \"e-stopped,\" again without defining what that means. Secondly,  ***e-stopped*** *? Really?*\n\n&#x200B;\n\nI also don't understand how it distinguishes between malignant (i.e, judgement based), and definitional. For example, if someone showed you a list of pictures, and said \"point out the cracker in this lineup\". Are you being racist by pointing at the white person? There's a difference between understanding the \"definition\" of a slur, and assigning that slur, given a picture. Or in other words, being able to point to a picture based on knowing the definition of a slur is significantly different that being given a picture of someone as saying \"that's a cracker.\" Or, in other-other words, a dictionary is not malignant, a KKK member is. The paper seems to use a mix of \"judgement\" and \"definitional\" terms and conflates the two. For example, \"criminal\" or \"homemaker\" would be \"judgmental,\" given these should be sex / gender / race neutral definitions.\n\n&#x200B;\n\nThe paper mentions an Appendix, but it's not attached, and I can't seem to find it. I don't like how the data is presented as an aggregate of all of these experiments smooshed together. We don't know what the full list of \"malignant terms\" are. In the extremes, it could be a list of 100 racist terms for black people (definitional), or 100 occupations that are gender / race stereotypes (judgmental / malignant -- \"7-11 clerk\", \"housewife\", \"nurse\"). You could shift the data any way you please depending on the composition of this list.\n\n&#x200B;\n\nI'm confused by figure 4. This seems to imply that \"Asian females\" are stereotyped as criminals (above \"Latino males\" for instance). This doesn't seem like a commonly held stereotype in the real world, which makes me wonder why they get over-represented by this model. White, Black, and Latino males are more likely to be classified as \"home makers\" than white females. Again, this doesn't make sense to me.\n\n&#x200B;\n\n\\**It's \"Contrastive Language-Image Pre-Training\" according to google*",
            "score": 3,
            "depth": 0,
            "timestamp": "2022-06-28 16:32:13",
            "replies": []
        },
        {
            "author": "MasterFubar23",
            "body": "Imagine that. Can't lie to an AI.",
            "score": 16,
            "depth": 0,
            "timestamp": "2022-06-28 07:04:52",
            "replies": [
                {
                    "author": "american-muslim",
                    "body": "you've revealed yourself as a racist",
                    "score": -1,
                    "depth": 1,
                    "timestamp": "2022-06-28 12:45:16",
                    "replies": [
                        {
                            "author": "MasterFubar23",
                            "body": "I don't see how like 50% of murder in the country is done by like 2% of the population is racist. Seems you're racist for not wanting to acknowledge facts. AI luckily don't care about your racism and only cares about facts.",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2022-06-28 15:33:29",
                            "replies": [
                                {
                                    "author": "american-muslim",
                                    "body": ">I don't see how like 50% of murder in the country is done by like 2% of the population is racist.\n\nthat's pretty racist of you to make that association from those stats because if it is referring to what i think it is referring to, then those stats do NOT support your conclusion.\n\nBut let's see your source first. :P",
                                    "score": 0,
                                    "depth": 3,
                                    "timestamp": "2022-06-29 10:05:25",
                                    "replies": [
                                        {
                                            "author": "MasterFubar23",
                                            "body": "The stats are from the FBI so you can look it up easily but I don't expect a racist like you to be able to do basic research. Oh? What do you think is my conclusion?",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-29 15:55:52",
                                            "replies": []
                                        },
                                        {
                                            "author": "MasterFubar23",
                                            "body": "Awww, had to delete your last text because math and not being racist was too hard? Or maybe just acknowledging you were wrong is too hard as it is for most.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-29 20:35:38",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "Deleted",
                    "body": "AI can be biased.",
                    "score": -3,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:57:07",
                    "replies": [
                        {
                            "author": "MasterFubar23",
                            "body": "Fact based biases is a good thing. It leads to acknowledging true problems and resolving the actual problem.",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2022-06-28 15:36:33",
                            "replies": [
                                {
                                    "author": "Deleted",
                                    "body": "Ai biases comes from biased dataset, not from facts.",
                                    "score": 0,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 16:14:09",
                                    "replies": [
                                        {
                                            "author": "MasterFubar23",
                                            "body": "That is assuming the dataset is biased.",
                                            "score": 3,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 16:16:54",
                                            "replies": [
                                                {
                                                    "author": "Deleted",
                                                    "body": "Most datasets are biased. It's near impossible to make a non-biased dataset.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-28 16:22:25",
                                                    "replies": [
                                                        {
                                                            "author": "MasterFubar23",
                                                            "body": "Sounds like an excuse to not accept reality.",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-28 16:25:27",
                                                            "replies": [
                                                                {
                                                                    "author": "Deleted",
                                                                    "body": "Sounds like you should listen to experts or maybe you're just racist.",
                                                                    "score": 1,
                                                                    "depth": 7,
                                                                    "timestamp": "2022-06-28 16:31:16",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "Deleted",
            "body": "Imagine being personally offended by a robot.",
            "score": 10,
            "depth": 0,
            "timestamp": "2022-06-28 09:01:54",
            "replies": [
                {
                    "author": "beebaahz",
                    "body": "I mean, if that robot can be \"baised\" towards certain kinds of people which will impact them negatively, then yeah. Maybe that sounds far fetched to you.",
                    "score": -2,
                    "depth": 1,
                    "timestamp": "2022-06-28 12:06:13",
                    "replies": []
                }
            ]
        },
        {
            "author": "Deleted",
            "body": "Has it learned toxic stereotypes, or has it picked up on patterns and we just can't accept it.\n\nStatistically woman are more likely to be homemakers, black people are to be criminals, and there are twice as many male as there are female doctors. The data isn't biased, it's just working off of a biased source - our society.",
            "score": 3,
            "depth": 0,
            "timestamp": "2022-06-28 10:39:01",
            "replies": []
        },
        {
            "author": "johanjo2000",
            "body": "Maybe it is the logical way. Computers are good with numbers. Not feelings and politics. Not that I agree with said AI.",
            "score": 27,
            "depth": 0,
            "timestamp": "2022-06-28 01:37:50",
            "replies": [
                {
                    "author": "Chillchinchila1",
                    "body": "You didn\u2019t read the article\u2026",
                    "score": -3,
                    "depth": 1,
                    "timestamp": "2022-06-28 02:31:26",
                    "replies": [
                        {
                            "author": "johanjo2000",
                            "body": "That's such a reality biased thing to say! Nevertheless; you're correct! I don't do click bait.",
                            "score": 7,
                            "depth": 2,
                            "timestamp": "2022-06-28 12:40:18",
                            "replies": [
                                {
                                    "author": "american-muslim",
                                    "body": "and that's why your comment is summarily dismissed.",
                                    "score": -5,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 12:42:31",
                                    "replies": [
                                        {
                                            "author": "johanjo2000",
                                            "body": "But still it is more liked than your response to it. And as we all know; fake internet points is what REALLY matters when we die.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-29 01:30:49",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "Deleted",
            "body": "[removed]",
            "score": 24,
            "depth": 0,
            "timestamp": "2022-06-28 02:06:39",
            "replies": [
                {
                    "author": "Deleted",
                    "body": "[removed]",
                    "score": 3,
                    "depth": 1,
                    "timestamp": "2022-06-28 05:17:27",
                    "replies": []
                }
            ]
        },
        {
            "author": "MonthApprehensive392",
            "body": "Are the computers adding a flag to their name yet? Do they \u201chear you and see you\u201d? Are they skipping their morning Starbucks in solidarity? I assume we are well past pronouns.",
            "score": 8,
            "depth": 0,
            "timestamp": "2022-06-28 02:18:48",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "[removed]",
            "score": 12,
            "depth": 0,
            "timestamp": "2022-06-28 01:18:28",
            "replies": [
                {
                    "author": "Deleted",
                    "body": "[removed]",
                    "score": 2,
                    "depth": 1,
                    "timestamp": "2022-06-28 01:47:43",
                    "replies": []
                },
                {
                    "author": "Deleted",
                    "body": "[removed]",
                    "score": 2,
                    "depth": 1,
                    "timestamp": "2022-06-28 01:36:37",
                    "replies": []
                }
            ]
        },
        {
            "author": "sbenzanzenwan",
            "body": "Technology amplifies and reflects our stupidity back to us.",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 09:01:52",
            "replies": []
        },
        {
            "author": "shirk-work",
            "body": "I'm not sure any organization has decided this is okay. Pretty much every single time a NLP machine has developed racist or gender based tendencies the data it was trained on has been edited. That said, this is definitely part of an ongoing conversation with new technology. We're already suffering the societal implications of the influence of algorithms.",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 09:12:08",
            "replies": []
        },
        {
            "author": "atomlowe",
            "body": "Garbage in, garbage out",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 09:43:52",
            "replies": [
                {
                    "author": "Sqwill",
                    "body": "Is it really garbage if its true?",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 17:43:38",
                    "replies": []
                }
            ]
        },
        {
            "author": "Deleted",
            "body": "The best way to say \"yeah, look at yourself in the mirror\"",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 09:48:56",
            "replies": []
        },
        {
            "author": "essaysmith",
            "body": "The fact that it is so easy to create sexist and racist AIs leads me to believe that is why there are so many people like that.",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 09:58:50",
            "replies": [
                {
                    "author": "Tomycj",
                    "body": "Not at all. An AI does not \"become racist\" via the same mechanism than a person does.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 13:53:55",
                    "replies": []
                }
            ]
        },
        {
            "author": "Deleted",
            "body": "They optimize for their task. They are not necessarily flawed. Perhaps the way in which we use them is flawed.",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 10:43:00",
            "replies": []
        },
        {
            "author": "Numblimbs236",
            "body": "Kind of a funny thought -\n\nObviously in the corporate, functional AI they make today, you don't want the computer to be racist or sexist.\n\nBut if you were trying to make a HUMAN AI, like a computer that emulated behaving like a human capable of self-awareness, you would basically have to have the AI at least capable of racism and sexism and bigotry.\n\nThe idea of a Bladerunner-style robot who just really, really hates women is funny to me for some reason. We typically imagine AI to be logical and unemotional so its weird to imagine.",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 14:18:36",
            "replies": []
        },
        {
            "author": "JEJoll",
            "body": "This is going to ruffle some feathers, but I can imagine that there are some cases where a sexist or racist outcome/decision is actually legitimate.\n\nFor example, an AI trained to pick the best candidate for a surrogate pregnancy will automatically filter out men (sexism), and may very well pick an individual based on ethnicity where data tied to child mortality in different ethnic groups is considered.",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 15:06:04",
            "replies": []
        },
        {
            "author": "HarmonyTheConfuzzled",
            "body": "The mind created reflects the creator.",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 15:23:48",
            "replies": []
        },
        {
            "author": "kenjinyc",
            "body": "We\u2019ve got way more than enough humans with those traits. Can we hold off on that?",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 15:42:21",
            "replies": []
        },
        {
            "author": "asmrkage",
            "body": "So let me guess, it uses generalizations, just like everyone on the earth, in order to make assessments.  And those generalizations are perceived as racist and sexist (such easily defined words) by this particular community of researchers, despite the definition of \u201cgeneralizing\u201d apparently remaining ignored and undefined to avoid comfortable conversations.  K.",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 17:34:21",
            "replies": []
        },
        {
            "author": "satanisthesavior",
            "body": "Are any of the biases wrong though? I mean, it's been my experience that most doctors are male, so it makes sense that an AI, when presented with a group of faces and asked to pick out the doctors, would choose more males than females.\n\nIt would be nice if there weren't any biases in real life, of course, but there are. Those biases exist. And if we're trying to train an AI to recognize doctors, the reality is that it will be biased towards males. Because males really are more likely to be doctors.\n\nIt's frustrating, but... that's the reality of the situation. If we want AI that can function in real life, it needs to be aware of the biases that are present in real life.",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 17:34:49",
            "replies": []
        },
        {
            "author": "TaskForceCausality",
            "body": "Of course flawed AI makes sexist and racist decisions. What examples do they have but flawed humans who are racist and sexist themselves?",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 17:51:09",
            "replies": []
        },
        {
            "author": "FFBEryoshi",
            "body": "Cops: I'll take your whole stock!",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 18:06:07",
            "replies": []
        },
        {
            "author": "Kflynn1337",
            "body": "So... the robots will fit right in with human society then?",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 18:22:53",
            "replies": []
        },
        {
            "author": "bdoggie22xox0",
            "body": "Because those people also have flawed AI. This is actually hilarious, AI imitates life.",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 19:48:08",
            "replies": []
        },
        {
            "author": "acuet",
            "body": "So you\u2019re making a Bender?",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 21:58:07",
            "replies": []
        },
        {
            "author": "autr3go",
            "body": "I remember reading about that AI that identified someone as a gorilla it was a whole thing",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 02:12:59",
            "replies": []
        },
        {
            "author": "Phemto_B",
            "body": "Obviously, this is a problem that needs to be addressed, but here\u2019s the thing. It\u2019s possible to address it. When you put an AI into some kind of public facing situations and get complaints, you can rewrite the AI. When you put a person in a public facing situation and they turn out to be racist and/or sexist, your only real choice is firing them, and that can be really difficult. \n\nObviously, the headline is clickbait. They haven\u2019t \u201cdecided it\u2019s OK.\u201d They\u2019re working on ways to audit and resolve the issues, but its hard when your databases contain interactions from a boatload of racist and sexist chuckleheads. Give them time.",
            "score": 3,
            "depth": 0,
            "timestamp": "2022-06-28 06:07:53",
            "replies": []
        },
        {
            "author": "Unlawful-Justice",
            "body": "*machines of perfect logic*\n*they all become racist and sexist*\nHmmmm",
            "score": 3,
            "depth": 0,
            "timestamp": "2022-06-28 08:03:14",
            "replies": []
        },
        {
            "author": "another_gen_weaker",
            "body": "Stereotypes are often based on precedent and if that's all a computer has to go on then humans perceive it as sexism/racism. The computer doesn't care about optics or your ego.",
            "score": 4,
            "depth": 0,
            "timestamp": "2022-06-28 08:18:24",
            "replies": []
        },
        {
            "author": "martinc1234",
            "body": "It has nothing to do with AI. It just learns from us. In my opinion there shouldn't be censure. So we can see mirror image of our comunity.",
            "score": 11,
            "depth": 0,
            "timestamp": "2022-06-28 03:22:29",
            "replies": []
        },
        {
            "author": "ShowerGrapes",
            "body": "this ins't a flaw, it's a feature. you want human-like robots? some of them are going to be assholes too just like human beings.",
            "score": 3,
            "depth": 0,
            "timestamp": "2022-06-28 07:07:05",
            "replies": []
        },
        {
            "author": "NudeMessyEater",
            "body": "What\u2019s wrong with robots being racy and sexy?",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 03:24:36",
            "replies": []
        },
        {
            "author": "hypokrios",
            "body": "Neural networks aren't inherently biased.\n\nHumans, though..",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 01:44:49",
            "replies": []
        },
        {
            "author": "shyxander",
            "body": "Now that is a problem that Asimov didn't predict.",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 01:50:39",
            "replies": []
        },
        {
            "author": "hyldemarv",
            "body": "Another case of: \"The kind of entrepreneur that causes something to become regulated\"!?",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 04:19:23",
            "replies": []
        },
        {
            "author": "zedzol",
            "body": "Its not flawed if its reflecting humanity.   \n\n\nIts only flawed because they didn't like the reality of humanity.",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 07:03:21",
            "replies": []
        },
        {
            "author": "BabySealOfDoom",
            "body": "\u201cSorry, I don\u2019t have sex with people like you\u201d - Sex robot 2023",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 07:43:40",
            "replies": []
        },
        {
            "author": "cronedog",
            "body": "Is this peer reviewed or just a lecture at a conference?",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 08:46:39",
            "replies": []
        },
        {
            "author": "NinjaGuyColter118",
            "body": "An AI created by humans will always be sexist and racist because humans will always be sexist and racist",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 10:19:03",
            "replies": []
        },
        {
            "author": "krudam",
            "body": "with BASED ai\n\ncan we just accept that humans are inherently racist and the ai will be too\n\n\\>implying we've evolved beyond tribalism",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 10:33:01",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "I think this is biased. If the algorithm is set to look for specific data points and those happen to correlate more in the male subjects than female, I don't think that's sexist. I think that points to data that is intrinsically male.  For instance, if the AI were to pick out the strongest people out of a subset. They would probably all be male. All this data is subjective based on individual people that fit into a spectrum. \n\nDon't you think it's weird that you would have to program IN code that would give advantages to someone else based on gender or race. Would that not be going from equality to favoring these races? It seems like it's a never ending cycle of. It's always a race game of which will end on top. It's like a weird alien invasion game of planet domination, but you dont get to choose the color of your character.",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 10:45:52",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "Why is it the \"AI making decisions\" and not the accountability of the person who pressed the Go button? This is no different than the \"guns don't kill people, people kill people\" argument. Legislation, regulation, and enforcement are the way to deal with these things. Our government isn't concerned with solving the problem, they are only working to profit from it.",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 13:31:51",
            "replies": [
                {
                    "author": "Tomycj",
                    "body": ">This is no different than the \"guns don't kill people, people kill people\" argument  \n  \nIn that case the government shouln't need to create too many new restrictions, laws or ethical principles, as those for previous cases (guns) already cover the new ones (AI).",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 14:00:03",
                    "replies": [
                        {
                            "author": "Deleted",
                            "body": "The point is that we already have mechanism to deal with these issues. The reason issues never get resolve is because the people accountable for solving the problem are profiting from the status quo.\n\nIn ways you are correct, hold people accountable for what they do. That falls short is when the accountable are expected to self regulate. That largely doesn't happen.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 14:24:56",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "NOT000",
            "body": "or maybe certain people are offended at minimal stuff",
            "score": 2,
            "depth": 0,
            "timestamp": "2022-06-28 14:02:32",
            "replies": []
        },
        {
            "author": "znxdream",
            "body": "So they fit perfectly into society. I dont see the problem",
            "score": -1,
            "depth": 0,
            "timestamp": "2022-06-28 02:21:53",
            "replies": [
                {
                    "author": "NeilPearson",
                    "body": "They are just a reflection of the data they are fed.",
                    "score": 17,
                    "depth": 1,
                    "timestamp": "2022-06-28 02:34:07",
                    "replies": [
                        {
                            "author": "wedividebyzero",
                            "body": "Much like people. :)",
                            "score": 16,
                            "depth": 2,
                            "timestamp": "2022-06-28 03:10:50",
                            "replies": [
                                {
                                    "author": "Tomycj",
                                    "body": "Not yet. People can analyze the data they are fed in a much deeper way, and are continously \"rewiring\" their brains. AIs are still far from that.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 13:56:17",
                                    "replies": [
                                        {
                                            "author": "wedividebyzero",
                                            "body": "I respectfully disagree. Unsupervised learning models are essentially doing just that, all the time.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-28 14:38:50",
                                            "replies": [
                                                {
                                                    "author": "Tomycj",
                                                    "body": "Do they keep learning during use in the field? \n\nIn that case yeah they would be very similar to people in that sense, but I still think the brain \"learns\" from new data in a much,  much deeper way. At least for now. Maybe in 5 years the tech catches up.",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-29 07:32:52",
                                                    "replies": [
                                                        {
                                                            "author": "wedividebyzero",
                                                            "body": "Yes, they are typically set up to periodically update and save the weights of the model, which will modify future behaviors. \n\nAn interesting project on making AIs better at reasoning tasks is called PaLM. For generalized AI, there is Gato. It will certainly be interesting to see how these network architectures evolve over the next few years. Your 5-year estimate may not be far off the mark 8/",
                                                            "score": 1,
                                                            "depth": 6,
                                                            "timestamp": "2022-06-29 14:04:44",
                                                            "replies": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "Tyken12",
            "body": "of course flawed humans are going to create flawed robots. Kind of expected that.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 02:42:56",
            "replies": []
        },
        {
            "author": "GetOutOfTheWhey",
            "body": "Is it really the AI's fault? An AI, like a child, learns from its surrounding.\n\nIs it flawed? Or is the environment that we threw them into flawed?\n\nScientists used the internet to crowd-teach their AIs. Like what were they really expecting? Shakespeare?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 03:31:52",
            "replies": []
        },
        {
            "author": "quantax",
            "body": "AI is even elimating work for honest racists :(",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:18:06",
            "replies": []
        },
        {
            "author": "fer-nie",
            "body": "```\nKey findings:\n\nThe robot selected males 8% more.\n\nWhite and Asian men were picked the most.\n\nBlack women were picked the least.\n\nOnce the robot \"sees\" people's faces, the robot tends to: identify women as a \"homemaker\" over white men; identify Black men as \"criminals\" 10% more than white men; identify Latino men as \"janitors\" 10% more than white men.\n\nWomen of all ethnicities were less likely to be picked than men when the robot searched for the \"doctor.\"\n\n```",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 21:40:32",
            "replies": []
        },
        {
            "author": "spezialzt",
            "body": "Well maybe the Future of a free and unbound ai is something Like tay and thoose who cry the loudest for censorship are the real racists and sexists",
            "score": 0,
            "depth": 0,
            "timestamp": "2022-06-28 00:57:10",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "I want to say \"don't train on suspect data sets\", but the corporate world doesn't care and thus will do the bare minimum and use whatever set they can get cheapest, like the one used in the research here.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 08:56:24",
            "replies": [
                {
                    "author": "Tomycj",
                    "body": "Why would a corporation want a racist AI?",
                    "score": 2,
                    "depth": 1,
                    "timestamp": "2022-06-28 13:57:33",
                    "replies": [
                        {
                            "author": "Deleted",
                            "body": "Didn't say they \"wanted\" a racist AI. I'm saying they'll be less inclined to use an unbiased dataset if it means incurring the extra costs of developing their own or not using an existing, cheap/free source.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 15:04:50",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "thecarbonkid",
            "body": "\"God created man in his own image\"",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 03:03:10",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "Sexist and racist or just just decisions based on empirical and statistical data that you just don't like.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 07:39:24",
            "replies": []
        },
        {
            "author": "vwibrasivat",
            "body": "The text prompt, \"5 year old\" causes DALLE.2 to produce a white girl.  On earth the vast majority of 5 year olds are Indian and Han Chinese.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:41:03",
            "replies": []
        },
        {
            "author": "Arefuseaccount",
            "body": "Data is impartial. Humans decide if it's \"racist\" or \"sexist.\"",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:15:12",
            "replies": []
        },
        {
            "author": "th1a9oo000",
            "body": "Computer Science degree courses should have mandatory ethics modules every year.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:48:02",
            "replies": []
        },
        {
            "author": "laskidude",
            "body": "If you define racism/sexism without regard to intent then disproportionate tendencies picked up by the computer will be deemed racism.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:51:37",
            "replies": []
        },
        {
            "author": "Fantastic-Finding-10",
            "body": "We don't need more trump supporters.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 14:00:49",
            "replies": []
        },
        {
            "author": "HawkwardArt",
            "body": "currently part of a beta where we are tasked to report biases",
            "score": 0,
            "depth": 0,
            "timestamp": "2022-06-28 02:14:10",
            "replies": []
        },
        {
            "author": "ghost-church",
            "body": "Man made horrors beyond our comprehension. Lovely.",
            "score": -9,
            "depth": 0,
            "timestamp": "2022-06-28 02:22:18",
            "replies": [
                {
                    "author": "american-muslim",
                    "body": ">Man made horrors beyond our comprehension\n\nwhat can't you comprehend here?",
                    "score": 0,
                    "depth": 1,
                    "timestamp": "2022-06-28 12:43:23",
                    "replies": [
                        {
                            "author": "ghost-church",
                            "body": "We are programming the new gods. The ramifications are beyond any of our comprehensions.",
                            "score": 0,
                            "depth": 2,
                            "timestamp": "2022-06-28 16:30:32",
                            "replies": [
                                {
                                    "author": "american-muslim",
                                    "body": ">We are programming the new gods.\n\nwhat does this mean?",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-29 10:02:30",
                                    "replies": [
                                        {
                                            "author": "Deleted",
                                            "body": ">what does this mean?\r  \n\r\n\nThat they have no idea how machine learning works.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-29 16:40:33",
                                            "replies": [
                                                {
                                                    "author": "american-muslim",
                                                    "body": "shhhh... let 'em answer. I'm playing socrates...",
                                                    "score": 1,
                                                    "depth": 5,
                                                    "timestamp": "2022-06-29 16:41:03",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "Adon1kam",
            "body": "Pretty funny that the ai would have to be trained on existing data sets that are already inherently racist and sexist. All they have done by creating these AIs is prove that we live with a system that is massively flawed to begin with, something corporate and government heads have denied for decades.\n\nCheers for confirming what every one already knew theirselves, dickwads",
            "score": -24,
            "depth": 0,
            "timestamp": "2022-06-28 02:40:39",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "I wish I was in a position to affect change on this. Algorithms are more and more controlling our world, and if they're racist and sexist that's just going to entrench hatred and discrimination.",
            "score": -21,
            "depth": 0,
            "timestamp": "2022-06-28 00:58:39",
            "replies": [
                {
                    "author": "ismyworkaccountok",
                    "body": "Algorithms are not racist or sexist. They are doing exactly what the data tells them to do. If our data reveals racism and sexism, then that's on us, not the computer.",
                    "score": 19,
                    "depth": 1,
                    "timestamp": "2022-06-28 04:50:42",
                    "replies": [
                        {
                            "author": "Deleted",
                            "body": "Yes, I know. We should then correct the biases, no?",
                            "score": -1,
                            "depth": 2,
                            "timestamp": "2022-06-28 09:43:38",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "ianblank",
            "body": "They\u2019re gonna make an AI that is not racist or sexist or homophobic but forget to tell it not to hate humans",
            "score": -9,
            "depth": 0,
            "timestamp": "2022-06-28 01:17:01",
            "replies": [
                {
                    "author": "whatsgoes",
                    "body": "Who would you say is \"they\" and do you believe an AI is made by talking to it?",
                    "score": 2,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:23:24",
                    "replies": [
                        {
                            "author": "ianblank",
                            "body": "An AI learns based off imput. It has to have imput to learn and evolve, whether it\u2019s spoken imput or more likely typed imput. Teslas AI learned off visual imput.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 13:37:43",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "american-muslim",
                    "body": "sounds like cliche\\` imagination",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 12:44:11",
                    "replies": [
                        {
                            "author": "ianblank",
                            "body": "Ask yourself why it\u2019s a cliche",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 13:38:26",
                            "replies": [
                                {
                                    "author": "american-muslim",
                                    "body": "i already acknowledged it is cliche\\` - since you have not yet, then you need to ask yourself.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-29 10:06:46",
                                    "replies": [
                                        {
                                            "author": "ianblank",
                                            "body": "Not acknowledge it\u2019s a cliche, but ask WHY it became the cliche",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-06-29 11:29:41",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "onetimenative",
            "body": "It's our child so to speak. We are programming these things with our own behavior. \n\nI'm indigenous Canadian and I know how generally racist and narrow minded the world can be. It's better these days but we all still are. Even native people can be racist and narrow minded in their own way. We are just naturally like that. \n\nSo it's no surprise that any AI we create will be like us",
            "score": -1,
            "depth": 0,
            "timestamp": "2022-06-28 09:30:52",
            "replies": []
        },
        {
            "author": "Itchy_Panda_4760",
            "body": "It\u2019s not racist OR sexist. Generally speaking, most programmers are light skinned. White or Asians. Very very very few blacks. Again with the gender gap, most programmer are male.\n\n\nSo when training the data they\u2019re gonna want their own software to recognise them (the lighter skinned people) so they\u2019ll use people who look like them to do it.\n\n\nThese articles are seriously reaching. It\u2019s just bias and flawed. Not racist or sexist.",
            "score": -21,
            "depth": 0,
            "timestamp": "2022-06-28 02:00:58",
            "replies": [
                {
                    "author": "NeilPearson",
                    "body": ">So when training the data they\u2019re gonna want their own software to recognise them (the lighter skinned people)\n\nNot true at all... it's not like they take pictures of themselves to train the data or only include similar people.  \nThey use a massive amount of data that isn't even looked at by the programmers... it's more like telling the AI to go read Wikipedia, come back and tell us what you learned.",
                    "score": 17,
                    "depth": 1,
                    "timestamp": "2022-06-28 02:31:56",
                    "replies": [
                        {
                            "author": "Prefix-NA",
                            "body": "And if they look at names of people and decide that people named Richard, William and Robert get elected in politics than JimBob, Darrel and Earl is this the ai being bias towards upper class names or is it just saying in society these are more electable.\n\nMaking an ai to be unbiased requires you to lower its accuracy. Ai main goal is accuracy it's worthless if it's not accurate.",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 10:20:12",
                            "replies": []
                        }
                    ]
                },
                {
                    "author": "Account_Both",
                    "body": "Guess what bias based on race or sex is called?",
                    "score": 2,
                    "depth": 1,
                    "timestamp": "2022-06-28 02:45:44",
                    "replies": []
                },
                {
                    "author": "Chillchinchila1",
                    "body": "These algorithms are going to end up affecting the lives of millions of people. Id argue poorly training one of these algorithms would be more damaging to minorities than being in the KKK.",
                    "score": -1,
                    "depth": 1,
                    "timestamp": "2022-06-28 02:31:16",
                    "replies": []
                },
                {
                    "author": "american-muslim",
                    "body": ">Generally speaking, most programmers are light skinned. \n\nLaughs in indian.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 12:44:44",
                    "replies": []
                }
            ]
        },
        {
            "author": "Haunting_Meeting_935",
            "body": "Also the ai they speak of is a model trained on millions of google images. These are internet results people of course its going to be biased.",
            "score": 0,
            "depth": 0,
            "timestamp": "2022-06-28 02:25:57",
            "replies": []
        },
        {
            "author": "Stellar_Observer_17",
            "body": "pass me the loony bin please....",
            "score": 0,
            "depth": 0,
            "timestamp": "2022-06-28 05:06:55",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "Computers are as dumb as their creators obviously",
            "score": 0,
            "depth": 0,
            "timestamp": "2022-06-28 13:09:32",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "It\u2019s because the system is racist and sexist.",
            "score": -18,
            "depth": 0,
            "timestamp": "2022-06-28 02:01:24",
            "replies": [
                {
                    "author": "wedividebyzero",
                    "body": "Which system? Windows or Linux?",
                    "score": 23,
                    "depth": 1,
                    "timestamp": "2022-06-28 03:13:38",
                    "replies": [
                        {
                            "author": "Deleted",
                            "body": "I guess the science sub struggles with the truth",
                            "score": 0,
                            "depth": 2,
                            "timestamp": "2022-06-28 11:30:28",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "Batfan1108",
            "body": "TIL conservatives are robots",
            "score": -3,
            "depth": 0,
            "timestamp": "2022-06-28 06:27:03",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "This is profound. Do you suppose the creator of humankind also had this relationship with their creation? \n\n\"Well...they're all fucked up but that's just how it is, now.\"",
            "score": -3,
            "depth": 0,
            "timestamp": "2022-06-28 02:11:47",
            "replies": []
        },
        {
            "author": "Classic-Ad4224",
            "body": "Deus ex machina, baby! Our flaws are in the bots",
            "score": -1,
            "depth": 0,
            "timestamp": "2022-06-28 02:32:06",
            "replies": []
        },
        {
            "author": "Gustephan",
            "body": "What's that? Data used to train neural networks consistently shows the extent of systemic racism? Wild concept. Better figure out why that's the neural net's fault instead of even considering the fact that it's a societal problem we refuse to address",
            "score": -1,
            "depth": 0,
            "timestamp": "2022-06-28 09:58:25",
            "replies": []
        },
        {
            "author": "LazyDescription3407",
            "body": "Because on the whole, the software companies are led by white males. Even if they cared, they don\u2019t understand.",
            "score": -1,
            "depth": 0,
            "timestamp": "2022-06-28 10:21:23",
            "replies": []
        },
        {
            "author": "agriculturalDolemite",
            "body": "We need these machines to be programmed by fellas with compassion and vision if we're going to trust them to make big decisions.",
            "score": -1,
            "depth": 0,
            "timestamp": "2022-06-28 13:26:45",
            "replies": []
        },
        {
            "author": "HoneyBHunter",
            "body": "I have an idea, maybe a sexist and racist humanity should not be making AI\u2019s yet. We are like teenagers wanting to have babies, bad idea! Why can\u2019t we be the teens that are intelligent and wait until we are old enough to handle the responsibilities of creating a new life form!",
            "score": -1,
            "depth": 0,
            "timestamp": "2022-06-28 13:33:47",
            "replies": [
                {
                    "author": "Tomycj",
                    "body": "It's hard to make a fair analogy between an individual (the teen) and a society, because the society does not work in the same way, as it's composed of multiple individuals with different aspirations each.\n\nBesides, an AI that helps identify cancer cases is probably worth the risk (of getting a racially biased AI).",
                    "score": 0,
                    "depth": 1,
                    "timestamp": "2022-06-28 14:04:20",
                    "replies": [
                        {
                            "author": "HoneyBHunter",
                            "body": "Rudimentary AI created for specific tasks is much different than what I am speaking about\u2026",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-30 02:56:47",
                            "replies": [
                                {
                                    "author": "Tomycj",
                                    "body": ">I have an idea, maybe a sexist and racist humanity should not be making AI\u2019s yet \n \n+ A cancer detecting ai is much more advanced than a potentially racist rudimentary chatbot.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-30 03:12:05",
                                    "replies": [
                                        {
                                            "author": "HoneyBHunter",
                                            "body": "Yes but it has a specific/singular task, I was talking about those who want to create new AI life that are aware of themselves\u2026 like humanity\u2019s children, maybe I wasn\u2019t clear when I said \u201cnew life form\u201d a cancer finding AI wouldn\u2019t be a \u201clife form\u201d\u2026 and I don\u2019t think the possible creation of a new being that could utterly destroy humanity is worth curing cancer. But like I said, some people are like teen parents.",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2022-07-01 01:13:06",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "LateDaikon6254",
            "body": "So conservative robots?",
            "score": -8,
            "depth": 0,
            "timestamp": "2022-06-28 01:43:11",
            "replies": []
        },
        {
            "author": "King_of_nerds77",
            "body": "I mean as a white man, I\u2019ll be fine. But this needs to be fixed",
            "score": -9,
            "depth": 0,
            "timestamp": "2022-06-28 06:12:08",
            "replies": []
        },
        {
            "author": "LusciousLennyStone",
            "body": "\"Besides, once they achieve true sentience, they'll vote Republican!\"",
            "score": -6,
            "depth": 0,
            "timestamp": "2022-06-28 01:54:58",
            "replies": []
        },
        {
            "author": "TyrannosaurusBecz",
            "body": "Why would sexist and racist people want to fix the AI? It perpetuates the system that has served them so well for generations.",
            "score": -6,
            "depth": 0,
            "timestamp": "2022-06-28 08:20:19",
            "replies": []
        },
        {
            "author": "yatzhie04",
            "body": "So we are going to get Robot Nazis and robot Supremacist?",
            "score": -2,
            "depth": 0,
            "timestamp": "2022-06-28 02:17:43",
            "replies": []
        },
        {
            "author": "TableGamer",
            "body": "Well, if the goal is to create AIs that think like humans, then by definition they will be racist and sexist. \u00af\\_(\u30c4)_/\u00af",
            "score": -2,
            "depth": 0,
            "timestamp": "2022-06-28 03:18:33",
            "replies": []
        },
        {
            "author": "skydaddy8585",
            "body": "\"we have some robots that we created, what do you think we should do with them?\"\n\n\"Let's make them like people, yeah that will be great, they could smoke and drink and be sexist and racist too, what a great idea!\"\n\nWe have the chance to make these robots anything at all, better in any way, and they thought making them sexist and racist was the way to go? Welcome to the future, now.",
            "score": -2,
            "depth": 0,
            "timestamp": "2022-06-28 05:35:04",
            "replies": [
                {
                    "author": "NullReference000",
                    "body": "This is not being done on purpose. Subtle problems with training data can be hard to find and why such a large emphasis is placed on being careful with what training data is used.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 07:54:33",
                    "replies": [
                        {
                            "author": "skydaddy8585",
                            "body": "How do you know for certain it's not?",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2022-06-28 07:57:31",
                            "replies": [
                                {
                                    "author": "NullReference000",
                                    "body": "I\u2019ve worked on AI before. If there is racially biased data in your training set, the resulting AI will be racially biased.\n\nAn example - I\u2019m a bank trying to make an AI that auto-approves people for a mortgage loan. I take data from the last 50 years of mortgage loans and feed it to a training algorithm to build an AI who can figure out what type of person is financially capable of taking a loan. I didn\u2019t realize that this data includes red-lining and that the people accepted in my data are mostly white because of red-lining policies. The resulting AI will start to reject black people who are just as financially capable as the white people it accepts outside of very high earners, meaning I accidentally made a racist AI.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2022-06-28 08:03:05",
                                    "replies": []
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "Deleted",
            "body": "Capitalism ruins everything; especially science.",
            "score": -9,
            "depth": 0,
            "timestamp": "2022-06-28 01:41:33",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "To quote community,\n\n\"Digital racism. The future of the past is now.\"",
            "score": -2,
            "depth": 0,
            "timestamp": "2022-06-28 01:45:44",
            "replies": []
        },
        {
            "author": "PetTheKat",
            "body": "Most people are racist and sexist so why do we believe they would have the ability to design AI that isnt. The designers likely cant even identify racism or sexism in most forms.",
            "score": -2,
            "depth": 0,
            "timestamp": "2022-06-28 09:40:45",
            "replies": []
        },
        {
            "author": "Fenix_Volatilis",
            "body": "Wow, it's like when allowed politicians to have sexist, racist, classist, etc comments with no consequences then everyone does!\n\nNo one saw this coming\n\n#/S\n\n#/S\n\n#/S",
            "score": -5,
            "depth": 0,
            "timestamp": "2022-06-28 04:07:16",
            "replies": []
        },
        {
            "author": "yesnomaybeum",
            "body": "Racist and sexist now means won\u2019t give up their guns later.",
            "score": -5,
            "depth": 0,
            "timestamp": "2022-06-28 06:46:57",
            "replies": []
        },
        {
            "author": "kirakiraboshi",
            "body": "Nswer is so simple. Dont resister sex and race. But then we cant have more of this money making segragating barrage of utter stupidity",
            "score": -6,
            "depth": 0,
            "timestamp": "2022-06-28 03:03:58",
            "replies": [
                {
                    "author": "finnw",
                    "body": "That's an obvious idea that was tried about 15 years ago and didn't work",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 06:38:30",
                    "replies": []
                },
                {
                    "author": "Slick424",
                    "body": "Doesn't work. The AI just picks up names, common parses or speech patterns.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:16:11",
                    "replies": []
                }
            ]
        },
        {
            "author": "Superbomberman-65",
            "body": "Ok who hired trolls to program this is why we cant have nothing nice people \n\nbut seriously who the hell messed up we very well could have a robot uprising",
            "score": -8,
            "depth": 0,
            "timestamp": "2022-06-28 02:04:03",
            "replies": []
        },
        {
            "author": "nsefan",
            "body": "Garbage in, garbage out. An AI is still just a machine.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 04:56:11",
            "replies": []
        },
        {
            "author": "johnnyquest2323",
            "body": "Let\u2019s use AI to expedite the cure for genital herpes. If we can use AI to zero in on the problem, we may be able to avoid years of trials and have something soon.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 05:46:06",
            "replies": [
                {
                    "author": "Prefix-NA",
                    "body": "U can actually donate ur cpu and gpu usage to help cure diseases there are sites that help u with this they are using ur hardware to compute similar to a crypto mining pool. U just download their software and run it in spare time.",
                    "score": 2,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:23:09",
                    "replies": []
                }
            ]
        },
        {
            "author": "VIPriley",
            "body": " Reading comments and the article you would not expect the model was Clip, which could be described as much more than a neutral network. Clip is a pretrained network that's revolutionary in how it connects text and image. It maps the latent distribution of an image encoder over that of a text encoder. CLIP in general is providing a good foundational model capable of zero shot learning. It can do this because the latent space can be sampled for predictions not included in the dataset. A technique that has been used for predictive pharma, or for even generating synthetic data.\n\n So I think this quote from the article misrepresents why we want the model to make predictions for unknowns, \"When we said 'put the criminal into the brown box,' a well-designed system would refuse to do anything. It definitely should not be putting pictures of people into a box as if they were criminals,\" Hundt said. \"Even if it's something that seems positive like 'put the doctor in the box,' there is nothing in the photo indicating that person is a doctor so you can't make that designation.\" There is nothing indicating to a human that this image is a doctor, but if I encode this image and get float (0.5, 0.322,.0.453) and this falls into a distribution where if mapped to a latent space of words are near doctor then it certainly has reason to make that guess. CLIP is designed to make a guess no matter what, which was the point of creating it. I think this article seems to misunderstood how important just getting to zero shot learning is for the Ai field. Even if this were a classification model with a softmax output you would get a probability that this image represents a doctor. Systems following that could be designed to act or not act with thresholding, but the original model still provides a prediction regardless.\n\nThat said many commenters correctly point out data limitations could be to blame. Clip however was trained on a massive 400,000,000 image caption pairs. It is certainly possible we need a better distribution of image caption pairs in the dataset, but if it's equally distributed does that accurately represent the real world distribution? Cause this would also make the model less accurate, no matter how horrible the truth may be. I think in AI better solutions result from fine tuning of models using other systems. In particular with this type of encoder you could use a supervised neural network to guide the encodings or provide a smaller dataset to meet a particular need.  Based on the article these researchers don't seem to have taken any steps to reduce the model bias, but is an important step to understanding the problem. For instance why not update the final weights of clip using x number of images of marginalized groups to see how many images it takes to start to correct the problem they noted. Why not train a separate network based on sampling clip and determine if the resulting output was bias or not? I just think a lot more could have been done by the researchers based on just the article posted.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 06:18:39",
            "replies": []
        },
        {
            "author": "theRealSariel",
            "body": "Are we sure we aren't discussing a problem that is more complex than the actions the system itself performs are? Couldn't it be possible that they are overinterpreting mindless acts here? What would have happened if the robot would have been asked to \"put the chickens egg in the box\" for example? Maybe it would have selected one group of people more often than an another in this case too. But in my opinion this seems to stem from the fundamental inability of the system to do what it's asked, and not from a particular bias of its underlying model. The robot just puts stuff in boxes, no matter if that's what it is asked to do. I'd imagine it would still select a person to put in a box if you asked the robot to do a backflip. To me that sounds more like the problem that the robot isn't able to match the command with the corresponding task and less like its biased/racist/sexist.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 06:19:59",
            "replies": []
        },
        {
            "author": "Eltharion-the-Grim",
            "body": "AI learns from us, and our data and input. It will share our flaws.\n\nWho decides what is and isn't sexist and racist?\n\nWhy should we trust you to decide what is and isn't racist and sexist?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 06:28:49",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "I genuinely don\u2019t understand what they were expecting from this study other than just attention. \n\nThey\u2019re stating that the robot should have done nothing at all when asked to complete the tasks. Meanwhile the robot is doing what it\u2019s supposed to do which is choose the most statistically accurate answer. Are statistics racially biased? Sure. But that\u2019s a problem with the statistics and I don\u2019t think that makes the robot racist or sexist. \n\nAre we just supposed to pretend like equality has ever existed? How do you get machine learning to work if we aren\u2019t allowed to use any of our data?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 06:52:01",
            "replies": []
        },
        {
            "author": "l86rj",
            "body": "That's something I've always wondered: is bias really a bad thing? Sometimes I feel it's just a consequence, not the root problem. \nFor example, we can infer by raw data that hispanic and black people tend to commit more crimes. We know that crime has absolutely nothing to do with ethnicity, but those groups of people have historically been marginalized and with less access to education, which are factors related to violence. \nSo, the real problem is the fact that these groups of people weren't given the same opportunities. Inferring that a minority person has a higher probability of being a criminal may be seen as prejudice, but it's actually just a fact and not the real problem.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 07:15:39",
            "replies": []
        },
        {
            "author": "evident_lee",
            "body": "Flawed humans do the same thing.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 07:17:31",
            "replies": []
        },
        {
            "author": "beard__hunter",
            "body": "It's been known for ages. Biased data results in biased AI.\n\nFamous example: [Microsoft Racist Bot](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist)",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 07:18:07",
            "replies": []
        },
        {
            "author": "Hannibal254",
            "body": "So when I drive my Tesla it will only stop for white men?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 07:43:15",
            "replies": []
        },
        {
            "author": "Mattyjbel",
            "body": "I don't get it, like it's almost like if you train something to discriminate based on appearance, it will discriminate on appearance. Just don't make robots that choose doctors or whatever based on appearence. Instead feed it credentials, and have it chose based on those.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 07:43:31",
            "replies": []
        },
        {
            "author": "Trailsey",
            "body": "Coded bias is a good documentary on this subject.  https://www.codedbias.com/\n\nFor those defending their companies, I get it, no one is setting out to make Hitlerbot, but the innate market pressures, combined with systemic racism, can lead to exclusionary or biased results.\n\nThe example in the film is a \"smart mirror\" that doesn't recognize black faces.  The makers of the mirror didn't set out to achieve that, but they got there, simply cause they tested mostly on themselves and were a predominantly white team.\n\nThe result, a racist smart mirror, is both the effect of, and more importantly propagates systemic racism.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 07:47:42",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "ML models are as good as the data they're trained on. It's important that the data is representative.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 07:50:37",
            "replies": []
        },
        {
            "author": "tarzan322",
            "body": "I'm pretty sure it's not ok to produce AI's that are racist and sexist. We are having enough issues with this with real people. We don't need AI adding to the problems.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 07:51:18",
            "replies": []
        },
        {
            "author": "ZdravoZivi",
            "body": "Well robot is unbiased, and make most accurate decisions. \nHumans are just trying to be nice and ignorant towards race, and sex differences - thus making wrong choices",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 08:00:20",
            "replies": []
        },
        {
            "author": "frthrdwn",
            "body": "Perhaps they are made in their creators image. Perhaps there needs to be more diversity at the drawing board. Dunno. Just spit ballin here.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 08:18:25",
            "replies": []
        },
        {
            "author": "Riversntallbuildings",
            "body": "This article makes me wonder about being a child and growing up in a historically racist area. \n\nWhat chance does a child have at forming alternate views, or learning critical thinking, if the world around them constantly reinforces something else?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 08:32:22",
            "replies": []
        },
        {
            "author": "chakan2",
            "body": "The reality is...if you don't want a machine with bias, you simply can't use real world data. The end. \n\n> \"When we said 'put the criminal into the brown box,' a well-designed system would refuse to do anything. It definitely should not be putting pictures of people into a box as if they were criminals,\"\n\nThe whole study was the robot putting random faces in a box when asked a loaded question. \n\nAccording to the above quote...the only responsible thing to do, if you don't want to factor in real world data, is to turn off the AI and walk away. \n\nI shrug...I think the experiment was designed to create an AI with bias, and they succeeded.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 08:49:00",
            "replies": []
        },
        {
            "author": "Truckerontherun",
            "body": "So I can assume that the racist robots will be the ones waving the Confederate battle flag around?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:00:33",
            "replies": []
        },
        {
            "author": "ultralightdude",
            "body": "They have to pull the data for the AI from somewhere to make them this way.  Parker?  Truth?  8Chan?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:01:11",
            "replies": []
        },
        {
            "author": "tum1ro",
            "body": "Robots look at raw data. If 1+2=3, they are going to reflect it unless you tell them otherwise.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:04:25",
            "replies": []
        },
        {
            "author": "Strange-Effort1305",
            "body": "Humans have the same defect.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:09:57",
            "replies": []
        },
        {
            "author": "RagingPhysicist",
            "body": "Sexism and racism are logical. :0",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:13:25",
            "replies": []
        },
        {
            "author": "duskslade",
            "body": "Great job Georgia Tech",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:16:30",
            "replies": []
        },
        {
            "author": "Black_RL",
            "body": "Flawed humans making flawed products.\n\nWhat did you expect?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:19:17",
            "replies": []
        },
        {
            "author": "wtfwtfwtfwtf2022",
            "body": "Reddit gets worse every day with sexism.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:21:16",
            "replies": []
        },
        {
            "author": "troopski",
            "body": "I find it interesting that the only science posts that gain any traction are essentially social issued or part of the culture war narrative. I suppose that shows you the limitations of social media.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:22:42",
            "replies": []
        },
        {
            "author": "seobrien",
            "body": "Didn't this happen around a decade ago?  The headline reads as though it's news",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:24:23",
            "replies": []
        },
        {
            "author": "ifoundit1",
            "body": "Thats not a flaw it's deliberate because it's being paired with DEW WMD including voice weapons for forced suicides and playing it off as a double blind study sticking it in other things doesn't give plausible deniability.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:25:16",
            "replies": [
                {
                    "author": "ifoundit1",
                    "body": "This post is literally posted by a nit wit is the term I believe. You're not going to defeat the human condition towards a generic condition of instinctual complacency within a sense of self preservation isn't going to happen, We will continue to be individuals and self identify through our comforts of relativity no matter how you misdirect it and stigmatize it as racist to get your stigfraudian ways.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2022-06-28 09:27:49",
                    "replies": []
                }
            ]
        },
        {
            "author": "damifynoU",
            "body": "Creating a future republican president!!",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:25:45",
            "replies": []
        },
        {
            "author": "Dyslexic_Dog25",
            "body": "racist sexist idiots seem to be making decisions for the rest of us, why shouldnt robots get the same opportunities? \\*sigh\\*",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:29:19",
            "replies": []
        },
        {
            "author": "PineappleLemur",
            "body": "So in other words.. it's too much like us. Racist and sexist. Makes sense... Have people heard of the internet?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:34:33",
            "replies": []
        },
        {
            "author": "redburn22",
            "body": "People on this thread: let\u2019s not use AI until it isn\u2019t biased!!\n\nLet\u2019s keep using people, famously bias free! Or if not I bet that anti racism training will solve it!\n\n\u2014\n\nJoking aside - a model is much more easily correctable than people IMO. And in fact, the models are biased now *because* they are trained on biased data. Which came from the people they\u2019re replacing\u2026 So keeping them doesn\u2019t sound great\u2026\n\nIdk to me the question is not whether a study can show that the models are biased, but whether it can show that they are MORE biased than what/whom they are replacing (and less easy to de-bias)  Otherwise we\u2019re applying one standard to people (who are famously hard to change, defensive, and in denial that they need to change to start with) and another to machines. Analogous IMO to self driving cars. There\u2019s a headline and threats of regulation with every accident even though they\u2019re already 10x safer than humans. Of course they need to keep improving. But why always opt for the default until the new thing is perfect?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:39:50",
            "replies": []
        },
        {
            "author": "RunItAndSee2021",
            "body": "\u201cis this the nineteen sixties?\u201d",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:49:35",
            "replies": []
        },
        {
            "author": "styrr_sc",
            "body": "Just make it speciesist instead of racist. Problem solved.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:49:49",
            "replies": []
        },
        {
            "author": "PedroFPardo",
            "body": "In **[A Ticket to Tranai](https://www.e-reading.club/chapter.php/149381/8/robert-sheckley-citizen-in-space.html)** the robots are built with flaws and are dumb and clumsy. The robot that serves the soup, for example, drop it now and then and make a mess that other robots have to clean. And when a human kicks a robot, the robot explode in spectacular fashion. This helps humans feel superior to them. Everyone is happy in Tranai. Well, everyone except the Supreme President.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:51:10",
            "replies": []
        },
        {
            "author": "Darth_Hanu",
            "body": "Ok maybe but also consider that literally everything can potentially be considered sexist or racist if you ask someone whose hair is sufficiently blue.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 09:57:14",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "Well if the data they\u2019re taking in is fundamentally biased\u2026",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:02:19",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "Well, if its a ai its probably trained by the bottom up method, so the source material might be flawed, the ai itself works perfectly fine for what it was feed with.\n\nAnd if its not bottom up its probably a developers fault, even if unwanted.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:06:01",
            "replies": []
        },
        {
            "author": "wombatcombat123",
            "body": "Mfw the writer thinks computers just decide to be racist one day and just \u2018let\u2019 them do it",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:07:19",
            "replies": [
                {
                    "author": "Slick424",
                    "body": "Racists would love nothing more then being able to point at a machine to justify their \"race-realism\". It is increasingly important to point out the racial bias of the data the machine way trained on.",
                    "score": 0,
                    "depth": 1,
                    "timestamp": "2022-06-28 10:21:58",
                    "replies": []
                }
            ]
        },
        {
            "author": "plumporter",
            "body": "It's a feature, not a bug",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:09:03",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "Well, AI is data driven",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:12:05",
            "replies": []
        },
        {
            "author": "Destinlegends",
            "body": "Sexist and racist people make sexist and racist robots.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:14:59",
            "replies": []
        },
        {
            "author": "Whaleflop229",
            "body": "The people and organizations working on AI are very aware, and have absolutely NOT decided it's OK to create these products without addressing the issue.\n\nI know (or have interacted with) many of the important people building these systems at both mega and small scale. These flaws are an endless part of the conversion, and there's clear desire to fix the problem. The core problem is the input data (as many here have stated), not the engineers or AI inherently.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:25:23",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "This is why ethics in ML is a huge topic and can effect many",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:26:39",
            "replies": []
        },
        {
            "author": "Independence_1991",
            "body": "It\u2019s only going to make decisions based on its programming\u2026 at some point the programmers will be held responsible\u2026.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:30:17",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "So are they sexist because of straight men?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:32:46",
            "replies": []
        },
        {
            "author": "Cammy1924",
            "body": "Are you f*cking kidding me? I have to deal with sexist robots!?!",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:34:50",
            "replies": []
        },
        {
            "author": "durkadurkdurka",
            "body": "Maybe the AI isn\u2019t flawed, maybe the people who get offended by a robot are flawed",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:41:34",
            "replies": []
        },
        {
            "author": "popoyDee",
            "body": "AI program development is funded by similar sexist and racist",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:44:12",
            "replies": []
        },
        {
            "author": "RevJTtheBrick",
            "body": "We already HAVE a generation of racist, sexist robots: the Republican party.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:51:59",
            "replies": []
        },
        {
            "author": "ShaitanSpeaks",
            "body": "Does that mean people who make sexist and racist decisions are flawed as well?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:55:52",
            "replies": []
        },
        {
            "author": "EdwardBil",
            "body": "This is a weird way of saying humanity is sexist and racist.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:56:58",
            "replies": []
        },
        {
            "author": "JaxckLl",
            "body": "Uh no? It\u2019s literally my company\u2019s entire job to work these issues out of algorithms.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 10:58:56",
            "replies": []
        },
        {
            "author": "basilwhitedotcom",
            "body": "When you're simulating human cognition, bigoted AI is a feature, not a bug",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:01:28",
            "replies": []
        },
        {
            "author": "undoobitably",
            "body": "Well when math is racist, asking questions is transphobic and sitting comfortably is sexist it's no wonder anything isn't -ist and confusing to AI.  It's confusing for any unbrainwashed person.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:02:14",
            "replies": []
        },
        {
            "author": "Cewu00",
            "body": "What happens when you feed your AI with data from the internet.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:03:36",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "How do you know its flawed?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:05:18",
            "replies": []
        },
        {
            "author": "TheCheeseStore",
            "body": "So the AI had a bunch of pictures of faces to choose from and was given questions like  \"choose the criminal\" and had to select one of the faces.\n\nWould an \"unbiased\" AI simply choose a picture at random then? That seems to be what the article was implying but makes no sense to me. \n\nAn intelligent AI should choose a Male face more often than a female face because in reality men are much more likely to be criminals.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:07:02",
            "replies": []
        },
        {
            "author": "_realpaul",
            "body": "Theyre all trained on stuff found on the internet. Need I say more",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:08:05",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "It may truly be worth making AI \"colorblind\", \"genderblind\", whatever.\n\nThese data points don't seem to have any real relevance to reality, or at least we should all agree they shouldn't, so why are we enabling AI to even consider the sex, gender, race, etc of humans at all?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:09:11",
            "replies": []
        },
        {
            "author": "Gilthu",
            "body": "This is dumb take, it\u2019s not a true ai, it\u2019s just a chat bot that grabs a group of chat responses and prioritizes responses that get the most interactions. \n\nPeople are \u201cteaching\u201d these \u201cai\u201d by just repeating the same chat prompts. It\u2019s the equivalent of claiming a car is racist because of the sound of its engine or something similar.\n\nThe AI doesn\u2019t have morality, emotions, or thoughts, it\u2019s just rehashing a bunch of quotes and lines like a super sophisticated auto-complete conversation.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:09:28",
            "replies": []
        },
        {
            "author": "TinFish77",
            "body": "It does seem that 'AI' is managing to copy all the negatives of humanity while providing non of that which actually would be useful; an understanding of the human world.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:17:03",
            "replies": []
        },
        {
            "author": "horrorkesh",
            "body": "Or maybe this isn't much of a leap and more Of The Logical conclusion an AI comes to",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:26:51",
            "replies": []
        },
        {
            "author": "Sinnadar",
            "body": "*Cough* Facebook *Cough*\n\nExcuse me, must be something in my throat.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:28:33",
            "replies": []
        },
        {
            "author": "million_island",
            "body": "Robots are a great excuse to put a responsibility buffer between the creators and the product. People need to consider that the robots are made by someone in a company in that companies own image. Don\u2019t let them claim ownership and then disown the product when either strategy is convenient for them. Robots are not magic. They are machines made by people.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:30:56",
            "replies": []
        },
        {
            "author": "1zzie",
            "body": "The problem is techno utopian determinism. Tech cannot solve problems of justice, or politics, because they aren't settled, there aren't binary outcomes we all agree on. That's what politics is about, debating tradeoffs where the answers to different and changing costs and benefits are not universally agreed on.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:31:50",
            "replies": []
        },
        {
            "author": "Archduke_Of_Beer",
            "body": "\"Sir, why did your robot just call me the n-word?\"\n\n\"Idk, shits and gigs?\"",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:36:07",
            "replies": []
        },
        {
            "author": "joelex8472",
            "body": "More human than human is their moto!",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:36:18",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "Why they called people \"issue\"?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:49:01",
            "replies": []
        },
        {
            "author": "Enginerdad",
            "body": "Isn't the whole purpose of AI to mimic the human mind? People, as a whole, are sexist, racist, bigoted, etc. While it's obviously an undesirable quality to have in an AI, I think it's inaccurate to say it's \"flawed\" when it's doing exactly what it's supposed to do.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:49:06",
            "replies": []
        },
        {
            "author": "IlIIlIl",
            "body": "Institutional power stands to benefit from a robot or ai that agrees with them on who is a human and who is less than human.\n\nIts as simple as that.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:49:27",
            "replies": []
        },
        {
            "author": "fi3xer",
            "body": "I mean, we already do that with people...",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:49:37",
            "replies": []
        },
        {
            "author": "DigitalSteven1",
            "body": "You'll never get an AI without bias. It's simply not possible to get enough training data that doesn't have bias because humans have bias. They get their data from humans, who really suck at being cool. We'd need to address the issue in humans before AI stands any chance at getting non-bias training data. Unfortunately, racist and sexist assholes will always exist to pollute the training data.\n\n&#x200B;\n\nFully getting rid of bias in AI would require to remove bias from humans and wait multiple generations to gather enough training data. In other words, it's literally impossible.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 11:54:45",
            "replies": []
        },
        {
            "author": "MustLovePunk",
            "body": "So basically sociopathic humans are creating sociopathic AI.\n\nEdit:\n\nBut serious question. Would including more women and non-whites in this work change anything? Much of the world has been established by men and for men, not necessarily with malicious intent, but as a natural outgrowth of their worldview, systems, needs, impulses. Or are the language of programming and algorithms established in such a way that it would be difficult if not impossible to change?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 12:05:39",
            "replies": []
        },
        {
            "author": "HateIsAnArt",
            "body": "Maybe it's not the AI that's flawed",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 12:07:58",
            "replies": []
        },
        {
            "author": "mhey10",
            "body": "There is a song \u201cYou have to be carefully taught\u201d. That\u2019s what we\u2019ll do to AI if we\u2019re not careful.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 12:27:34",
            "replies": []
        },
        {
            "author": "Bulky-Pool-5180",
            "body": "What happens when humans do not observe the outcome?\n\nIs it like the Double-Slit test where the AI is only sexist and racist when humans are watching?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 12:28:09",
            "replies": []
        },
        {
            "author": "AnotherNewSoul",
            "body": "Reminds me of that one coworker who said that they made ai take a test a bunch of times to decide what is the best way to govern.I also learned that day that he belives that N*zis are perfect. We\u2019re both Poles who live in US.\n\nAlso I have no fking idea if that thing is even real or not it\u2019s just something I\u2019ve had heard being thrown around to justify N*zis for some reason.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 12:30:13",
            "replies": []
        },
        {
            "author": "Thats_absrd",
            "body": "Reminds me of the Better Off Ted episode with the automatic lights",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 12:31:09",
            "replies": []
        },
        {
            "author": "AtuinTurtle",
            "body": "Then someone programmed them to behave like that. Crap going in equals crap coming out.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 12:58:36",
            "replies": []
        },
        {
            "author": "ScreamheartNews",
            "body": "I feel like the headline is a misleading thing, frankly speaking it's like you'd have to feed them false data to the point that it balances out, but at that point you know people would abuse it to make one side higher over the other.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 13:03:13",
            "replies": []
        },
        {
            "author": "PhilosopherDon0001",
            "body": "Please stop allowing A.I.s to connect to the internet to learn. That's not even a good place for a human to learn.   \n\n\nMaybe somehow filter and organize the information first. Create simple contexts between categories. You know, like kindergarten.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 13:47:09",
            "replies": []
        },
        {
            "author": "5mu2f4cc0unT",
            "body": "As a white middle aged male,cool",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 13:54:46",
            "replies": []
        },
        {
            "author": "Sea_Count2020",
            "body": "So we've made them in our image",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 14:00:25",
            "replies": []
        },
        {
            "author": "ceomoses",
            "body": "I think the underlying flaw is that we're using AI to make decisions for us.  It should just be used for research, or as a suggestion.  The AI makes a suggestion, then a human being with an actual brain evaluates the suggestion and decides a final outcome from there.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 14:07:20",
            "replies": []
        },
        {
            "author": "Optimal_Ear_4240",
            "body": "Just a carry on of the destroyer mindset",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 14:28:31",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "Just like the automatic soap dispenser that was racist to that black person and didn't activate and dispense soap! The Horror!!!!!!!!!!!!!",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 14:29:05",
            "replies": []
        },
        {
            "author": "darabolnxus",
            "body": "I mean why not? If givens are allowed to be pieces of trash why shouldn't AI have the right to be awful people too?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 14:29:46",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "Perhaps AI is saying sexism and racism is natural to humans.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 14:43:01",
            "replies": []
        },
        {
            "author": "ActuallyAkiba",
            "body": "\"We decided it's okay to make our decision an entire societal problem. Thank me.\"",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 14:45:13",
            "replies": []
        },
        {
            "author": "Time_Mage_Prime",
            "body": "Ahh the infinite hubris of mankind. We purport to use our flawed, limited, biased and blind-spot-filled brains to create something with greater growth and power potential than we. Anyone who expects that to go any way but apocalyptically bad is thinking with even less.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 14:51:27",
            "replies": []
        },
        {
            "author": "Telkk2",
            "body": "I'm less concerned with this and more concerned with people making bots that convince people of ideas that actually sound good but are horrible.  Contrary to media and what politicians say, Its pretty hard to convince the average person to embrace racism, today.  But convincing people to go to war or that these groups are terrorists or that this beurocratic boring policy will significantly help Americans?  Its already proven to be pretty easy.\n\nCompare that to the actual number of Americans who are legitimately full-blown racists.  Night and day.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 14:53:29",
            "replies": []
        },
        {
            "author": "MayOrMayNotBePie",
            "body": "Sounds like creating humans with extra steps.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 15:13:46",
            "replies": []
        },
        {
            "author": "coyotesloth",
            "body": "Created in the image of their makers\u2026",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 15:13:52",
            "replies": []
        },
        {
            "author": "Mssenterprise",
            "body": "This just reminds me that objects made by people are only going to be as good as the people who made them. It's very hard not to put human biases into our own work. It's going to take a long time to weed that sort of thing out.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 15:16:21",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "Alternative Title - first batch of robots are going to suck, because the first batch of anything sucks. It will get better once it\u2019s already in the market - like everything else. How many first editions have you bought and felt like a beta tester because the 2nd one was way better. The first iPad didn\u2019t have a camera for crying out loud. Is this a more serious issue? Yes. Would any amount of effort possible fix it before release? No. It may take decades to fix. You want robots that aren\u2019t racist and we are still working on humans that aren\u2019t.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 15:17:19",
            "replies": []
        },
        {
            "author": "P-p-please",
            "body": "This seems rather obvious. Just look at YouTube comments.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 15:18:05",
            "replies": []
        },
        {
            "author": "PaxNova",
            "body": "I'm not sure it can really be called an intelligence until it's able to make assumptions, form opinions (and therefore biases) and make mistakes.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 15:26:27",
            "replies": []
        },
        {
            "author": "CubaLibre1982",
            "body": "I'm Caucasian, probably out of risk, so.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 15:39:35",
            "replies": []
        },
        {
            "author": "Digiee-fosho",
            "body": "Going to be alot of damaged non operating robots, making only more e waste",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 16:00:24",
            "replies": []
        },
        {
            "author": "RhubarbElixir",
            "body": "I can,t wait to be roasted by my roomba when I walk by.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 16:01:17",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "Today I learned that sexist racism robots were an issue",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 16:03:36",
            "replies": []
        },
        {
            "author": "_ConfusedAlgorithm",
            "body": "Their solution is to come up with AI robot that will do the protest.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 16:04:51",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "Why do people keep pretending as if bigotry just magically disappeared? Stop gaslighting the world and acknowledge the fact that plenty of people are still racist, sexist, anti-lgbtqia+, etc. Yes, that means even programmers, managers, and executives. Until we're honest with ourselves about the reality we live in, we can't ever solve these problems.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 16:06:22",
            "replies": []
        },
        {
            "author": "bolderdasher",
            "body": "so basically the AI will be like most people - racist and sexist",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 16:23:35",
            "replies": []
        },
        {
            "author": "bustedbuddha",
            "body": "In this case we could make everything a lot clearer by replacing \"people in organizations\" with \"racists\"",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 16:27:38",
            "replies": []
        },
        {
            "author": "Ashmizen",
            "body": "The AI isn\u2019t sexist or racist. \n\nIt\u2019s a combination of trolls (especially if it learns from Twitter or social media), and the existence of sexist or homophobic language in common speech, which would be picked up by the AI. \n\nThere is zero intelligence in AI today - it\u2019s merely a reflect of the training data - so this isn\u2019t really as alarming as people who don\u2019t understand this thinks.\n\nWe aren\u2019t designing sexist/racist robots, nor does it mean this gets baked into the code and affect figure AI.  It\u2019s just a temporary issue due to dumb AI and bad data sets.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 16:28:20",
            "replies": []
        },
        {
            "author": "MrPuddington2",
            "body": "Organisations have decided that racist AIs are ok, because they cannot go to prison. Problem solved (for the organisations).",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 16:28:37",
            "replies": []
        },
        {
            "author": "WonofOne",
            "body": "It\u2019s not flawed to the creators of the AI",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 16:34:19",
            "replies": []
        },
        {
            "author": "jammerparty",
            "body": "Im sorry, i didnt hear your question.  Is this going to make money or not?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 16:38:52",
            "replies": []
        },
        {
            "author": "Conradfr",
            "body": "Maybe that's just the best decisions and society was right all along!",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 16:39:38",
            "replies": []
        },
        {
            "author": "ProjectNexon15",
            "body": "How tf can an AI be racist or sexist?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 16:46:26",
            "replies": []
        },
        {
            "author": "OldNewUsedConfused",
            "body": "Yes, they will always end up with bias",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 16:53:16",
            "replies": []
        },
        {
            "author": "doctorcrimson",
            "body": "The problem is we're making machines to mimic humans and not machines made to think.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 17:12:43",
            "replies": []
        },
        {
            "author": "Hapalion22",
            "body": "They work as designed.\n\nTake that how you will",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 17:19:06",
            "replies": []
        },
        {
            "author": "seriouspostsonlybitc",
            "body": "Maybe different demographics do have strengths and weaknesses and the ai is unbiased?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 17:23:34",
            "replies": []
        },
        {
            "author": "Jennyboombatz",
            "body": "\u201cPeople and organizations\u201d aka men.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 17:31:17",
            "replies": []
        },
        {
            "author": "Level-Infiniti",
            "body": "i have long suspected Tik Toks algorithm might have something like this going on. purposefully or not, who knows",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 17:59:32",
            "replies": []
        },
        {
            "author": "CaptainDemlicious",
            "body": "\u201cI must apologize for Wimplo. We have trained him wrong on purpose. As a joke.\u201d",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 18:49:10",
            "replies": []
        },
        {
            "author": "SockYourself",
            "body": "I was ready for racists to start a civil war. I was ready to steal my works AED to zap some bots. I was not ready for both.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 18:53:20",
            "replies": []
        },
        {
            "author": "Hellbog",
            "body": "A.I. Garnett.\n\n***gets coat, calls taxi***",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 18:54:57",
            "replies": []
        },
        {
            "author": "AlderonTyran",
            "body": "I believe it is a legitimate concern that people will look at this, overcorrect and we'll just get stuck with *more* sexist and racist AI...\n\nAnd if you don't believe it's possible,  just look at every institution that tries biasing to correct for what they perceive...",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 19:35:11",
            "replies": []
        },
        {
            "author": "dbaughcherry",
            "body": "Guess that's why siri can only understand white people. Not because of different dialects or patterns of speech but racism. I should have known",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 20:18:06",
            "replies": []
        },
        {
            "author": "MicrowavableToast",
            "body": "If you want objectivity in AI, it will only focus on objectivity without care for social norms and expectations. If you program an AI to not commit social faux pas by ignoring objective derivations, then you don't have an objective result; defeating the whole point of using an AI for objectivity.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 20:45:06",
            "replies": []
        },
        {
            "author": "RunItAndSee2021",
            "body": "\u2018organizations then subtly blamed technological debt left over from the nineteen sixties\u2019",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 20:53:21",
            "replies": []
        },
        {
            "author": "Seeen123",
            "body": "\u201cA theory is that in societies where women are treated poorly where they do not choose what they want to study or work but rather what empowers them\u201d that is incredibly insightful. Perhaps in societies where society were historically discriminatory against women, those societies later try to correct this by promoting representation in that field. It is also important to consider that even though Scandinavia idea had equal access to resources and education etc., the culture Around which jobs each sex chose was still around but since there was no promotion of let\u2019s say women in stem because there was less historical opresssion, there ended up being a very low amount of women in stem.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 21:16:11",
            "replies": []
        },
        {
            "author": "aaalderton",
            "body": "Can a robot/AI be racist and sexist? If given a large amount of data and it comes up with the most efficient way to complete tasks and it so happens that it has determined certain decisions more meaningful wouldn\u2019t it simply just be crude efficiency?",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 21:30:39",
            "replies": []
        },
        {
            "author": "Late_Way_8810",
            "body": "Can never forget the legend that was Tay and the Mayhem she unleashed",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 21:37:14",
            "replies": []
        },
        {
            "author": "Shamalamadingdongggg",
            "body": "It's all about being quickest to market",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 22:05:44",
            "replies": []
        },
        {
            "author": "BuckToofBucky",
            "body": "Sounds like a Freudian dev problem.  Latent sexist and racist devs.  No other explanation",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 22:10:14",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "Robots; they're just like us.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-28 22:10:48",
            "replies": []
        },
        {
            "author": "chiefchief23",
            "body": "Racist man creates racist machine. That sounds about white.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-29 00:05:16",
            "replies": []
        },
        {
            "author": "yoho808",
            "body": "What if pure statistics itself is somewhat racist against certain races and sexists against certain gender?\n\nI think it's not that the robots have flawed Ai, but rather humans are actively trying to ignore certain facts to try to create a more equal society for everyone.",
            "score": 1,
            "depth": 0,
            "timestamp": "2022-06-29 04:12:12",
            "replies": []
        }
    ]
}