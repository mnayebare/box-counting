{
    "post_title": "[D] Does RLHF really work? why do you use it?",
    "post_timestamp": "2024-04-03 15:58:25",
    "last_comment_timestamp": "2024-04-07 19:06:00",
    "time_difference": "4 days, 3:07:35",
    "comments": [
        {
            "author": "ARogueAI",
            "body": "Use Direct Policy Optimization for 99% of tasks you're probably doing. Unless you're working with some pretty insane training/data collection budgets DPO is going to be better because it doesn't require training a separate model, and is pretty similar (exactly the same at convergence actually).",
            "score": 61,
            "depth": 0,
            "timestamp": "2024-04-03 16:52:14",
            "replies": [
                {
                    "author": "Smallpaul",
                    "body": "Is there actually a definable 1% where RLHF is better? Or are you just hedging your bets out of uncertainty?",
                    "score": 6,
                    "depth": 1,
                    "timestamp": "2024-04-03 17:09:39",
                    "replies": [
                        {
                            "author": "ARogueAI",
                            "body": "Mixture of both. At the sort of cutting edge research stuff (GPT4/5, Llama, Claude, etc...) it can get more complicated. Certainly RLHF is a lot more flexible than DPO with respect to variants, you just need to make changes to the reward function, compared to having to do that + trying to rederive a similar DPO formula (can be done, but just harder). There is also a compelling argument related to the fact the on a per sample basis RLHF gives the model wayyy more information than DPO. RLHF evaluates each data point independently, compared to DPO which just compares 2 (Imagine it like RLHF tells you what's good vs bad with your sample compared to DPO which just tells you this is bad and that is good). I'm sure the big labs have all run extensive experiments on it but those results aren't public obviously. That being said if you're at the level to be in charge of the high level training procedure for a multi-million dollar model you probably know more than me and aren't posting on reddit asking for advice about it, so just use DPO.\n\n&#x200B;\n\nEdits for clarity + grammar",
                            "score": 15,
                            "depth": 2,
                            "timestamp": "2024-04-03 18:32:29",
                            "replies": [
                                {
                                    "author": "SchweeMe",
                                    "body": "Do yk how I can change DPO to work with a different type of model? For example I made a conditional GAN for image generation that I want to start doing DPO on, but all the examples of DPO are for text to media models.",
                                    "score": 1,
                                    "depth": 3,
                                    "timestamp": "2024-04-03 23:16:28",
                                    "replies": []
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "author": "spezjetemerde",
            "body": "do you have a kenyan army paid 2$ an hour?",
            "score": 118,
            "depth": 0,
            "timestamp": "2024-04-03 16:15:53",
            "replies": [
                {
                    "author": "WhichOfTheWould",
                    "body": "What kind\u2019ve dataset does it take to train the reward model? I\u2019m really a bit unsure on how much data/compute it takes for PPO to be effective in general.",
                    "score": 12,
                    "depth": 1,
                    "timestamp": "2024-04-03 16:30:55",
                    "replies": [
                        {
                            "author": "spezjetemerde",
                            "body": "For simple tasks, you might get by with a few thousand samples; complex tasks may require millions. PPO's effectiveness varies greatly with the task complexity",
                            "score": 17,
                            "depth": 2,
                            "timestamp": "2024-04-03 16:34:52",
                            "replies": [
                                {
                                    "author": "qu3tzalify",
                                    "body": "I\u2019ve always seen datasets for alignment being in the 100k\u2019s not much more. Do you know a paper using 1M+ samples?",
                                    "score": 3,
                                    "depth": 3,
                                    "timestamp": "2024-04-03 19:50:57",
                                    "replies": [
                                        {
                                            "author": "Impossible-Manager-7",
                                            "body": "Hi, could you share where you found info on datasets in the 100k for alignment?",
                                            "score": 1,
                                            "depth": 4,
                                            "timestamp": "2024-04-07 19:01:32",
                                            "replies": [
                                                {
                                                    "author": "qu3tzalify",
                                                    "body": "[https://github.com/glgh/awesome-llm-human-preference-datasets](https://github.com/glgh/awesome-llm-human-preference-datasets)\n\nMost of them are a couple of 100k's",
                                                    "score": 2,
                                                    "depth": 5,
                                                    "timestamp": "2024-04-07 19:06:00",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "author": "pm_me_your_pay_slips",
                    "body": "For a really good dataset you need Americans making a bit more than that. Evaluating text quality isn't trivial.",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2024-04-04 09:52:01",
                    "replies": [
                        {
                            "author": "spezjetemerde",
                            "body": "i was referring to what openai used\n\nhttps://time.com/6247678/openai-chatgpt-kenya-workers/",
                            "score": 2,
                            "depth": 2,
                            "timestamp": "2024-04-04 10:01:00",
                            "replies": []
                        },
                        {
                            "author": "Calm_Ad_5133",
                            "body": "Americans are possibly the worst cost to perf ratio",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2024-04-07 18:53:51",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "TransitoryPhilosophy",
            "body": "I\u2019ve been doing paid RLHF on Gemini and other Google models for over a year. They have employed 100,000 people to do it. It definitely works.",
            "score": 26,
            "depth": 0,
            "timestamp": "2024-04-03 17:39:24",
            "replies": []
        },
        {
            "author": "Status-Effect9157",
            "body": "Can you elaborate how it doesn't work? How did you test it? What were the benchmarks? \"Vibe-based\"? \n\nHow large is your preference data? Did you generate it yourself? Or did you use another dataset? Which one?\n\nWhat are your training params? Maybe there was an issue during training? \n\nFunny and frustrating that most comments went directly to giving suggestions and comments without understanding or verifying OP's claim.",
            "score": 12,
            "depth": 0,
            "timestamp": "2024-04-03 20:03:31",
            "replies": [
                {
                    "author": "belabacsijolvan",
                    "body": "!remindme 2 days",
                    "score": 1,
                    "depth": 1,
                    "timestamp": "2024-04-03 20:53:52",
                    "replies": [
                        {
                            "author": "RemindMeBot",
                            "body": "I will be messaging you in 2 days on [**2024-04-06 00:53:52 UTC**](http://www.wolframalpha.com/input/?i=2024-04-06%2000:53:52%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/MachineLearning/comments/1bv2jgm/d_does_rlhf_really_work_why_do_you_use_it/kxy00pw/?context=3)\n\n[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F1bv2jgm%2Fd_does_rlhf_really_work_why_do_you_use_it%2Fkxy00pw%2F%5D%0A%0ARemindMe%21%202024-04-06%2000%3A53%3A52%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201bv2jgm)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
                            "score": 1,
                            "depth": 2,
                            "timestamp": "2024-04-03 20:54:39",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "DEGABGED",
            "body": "Not me, but I know SynthV (a vocal synth like Vocaloid which essentially generates sung human voices from MIDI notes and lyrics) uses RLHF to improve their voice banks. Not too sure of the specifics though\n\nhttps://www.youtube.com/watch?v=ZKwGR08kCSk",
            "score": 2,
            "depth": 0,
            "timestamp": "2024-04-04 05:19:27",
            "replies": []
        },
        {
            "author": "Deleted",
            "body": "There is no human in your loop lol",
            "score": 1,
            "depth": 0,
            "timestamp": "2024-04-05 22:51:39",
            "replies": []
        }
    ]
}