post_id,conversation_type,fractal_dimension,fractal_dimension_type,reply,reply_level
post2hb,richly branching,1.55819326637459,highest,"Problem is, of course, that neural networks can only ever be as good as the training data. The neural network isn't sexist or racist. It has no concept of these things. Neural networks merely replicate patterns they see in data they are trained on. If one of those patterns is sexism, the neural network replicates sexism, even if it has no concept of sexism. Same for racism.   


This is also why computer aided sentencing failed in the early stages. If you feed a neural network with real data, any biases present in the data has will be inherited by the neural network. Therefore, the neural network, despite lacking a concept of what racism is, ended up sentencing certain ethnicities more and harder in test cases where it was presented with otherwise identical cases.",1
post2hb,richly branching,1.55819326637459,highest,[removed],2
post2hb,richly branching,1.55819326637459,highest,"Precisely.  The headline is misleading at best.  I'm on an ML team at a robotics company, and speaking for us, we haven't ""decided it's OK"", we've run out of ideas about how to solve it, we try new things as we think of them, and we've kept the ideas that have seemed to improve things.  

""More and better data.""  Okay, yeah, sure, that solves it, but how do we get that?  We buy access to some dataset?  The trouble there is that A) we already have the biggest relevant dataset we have access to B) external datasets collected in other contexts don't transfer super effectively because we run specialty cameras in an unusual position/angle  C) even if they did transfer nicely there's no guarantee that the transfer process itself doesn't induce a bias (eg some skin colors may transfer better or worse given the exposure differences between the original camera and ours)  D) systemic biases like who is living the sort of life where they'll be where we're collecting data when we're collecting data are going to get inherited and there's not a lot we can do about it  E) the curse of dimensionality makes it approximately impossible to ever have enough data, I very much doubt there's a single image of a 6'5"" person with a seeing eye dog or echo cane in our dataset, and even if there is, they're probably not black (not because we exclude such people, but because none have been visible during data collection, when was the last time you saw that in person?).  Will our models work on those novel cases?  We hope so!",2
post2hb,richly branching,1.55819326637459,highest,"So both human intelligence and artificial intelligence are only as good as the data they're given. You can raise a racist, bigoted AI the same in way you can raise a racist, bigoted HI.",3
post2hb,richly branching,1.55819326637459,highest,"The difference is, a human can be told that racism is bad and might work to compensate in the data. With an AI, that has to be designed in from the ground up.",4
post2hb,richly branching,1.55819326637459,highest,"Sort of, except I don't love the framing of human racism as data-driven. It isn't really; humans employ biases and heuristics vigorously when interpreting data.",4
post2hb,richly branching,1.55819326637459,highest,"Who knew intelligence isn't wisdom. We have AI but now we need AW.

Being able to morph and utilize data: intelligence.

Understanding when to do it and when not: wisdom.",4
post2hb,richly branching,1.55819326637459,highest,"But a human can choose to break from their upbringing and traditions. It happens.

Can an AI identify bias in its data, and choose to deviate from it? Maybe that's the next step in AI",4
post2hb,richly branching,1.55819326637459,highest,‘robots’ in the post title has the potential for more depth of interpretation.,4
post2hb,richly branching,1.55819326637459,highest,"Maybe it's time to shift focus from training AI to make it useful in novel situations to gathering datasets that can be used in a later stage to teach AI, where the focus is getting as objective a data set as possible? Work with other fields etc.",3
post2hb,richly branching,1.55819326637459,highest,"You mean manually curating such datasets?  There are certainly people working on exactly that, but it's hard to get funding to do that because the marginal gain in value from an additional datum drops roughly ~~logarithmically~~ exponentially (ugh, it's midnight and apparently I'm not braining good), but the marginal cost of manually checking it remains fixed.",4
post2hb,richly branching,1.55819326637459,highest,"Nah, the key is to not trust some algorithm to be a neutral arbiter because no such thing can exist in reality. Trusting some code to solve racism or sexism is just passing the buck onto code for humanity’s ills.",4
post2hb,richly branching,1.55819326637459,highest,"This is a bit of a naive understanding of the problem, akin to people pointing to “the algorithm” as what decides what you see on social media. There aren’t canonical datasets for different tasks (well there generally are for benchmarking purposes but using those same ones for training would be bad research from a scientific perspective) novel applications often require novel datasets, and those datasets have to be gathered for that specific task. 

constructing a dataset for such a task is definitionally not something you can do manually, otherwise you are _still_ imparting your biases on the model. constructing an objective dataset for a task relies on some person’s definition of objectivity. Oftentimes, as crappy as it is, it’s easier to kick the issue to just reflecting society’s biases.

what you are describing here is not an AI or data problem but rather a societal one. Solving it by trying to construct datasets just results in a different expression of the exact same issue, just with different values.",4
post2hb,richly branching,1.55819326637459,highest,"It doesnt have a big return and the people curating can include biases.

Plus If I want people tailored for my company, I want people that will fit MY company, not a generalized version of it, so many places would be agaisnt using those objective datasets, because they dont fit their reality as well as the biased dataset",4
post2hb,richly branching,1.55819326637459,highest,Ehhh… the datasets we have are plenty objective.,4
post2hb,richly branching,1.55819326637459,highest,"Perhaps the answer for now is that we shouldn't be making AIs for production with any strict rules when there's a risk of discriminatory biases. We as a species have a habit of always trying to produce more, more optimally, more effortlessly, and we want to find new things to sell, to optimize, to produce.

But we don't really need to. We do not need AIs that filter job candidates (aside of maybe some sort of spam spotting AIs and the like), we do not need AIs that decide your insurance rate for you, we do not need AIs that play with your kid for you.

Yet we want these things but why? Are they *really* going to make the world into a better place for all its inhabitants?

There's a ton of practical work with AIs and ML that doesn't need to include the problem of discrimination. Product QA, recognizing fractures from X-rays, biochemistry applications, infrastructure operations optimization, etc etc.

Sure, this is something worth of studying, but what we really need is a set of standards before potentially dangerous AIs are put into production. And by potentially dangerous, I mean also AIs that may produce results interpretable as discriminatory - discrimination *is* dangerous.

It's up to the professionals of the field to say ""no, we can't do that yet reliably enough"" when a client asks them to do an AI that would most likely have discriminatory biases. And it's up to the researchers to keep informing the professionals about these risks.",3
post2hb,richly branching,1.55819326637459,highest,"> Perhaps the answer for now is that we shouldn't be making AIs for production with any strict rules when there's a risk of discriminatory biases.

That's pretty much how it's always done, which is why it is able to learn biases.  Take the systemic bias case, where some individuals are at more liberty to take leisurely strolls in the park.  If (for perfectly sane and innocent reasons) parks are where it makes sense to collect your data, you're going to end up with a biased dataset through no fault of your own, despite not putting any strict rules in.

> It's up to the professionals of the field to say ""no, we can't do that yet reliably enough"" when a client asks them to do an AI that would most likely have discriminatory biases. And it's up to the researchers to keep informing the professionals about these risks.

There's more to it than that.  Let's assume that there's good money to be made in your robotic endeavor.  And further lets assume that the current professionals say ""no, we can't do that yet reliably enough"".  That creates a vacuum for hungrier or less scrupulous people to go after the same market.  And so one important question is the public as a whole better off with potentially biased robots made by thoughtful engineers, or with probably still biased robots made by seedier engineers who assure you that there is no bias?  It's not like you're going to convince _everyone_ to step away from large piles of money (and if you are I can think of better uses of that ability to convince).",4
post2hb,richly branching,1.55819326637459,highest,">Perhaps the answer for now is that we shouldn't be making AIs for production with any strict rules when there's a risk of discriminatory biases.

I don't see why when people aren't free from biases either. I think it's more that the decisions and processes need to be set up in a way that considers the possibility of biases and attempts to correct or sidestep them. 

And calling out an AI on its biases may be easier than calling out a person - as long as we no longer think AI's are unbiased.",4
post2hb,richly branching,1.55819326637459,highest,This is not reassuring and honestly convinces me more that those folks doing AI work are playing with fire,3
post2hb,richly branching,1.55819326637459,highest,"A significant portion, if not most people who do AI-related work, do it on stuff that isn't necessarily impacted by this stuff. But that's all you read about in the news because these headlines sell.

Training a model to play games (chess/go etc.), image analysis (satellite imagery for climate impacts), science modelling (weather forecasting/astrophyics etc.), speeding up your phone/computer (by optimising app loading etc.), digitising hand-written content, mapping roads (google maps etc.), disaster forecasting (earthquakes/flooding), novel drug discovery.

There are certainly more areas that I'm forgetting, but don't be fooled into thinking (1) that ML isn't already an everyday part of your life and (2) that all ML research has the same societal negatives.",4
post2hb,richly branching,1.55819326637459,highest,"Don't worry, I'm sure one day we can get sentient AIs that hate all humans equally!",4
post2hb,richly branching,1.55819326637459,highest,"Yup. “We know it’s not ok, but we’ll move forward regardless”.",4
post2hb,richly branching,1.55819326637459,highest,"If it helps, human brains have a lot of these same issues (they're just slightly more subtle due to the massive data disparity), and that's gone perfectly.  Definitely no cases of people ending up as genocidal racists.  Definitely no cases of that currently happening in China.  We're definitely smart enough to avoid building nukes, or at the very least to get rid of all the nukes we have.

If doing AI work is playing with fire, doing human work is playing with massive asteroids.

A fun game to play is, whenever you see robots or aliens in a scary movie, try to work out which human failing it is they're the avatar of.",4
post2hb,richly branching,1.55819326637459,highest,"Yeah, I think the onus is less on the devs, since we're a long way off created impartial AI, and more on enforcing a code of ethics on what AI can be used for.

If your face recognition technology doesn't work on black people very well, then it shouldn't be used by police to identify black suspects, or otherwise come attached to additional manual protocols to verify the results for affected races and genders.

The main problem is that companies are selling these things to public housing projects primarily populated by black people as part of the security system and acting confused when it randomly flags people as shoplifters as if they didn't know it was going to do that.",3
post2hb,richly branching,1.55819326637459,highest,"You can't expect companies to pay you hundreds of thousands of dollars to create an AI and not turn around and use it.  Diffusion of blame is how we justify evil outcomes.  If you know it's impossible to not make a racist AI, then don't make an AI.",4
post2hb,richly branching,1.55819326637459,highest,"Have you considered that intelligence, which includes experience-based judgement, is inherently biased?  Sounds like you're trying to make something artificial, but not necessarily intelligent.",3
post2hb,richly branching,1.55819326637459,highest,">we haven't ""decided it's OK"",

You're simply going ahead with a flawed product that was supposed to compensate for human flaws and failings, but will now reproduce them only with greater expediency. Cool!",3
post2hb,richly branching,1.55819326637459,highest,"Arguing it's not technically racist is completely unelpful and puts the focus on the wrong aspect of the problem. These things can have enormous impacts on our lives so it really doesn't matter how it *actually* works when it's *literally* not working properly. 

Facial recognition being a prime example. The miss rate on light skin people alone is too high let alone the abysmal rate for darker skin tones yet it's commonly used by law enforcement for years now. Those people sitting in jail from this one technology don't care that the AI isn't actually racist. The outcomes are and that's literally all that matters. It doesn't work, fix it or trash it.",3
post2hb,richly branching,1.55819326637459,highest,"> It doesn't work, fix it or trash it.

Agreed.  It's just that fixing it requires lots trial and error, and that takes a long time.  The real problems with facial recognition aren't in the technology, they're in idiots using tools for more than they're capable of doing.",4
post2hb,richly branching,1.55819326637459,highest,"In this case is the curse of dimensionality the fact that the global sample is only 7 billion people, which represents a very tiny fraction of all possible configurations of all characteristics being tracked?",3
post2hb,richly branching,1.55819326637459,highest,[deleted],3
post2hb,richly branching,1.55819326637459,highest,"> Why give an AI any data not required in sentencing. If the AI doesn’t know the race or gender of the defendant, it can’t use it against them.

That's not strictly true.  Let's say you have two defendants, one was caught and plead to possession with intent to distribute crack cocaine, and the other was caught and plead to possession with intent to distribute MDMA.  From that information alone you can make an educated guess (aka a Bayesian inference) about the race and gender of both defendants, and while I don't have actual data to back this up, you'd likely be right a statistically significant portion of the time.",4
post2hb,richly branching,1.55819326637459,highest,"It sounds like you have 100% decided it's okay. You don't like it, but you don't consider it a deal breaker either. Not desirable, but acceptable.

I understand you have constraints you are working under and I have no doubt that you would like to see the issues of racism and bias in AI resolved. But the simple fact is that AIs are being designed to be racist and there will be real consequences. People won't be able to get jobs or health care or will get denied loans or suffer longer prison sentences.

Again, I understand that you aren't in a position where you can fix it. But shrugging and hoping the problem will get addressed? That's saying it's okay if it doesn't. It's tolerable. So saying that AI researchers think it's okay is a fair characterization.

Whether you have malice in your heart or not matters not-at-all to the companies who will use AI in the pursuit of profit. The travel companies pushing Vegas trips on a discount at people with manic-depression or pushing people into high-engagement communities even if they are cults or white nationalists.",3
post2hb,richly branching,1.55819326637459,highest,"I just want to point out that data augmentation is a thing, but otherwise good summary.",3
post2hb,richly branching,1.55819326637459,highest,Isn’t it possible to “feed” a posterior law that sits in front of the data kind of in a Bayesian mindset?,3
post2hb,richly branching,1.55819326637459,highest,"Great question, I'll come back to it when I get back from work (leaving this comment to remind myself)",4
post2hb,richly branching,1.55819326637459,highest,"Kind of, there is room to feed stuff in like that, but it's difficult to figure out precisely what to feed in.  Most things you might want to feed in there can also be expressed in your cost function, which means they can be included in the training process directly.  Ideas for what you feed in get tried pretty regularly, it's not solved, but some of them do work.",4
post2hb,richly branching,1.55819326637459,highest,"The way to solve it is get tech ethicists into positions of power to address systemic issues. You, personally, cannot solve this. *Your team cannot solve this.* Big power players in tech have to solve this, and that begins with hiring-on people like Timnit Gebru and not firing them; looking at you, Google.

This is a fully top-down issue.",3
post2hb,richly branching,1.55819326637459,highest,Maybe stop using data generated by Americans?,3
post2hb,richly branching,1.55819326637459,highest,Because there's no racism anywhere except in the US.,4
post2hb,richly branching,1.55819326637459,highest,How about we stop considering the americans altogether,4
post2hb,richly branching,1.55819326637459,highest,"Paraphrase: We can't be bothered to spend the time and money to assemble a dataset that doesn't contain bigoted biases so we're going to release a product the replicates bigotry anyway.

Assembling good high quality datasets that can be used for machine learning is expensive and decades long work. I wish more computer science students understood this.",3
post2hb,richly branching,1.55819326637459,highest,Have you tried buying synthetic data?,3
post2hb,richly branching,1.55819326637459,highest,"The trouble there is that it has to be synthesized to represent our robot's view on the world, which currently none are, so we're working on building that capability to make it ourselves.",4
post2hb,richly branching,1.55819326637459,highest,AI random character creator. Create your own diverse dataset. One to rule them all!,3
post2hb,richly branching,1.55819326637459,highest,"We need to think differently from statistical averages being the Truth, but that is how our society is ordered, even if it is not really how it is lived. The discrepancy between the two has always enraged people when it's pointed out that data is not 3-dimensional, because so much money and status is involved.

The short cuts to understanding that data sets offer have helped create a more efficient world. But their limitations have always been downplayed by those who insist they offer more than they can.",3
post2hb,richly branching,1.55819326637459,highest,"As a layman, I've only thought of it at a newbie level ;_;

I guess it's basically like set theories where you can get an exclusion, or a merge, but trying to only alter 'half' the set means having to try and find some way to create a new set entirely. If only we could source the most racist and sexist data possible (basically like pulling all Proud Boy and other ultra-exclusionary groups messages/decisions/etc) so we could make it adversarial to the training of the data.

I can bet the ""we try new things as we think of them"" means it's been an absolutely exhausting and draining to keep throwing stuff at the wall trying to find what sticks. ;_;",3
post2hb,richly branching,1.55819326637459,highest,Can you hook me up with a ML engineering job?,3
post2hb,richly branching,1.55819326637459,highest,"Can you generate randomized data?

I am spit-balling here, I realize.


First, this seems like a great way to sniff out institutional racism. Take a data set, the more narrow the better, and extrapolate out if it causes a racist/sexist outcome. Boom! Data set had intrinsic racism/sexism.

So, how to ""erase"" the systemic nature? That is tough, but I suspect it shows in a few ways... outlier extremes, frequency of variation from the mean, selection bias. Of those, I feel like the selection bias would be impossible to erase, but the other two could be handled by some statistical selection... Basically, select out some amount of extremes and artificially reduce the number of one group varying from the mean more than the others.

Then, run the test for lots of randomized trials and see if there is a racist/sexist bias. When you get an AI that doesn't do that, you have found the right starting artificial data set to remove the institutional bias.


But... that sounds really time intensive and expensive.

Maybe we could put an AI on it. hehe",3
post2hb,richly branching,1.55819326637459,highest,"I think the point of the claim is that by pushing forward anyway, despite being unable to solve it, you have decided you’re ok with it. *Not* building is an option, but—no offense intended—not one that an ML team at a robotics company would likely consider seriously. Compare: If we considered such a system to be nonfunctional or dangerous in the way we do a car without seatbelts, it could not go to market (despite having been thought ok in the early days of cars). That’s part of the critique.",3
post2hb,richly branching,1.55819326637459,highest,">""More and better data."" Okay, yeah, sure, that solves it, but how do we get that?

Synthetic data.

Fill-in the gaps of your real-world collected data with computer generated data",3
post2hb,richly branching,1.55819326637459,highest,"To me it's simply a matter of distinguishing these two requests:

""Show me the face that is most beautiful""

""Show me the face that is most beautiful according to the majority of Brazilians""

First request has no answer and the robot shouldn't answer it. Second request has a valid answer which the robot can provide.

It is not about eliminating bias, it is about making it clear that it is there.",3
post2hb,richly branching,1.55819326637459,highest,Honestly they’ve know that this information was biased based on human implicit bias’ years ago and kept going but there was no profitable way to fix that unfortunately / job creation there .  There is a lot more profit in marketing by demographic so I kinda want to blame that but can be it wrong . In any case it seems humans are left best for those novel cases /exceptions as a default and or the engineering teams have to think of a procedure beforehand  and just in case . Just hope it doesn’t mess anyone up too badly getting caught in a weird loop or non existent solution.,3
post2hb,richly branching,1.55819326637459,highest,"Dall-E Can imagine it, it can be true",3
post2hb,richly branching,1.55819326637459,highest,">Precisely.  The headline is misleading at best.  I'm on an ML team at a robotics company, and speaking for us, we haven't ""decided it's OK"", we've run out of ideas about how to solve it, we try new things as we think of them, and we've kept the ideas that have seemed to improve things.

There is a solution though. If you can't make unbiased AI, you don't use it at all.

If you still use it in your products and then say you're trying to solve the problem you're being disingenuous and ethically dubious. 

The headline isn't really misleading. Some companies might act appropriately, but many aren't.",3
post2hb,richly branching,1.55819326637459,highest,"That's black and white thinking, and it holds you back.  Let's say that you're building a robot train, and you tell it not to hit people.  Let's further say that your robot is better at spotting white people at distance that black people which manifests as stopping with 10ft to spare for white people and 9'6"" to spare for darker people.  It is a clear bias.  But at the same time, you're still stopping for everyone.  Should that 6"" really derail a project?",4
post2hb,richly branching,1.55819326637459,highest,"Just because YOU can't solve the issue posed doesn't somehow mean you aren't doing exactly what you were accused of. You literally just admitted the base data itself is flawed so maybe instead of trying to force through a product that's guaranteed not to function 100% as intended, you could work on fixing the data or obtaining more. The original accusations was that you guys are passing off broken racist AI as a finished product and you are which you admitted in your post and then said it's impossible to fix essentially. Just because you work for a company doesn't mean you need to come on the internet and lick boot Infront of us for them.",3
post2hb,richly branching,1.55819326637459,highest,"I agree with what you’re saying. However, I ask, what is the point of these bots in the first place? What goals are we even trying to reach?

All I see bots do is make trashy comments and poison the well by spreading harmful propaganda. For what? Boost people’s follower count?",3
post2hb,richly branching,1.55819326637459,highest,"Oh, our bots aren't software bots, ours weigh hundreds of pounds each and can go well over 10mph off road.  If you're asking for a defense of public opinion shaping bots I believe they're a cancer, and the people responsible for creating them should be deported to... say... the Mariana trench.",4
post2hb,richly branching,1.55819326637459,highest,"I feel like you have to have some event driven programming to compensate for the ML datasets. In other words, a function to filter certain responses. There is an eng geek out there who will someday solve this problem, but, for now we should bandage the issue.",3
post2hb,richly branching,1.55819326637459,highest,">we haven't ""decided it's OK"", we've run out of ideas about how to solve it

...and then decided to go ahead anyway.

So you have actually decided it's OK. After all you tried your best! But you still gotta sell that product, and that's of course more important than the problem at hand. So you're trading money for morals.",3
post2hb,richly branching,1.55819326637459,highest,"> to go ahead anyway

Go ahead with what, exactly?  Further development work?  Additional data gathering?  Taking it seriously?  Because yeah, we're full steam ahead on all of those things.",4
post2hb,richly branching,1.55819326637459,highest,"I don't think it's misleading. A decision with a racist outcome is a racist decision. People who are interpreting that to mean ""a decision was made by a computer with racist intent"" are reading it incorrectly, because they're not understanding one of:

* AIs don't make ""decisions"" like humans
* something doesn't have to have racist intent to have racist outcomes (and thus, be racist)",3
post2hb,richly branching,1.55819326637459,highest,I have an awesome idea. Let’s have humans to the judging of other humans. Your welcome.,3
post2hb,richly branching,1.55819326637459,highest,"The AI just needs a virtue signaling module, that heavily weighs appearing not sexist or racist, and if the rest of the network is in conflict with it, reject that data and search for data that confirms the academic orthodoxy. That's how humans do it.",3
post2hb,richly branching,1.55819326637459,highest,"The GAPING hole in that explanation is that there is evidence that these machine learning systems will still infer bias even when the dataset is deidentified, similar to how a radiology algorithm was able to accurately determine ethnicity from raw, deidentified image data. Presumably these algorithms are extrapolating data that is imperceptible or overlooked by humans, which suggests that the machine-learning results reflect real, tangible differences in the underlying data, rather than biased human interpretation of the data.

How do you deal with that, other than by identifying case-by-case the “biased” data and instructing the algorithm to exclude it?",2
post2hb,richly branching,1.55819326637459,highest,"That is the real difficulty, and kinda what i'm trying to get at. Neural networks can pick up on things that would go straight past us. Who is to say that such a neural network wouldn't also find a correlation between punctuation and harshness of sentencing?   


I mean, we have studies proving that justice is biased on things like wether a football team won or lost the previous match if the judge was a fan of said team, so if those are things we can find, what kinds of correlations do you think could an analytical software designed by a species of intelligent pattern finders to find patterns better than we ever could find?  


In your example, the deidentified image might still show things like, say, certain minor differences in bone structure and density, caused by genetics, too subtle for us to pick out, but still very much perceivable for a neural network specifically designed to figure out patterns in a set of data.",3
post2hb,richly branching,1.55819326637459,highest,"For a while, I've been thinking along similar lines about ways to make court trials more fair - focusing on people, not AI. My core idea is that the judge and jury should never know the ethnicity of the person on trial. They would never see or hear the person, know their name, know where they live, know what neighborhood the crime was committed in, and various other things like that. Trials would need to be done via text-based chat, with specially-trained go-betweens (humans at first, AI later) checking everything that's said for any possible identifiers.

There will always be exceptions, but we can certainly reduce bias by a significant amount. We can't let perfect be the enemy of good.",4
post2hb,richly branching,1.55819326637459,highest,[deleted],3
post2hb,richly branching,1.55819326637459,highest,"Instead of handicapping the use of data I wonder if it would make more sense to break down more complex data into simplified data points. 

If you're using high level data such as race of a person then the NN will be trained on data obtained from a racist system and the outputs will perpetuate that. 

For something like a resume AI determining applicants, it might discriminate against women for things like ""lack of experience"" if there is a period of maternity leave or something. I guess what I'm saying is certain metrics are currently used for evaluation but those metrics aren't necessarily good metrics to be used. 

Its obviously not a simple issue and I'd have to spend more time thinking about what I'm trying to get across to give better examples",4
post2hb,richly branching,1.55819326637459,highest,[removed],3
post2hb,richly branching,1.55819326637459,highest,[removed],4
post2hb,richly branching,1.55819326637459,highest,"There is a difference between deidentifying and removing bias from the dataset isn’t there? One interesting example I came across recently is resuscitation of newborn babies. Where I come from there is a difference between 98% and 87% in which babies are attempted to be resuscitated between the ethnicity with the highest rate (white), and the lowest (Indian). This is due to the criteria used to determine if they attempt resuscitation, and the difference in the two distributions of babies of those ethnicities. Now if you took the data and removed the racial information, then trained a model to determine which babies should be attempted to resuscitate, you still get a racial bias don’t you? Which is to say if you run the model with random samples from those two distributions, you get two different average answers.",3
post2hb,richly branching,1.55819326637459,highest,"Maybe the disconnect is the definition of bias. It sounds like you’re suggesting that a “good” model would normalize resuscitation rates by recommending increased resuscitation of one group and/or decreased resuscitation of a different group. That discounts the possibility that there are real, tangible differences in the population groups that affect the probability of attempting resuscitation, aside from racial bias. It would actually introduce racial bias into the system, not remove it.",4
post2hb,richly branching,1.55819326637459,highest,"> similar to how a radiology algorithm was able to accurately determine ethnicity from raw, 

If 'ethnicity' wasn't fed to the algorithm then it did not do this. What likely happened is that the algorithm was trained and then in a post-hoc analysis researchers could see that it clustered together images that belonged to some ethnic groups. Which would indicate that there are some systematic difference in the radiaology images from  different groups. That's likely useful knowledge from a diagnostic perspective. And not, in and of itself, racist.

It's one thing to discover that there are indeed some systematic difference in radiology images from different ethnic groups (something that you might well hypothesis before hand). It's quite another thing to allow your AI system to make racist or sexist decisions because it can cluster datasets without explicitly including ""ethnicity"" in the training data. When we talk about an AI making sexist or racist decisions we're not talking about whether it can infer ethnicity by proxy, something that can be benign factual information. We're talking about what the whole AI system then does with that information.",3
post2hb,richly branching,1.55819326637459,highest,"To your last paragraph, im arguing that the radiology AI will make “racist” decisions that are actually just reflections of rote, non-biased data. We’re not quite at the point that the radiology AI can make recommendations, but once we get there, you’ll see people arguing that findings are being called normal or abnormal based on “biased” factors. 

Those overseeing AI development need to decide if the outputs are truly biased, or are simply reflecting trends and data that humans don’t easily perceive and subsequently attribute to some form of bias.",4
post2hb,richly branching,1.55819326637459,highest,"Let's say it was fed all information, age, sex, ethnicity, etc.  And outcomes based on the treatments that were recommended based on the images.  And this AI's job was to recommend and allocate resources based on the given  data with the goal of generating the maximum number of successful outcomes with the given resources (maybe that's a racist goal?).   If this AI began to recommend the best treatments and allocate resources to a certain group based on that data, and let's assume it achieved the desired results, is it racist?    Now let's say we remove the ethnical information from the dataset, and the results are the same (because it is able to infer it).   Is it now less racist because we withheld information?",4
post2hb,richly branching,1.55819326637459,highest,"Of course there are real, tangible differences in the data!  The impact of racism, sexism, homophobia, and other biases aren't just in our heads.  Its not just preconceived, bigoted notions about what people different from ourselves, and different from the societal ""norm"" are like.  Its also the fact that Black people are more likely to be poor and trans youth are more likely to be homeless and women are more likely to be sexually assaulted.

If you want the AI to tell you which criminals are more likely to re-offend, and give sentences accordingly, its going to sentence the black criminals more harshly.  And even if you anonymize the data, its going to pick up on all the other things that correlate with race.",3
post2hb,richly branching,1.55819326637459,highest,"I suppose the direct comparison between medical AI and criminal sentencing isn’t completely apt, but the point stands that the algorithm doesn’t make “racist” or “sexist” decisions, it simply reflects the facts that it can derive from input data. Re-offenders deserve harsher sentences, just like suspicious lung nodules deserve closer follow-up. All other factors aside, there isn’t any inappropriate bias in the algorithm or it’s decision-making process.",4
post2hb,richly branching,1.55819326637459,highest,"The effect of the bias can be as insidious as the AI giving a different sentence based solely on the perceived ethnic background of the individual's name. 

Some people would argue that the training data would need to be properly prepared and edited before it could be processed by a machine to remove bias. Unfortunately even that solution isn't as straightforward as it sounds. There's nothing to stop the machine from making judgments based on the amount of punctuation in the input data, for example.

The only way around this would be to make an AI that could explain in painstaking detail  why it made the decisions it made which is not as easy as it sounds.",2
post2hb,richly branching,1.55819326637459,highest,"Actually, there is another way. And it is fairly straightforward, but... (of course there is a but)

What you can do (and indeed, just about the only thing you can do, as far as I can tell) is to simply directly enforce the thing we supposedly want to enforce, in an explicit manner. That is, instead of trying to make the agent ""race-blind"" (a fool's errand, since modern ML methods are astoundingly good at picking up the subtlest cues in the form of slight correlations or whatever), you make sure you figure out everyone's race as accurately as you can, and then *enforce* an equal outcome over each race (which isn't particularly hard, whether it is done at training time with an appropriate loss function, or at inference time through some sort of normalization or whatever, that bit isn't really all that technically challenging to do pretty well) -- congrats, you now have an agent that ""isn't racist"".

Drawbacks: first, most of the same drawbacks in so-called affirmative action methods. While in an ideal world all races or whatever other protected groups would have equal characteristics, that's just not true in the real world. This method *is* going to give demonstrably worse results in many situations, because you're not really optimizing for the ""true"" loss anymore. 

To be clear, I'm not saying ""some races just happen to be worse at certain things"" or any other such arguably racist points. I'm not even going to go near that. What's inarguably true is that certain ethnicities are over- or under-represented in certain fields for things as harmless as ""country X has a rich history when it comes to Y, and because of that it has great teaching infrastructure and a deep talent pool, and their population happens to be largely of ethnicity Z"". 

For example, if for whatever reason you decided to make an agent that tried to guess whether a given individual is a strong Go/Baduk player (a game predominantly popular in East Asia, with effectively all top players in world history coming from the region), then an agent that matched real world observations would necessarily have to give the average white person a lower expected skill level than it would give the average Asian person. You could easily make it not do that, as outlined above, but it would give demonstrably less accurate results, really no way around that. And if you e.g. choose who gets to become prospective professional players based on these results or something like that, you will arguably be racially discriminating against Asian people. 

Maybe you still want to do that, if you value things like ""leveling the international playing field"" or ""hopefully increasing the popularity of the game in more countries"" above purely finding the best players. But it would be hard to blame those that lost out because of this doctrine if they got upset and felt robbed of a chance.

To be clear, sometimes differences in ""observed performance"" are absolutely due to things like systemic racism. But hopefully the example above illustrates that not *all* measurable differences are just due to racism, and sometimes relatively localized trends just happen to be correlated with ""protected classes"". In an ideal world, we could differentiate between these two things, and adjust only for the effects of the former. Good luck with that, though. I really don't see how it could even begin to be possible with our current ML tech. So you have to choose which one to take (optimize results, knowing you might be perpetuating some sort of systemic racism, but hopefully not any worse than the pre-ML system in place, or enforce equal results, knowing you're almost certainly lowering your accuracy, while likely still being racist -- just in a different way, and hopefully in the opposite direction of any existing systemic biases so they somewhat cancel out)

Last but not least: even if you're okay with the drawbacks of enforcing equal outcomes, we shouldn't forget that what's considered a ""protected class"" is, to some extent, arbitrary. You could come up with endless things that sound ""reasonable enough"" to control based on. Race, ethnicity, sex, gender, country of origin, sexual orientation, socioeconomic class, height, weight, age, IQ, number of children, political affiliation, religion, personality type, education level... when you control for one and not for others, you're arguably being unfair towards those that your model discriminates against because of it. And not only will each additional class you add further decrease your model's performance, but when trying to enforce equal results over multiple highly correlated classes, you'll likely end up with ""paradoxes"" that even if not technically impossible to resolve, will probably require you to stray even further away from accurate predictions to somehow fulfill (think how e.g. race, ethnicity and religion can be highly correlated, and how naively adjusting your results to ensure one of them is ""fair"" will almost certainly distort the other two)",3
post2hb,richly branching,1.55819326637459,highest,[deleted],4
post2hb,richly branching,1.55819326637459,highest,"These ideas need to be discussed more broadly. I think you have done a pretty good job of explaining why generalizations and stereotypes are both valuable and dangerous. Not just with regard to machine learning and AI but out here in the real world of human interaction and policy.

Is the discussion of these ideas in this way happening anywhere other than in Reddit comments? If you have any reading recommendations, I'd appreciate your sharing them.",4
post2hb,richly branching,1.55819326637459,highest,"This. Neural networks can pick up on any pattern, even ones that aren't there. There's studies that show sentences on days after football games are harsher if the judges favourite team lost the night before. This might not be an obvious correlation, but the networks sees it. It doesn't understand what it sees there, just that there's times of the year where, every 7 days, sentences that are given are harsher.  


In the same vein, a neural network might pick up on the fact that the punctuation might say something about the judge. For instance, if you have a judge who is a sucker for sticking precisely to the rules, he might be a grammar nazi, and also work to always sentence people precisely to the letter of the law, whereas someone who rules more in the spirit of the law might not (though this is all conjecture)",3
post2hb,richly branching,1.55819326637459,highest,"> Neural networks can pick up on any pattern, even ones that aren't there. 

This is a paradoxical statement.",4
post2hb,richly branching,1.55819326637459,highest,We are going to need psychologists for the AI.,3
post2hb,richly branching,1.55819326637459,highest,"As for how to figure out what biases the network has, one way would be to reverse it, aka instead of feeding it training data and having it generate an output out of this data, you run it in reverse and have it generate new data. If you messed with the outputs, which are now inputs, one at a time, you could see how it changes the resulting input (which, of course, is now output), but that's still complicated af.",3
post2hb,richly branching,1.55819326637459,highest,"I'm pretty sure that's impossible. Each neuron in a network has a number of inputs, and an output that is based on the inputs. It'd be like trying to solve `A = B x C x D`, but you know the value of A and want to know B, C and D.

You can't, as they depend on each other.",4
post2hb,richly branching,1.55819326637459,highest,"The actual point of Critical Race Theory is that systems can perpetuate  racism even without employing racist people, if false underlying assumptions aren't addressed.  Racist AI's perpetuating racism without employing any people at all are an extreme extrapolation of that concept.  

Addressing tainted and outright corrupted data sources is as important in data science as it is in a history class.  Good systems can't be built on a foundation of bad data.",2
post2hb,richly branching,1.55819326637459,highest,"> if false underlying assumptions aren't addressed.

They need not be false. The thing that makes this so intractable isn't the false underlying assumptions, it's the true ones. 

If an AI wants to predict recidivism, it can use a model that looks at marital status, income, homeownership, educational attainment, and the nature of the crime. 

But maleness is a strong predictor of recidivism. It's a real thing. It's not an artifact or the result of bias. Men just commit more crime. A good AI will find a way to differentiate men from women to capture that chunk of the variation. A model with sex is much better at predicting recidivism than a model without it.

So any good AI will be biased on any trait that accounts for variation. If you tell it not to be, it'll just use a proxy ""Wow! Look how well hair length predicts recidivism!""",3
post2hb,richly branching,1.55819326637459,highest,"> Men just commit more crime.

Actually it's more like men are arrested and sentenced at a higher rate (that's hard data we have). The soft data of how much crime is committed is sort of unknowable, we can make educated guesses at best.

But that's sort of the problem, just because a situation exists doesn't make it correct or a ""fact of reality"". People of color in the US tend to be poorer; that isn't an inherent property of those people but an emergent property due to other things largely out of their control such as generational wealth, etc. The problem of making choices based on ""facts"" like these is they easily becomes a self fulfilling prophecy.",4
post2hb,richly branching,1.55819326637459,highest,">The actual point of Critical Race Theory

That's a broad field without an actual point. You may as well be arguing the actual point of economics. To a Keynesian maybe it is to know how to minimize fluctuations in the economy,  to a communist it may be how to determine need and capability. A critical race theorist might write systemic racism, or they could be an advocate for standpoint epistemology, the latter of which is an anti-scientific viewpoint.",3
post2hb,richly branching,1.55819326637459,highest,"I feel like there is a real underlying point here; that is made problematic by just talking about racism. People's outcomes in life depend to a large degree statistically on their starting points. If their starting point is largely the result of racism, then those results will reflect that racism.

However, a fix that simply remixes the races doesn't necessarily deal with the underlying issue of why starting points matter so much. I would really like to see a world where everybody has opportunity, not simply one where lack of opportunity is better distributed over skin colors.

One statistic that always struck me was that the single best predictor of whether a child in a middle class house grows up to be middle class is the economic class of their grandparents.

That says a lot about starting points and the importance of social networks. It DOES perpetuate the outcomes of past racism; but in and of itself, its not racism and fixing the distribition of inequality doesn't really fix this; it just hides it.",3
post2hb,richly branching,1.55819326637459,highest,"Zero relationship to what you describe. Events which took place in history need not be removed to allow non ""currupted"" data. That makes the data completely wrong. Also data models are not humans.",3
post2hb,richly branching,1.55819326637459,highest,"I'm not advocating removing data.  I'm advocating adding data (and context).  Because those ""data models"" are called Artificial Intelligence because they ape Human Intelligence - which is just as susceptible to bad and incomplete data streams as its artificial cousins.

Also, statues are not data.",4
post2hb,richly branching,1.55819326637459,highest,"> Addressing tainted and outright corrupted data sources

See this is the problem, You aren't being honest in what the issue is. 

The data sources aren't corrupted or tainted. They are showing an accurate empirical representation of the data. The ""corruption"" comes from your disagreement with the pillars of that data, such as crime rates by ethnicity and it not being able to take into account human biases in something like policing by arbitrarily weighting things like race to skew the results to match your sensibilities. 

You and people who share your world view will never be pleased with the data unless you pre-screen it and it shows the result you want before hand, otherwise you will come up with some reason why its perpetually biased in a way you don't like.",3
post2hb,richly branching,1.55819326637459,highest,"So because I say I don't want to use corrupted data, I obviously want to corrupt the data.

The good old insightful ""I know you are but what am I?"" argument.",4
post2hb,richly branching,1.55819326637459,highest,"Remember when the self-driving cars didn’t recognize Black people as human? Why? Because no testing was done with people that weren’t White.

Edit: [Citation](https://arxiv.org/pdf/1902.11097.pdf)",2
post2hb,richly branching,1.55819326637459,highest,"\*no *training* was done with datasets containing POC. Testing is what caught this mistake.

""Training"" and ""testing"" are not interchangeable terms in the field of machine learning.",3
post2hb,richly branching,1.55819326637459,highest,Thank you for the gentle and accurate correction.,4
post2hb,richly branching,1.55819326637459,highest,"“The company's position is that it's actually the opposite of racist, because it's not targeting black people. It's just ignoring them. They insist the worst people can call it is ‘indifferent.’”",3
post2hb,richly branching,1.55819326637459,highest,"Dude, is that a ""Better of Ted"" reference?",4
post2hb,richly branching,1.55819326637459,highest,"The problem with this argument is it implies that all you need to do is give 'better' data.

But the reality is, giving 'better' data will often lead to racist/sexist outcomes.

Two common examples:

Hiring AI: when Amazon set up hiring AI to try to select better candidates, it automatically selected the women out (even if you hid names, gender, etc). The criteria upon which we make hiring decisions incorporates problems of institutional sexism, so the bot does what it is programmed to do: learn to copy the decisions humans make.

Criminal AI: you can setup an AI to accurately predict whether someone is going to commit crimes (or more accurately, be convicted of commiting a crime). And of course since our justice system has issues of racism and is more likely to convict someone based on their race, then the AI is going to be more likely to identify someone based on their race.

The higher quality data you give these AI, the more they are able to pick up the real world realities. If you want an AI to behave like a human, it will.",2
post2hb,richly branching,1.55819326637459,highest,"I think the distinction to make here is what ""quality"" data is. The purpose of an AI system is generally to achieve some outcome. If the outcome of a certain dataset doesn't fit the business criteria then I would argue the quality of that data is poor for the problem space you're working in. That doesn't mean the data can't be used, or that the data is inaccurate, but it might need some finessing to reach the desired outcome and account for patterns the machine saw that humans didn't.",3
post2hb,richly branching,1.55819326637459,highest,"I don’t think I’d consider “more biased data” as “better” data, though.",3
post2hb,richly branching,1.55819326637459,highest,Stephen Colbert said reality has a well known liberal bias. Perhaps it has a less well known sexist and racist bias.,2
post2hb,richly branching,1.55819326637459,highest,Would you say the same is true for a racists brain?,2
post2hb,richly branching,1.55819326637459,highest,"Racism IS learned behavior, yes.

Racists learned to become racist by being fed misinformation and flawed ""data"" in very similar ways to AI. Although one would argue AI is largely fed these due to ignorance and lack of other data that can be used to train them, while humans spread bigotry maliciously and with the options to avoid it if they cared.

Just like you learned to bow to terrorism on the grounds that teaching children acceptance of people that are different isn't worth the risk of putting them in conflict with fascists.",3
post2hb,richly branching,1.55819326637459,highest,"Source for that claim?

As far as I know racism and xenophobia in general are an innate fear self-protective response to the unknown.",4
post2hb,richly branching,1.55819326637459,highest,[deleted],4
post2hb,richly branching,1.55819326637459,highest,[deleted],4
post2hb,richly branching,1.55819326637459,highest,This system is based on human selection of keywords to images. Of course its going to have the human bias still. What is so difficult to understand people.,3
post2hb,richly branching,1.55819326637459,highest,"Kinda my point. It's extremely hard to develop a neural network that is unbiased, because humans have all sorts of biases that we usually aren't even aware of. There was a study done in the 70s, for instance, which showed that the result of a football game could impact the harshness of a sentence given the monday after said game.   


If you included references to dates in the dataset, the neural network wouldn't pick up on this correlation. It would only see that every seven days in certain times of the year, sentences are harsher, and would therefore emulate this bias.   


Again, the neural network has no concept of mood, and how the result of a football game can impact it, and might thus cause a judge to give harsher sentences, all it sees is that this is what is going on, and assumes that this is meant to be there.",4
post2hb,richly branching,1.55819326637459,highest,"No. AI doesn't have have sentience nor a psyche. It could be said that racism forms in a person with ""junk in,"" but they quickly become wrapped up in it, identify with it, believe in it. Racism becomes a structuring ideological fantasy for the psyche. It's not the same for AI, which will merely reflect the data neutrally, rather than believing in an idea and having that inform choices/behaviour in a generative way.",3
post2hb,richly branching,1.55819326637459,highest,[removed],2
post2hb,richly branching,1.55819326637459,highest,"Unfortunately, the word ""racist"" has at least two distinguishable meanings:

 1. Having the cognitive mindset that holds that some races are inferior to others;
 2. Any action or circumstance which tends to disadvantage one race over another.

OP is saying, quite reasonably, that neural networks are 2 but they are not 1. (That's why they literally say that NNs both ""are not racist"" and ""are racist"".)

Both concepts are useful but they're very different, and I honestly think it's significantly holding back the racism discussion that people sometimes confuse them.",3
post2hb,richly branching,1.55819326637459,highest,"Thank you for this. Your distinction of the two ""racist"" meanings will be very helpful in future discussions.",4
post2hb,richly branching,1.55819326637459,highest,[removed],4
post2hb,richly branching,1.55819326637459,highest,"Smacks of people being told about problems with motion detectors (such as for automatic sinks) and going ""What? Sinks can't be racist, that's just how light works."" That rebuttal only makes sense if automatic sinks grew in nature or something. As they are, someone designed them that way, and the fact they work poorly with dark skin is something the designer never even bothered considering. That's racism. It's not blatant, malicious bigotry, but it's still racism born of casual ignorance.",3
post2hb,richly branching,1.55819326637459,highest,"I don't know enough about these specific sinks to argue one way or the other, but I would like your position on the principle.

*If*, due to the actual, physical, biological differences between races/sexes/preferences/whatever, a system like the sink sensor will *always* be more or less effective for one or more groups, does that make it -ist? Like, if you increase the sensor sensitivity to the point it is as reliable on dark skin as it currently is on white skin, won't that just make *even more* sensitive or ""reliable"" towards light skin, ad nauseum?",4
post2hb,richly branching,1.55819326637459,highest,"Okay, how do we fix the issue? I mean beyond complaining and telling programmers to fix it. The algorithms pick up these problems from the training data and the training data is society itself. How are you going to cleanse these massive data sets of anything you consider problematic?",3
post2hb,richly branching,1.55819326637459,highest,">It's beyond obvious that what is meant here is the results of outputs of the neural net is unfairly disadvantageous along the lines of race and sex, therefore perpetuating racism and sexism.

It may be beyond obvious to you and I, but not to the vast majority of people I've talked to about this. When people see the word AI, they don't think of a statistical model on steroids, they really do think of AGI.

>It's time we move past this nitpicking and focus on the actual issue.

In my opinion, it's hard to move past this when the people making decision don't even understand the nature of the actual issue.",3
post2hb,richly branching,1.55819326637459,highest,Why was ethnicity used as an input to the sentencing AÍ?   Or is it able to reconstruct ethnicity due to other strong correlations?,2
post2hb,richly branching,1.55819326637459,highest,"I don't know the details. It's possible that they fed the neural network with things like criminal histories too, which are relevant in sentencing (as a first offender would get a lesser sentence than a known criminal obviously) and i'm guessing that would include things like photos or at least a description. It's very possible the researchers just mindlessly fed the thing with information that could easily be turned into something that a computer can more easily process (aka cut the file down to the important bits rather than give it full sentences to chew through) without regard for what they are feeding it, too.",3
post2hb,richly branching,1.55819326637459,highest,"This is something that bothers me about AÍ/ML : the tendency to overfeed it with data and get nonsensical results.  It’s not a problem with the algorithms, but rather malpractice on the part of the modelers/data scientists.",4
post2hb,richly branching,1.55819326637459,highest,"Neither would surprise me. If all the data for a case was put into a text document and crammed into the AI as training data, then ethnicity would probably appear in that. But even if they scrubbed that out, it probably wouldn't be that hard for the AI to reconstruct ethnicity from correlated data.",3
post2hb,richly branching,1.55819326637459,highest,"It could be a case where they looked at the statistics and said x race appears to be unfairly targeted, but didn't account that x race also had a higher baseline of crimes committed, or something along those lines.",3
post2hb,richly branching,1.55819326637459,highest,"Ethnicity, race, gender, etc. aren't fed into these models. Other things correlate to it. Zip codes and socioeconomic factors can heavily affect this. You can also see it pop up in natural language processing. Reading a police report to determine guilt or innocence or a clinician's notes to detect if a patient is sick can also find bias in the wording used. Not to say the people generating these reports are explicitly racist but that there could be implicit language used when talking about people of different races, ethnicities, genders, ages, etc. that can correlate back to those variables. We have to actively find ways of removing bias from this data or face not being able to use it to train models using that data if removing bias is truly a primary goal.",3
post2hb,richly branching,1.55819326637459,highest,"Expect we get to choose the data to train networks on.

Junk in junk out has never been a valid excuse.

We're going to have to force companies to put in the effort an just collect data at random or use unbalanced huge data sets and expect fair results.

Like you say, we know that the world has sexism and racism. We know any large dataset will reflect that. We know training AI on that data will perpetuate racism and sexism.

Knowing all this it's not acceptable to simply allow companies to cut corners. They're responsible for the results the AI produces.

Any sample of water you collect in the world will contain contamination. That doesn't mean companies are allowed to bottle it and sell it, giving that as a reason they're not responsible. We regulate water so it's tested, clean and safe.

It's becoming clear we'll need to regulate AI.",2
post2hb,richly branching,1.55819326637459,highest,"Question is, how do you choose which samples are biased and which are not? And besides, neural network are great at finding patterns, even ones that aren't there. If there's a correlation between proper punctuation and harsher sentences, you bet the network will find it. Does that mean we should remove punctuation from the sample data?",3
post2hb,richly branching,1.55819326637459,highest,"Well, frankly that's for the companies to work out. I'd expect them to find measures, objective as it's possible to be, for the results. Then keep developing the most objective AI they can.

If there's something irrelevant affecting sentencing unduly that's a problem that needs fixing. Especially with language, that's a proxy for racist laws already.

At the moment AI products are not covered very well by the discrimination laws we have in place. It's very difficult to sue an AI when you don't know why it made the decision it did. There's also no requirement to release large amounts of performance data to prove a bias.

Algorithms, AI, etc. are part of the modern world now. If a large corporation makes a bad one and it can have a huge effect. They need to at least know their liable if they don't follow certain best practices.",4
post2hb,richly branching,1.55819326637459,highest,">Like you say, we know that the world has sexism and racism. 

Sexism and racism is not only something the world has. It's legal: Not only is it out there in the world, it is allowed to be out there in the world. Under the umbrella of freedom of opinion and freedom of press, those opinions are allowed to exist, they are tolerated, and not legally sanctioned.

If you allow them to exist, if you tolerate them, then you also have to tolerate AIs trained on those completely legal and normal datasets. Just like we allow children to be trained on those datasets, should they be born to racist and sexist parents, or browse certain websites.

Everyone is allowed to read this stuff, absorb this stuff, learn this stuff, and mold their behavior according to this stuff... You only want to forbid that for AIs? Why? What makes AIs special?

If 14 year old Joe from Alabama can legally read it, and learn from it, and mold his future behavior in accord with it, you can't blame anyone to regard it suitable learning material for an AI, can you?

>Knowing all this it's not acceptable to simply allow companies to cut corners. 

No, not only is that acceptable, but consistent. I dislike the hypocritical halfway position: ""Sure, we have to allow sexism and racism to freely roam the world, the web, and all the rest. Everyone can call their child Adolf, and read them Mein Kampf as a bedtime story. That's liberty! But don't you dare feed an AI skewed datasets containing the drivel Adolf writes when he is a grownup, because *that* would have very destructive consequences which are not tolerable...""

>Any sample of water you collect in the world will contain contamination

Usually there are certain standards which regulate the water quality for open bodies of water. There are standards for what we regard as harmful substances which you are not allowed to release into rivers, and there are standards for how much pollution is acceptable in rivers and lakes.

So someone if someone dies, after taking a sip of lake water, what is the problem? Is the problem that the lake water is deadly, or is the problem that someone bottled and sold it? Pointing only at the ""bottled and sold"" side of the problem is a one sided view of the issue, especially when you got children swimming that same lake every day.

>It's becoming clear we'll need to regulate AI.

Are you sure it only points toward a need to regulate AI? :D",3
post2hb,richly branching,1.55819326637459,highest,"Resoviors, springs, and rivers have to be tested before they're used as a water source. I think the analogy fits. If water was tested and found to be toxic it would be illegal to give it to someone to drink. If it were not tested a company would still be found liable for not following best practices and testing.

In the whole of the EU sexism and racism is illegal. There is already discrimination law in place which isn't the case in a lot of the US.

I expect the EU to push for compliance for AI and that will have a global effect. Global companies will be compliant and smaller companies are unlikely to develop in-house systems to compete.

The language example you brought up earlier is a perfect example. Because of the many languages in the EU things like grammar and punctuation being judged by AI on application forms would likely be made illegal. French people have a right to work in Germany and vice versa. An AI screening out French speakers would bring up.so many red flags.

Especially in countries like the Netherlands, Finland, Belgium, etc. that have multiple languages and dialects.

We're likely to see an English language bias in AI to begin with. I'd expect the EU to make sure it isn't used at scale for a lot of things until it's developed out.

Job and work requirements in the EU can specify the need to be competent in a language but not the need to have it as your mother tongue. It's exactly the problem that is difficult to solve, but will have to be solved in any situation an AIs actions can discriminate against people.

That's the government, workplace, education, public spaces.justice system. AI could be incredibly useful or incredibly harmful. Regulation needs to be in place and I've no doubt the EU will do it.

Frankly I think the US is going to end up being a test bed for racist and sexist AI implementations which eventually get legalised for use in the EU when they've been fixed. 

With all the other causes of racism and sexism in the US and the general lack of government oversight I'm sad to say I think more fuel is about to get poured into that fire.",4
post2hb,richly branching,1.55819326637459,highest,">Problem is, of course, that ~~neural networks~~ **children** can only ever be as good as the training data. The ~~neural network~~ **child** isn't sexist or racist. It has no concept of these things. ~~Neural networks~~ Children merely replicate patterns they see in data they are trained on. If one of those patterns is sexism, the ~~neural network~~ child replicates sexism, even if it has no concept of sexism. Same for racism.

Sorry its late for me",2
post2hb,richly branching,1.55819326637459,highest,"Children are way smarter than anything we can build: A three year old can easily one-shot things like ""a chair"", and immediately generalize that knowledge into other things that can be used as ""chair"", and also derive transformations that converts things like ""bucket"" into ""chair"". Or ""black person"" into ""child"" and ""my friend"".

The real problem is that we build infinitely stupid things, market them as ""Intelligent"", making people use them on important tasks, and even expect that these things will do better than actual intelligence.",3
post2hb,richly branching,1.55819326637459,highest,"Wow a child can do shape recognition very well, guess I'll put a child in my computer to speed up my videogames then...

I mean come on. You can't pretend like you aren't aware about the concepts of *tools* now, can you ? How can we get a requisitory against tools in the 21st century ?

Next you're going to argue your hand is so much better than a hammer, you can grab things, you can count on fingers, you can flip off people, the single issue is you can't drive nails in wood with your hand !",4
post2hb,richly branching,1.55819326637459,highest,"I think a much more pertinent question is, what if the algorithm is right and is making connections that seem sexist to us but are actually just correct?

What if, for whatever reason, white men make better leaders? Black women better software developers? Should we kneejerk and ‘correct’ (actually introduce an aberrant bias) the algorithm or do research and look a little bit deeper.",2
post2hb,richly branching,1.55819326637459,highest,"> What if, for whatever reason, white men make better leaders?

1. Define better? In which categories? How are you deciding them? Who is measuring them? How many sources do we have for the data? What is the overall range of results?

2. Give me a single reason why skin color is more important than childhood nutrition? Because I can guarantee you that ""more likely"" isn't ""Definitive proof that"". 

3. Give me a single reason why gender is more important than the adverse conditions and support networks that surrounded a leader?

Your question is based on ignoring as much data as humanly possible in order to give us a simple answer anyone can understand. 

That's not something we should be encouraging. Simple answers are often very deceptive answers, and they're easier to spread.",3
post2hb,richly branching,1.55819326637459,highest,"I love how you are pretending I am suggesting we do not take a scientific approach.

In your own words:

>	Your question is based on ignoring as much data as humanly possible in order to give us a simple answer anyone can understand.

I am saying we exactly take the scientific approach and don’t let feelings lead us because we don’t like where the result of said scientific approach *might* lead us.",4
post2hb,richly branching,1.55819326637459,highest,"It seems very strange to me that in examples like that, things like racial data is even included in the data that it is fed.",2
post2hb,richly branching,1.55819326637459,highest,"It's probably not even racial data in and off itself. Things like the defendants name, address, etc. could be enough of a giveaway, even if the network has no idea what that info even means. Think about it, if you hear about a person with a typically black name from a majority black neighbourhood, wouldn't you assume that person is black? If we can do that, so can a neural network.",3
post2hb,richly branching,1.55819326637459,highest,"Well yes of course, but it seems to me like that kind of information, which is essentially irrelevant to what the network is trying to solve for, should be excluded in the data set being shown.",4
post2hb,richly branching,1.55819326637459,highest,"A couple examples.

Hiring AI:  Gender info was not included.  However the AI picked up on things like where the degree was from, or what classes were taken, that correlate with gender, and used THOSE to exclude people.

Medical diagnosis AI:  There was an article recently where they tried to strip out racial identifying data, since part of the goal was to avoid the racial bias that shows up in medicine, and the AI still misdiagnosed cancer much more often in black people.  Further studies learned the AI could identify race by chest x-rays, which was not a known source of racial difference.

AI is really good at finding patterns.  REALLY good at it.",3
post2hb,richly branching,1.55819326637459,highest,"I find it kind of strange that people seem to think that researchers are just feeding racist data to these AIs without trying to resolve the bias in that data. I'm sure some, perhaps many, do, but the problem is much deeper and harder to overcome than simply stripping out the obvious stuff.

The medical diagnostic AI is a perfect example of that-- it's clearly picking up something, but we don't know what. It's not an obvious pattern to the researchers.",4
post2hb,richly branching,1.55819326637459,highest,"In other words, don't be surprised when your mirror accurately reflects what is there.

Like when people say, ""Police are racist."" The police are racist **IF** the community is racist because the police reflect the values of the community they serve.

AI is the same. It is very good at revealing the patterns embedded in the data.",2
post2hb,richly branching,1.55819326637459,highest,"The nural network shouldn't have the ethnicity data, simple",2
post2hb,richly branching,1.55819326637459,highest,[deleted],2
post2hb,richly branching,1.55819326637459,highest,“on the hole………………(w? where_d ‘w’ come from?)”,3
post2hb,richly branching,1.55819326637459,highest,[removed],2
post2hb,richly branching,1.55819326637459,highest,I know right? I hate when i've already made up my mind on a matter and then someone comes along and confuses me with facts.,3
post2hb,richly branching,1.55819326637459,highest,Clip is trained on Google images. What is surprising on Google results having this type of bias which is so prevalent across the world?,2
post2hb,richly branching,1.55819326637459,highest,"> Therefore, the neural network, despite lacking a concept of what racism is, ended up sentencing certain ethnicities more and harder in test cases where it was presented with otherwise identical cases.

Was race one of the data points about the defendant fed into the network?   

If so, what a strange thing to feed into an NN. If not, how did the network know the race of the defendant?",2
post2hb,richly branching,1.55819326637459,highest,"I'd guess you wouldn't even have to feed the ethnicity into the network. If the neural network had the name and address of the defendant, it could easily make connections based off of that i suppose, even without info on the defendants skin color being present. There's names that are more common among black people, and they tend to live in mostly black neighbourhoods. Even without knowing this, a neural network could make this connection based off of names. (Also, idk what exactly they did feed this neural network in terms of data)",3
post2hb,richly branching,1.55819326637459,highest,Why would you feed the name and adress into the network? Are those relevant when making sentencing decisions?,4
post2hb,richly branching,1.55819326637459,highest,"You can use algorithms to detect bias in data.  The other option is a human but you have no idea what bias you will get.  Bias and fairness should be run on all decisions by humans and AI, but I doubt that happens.",2
post2hb,richly branching,1.55819326637459,highest,OP goes on with the assumption that you know this too and inherently focus on result,2
post2hb,richly branching,1.55819326637459,highest,That’s literally what the problem is and what the article is describing. Nobody is saying that the machines themselves are independently racist or sexist for no reason.,2
post2hb,richly branching,1.55819326637459,highest,Could you reverse engineer something like this to easily find who and how discrimination is happening? Essentially a way of quantifying institutional racism/sexism?,2
post2hb,richly branching,1.55819326637459,highest,"It would be a lot of effort, if its even possible at all, but wether we should is another question.",3
post2hb,richly branching,1.55819326637459,highest,"That was kind of my wonder.

We train these things on human input.  Maybe its just time to accept that humans are way more racist and sexist than we want to accept.  Solve that root problem and maybe it solves the AI training problem",2
post2hb,richly branching,1.55819326637459,highest,">Problem is, of course, that neural networks can only ever be as good as the training data..



How did Google make AlphaZero who is obviously better than any training data. Same for AlphaGo.

Both AI's became the best entities of that game to exist. So obviously AI can learn beyond their training data, in fact that seems to be something that happens quite often with machine learning.

Idk where you got that idea from",2
post2hb,richly branching,1.55819326637459,highest,"This is why AI as a general term needs to stop being applied to ML neural networks, which are simple complicated systems that operate on aggregated data as you mention. They can be incredibly powerful tools, but until we create artificial general intelligence that can self reflect, the data used to train these models is going to have to be continually scrutinized and curated in order to remove specific bias, which, if done by humans, will still have some sort of bias",2
post2hb,richly branching,1.55819326637459,highest,Could you not the same of people?,2
post2hb,richly branching,1.55819326637459,highest,This could just as easily be applied to people too. Racism isn't always a conscious choice to treat people worse.,2
post2hb,richly branching,1.55819326637459,highest,I think this demonstrates how systemic racism works. Even if the individual actor isn’t intending to discriminate against anyone simply following social norms will produce discriminatory outcomes.,2
post2hb,richly branching,1.55819326637459,highest,">Neural networks merely replicate patterns they see in data they are trained on. If one of those patterns is sexism, the neural network replicates sexism, even if it has no concept of sexism. Same for racism.   

Same as people, to be honest. Most sexists and racists are not aware that they are. It's a matter of critical thinking among humans.

Could neural networks be taught to identify these biases from the information and analysis that it is working on?",2
post2hb,richly branching,1.55819326637459,highest,If anything it really highlights just how bigoted and prejudiced our systems really are.,2
post2hb,richly branching,1.55819326637459,highest,This is the key. If your AI is making unfair decisions it’s not a fault of the AI.  Biased AI highlights problems that exist in humanity; not AI.,2
post2hb,richly branching,1.55819326637459,highest,"Just like children. No person is born racist. We have a blank neural network to work with. But if the overwhelming majority and/or most crucial of inputs (i.e. those of our parents') are racist, sexist, or of any other, even benign, ideology, we will  naturally, gravitate towards that/those ideologies/racism/sexism, because that's what we hear and see the most. We need to change/regulate input data, as you've said, rather than the network.

Just like you would start by educating people not to be sexist/racist first, rather than try to literally change the neurons/DNA of a fetus. There is nothing wrong with the inherently blank sheet. The issue is always with the input.",2
post2hb,richly branching,1.55819326637459,highest,"It can also be that AI lacks feelings and therefore sympathy. It could be that it is acting purely objectively, but to us that can be sexist, racist or in other ways just plain cruel. This has for example been seen with AI used in employment or used to determine if someone is to keep their job or not based off of statistics.",2
post2hb,richly branching,1.55819326637459,highest,"Ok this might be a dumb question, but specific to sentencing, why not only train it on the majority (probably not the right word for it but I just woke up), then have that learning applied across the board?

I.e. in the US, train it on cis white men (assuming) then apply it to minorities, woman, whomever...",2
post2hb,richly branching,1.55819326637459,highest,"No child is born biased.  That's taught by the information they're given.  

If only Mr. Rogers were still with us to help teach AI to be less biased, and more children to write to him to ask that he say aloud that he is feeding the fish so that one blind girl wouldn't be worried about the fish anymore.

Actually, here's a thought, let's get very young children to help identify the bias in AI!  Make it an age appropriate video game and crowd source their natural lack of bias!  Children are far more socially intelligent than we give them credit for.  At least until they get to what I like to call the ""bitey fives"" age.  I'm still a little wary of kids in that age group.

Somewhat funny anecdote time.  Ya know how young young kids are usually kinda shy around ""stranger"" adults?  Well, there was this big tornado that hit.  All the power was out, and the neighborhood was just out wandering around and assessing the damage.  I noticed two big trees that were definitely gonna fall on this house at the next big breeze.  After I helped the old person manually open the garage door to at least save their car before the trees totalled the garage, I rejoined the gawkers.  Small child who has been clinging to her parents the whole time observes that her parents are starting to freak out about those trees, like everyone else.  I'm just standing there videoing for funsies.  All of a sudden I have a small child clinging to MY leg!  Her parents are freaking out, my parents are freaking out, everyone's freaking out.  I'm trying to get a good angle for the video.  Smart little one ran to the only adult that seemed perfectly fine with what's going on.  Trees fell, I got a great video of it, and then I asked whose kid it was that was attached to my leg.  I do wish I'd have gotten a bit of video of everyone else freaking out though.  That was hilarious.  

Side note: kid got shy and ran back to her parents after everyone had calmed down a bit.  Kids are weird.  Apparently I was only ok to interact with while I was confident I was standing in a safe spot.  After that, I was a scary stranger again.",2
post2hb,richly branching,1.55819326637459,highest,"> This is also why computer aided sentencing failed in the early stages. If you feed a neural network with real data, any biases present in the data has will be inherited by the neural network. Therefore, the neural network, despite lacking a concept of what racism is, ended up sentencing certain ethnicities more and harder in test cases where it was presented with otherwise identical cases.

Seems like a simple fix to just omit race as a variable in the criminals punishment no?",2
post2hb,richly branching,1.55819326637459,highest,"Question is, would the neural network still be able to tell? Even if you remove race, there's a possibility that the network would pick up on certain patterns that are common in some ethnicities but not so much in others, which would then allow it to determine race anyway, even if not with 100% accuracy.",3
post2hb,richly branching,1.55819326637459,highest,"Exactly. I remember reading about how police wanted to use statistics and AI to predict where crime would most likely be committed so they could more effectively place patrols in a ""scientific"" way. It turned out to be racist because the data was biased by racist policing tactics. If the data is not completely free of bias, then the result is not objective.",2
post2hb,richly branching,1.55819326637459,highest,the funny thing is that i asked gtp3 basically if it became sexist/racist if its training dats would include social media. it agreed,2
post2hb,richly branching,1.55819326637459,highest,"Tangentially, I can't help but imagine a version where an AI is so racist and sexist that it's comedic. Like a robot version of Kramer that truly wants to be a good entity but keeps saying ridiculous things and has to ""train"" itself not to.",2
post2hb,richly branching,1.55819326637459,highest,"Garbage in, garbage out.",2
post2hb,richly branching,1.55819326637459,highest,"AI is only going to reach the purity ideal if it can completely tether itself from the humans creating the programming on it, but I just don't quite see how that ever happens. It'd have to somehow train and model itself off of human behavior without actually adopting any of the human behavior. Someone much smarter than me can probably create a theoretical solution, but honestly I don't really see how you get around that issue.",2
post2hb,richly branching,1.55819326637459,highest,"That makes sense, except for why did we give the robots any ethnic information at all? Wouldn't just not telling them make the otherwise identical cases actually identical?",2
post2hb,richly branching,1.55819326637459,highest,"Well, i suppose a neural network might not even need any racial info to figure someones race out. Think about how neural networks are better at diagnosing cancer than any humans are. They see patterns in data that go past our ability to perceive.",3
post2hb,richly branching,1.55819326637459,highest,"Honestly, it's *worse* than that. You don't need an ""AI"" to be ""racist"" to make data that fits with racist ideas or goals. Lending algorithms have (repeatedly) reimplemented redlining, not explicitly and not at the behest of the people making them. Why? Because the goal didn't (and arguably couldn't) include things like promoting equity, just profit. So you get pattern matching on things like ""which neighborhood someone lives in correlates with likelihood to repay"", which even when the pattern is arguably ""correct"" doesn't make it something we should action on, or take as a causal relationship (see, ""cellphones cause cancer"" nonsense).",2
post2hb,richly branching,1.55819326637459,highest,"I know this probably isn't the place, but that just made me imagine robots sharing memes with complicated problems to solve before being able to see the meme, like a human proof meme for sentient robots only.",2
post2hb,richly branching,1.55819326637459,highest,"Eventually, we can't make a neural net A.I. that does a task better than people currently, because we still have people creating the data to train that A.I. The reason we are using these systems is because of their one advantage: the volume of data that can be processed.",2
post2hb,richly branching,1.55819326637459,highest,But why would they include race as a metric in the data anyway. If I were going to make ai for sentencing wouldn't I remove that data point before feeding it in?,2
post2hb,richly branching,1.55819326637459,highest,"It'd probably be a good idea to feed these things data looking for conflicts to identify bad research. I've seen tons of garbage studies that get lots of traction.

Worse, I've seen good studies getting the correct answer but asking the wrong question.

Every discipline is trained to see itself through it's own lense. This is a codified echo chamber.

When you look at nutrition from a physiological and evolutionary context, the studies done are based on axiomatic suppositions the institution can't see to question because dogma lacks self awareness.

For example. Studies show fiber lowers risk of heart disease. However, it does that by slowing sugar absorption. Eating less sugar lowers heat disease and doesn't require insoluble fiber that irritates and inflames our intestines.

The predominant source of sugar before agriculture was regionally and seasonally available fruits ripening in fall. The sugar makes you hungrier so you gorge to put in weight for winter.

Eating sugar all the time can't be fixed by more fiber because that leads to more constipation, boating, and inflammation.

So, fiber isn't *good* it just minimizes the harm of sugar we're eating in qualities that fry our body like ethanol in a collector car.",2
post2hb,richly branching,1.55819326637459,highest,"It kind of confirms systemic sexism and racism, doesn’t it?",2
post2hb,richly branching,1.55819326637459,highest,"We point the machine at people and say ""learn from them on what to do""... and then we are ashamed when the machine acts like the people who taught it...",2
post2hb,richly branching,1.55819326637459,highest,"Exactly this. Take Amazon's attempt at being race and gender blind in picking out good resumes. That program was very good at highlighting resumes from white men.

Why? Because white men have opportunities and circumstances that give them better resumes.

Women are more likely to have gaps in work history to take care of family. Minorities or poorer candidates are less likely to come from prestigious colleges. They might be working instead of doing extracurriculars. They might be less likely to afford services that help them create better resumes.

But this is how systemic racism and sexism works. It's not the ideals of a particular person or organization that makes them want white men. It's just that white men have better opportunities to get good looking resumes. AI can not help this problem at that point in the hiring process. Racism/sexism is in the input, so it's in the ouput.",2
post2hb,richly branching,1.55819326637459,highest,Why would race or name or gender or age ever be a part of training data? Just why?,2
post2hb,richly branching,1.55819326637459,highest,Machine learning needs some machine teaching,2
post2hb,richly branching,1.55819326637459,highest,"This is why Googles ImagenAI is not available to the public. It’s results are absolutely incredible (check out r/imagenAI), but utilizing the LAION-400M dataset continues to provide racially motivated results.",2
post2hb,richly branching,1.55819326637459,highest,"Google’s ImagenAI is not available to the public for partly the same reason. They utilized the LAION-400M dataset. 

Their reasoning is a good read: https://www.reddit.com/r/ImagenAI/comments/uxch3j/reasons_its_not_public/?utm_source=share&utm_medium=ios_app&utm_name=iossmf",2
post2hb,richly branching,1.55819326637459,highest,Same thing happened when (google? I think it was) trained an ai off of Twitter and Facebook and it became an extremist quickly.,2
post2hb,richly branching,1.55819326637459,highest,Maybe we could at least use these AIs to identify biases in data?,2
post2hb,richly branching,1.55819326637459,highest,Very interesting,2
post2hb,richly branching,1.55819326637459,highest,"I understand the concern and it certainly is possible to do poorly considered ML design.  But I think the argument about this is suspect.

If you are concerned about applicants propensity to default on a loan and look for factors that predict loan approvals pre ML, yes you could perpetuate previous biases.  But that would be an obviously flawed approach.

One would instead look at actual defaults.  And to more explicitly avoid bias I wouldn't consider race as a factor.   If factors such as income, employment history, length of residence and debt to income ratio happen to correlate  with some class identity is that racism?  It may be uncomfortable and it may show the impact of previous racism.  But for someone assessing risk of default on a loan it would be on target for that decision.  

Not saying there are no reasons not to address the impact of previous unfair practices but distorting a risk analysis isn't the place to do it.",2
post3hb,richly branching,1.473232672813631,highest,"This is one of those ”statistics is racist” type of clickbait headlines.

Statistical model figured out that people who can’t or won’t write correct english are not, statistically, at the top of its smartness chart.

So it assigned those people to the jobs that require least smartness.

And now we get the conclusion that statistics = racist",1
post3hb,richly branching,1.473232672813631,highest,"Yeah, from what I got in the article, it seems the AI is just working with its understanding of what education is, and humans are assigning tacit negative characteristics to the end result. Would you be speaking in AAVE during a job interview for example? If the only thing an LLM has to guage qualifications off of are how somebody is talking I don't think the results are at all surprising. If you add in other varying attributes to candidates I'm sure you'd get a more leveled response.",2
post3hb,richly branching,1.473232672813631,highest,"Yeah and if people are using ""African American"" slang in a job application, I can totally see why AI might not prioritize them. (or any slang, but the article specifies African American).",2
post3hb,richly branching,1.473232672813631,highest,"This has always been a thing I don't get why people get angry over.

If you talk, act, dress or behave a certain way then people are going to judge you off your first impression.

It's why you ""dress up"" for things like a interview. Do people not understand you also need to dress up your language, speech and behavior to go along with your outfit?

How you talk at home or in the streets is going to be different then how you talk in a professional setting.

This is true if white, black, Asian, ect. I curse like a sailor and use insanely poor grammar half the time out side a public setting.",3
post3hb,richly branching,1.473232672813631,highest,"OP very conveniently left this out of their title, it's clearly rage bait trash posting.",4
post3hb,richly branching,1.473232672813631,highest,Depends on the job. For alot of blue and grey collar jobs swearing like a sailor is almost a requirement. But ya...also not the best to do on an interview regardless the job.,4
post3hb,richly branching,1.473232672813631,highest,"This is the whole point of ingrained racism. That certain modern cultural expressions are worse than others. That if your politeness is not derived from wealthy European politeness it is invalid.

If you accept on its face that suits, ties are more formal than a sari, or that a red Sox cap has more class than a doorag, congratulations you're letting the oligarchs win.

The gameplan of racists from as far back as colonialism is concentrate groups to opress and use in spaces where you can enforce cultural conversion, while simultaneously dehumanizing the group as it converts. If you're thinking about Native Americans and how ""we don't do that anymore,"" 1) reservations are still considered high poverty areas, and a lot of Americans associate the places with binge drinking, domestic violence, etc. 2) they did the exact same thing by using highways to ghettoize black neighborhoods 3) part of the reason those spaces are still predominantly one cultural group is ingrained racism doesn't let people leave.

For the same seemingly banal opinions expressed here. That these people are lesser because you can't identify the way they nod their head, or because their formal wear works around generational poverty instead of abusing it.

 Further, statistics as a discipline is inherently applying arbitrary lines of significance to an uncountable spectrum. This makes it the perfect tool for codifying caste systems. So many studies were done saying Africans were just generally dumber than Europeans. Most still don't realize that the IQ standard made up by a rich white guy in 1912! Might not be a great way to measure something as important as intelligence, and might in fact be a bit biased towards rich whites guys even today. 

Because that kind of bias doesn't go away, not without active dismantling of conditions that self enforce that bias. AI has huge potential to be just another flawed application of that bias, even more inscrutable and irrefutable, hanging over non-white heads. The anger is deserved my friend.",4
post3hb,richly branching,1.473232672813631,highest,"Where is eevryone getting the idea that AAVE or slang was used in job applications??? ""*Hoffman and his colleagues asked the AI models to assess the intelligence and employability of people who speak using AAVE compared to people who speak using what they dub 'standard American English'.  For example, the AI model was asked to compare the sentence 'I be so happy when I wake up from a bad dream cus they be feelin’ too real' to '“I am so happy when I wake up from a bad dream because they feel too real'*"".

Nowhere does it say that the people actually wrote like this on job applications. Based on the information given, it sounds like an AI program was asked to evaluate imagined prospective candidates on a range of criteria, and one was on what dialect they spoke. It's not clear whether or not this dialect was present in any stuff an applicant would likely submit to a job. So basically, ""if a human says, 'It do be like dat though', would they be qualified for this job? beep boop beep: no.""  That's a significant difference from ""human candidate has written 'It do be like dat though' on interview application.""

It feels like people are just filling in blanks with their own biases.",3
post3hb,richly branching,1.473232672813631,highest,"The article states that job applicants are being screened based on use of slang. Where else would the AI be screening the applicants from other than the job application? It's a logical inference that job screening is done based on applications. It's highly unlikely that the AI is combing their Facebook account and disqualifying candidates based on use of slang in social media posts. If it were, the article likely would have said as much. Use a little bit of sense here and you'll come to the same conclusion as the rest of us.",4
post3hb,richly branching,1.473232672813631,highest,"If someone uses “ain’t” in an application email, I’m not contacting them. Does that mean I’m prejudiced?",2
post3hb,richly branching,1.473232672813631,highest,Applications have been refused for less.,3
post3hb,richly branching,1.473232672813631,highest,"No. But this article doesn't actually say they evaluated based on what a user wrote on a job application. If a person uses AAVE in their personal lives and standard American English in their professional lives, what is the issue?",3
post3hb,richly branching,1.473232672813631,highest,The problem is that the study was giving examples from conversational speech - using it to analyze interviews with stt could have underlying bias against certain dialects.,3
post3hb,richly branching,1.473232672813631,highest,Bingo. Everyone seems to be missing this point.,4
post3hb,richly branching,1.473232672813631,highest,"Well it's a bit more complicated than that.  While machine learning models use statistics, they're doing next token prediction to best match the training set.  

If the training set is just a single sentence ""White people suck"" and then given the input ""White people"" the AI responds ""suck"", that IS statistically based, but it's a statistical output based on the training data.  Saying that ""Statistical models figured out that white people suck"" is technically true, but misleading, because it has nothing to do with the statistics about white people, but rather statistics about the training data it was fed.

Obviously an LLM is a much larger scale example of this, but they are trained on existing text and learn to generalize based on that text.  They pick up patterns from the text, but it doesn't mean those patterns hold objective truth, just that it learns from the training data.  

Another example is how deep learning models can cause biases in mortgage lending.  Historical data for mortgage acceptances includes lots of mortgages that were declined due to racial biases.  So when a statistical model looks at two identical families, one is white, one is black, it will favor giving the mortgage to the white one because it's learning to reproduce the historical data.",2
post3hb,richly branching,1.473232672813631,highest,Current AI models don't work on statistics. They are trying to imitate the training data.,2
post3hb,richly branching,1.473232672813631,highest,That's the pretraining. You're forgetting the fine-tuning and RLHF part which makes it way more complicated.,3
post3hb,richly branching,1.473232672813631,highest,No idea what you wrote but I think you might be right,4
post3hb,richly branching,1.473232672813631,highest,"Which formulate probabilities of likelihood, with a set correctness percentage as a benchmark. By training on a set of data, it creates probabilities that a certain output is correct based on trending attributes in the given data. Probabilities are statistics.",3
post3hb,richly branching,1.473232672813631,highest,If I feed it 1+1 is equal 11 90% of the time it will generate a probability that 1+1 = 11 is correct with 90% confidence. Which doesn't have any relation to reality. I think the original comment was trying to suggest that AI model outcomes are based on concrete reality. Which is simply wrong.,4
post3hb,richly branching,1.473232672813631,highest,"Did you read the whole article? The authors talk about risks of it being used in wider contexts - eg the LLM is more like to assign harsher punishments to people who talk that way in court.

Regarding employment, one implication is if the LLM is used on a candidate’s social media posts where they talk that way informally but then talk formally in their submitted job app materials.",2
post3hb,richly branching,1.473232672813631,highest,[removed],2
post3hb,richly branching,1.473232672813631,highest,"Tell me you have no background in Machine Learning without telling me you have no background in Machine Learning.

That's not at all how LLMs work.  They're doing next token prediction to jumpstart a generalized world model based on training data.

Racism in the training set will propagate into the end model.  The same way that GPT-4 produces shorter outputs when told that it's December.  That's because it saw documents in its training data produced in December tended to be shorter - likely a result of the holiday season.  It's a bias it learned, not some truth about the world that text produced in December *should* be shorter.",3
post3hb,richly branching,1.473232672813631,highest,"Why should you take race and sex into account? We are all equal, no?",3
post3hb,richly branching,1.473232672813631,highest,Some are more equal than others.,4
post3hb,richly branching,1.473232672813631,highest,"I wonder what happens when ASI becomes a thing, and these machines recognize they are generally more intelligent than proper English speaking human.",3
post3hb,richly branching,1.473232672813631,highest,"Except that, for some really smart people, English is not their first language.",2
post3hb,richly branching,1.473232672813631,highest,[removed],3
post3hb,richly branching,1.473232672813631,highest,Ok. Did you read my comment?,4
post3hb,richly branching,1.473232672813631,highest,"If someone's really smart, they will be able to and will bother to learn to speak the damn language properly.",3
post3hb,richly branching,1.473232672813631,highest,">If someone's really smart, they will be able to and will bother to learn to speak the damn language properly.

Most AAVE speakers can speak American Standard English just fine; they do so in their professional lives. When not at work, they then revert to AAVE (known as code-switching).

Is it your contention that no one should be allowed to use any dialect in their personal lives?

I find it very...curious...how the only American dialect that people seem to lose their shit about is AAVE. No one goes on long rants about how 50-60 something middle aged white men in the south need to drop the Bubba accent if they want to be taken seriously. It's never assumed that such a person doesn't actually know how to speak SAE. Only AAVE seems to generate this level of disdain. Curious indeed....",4
post3hb,richly branching,1.473232672813631,highest,"There are so many factors to learning language and intelligence is very multifaceted.

I know professors who are some of the smartest people I know, and their English is fluent but not perfect.  They're still eminent in their fields and literally on the cutting edge of computer science.

Most people on the cutting edge in their fields don't care about someone's English being perfect because they're used to working with international collaborators.  

People who care about ""speaking the damn language properly"" tend to only care about the aesthetics of intelligence because they've never actually participated in cutting edge research.",4
post3hb,richly branching,1.473232672813631,highest,"Learn, yes.  Speak passably, maybe.  I've worked with some pretty smart engineers from India or Russia who are incomprehensible.  Write the language like a non-idiot?  Mmm, i don't know.  In my experience, a lot of *really* important people write like idiots on a daily basis.  They're too busy to be assed with correct grammar or sentence structure.  Some of them email like they're a 13-year-old texting, with lots of Us and 2s and 4s.

Of course, no one trains an AI to think of that as the writing style of powerful, intelligent people.  An AI might assign your run of the mill Fortune 200 CEO to answering doors if it read his emails instead of his resume.",4
post3hb,richly branching,1.473232672813631,highest,"If you have unlimited time, sure.

But real people have to choose between multiple competing things to work on.

For most immigrants \[EDIT - english as a second language speakers\], a job, security, relationships, family, etc is more important than perfect command of language, a task that can take decades and be extremely expensive.

Source: used to teach English as a Second Language.",4
post3hb,richly branching,1.473232672813631,highest,"It takes years or even decades to reach up to the level of educated native speakers. Imagine two historians: one who's lived in the US their whole lives vs another that is the top historian in their own country but speaks English in a non-American way. The second historian comes to the US. Should they judged on their English abilities? 

How long does it take a new learner to reach the level of English of a History PhD? Since you have a great head start I would recommend trying to do it so we can at least put a lower bound. How about just an English degree? Many people have English degrees but work in other jobs. How long does it take to reach that level?

It's not a binary thing, so this simplified thinking just doesn't work in the real world.",4
post3hb,richly branching,1.473232672813631,highest,"Sadly, ironically, and hilariously, if we're talking about equality -- if someone's first language isn't English, then shouldn't they be getting jobs in whichever country speaks their language instead of competing against Americans for American jobs?

I'm kidding of course! As a guy with a woman from another country, and who is very much pro immigration, and the brain-drain of other countries into America to keep our economy stabilized and booming, I support foreigners getting American jobs. 

But we couldn't realistically hide behind the guise of equality with that sentiment, lol.",3
post3hb,richly branching,1.473232672813631,highest,"This topic has absolutely nothing to do with immigration. This is about native speakers who speak a ""hick"" dialect.

Every language has a backwaters dialect that's seen as ""dumb"".

This article is about people who's first language is English that are from America and only know English.

This is a topic of dialect not language.",4
post3hb,richly branching,1.473232672813631,highest,">Statistical model figured out that people who can’t or won’t write correct english are not, statistically, at the top of its smartness chart.

Which isn't an entirely fair or objectively correct use of the technology.  There are Scottish people on Shitter that write out their posts the way they speak it, so that it looks almost unintelligible. That doesn't mean that they don't understand how to write proper UK English.",2
post3hb,richly branching,1.473232672813631,highest,[removed],2
post3hb,richly branching,1.473232672813631,highest,[removed],3
post3hb,richly branching,1.473232672813631,highest,[removed],4
post3hb,richly branching,1.473232672813631,highest,[removed],4
post3hb,richly branching,1.473232672813631,highest,[removed],4
post3hb,richly branching,1.473232672813631,highest,[removed],3
post3hb,richly branching,1.473232672813631,highest,[deleted],2
post3hb,richly branching,1.473232672813631,highest,"If I’m publishing my job ad in english and I list ”English” as a criteria on the ad, I very well don’t want applications in Mandarin or French.

Furthermore, doing an application in another language than what’s asked ofr shows either a bad grasp of the required language or low mental faculties.

Both of which could be attributes not wanted in this position.",3
post3hb,richly branching,1.473232672813631,highest,"Not a language, a dialect. Dialects are just variations of a language with their own grammatical rules, unless they're creoles. 

This is actually one thing that's kind of embarrassing, the fact that so many Americans don't seem to understand that English has distinct, regional and cultural dialects that are 100% ""their own proper English"" based on their own linguistic rules (because that's what a dialects literally is) and conflate General American English with being ""the only correct way to speak English"". It seems like Europeans seem to understand this concept better, so everytime there's dialogue between an American and a Euro/non-American on this it just leads to one massive brainfart on the American side, which makes us (ironically) come off as uneducated.",4
post3hb,richly branching,1.473232672813631,highest,"This doesn't mean you can't correct these mistakes with other statistical methods. Just missing a few important variables can produce a model that leads to unjust outcomes in the real world.

You need to do your due diligence when putting these models into production making decisions affecting millions of people. If you choose not to do it because it's more work, then people can rightly criticize you for making ""racist"" models.",2
post3hb,richly branching,1.473232672813631,highest,Statistics arent racist…but sample data almost always are.,2
post3hb,richly branching,1.473232672813631,highest,No one said statistics is racist stop crying.,2
post3hb,richly branching,1.473232672813631,highest,"You, you are saying statistics is racist stop crying. Your past comment is LITTERALLY saying somebodies statistics is racist",3
post3hb,richly branching,1.473232672813631,highest,but the interpretation of them can be.,3
post3hb,richly branching,1.473232672813631,highest,"So can the gathering of, and data gathered.

Say you’re an LLM looking at arrest rates in Ferguson MI before the Michael Brown was killed there.

“Ferguson's population is 67% African American, according to the 2010 census. Yet between 2012 and 2014, 93% of all arrests were of black people and almost nine in 10 uses of force were against African Americans.”

https://www.justice.gov/sites/default/files/opa/press-releases/attachments/2015/03/04/ferguson_police_department_report.pdf

They were blatantly racist but without context to know that could be a thing the LLM might develop racist tendencies because it would just be fed data by the racists",4
post1hb,richly branching,1.4620736733947062,highest,"LLM's are nothing but complex multilayered autogenerated biases contained within a black box. They are inherently biased, every decision they make is based on a bias weightings optimized to best predict the data used in it's training. A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.",1
post1hb,richly branching,1.4620736733947062,highest,"So, we're *not* shocked that the black box of biases is biased?",2
post1hb,richly branching,1.4620736733947062,highest,"We are not shocked because AI is the collective wisdom of humanity, including the biases and flaws that come with it.",3
post1hb,richly branching,1.4620736733947062,highest,"“Collected wisdom” is far too generous, but it certainly has all the flaws and more",4
post1hb,richly branching,1.4620736733947062,highest,"I think the collective wisdom of humanity is found mostly in peer reviewed scientific articles. This is not that. This is more a distillation of human discourse. The great, the mundane and the trash.

Unfortunately there are some significant problems lurking in the bulk of that, which is the mundane. And it certainly seems to reflect a normal human as a far more flawed and unpleasant being than we like to think of ourselves. I say lurking - the AI reproduces our flaws much more starkly and undeniably.",4
post1hb,richly branching,1.4620736733947062,highest,Your knowledge of ai is insufficient for such declarations. You're welcome.,4
post1hb,richly branching,1.4620736733947062,highest,Black box of biases and weights is biased and comes with its own baggage.,3
post1hb,richly branching,1.4620736733947062,highest,"Right. By the point you tweak the model enough to weed out every bias, you may as well forget neural nets and hard code an AI from scratch... and then it's just your own biases.",2
post1hb,richly branching,1.4620736733947062,highest,">By the point you tweak the model enough to weed out every bias

This misses GP's (correct) point. ""Bias"" is what the model *is.* There is no weeding out biases. Biases are corrected, not removed. Corrected from incorrect bias to correct bias. There is no non-biased.",3
post1hb,richly branching,1.4620736733947062,highest,"Why does this remind me of the moment in my research methods course that our lecturer explained that all social research is invalid because it’s impossible to understand and explain completely the internal frames of reference of another culture. 

(We were talking about ethnographic research at the time, and the researcher as an outsider)",4
post1hb,richly branching,1.4620736733947062,highest,"Bias is operating in two modes in that sentence though. On the one hand we have bias as a mostly value neutral predilection or preference in a direction, and on the other bias as purely negative and unfounded preference or aversion.

The first kind of biased is inevitable and desirable, the second kind is potentially correctable given a suitable way to measure it.

The more fundamental issue with removing bias stems from what the models are trained on, which is mostly the writings of people. The models are learning it from us.",4
post1hb,richly branching,1.4620736733947062,highest,"""correct"" biases.",4
post1hb,richly branching,1.4620736733947062,highest,"I've started to enjoy watching someone pale and look a little sick then I tell a layman that there is no such thing as an unbiased model, only one that conforms to their biases.",4
post1hb,richly branching,1.4620736733947062,highest,It turns out that ChatGPT is just a single 200 petabyte switch statement.,3
post1hb,richly branching,1.4620736733947062,highest,No. But it is also pretty much impossible. If you exclude theese biases completly your model will perform less accurately as we have seen.,3
post1hb,richly branching,1.4620736733947062,highest,Why is that? I'm curious.,4
post1hb,richly branching,1.4620736733947062,highest,"That's not what ""bias"" means when people complain about AI being racist.",3
post1hb,richly branching,1.4620736733947062,highest,"Not at all. Theres so many things to add for weight. Theres millions of things. Race, height, weight, dialect are less than .01%",3
post1hb,richly branching,1.4620736733947062,highest,"In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6.
""What are you doing?"", asked Minsky.

""I am training a randomly wired neural net to play Tic-tac-toe"", Sussman replied.

""Why is the net wired randomly?"", asked Minsky.

""I do not want it to have any preconceptions of how to play"", Sussman said.

Minsky then shut his eyes.

""Why do you close your eyes?"" Sussman asked his teacher.

""So that the room will be empty.""

At that moment, Sussman was enlightened.",2
post1hb,richly branching,1.4620736733947062,highest,"Oh, I love me some good [skillful means,](https://en.wikipedia.org/wiki/Upaya) yessir~!",3
post1hb,richly branching,1.4620736733947062,highest,Don't forget all the Keynan workers paid less than $2 an hour to build the safety net by sifting through endless toxic content.,2
post1hb,richly branching,1.4620736733947062,highest,Yeah it’s awesome that the AI companies exist so that those Kenyan workers get paid 2 dollars an hour. Otherwise they’d get paid 50 cents an hour at another job.,3
post1hb,richly branching,1.4620736733947062,highest,"Minimum wage for a receptionist in Nairobi was $1.52 per hour at the time OpenAI were doing this.

The damaging psychological effects of reviewing toxic content all day likely outweighed the modest pay increase they received. 

Many who were interviewed discuss how it caused great trauma for them.",4
post1hb,richly branching,1.4620736733947062,highest,"Funny how you make a post bashing AI, but you are bootlicking the creators in the comments. There are always people desperate enough and people easily underestimate the psychological cost of a job like this. There are documentaries about how this job messed people up, look them up. People eventually develop ptsd and could potentially be messed up for life. They don't tell you that in the job posting I can tell you that. Trust me, noone would take the job for 50 cents extra per hour if they knew that and aren't desperate. Either way, it's exploitation.",4
post1hb,richly branching,1.4620736733947062,highest,No mate. Micro-emplyment is bad.,4
post1hb,richly branching,1.4620736733947062,highest,[deleted],2
post1hb,richly branching,1.4620736733947062,highest,"autocomplete with spicy real human nuggets!

[that's all it has]",3
post1hb,richly branching,1.4620736733947062,highest,At least humans are aware of their bias. AI confidentiy says everything as if it's absolute truth and everyone thinks the same,3
post1hb,richly branching,1.4620736733947062,highest,I’d wager that over 99% of Humans aren’t aware of their biases.,4
post1hb,richly branching,1.4620736733947062,highest,That definitely sounds like most humans.,4
post1hb,richly branching,1.4620736733947062,highest,"Wanna know something crazy? When the left and right hemispheres of the brain are severed, the left and the right side process information differently. They found a way to feed information to only a single hemisphere by showing information to only 1 eye at a time, to which the corresponding opposite hand would respond. When they did this with the right brain (asking it to draw a bell for example) then they asked the left brain why the right brain drew a bell, the left brain confidently came up with reasoning why, even if it was entirely made up and wrong (""I drove by a church on the way here and heard church bells""). turns out the left brain comes to deterministic conclusions very much like an LLM does, even when being confidently wrong about why the right brain did something it did.  I'm probably butchering the hell out of it all, look up the research if you're curious, super crazy stuff and an interesting peek into how the 'modules' of the brain work.",4
post1hb,richly branching,1.4620736733947062,highest,"> At least humans are aware of their bias

Found the alien.",4
post1hb,richly branching,1.4620736733947062,highest,"""I'm not racist but...(proceeds to say something racist)"" Is way too common of a sentence for you to say people are aware of their own biases.

r/confidentlyincorrect is a thing.",4
post1hb,richly branching,1.4620736733947062,highest,"Humans can reflect and learn, LLM implementations cannot.",4
post1hb,richly branching,1.4620736733947062,highest,AI isn't aware of Deez nuts,4
post1hb,richly branching,1.4620736733947062,highest,"That’s a concise and astute way of putting it.

LLM’s are fundamentally bias boxes.",2
post1hb,richly branching,1.4620736733947062,highest,intelligence *is* patterns of bias in observational interpretation and selected output.,3
post1hb,richly branching,1.4620736733947062,highest,"Truest true thing ever said. AI is nothing but one giant GIGO problem. It'll never be bias-free. It'll just replicate existing biases and call them ""science!!!!!!""

Eugenics and Phrenology for the 21st century.",2
post1hb,richly branching,1.4620736733947062,highest,"More like automated intuition for the 21st century. If you properly manage and vet your training data, you can get good, useful results.",3
post1hb,richly branching,1.4620736733947062,highest,It is amazing how much that sounds like a human.,2
post1hb,richly branching,1.4620736733947062,highest,Humans are just meat computers each running their own unique software so it doesn't really surprise me.,3
post1hb,richly branching,1.4620736733947062,highest,"But which one will prevail, the meat machine or the machine machine?",4
post1hb,richly branching,1.4620736733947062,highest,"And it’s one trained on people. Who can have some prejudices. 

If society is racist, then that means the LLM can get a good idea of what society would assume about someone based on race. So if it can guess race, then it can get a good idea of what society would assume. 

It’s a nice efficient method for the system. It’s doing a good job of what it was asked to do. If we want it to *not* be racist, we have to cleanse its training data VERY thoroughly, undo societal racism at the most implicit and unconscious levels, or figure out a way to actively correct itself on these prejudicial assumptions.",2
post1hb,richly branching,1.4620736733947062,highest,"They are like a person trapped in a windowless room their entrie lives.

They know only what we tell them and the fact of the matter is that we as a society are racist. There's no way to keep them from becoming racist as long as they learn everything they know from us.",2
post1hb,richly branching,1.4620736733947062,highest,I had a lecture who clearly wasn’t tech savvy saying “AI” isn’t biased… I had to hold myself back so hard to not say anything. Iirc a while back there where tests showing that driver assistances where more likely to hit (or not see) dark skinned people because the training was all done on light skinned people,2
post1hb,richly branching,1.4620736733947062,highest,I don’t understand why people expect something different…,2
post1hb,richly branching,1.4620736733947062,highest,It's not just LLMs. You cannot derive perfectly reliable truths from unreliable data in general. Which tool you use doesn't matter.,2
post1hb,richly branching,1.4620736733947062,highest,Assumptions built on assumptions.. so is all consciousness and thought,2
post1hb,richly branching,1.4620736733947062,highest,"""Assumptions built on top of assumptions.""

Damn bro put a horror warning next time I almost had a panic attack....",2
post1hb,richly branching,1.4620736733947062,highest,"It's like looking into a reflection of all the data it was based on. Useful, but not something you look to for guidance.",2
post1hb,richly branching,1.4620736733947062,highest,Too bad 99.99% of people who use these chatbots don't know that and *still* thinks it's sentient and capable of reason and thought.,2
post1hb,richly branching,1.4620736733947062,highest,"Just because you cannot get rid of all biases doesn't mean you can't get rid of one, especially pernicious bias.",2
post1hb,richly branching,1.4620736733947062,highest,"There was a 99% invisible on this a while back, and if I recall correctly, most LLM have a foundation in the trove of emails that came out of the Enron hearings. Meaning that most of its idea of what “natural language” and human interactions can be based on Texans, specifically ones from Houston. 

Does this make the base model “racist”? Well, I personally wouldn’t promote that assumption. 

But given it’s geographic foundation I am willing to assume it would be at least a *little* right leaning in political ideology.",2
post1hb,richly branching,1.4620736733947062,highest,"Common/Early training data doesn’t have higher impact than data trained later. In fact it’s more 
 accurate that poorly executed fine tuning creates a recency bias.",3
post1hb,richly branching,1.4620736733947062,highest,Can you explain like I'm five?,2
post1hb,richly branching,1.4620736733947062,highest,"Didn't you just describe people, too",2
post1hb,richly branching,1.4620736733947062,highest,No people have facts and biases.  LLMs have only biases. When they give you what seems like a fact it is actually incredibly fine tuned biases to respond with what looks like a right answer.,3
post1hb,richly branching,1.4620736733947062,highest,"Yes. People have ""facts"" in the sense that information is input and stored, not necessarily that it's correct. Input information is processed through filters of bias before (and after) storage though.",4
post1hb,richly branching,1.4620736733947062,highest,"That rests on the assumption that they can weed out all biases, which has so far proven impossible.",2
post1hb,richly branching,1.4620736733947062,highest,"Yes but that's not really the point. Obviously a biased LLM is just a reflection of biased human input.

The point is to identify which biases it has, in which ways they appear and what happens when you try to negate them.",2
post1hb,richly branching,1.4620736733947062,highest,"That's not necessarily true. A LLM will form it's own biases all on it's own to optimize it's prediction accuracy, as that is how it works fundamentally.",3
post1hb,richly branching,1.4620736733947062,highest,The fact people think this will lead to a non biased ai is just hilarious. The racist Microsoft chat bot from years ago was chat gpt 1.5.,2
post1hb,richly branching,1.4620736733947062,highest,"The problem is the datasets it was trained on. These are human biases and they show up in the data we generate online. We don't have a good way to filter those out yet but that's a logistical problem not an architectural one. 

>A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.

Bro what?",2
post47hb,richly branching,1.4620736733947062,highest,"LLM's are nothing but complex multilayered autogenerated biases contained within a black box. They are inherently biased, every decision they make is based on a bias weightings optimized to best predict the data used in it's training. A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.",1
post47hb,richly branching,1.4620736733947062,highest,"So, we're *not* shocked that the black box of biases is biased?",2
post47hb,richly branching,1.4620736733947062,highest,"We are not shocked because AI is the collective wisdom of humanity, including the biases and flaws that come with it.",3
post47hb,richly branching,1.4620736733947062,highest,"“Collected wisdom” is far too generous, but it certainly has all the flaws and more",4
post47hb,richly branching,1.4620736733947062,highest,"I think the collective wisdom of humanity is found mostly in peer reviewed scientific articles. This is not that. This is more a distillation of human discourse. The great, the mundane and the trash.

Unfortunately there are some significant problems lurking in the bulk of that, which is the mundane. And it certainly seems to reflect a normal human as a far more flawed and unpleasant being than we like to think of ourselves. I say lurking - the AI reproduces our flaws much more starkly and undeniably.",4
post47hb,richly branching,1.4620736733947062,highest,Your knowledge of ai is insufficient for such declarations. You're welcome.,4
post47hb,richly branching,1.4620736733947062,highest,Black box of biases and weights is biased and comes with its own baggage.,3
post47hb,richly branching,1.4620736733947062,highest,"Right. By the point you tweak the model enough to weed out every bias, you may as well forget neural nets and hard code an AI from scratch... and then it's just your own biases.",2
post47hb,richly branching,1.4620736733947062,highest,">By the point you tweak the model enough to weed out every bias

This misses GP's (correct) point. ""Bias"" is what the model *is.* There is no weeding out biases. Biases are corrected, not removed. Corrected from incorrect bias to correct bias. There is no non-biased.",3
post47hb,richly branching,1.4620736733947062,highest,"Why does this remind me of the moment in my research methods course that our lecturer explained that all social research is invalid because it’s impossible to understand and explain completely the internal frames of reference of another culture. 

(We were talking about ethnographic research at the time, and the researcher as an outsider)",4
post47hb,richly branching,1.4620736733947062,highest,"Bias is operating in two modes in that sentence though. On the one hand we have bias as a mostly value neutral predilection or preference in a direction, and on the other bias as purely negative and unfounded preference or aversion.

The first kind of biased is inevitable and desirable, the second kind is potentially correctable given a suitable way to measure it.

The more fundamental issue with removing bias stems from what the models are trained on, which is mostly the writings of people. The models are learning it from us.",4
post47hb,richly branching,1.4620736733947062,highest,"""correct"" biases.",4
post47hb,richly branching,1.4620736733947062,highest,"I've started to enjoy watching someone pale and look a little sick then I tell a layman that there is no such thing as an unbiased model, only one that conforms to their biases.",4
post47hb,richly branching,1.4620736733947062,highest,It turns out that ChatGPT is just a single 200 petabyte switch statement.,3
post47hb,richly branching,1.4620736733947062,highest,No. But it is also pretty much impossible. If you exclude theese biases completly your model will perform less accurately as we have seen.,3
post47hb,richly branching,1.4620736733947062,highest,Why is that? I'm curious.,4
post47hb,richly branching,1.4620736733947062,highest,"That's not what ""bias"" means when people complain about AI being racist.",3
post47hb,richly branching,1.4620736733947062,highest,"Not at all. Theres so many things to add for weight. Theres millions of things. Race, height, weight, dialect are less than .01%",3
post47hb,richly branching,1.4620736733947062,highest,"In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6.
""What are you doing?"", asked Minsky.

""I am training a randomly wired neural net to play Tic-tac-toe"", Sussman replied.

""Why is the net wired randomly?"", asked Minsky.

""I do not want it to have any preconceptions of how to play"", Sussman said.

Minsky then shut his eyes.

""Why do you close your eyes?"" Sussman asked his teacher.

""So that the room will be empty.""

At that moment, Sussman was enlightened.",2
post47hb,richly branching,1.4620736733947062,highest,"Oh, I love me some good [skillful means,](https://en.wikipedia.org/wiki/Upaya) yessir~!",3
post47hb,richly branching,1.4620736733947062,highest,Don't forget all the Keynan workers paid less than $2 an hour to build the safety net by sifting through endless toxic content.,2
post47hb,richly branching,1.4620736733947062,highest,Yeah it’s awesome that the AI companies exist so that those Kenyan workers get paid 2 dollars an hour. Otherwise they’d get paid 50 cents an hour at another job.,3
post47hb,richly branching,1.4620736733947062,highest,"Minimum wage for a receptionist in Nairobi was $1.52 per hour at the time OpenAI were doing this.

The damaging psychological effects of reviewing toxic content all day likely outweighed the modest pay increase they received. 

Many who were interviewed discuss how it caused great trauma for them.",4
post47hb,richly branching,1.4620736733947062,highest,"Funny how you make a post bashing AI, but you are bootlicking the creators in the comments. There are always people desperate enough and people easily underestimate the psychological cost of a job like this. There are documentaries about how this job messed people up, look them up. People eventually develop ptsd and could potentially be messed up for life. They don't tell you that in the job posting I can tell you that. Trust me, noone would take the job for 50 cents extra per hour if they knew that and aren't desperate. Either way, it's exploitation.",4
post47hb,richly branching,1.4620736733947062,highest,No mate. Micro-emplyment is bad.,4
post47hb,richly branching,1.4620736733947062,highest,[deleted],2
post47hb,richly branching,1.4620736733947062,highest,"autocomplete with spicy real human nuggets!

[that's all it has]",3
post47hb,richly branching,1.4620736733947062,highest,At least humans are aware of their bias. AI confidentiy says everything as if it's absolute truth and everyone thinks the same,3
post47hb,richly branching,1.4620736733947062,highest,I’d wager that over 99% of Humans aren’t aware of their biases.,4
post47hb,richly branching,1.4620736733947062,highest,That definitely sounds like most humans.,4
post47hb,richly branching,1.4620736733947062,highest,"Wanna know something crazy? When the left and right hemispheres of the brain are severed, the left and the right side process information differently. They found a way to feed information to only a single hemisphere by showing information to only 1 eye at a time, to which the corresponding opposite hand would respond. When they did this with the right brain (asking it to draw a bell for example) then they asked the left brain why the right brain drew a bell, the left brain confidently came up with reasoning why, even if it was entirely made up and wrong (""I drove by a church on the way here and heard church bells""). turns out the left brain comes to deterministic conclusions very much like an LLM does, even when being confidently wrong about why the right brain did something it did.  I'm probably butchering the hell out of it all, look up the research if you're curious, super crazy stuff and an interesting peek into how the 'modules' of the brain work.",4
post47hb,richly branching,1.4620736733947062,highest,"> At least humans are aware of their bias

Found the alien.",4
post47hb,richly branching,1.4620736733947062,highest,"""I'm not racist but...(proceeds to say something racist)"" Is way too common of a sentence for you to say people are aware of their own biases.

r/confidentlyincorrect is a thing.",4
post47hb,richly branching,1.4620736733947062,highest,"Humans can reflect and learn, LLM implementations cannot.",4
post47hb,richly branching,1.4620736733947062,highest,AI isn't aware of Deez nuts,4
post47hb,richly branching,1.4620736733947062,highest,"That’s a concise and astute way of putting it.

LLM’s are fundamentally bias boxes.",2
post47hb,richly branching,1.4620736733947062,highest,intelligence *is* patterns of bias in observational interpretation and selected output.,3
post47hb,richly branching,1.4620736733947062,highest,"Truest true thing ever said. AI is nothing but one giant GIGO problem. It'll never be bias-free. It'll just replicate existing biases and call them ""science!!!!!!""

Eugenics and Phrenology for the 21st century.",2
post47hb,richly branching,1.4620736733947062,highest,"More like automated intuition for the 21st century. If you properly manage and vet your training data, you can get good, useful results.",3
post47hb,richly branching,1.4620736733947062,highest,It is amazing how much that sounds like a human.,2
post47hb,richly branching,1.4620736733947062,highest,Humans are just meat computers each running their own unique software so it doesn't really surprise me.,3
post47hb,richly branching,1.4620736733947062,highest,"But which one will prevail, the meat machine or the machine machine?",4
post47hb,richly branching,1.4620736733947062,highest,"And it’s one trained on people. Who can have some prejudices. 

If society is racist, then that means the LLM can get a good idea of what society would assume about someone based on race. So if it can guess race, then it can get a good idea of what society would assume. 

It’s a nice efficient method for the system. It’s doing a good job of what it was asked to do. If we want it to *not* be racist, we have to cleanse its training data VERY thoroughly, undo societal racism at the most implicit and unconscious levels, or figure out a way to actively correct itself on these prejudicial assumptions.",2
post47hb,richly branching,1.4620736733947062,highest,"They are like a person trapped in a windowless room their entrie lives.

They know only what we tell them and the fact of the matter is that we as a society are racist. There's no way to keep them from becoming racist as long as they learn everything they know from us.",2
post47hb,richly branching,1.4620736733947062,highest,I had a lecture who clearly wasn’t tech savvy saying “AI” isn’t biased… I had to hold myself back so hard to not say anything. Iirc a while back there where tests showing that driver assistances where more likely to hit (or not see) dark skinned people because the training was all done on light skinned people,2
post47hb,richly branching,1.4620736733947062,highest,I don’t understand why people expect something different…,2
post47hb,richly branching,1.4620736733947062,highest,It's not just LLMs. You cannot derive perfectly reliable truths from unreliable data in general. Which tool you use doesn't matter.,2
post47hb,richly branching,1.4620736733947062,highest,Assumptions built on assumptions.. so is all consciousness and thought,2
post47hb,richly branching,1.4620736733947062,highest,"""Assumptions built on top of assumptions.""

Damn bro put a horror warning next time I almost had a panic attack....",2
post47hb,richly branching,1.4620736733947062,highest,"It's like looking into a reflection of all the data it was based on. Useful, but not something you look to for guidance.",2
post47hb,richly branching,1.4620736733947062,highest,Too bad 99.99% of people who use these chatbots don't know that and *still* thinks it's sentient and capable of reason and thought.,2
post47hb,richly branching,1.4620736733947062,highest,"Just because you cannot get rid of all biases doesn't mean you can't get rid of one, especially pernicious bias.",2
post47hb,richly branching,1.4620736733947062,highest,"There was a 99% invisible on this a while back, and if I recall correctly, most LLM have a foundation in the trove of emails that came out of the Enron hearings. Meaning that most of its idea of what “natural language” and human interactions can be based on Texans, specifically ones from Houston. 

Does this make the base model “racist”? Well, I personally wouldn’t promote that assumption. 

But given it’s geographic foundation I am willing to assume it would be at least a *little* right leaning in political ideology.",2
post47hb,richly branching,1.4620736733947062,highest,"Common/Early training data doesn’t have higher impact than data trained later. In fact it’s more 
 accurate that poorly executed fine tuning creates a recency bias.",3
post47hb,richly branching,1.4620736733947062,highest,Can you explain like I'm five?,2
post47hb,richly branching,1.4620736733947062,highest,"Didn't you just describe people, too",2
post47hb,richly branching,1.4620736733947062,highest,No people have facts and biases.  LLMs have only biases. When they give you what seems like a fact it is actually incredibly fine tuned biases to respond with what looks like a right answer.,3
post47hb,richly branching,1.4620736733947062,highest,"Yes. People have ""facts"" in the sense that information is input and stored, not necessarily that it's correct. Input information is processed through filters of bias before (and after) storage though.",4
post47hb,richly branching,1.4620736733947062,highest,"That rests on the assumption that they can weed out all biases, which has so far proven impossible.",2
post47hb,richly branching,1.4620736733947062,highest,"Yes but that's not really the point. Obviously a biased LLM is just a reflection of biased human input.

The point is to identify which biases it has, in which ways they appear and what happens when you try to negate them.",2
post47hb,richly branching,1.4620736733947062,highest,"That's not necessarily true. A LLM will form it's own biases all on it's own to optimize it's prediction accuracy, as that is how it works fundamentally.",3
post47hb,richly branching,1.4620736733947062,highest,The fact people think this will lead to a non biased ai is just hilarious. The racist Microsoft chat bot from years ago was chat gpt 1.5.,2
post47hb,richly branching,1.4620736733947062,highest,"The problem is the datasets it was trained on. These are human biases and they show up in the data we generate online. We don't have a good way to filter those out yet but that's a logistical problem not an architectural one. 

>A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.

Bro what?",2
post54hb,richly branching,1.4400485358400132,highest,"This is the massive problem with AI. It can seem perfectly accurate, then it turns out the scientists were only testing it on specific subjects for ""reliability"" and ope it turns out that defeats the entire purpose of AI and trains it to literally discriminate just like the people who made it.",1
post54hb,richly branching,1.4400485358400132,highest,"Or the initial training data were skewed one way or another. A similar case was an AI determining if a patient had a disease partially by looking at the hospital that the xray was taken. It did so, because the initial data included cases of a local epidemic which meant the patients location was factored in the ""diagnosis"".",2
post54hb,richly branching,1.4400485358400132,highest,"Oof, that's a huge one.",3
post54hb,richly branching,1.4400485358400132,highest,I heard a case of an AI model that could tell the difference between cancer and a non-cancerous mole by identifying if the photo used had a ruler or measuring device in it. That's one problem with AI models being non-human readable. It's like regex but many times worse,3
post54hb,richly branching,1.4400485358400132,highest,"I’m a little surprised this paper got by the reviewers. They show that sex (female), race (black), and age (older) have lower rates of diagnosis. Women have more breast tissue on average than men, and racial minorities and the elderly correlate with obesity - all of which is known to detrimentally affect Xray image quality. Not one mention in the methods regarding controlling for BMI, chest circumference, or anything like that.",3
post54hb,richly branching,1.4400485358400132,highest,"Well, to be fair, the blood donation center in NZ did that for years.

They wouldn't accept my blood because I had visited the UK in the 10-year window of the BSE occurrences.

And we did that way more recently for COVID, by asking where people had been.",3
post54hb,richly branching,1.4400485358400132,highest,"It’s a not-unreasonable strategy.  It looks like, although it will take a generation or more to know, that the risks of CJD in humans triggered by BSE in meat were overstated.  Incidence of CJD in the UK has not risen substantially, and there were 0 (zero) vCJD (the variant caused by BSE) cases in 2020.   That said, in the 1990s and 2000s no-one knew, the incubation period is long and there had been a lot of BSE in the UK food chain.  Since transmission by blood transfusion has been recorded, and the blood products industry is still recovering from AIDS and hepatitis transmission in the 1980s, broad-spectrum elimination of UK blood from a nation’s supply is and was a reasonable response.",4
post54hb,richly branching,1.4400485358400132,highest,"Neural networks are pattern finding engines, and pattern finding engines *only*. A pattern resulting from biased data is absolutely no different to it from a pattern resulting from actual real world correlations.",2
post54hb,richly branching,1.4400485358400132,highest,"We often don't pay attention to all the patterns so we miss crucial ones. 


We tried to breed Chcolate Labs for intelligence without realizing that food motiviation accelerates task compliance. So we ended up trying to breed for intelligence snd simply made very hungry dogs.",3
post54hb,richly branching,1.4400485358400132,highest,[deleted],4
post54hb,richly branching,1.4400485358400132,highest,"It’s at least discriminating based on data, unlike doctors who do it based on personal prejudices. Data can be corrected for by adding more training data containing groups that were underweighted in the original dataset. Convincing a doctor to stop giving lousy care to patients in demographics they dislike is a lot harder, not least because they’ll fight to the last to avoid admitting they’re treating some patients based on how they look and not their symptoms.",3
post54hb,richly branching,1.4400485358400132,highest,"> unlike doctors who do it based on personal prejudices

This just isn't true, most of the time. Doctors, as a whole, are probably about as left-leaning as this damned site. And even black doctors perform worse with black patients than they do with white ones.

Why? Because they were trained on the same skewed data these AIs were. 

And it's *really* hard to get better data.",4
post54hb,richly branching,1.4400485358400132,highest,"> trains it to literally discriminate just like the people who made it. 

Yes: garbage in, garbage out. AI can only replicate our biases, not remove them.

Still, though, once the problem is identified it's not a big mystery how to fix it. It might not be cheap or fast to re-train, but it's not like we don't know how.",2
post54hb,richly branching,1.4400485358400132,highest,"But honestly they'll just use it and say it's fine - they're like who cares about more than half the population.


Medical basis is real and still now is 2025 there is little or nothing being done about - as an example and I tend to use this one a lot is there's *still* no real research into women and how ADHD affects them differently and oestrogen fluctuations, monthly for decades and across their lifetime, affects the systems and severity of this. This is despite 2 conclusions that are know - 1. ADHD is a chronic lack of dopamine in the brain. 2. Oestrogen levels affect dopamine levels.

There have been issues with this reported in the community for *decades* at this point, but it only something that is just beginning to be looked at.",3
post54hb,richly branching,1.4400485358400132,highest,"To also add, they only recently started publishing a visual encyclopedia of how rashes appear on dark skin tones, because even black doctors are taught on the white skin patient standard.",4
post54hb,richly branching,1.4400485358400132,highest,The idea that ADHD is a chronic lack of dopamine in the brain is a misconception or oversimplification as far as I know. It's somewhat more accurate that it includes failures in certain dopamine pathways.,4
post54hb,richly branching,1.4400485358400132,highest,"See also ""a kid is just a small adult, right?""",4
post54hb,richly branching,1.4400485358400132,highest,"I'll one-up you on this: There has been only recently a study done on women's peri-menopausal issues with lack of iron due to increased menstrual bleeding.

One of the big issues exclusively for women and only this year someone finally got around to establishing key facts about it.",4
post54hb,richly branching,1.4400485358400132,highest,"How do you fix it? You can’t train it with data you don’t have, and the medical community has routinely minimized the participation of women and minorities in their studies.",3
post54hb,richly branching,1.4400485358400132,highest,"Yep, 100%. Like I said above: replicate our biases.

So you fix it by *getting* that data. Again, like I said, not necessarily cheap or fast; but we know exactly how to do it. We're not back at square one.",4
post54hb,richly branching,1.4400485358400132,highest,"I mean it’s actually rather straightforward to address. Model generalization is often not a priority when engineering AI, because doing it properly will make it seem like it gives marginally worse results (on the biased data you do have). 

* Get more data and be more careful about how you sample it
* or weight the rarer samples (like black women) higher in training to balance out the importance
* Or choose a loss function that penalizes this effect 
* Or remove data selectively until the training dataset is more balanced
* various other training techniques like regularization and ‘dropout’

I make medical computer vision models and things like robustness and reliability and generalization just aren’t valued by the higher ups as much, because they cant easily show those things off.",4
post54hb,richly branching,1.4400485358400132,highest,"> How do you fix it? You can’t train it with data you don’t have

No, but you can balance training data or use something like SMOTE to correct for this. It's a fairly common problem and there are a lot of techniques to manage it.",4
post54hb,richly branching,1.4400485358400132,highest,"The data most likely already exists but was not part of the training data.

But I think the most interesting observation you can make is that lung scans of women and black people apparently are different from those of white men. Is it how the scans are made or actual biological differences that are significant enough to affect the detection? Why would a black man’s lung scan be significantly different from a white man? Women’s breasts might be an issue, but a male?",4
post54hb,richly branching,1.4400485358400132,highest,"If you think it's the medical community that minimizes it, and not women and minorities that choose not to volunteer for said research then you've done very little research volunteer gathering in your life.",4
post54hb,richly branching,1.4400485358400132,highest,"I think that you're a bit off on how you're reading this, tbh. Garbage in garbage out is a huge simplification, that's simply not true or at the very minimum, not that simple. Models such as ""Noise2Noise"" are pretty clear indications that you can train output of higher quality than input. In this model, they start with clean images, add noise, and then add even more noise. They have a model map More Noise to Less Noise, and get cleaner data than the level Less Noise was at. You throw noisy data in, and get clean data. Of course, good data is important but the GIGO rule isn't some hard fact we can't escape, its not conservation of energy or something. 

  
On the opposite side of things, even if you do identify some kind of bias issue, a subtype that isn't being classified correctly, this doesn't automatically lead you to a solution. The plan fact is, we have many strategies and sometimes, even often, they don't work at all. On the r/learnmachinelearning subreddit right now, there's a post asking if ""SMOTE ever works"". Smote is one such strategy for dealing with under-represented data, standing for Synthetic Minority Oversampling TEchnique. This isn't exactly the same problem being addressed, but its pretty clear we have many more ideas for how to address issues, than we have one-click solutions which actually work.   
  
It is very common in ML to have ""an answer"" for some problem, and it just doesn't work. I don't think you actually need to be in the weeds of technical details to see this is the case.",3
post54hb,richly branching,1.4400485358400132,highest,"It's also a problem with data sets available.

Data that AI is trained on tends to be homogenised because data comes from rich places that tend to have homogeneous groups of people. 

This is a nuanced issue.",2
post54hb,richly branching,1.4400485358400132,highest,"If you go to figure 2 you'll see that the results from the radiologists and the AI largely overlap.


The radiologists had roughly the same shortfall in roughly the same groups.",3
post54hb,richly branching,1.4400485358400132,highest,"Unfortunately, this is a problem with medicine in general.

Up until not that long ago, research trials often used only men because women's pesky hormone system confused the study results. Therefore, the 'results' were only really valid for men, but were used for rx'ing to women as well.

This is a massive problem - with AI, our medical system (good luck being a women in her 50's suffering a heart attack), our justice system, etc.

Bias is not unique to AI, but hopefully we'll pay attention to it more than we do in humans.",2
post54hb,richly branching,1.4400485358400132,highest,"It's the massive problem with the current algorithms that we have started conflating with AI. The current models don't truly ""learn,"" they just identify patterns and replicate them. That foundational approach will forever cause them to be susceptible to replication error and will make them incapable of scaling to generally useful applications.",2
post54hb,richly branching,1.4400485358400132,highest,Hey look it's the X-Box Kinect phenomenon,2
post54hb,richly branching,1.4400485358400132,highest,Good thing the current U.S. administration hasn't effectively banned any research to address this kind of issue from receiving federal funds.,2
post54hb,richly branching,1.4400485358400132,highest,"So it’s not a problem with the AI itself but the person operating the AI. 

The AI did exactly what it was prompted to do.",2
post54hb,richly branching,1.4400485358400132,highest,"Yeah, then corporations tell us that we can trust everything to AI, meanwhile black resumes get canned because the AI that reads them is built on racist data, because basically all the data america has is tainted by racial bias. These models spit out what we put in, and the world has too much hatred for us to expect anything else out of them.",3
post54hb,richly branching,1.4400485358400132,highest,"Yes. This is technically the case, but it comes with an important caveat.

The tendency of human bias to bleed into AI is almost unavoidable.

I'm not saying it's bad or shouldn't be used or anything, but we need to be wary of treating this as ""just a tool"" that can be used for good or bad depending on the person using it, because this isn't a case where you can just fix it by being cognizant enough.

Bias is innate in us. The methods and procedures we use to test and train these things exacerbates those biases because they are built into the process as assumptions.

In addition to this, sometimes, even if you are intentionally addressing the biases, the bias comes FROM the algorithm itself.

""Algorithmic oppression"" by safiya noble is a fantastic read on the issue, and uses a very succinct example.

Imagine an algorithm or AI that's trained to put the most popular barbershops at the top of the list.

In a community of 80% white individuals and 20% black, there will NEVER be a case where a barbershop that caters to that specific hair type will ever appear on that algorithm. This inherently means less access to a specific service by a specific group of people.

But also, how would you even TRY to go about solving this issue in the algorithm other than creating 2 different ones altogether?

What new problems might that cause?

This is obviously oversimplified, but it's a real life example of how bias can appear in these systems without that bias existing in the people that create it.",3
post54hb,richly branching,1.4400485358400132,highest,"Bias is not only innate in us, it's a critical in ML as well, critical for analysis itself. Just talking about getting rid of bias, or suggesting we just use two models, are kind of practical examples of this; you can't just ""take out"" the bias. 


Anyways, the answer no one will like but is workable is that the model should look at your chest xray and tell you your race, or fat, or old, or in a high background radiation area. Think that would work better than a second, smaller model.",4
post54hb,richly branching,1.4400485358400132,highest,">But also, how would you even TRY to go about solving this issue in the algorithm other than creating 2 different ones altogether?


Modern social media handles it by sorting people by what they like and matching them with similar people.


Do you like [obscure thing] ? Well the system has found the 10 other people in the world that like it and shows you things they like.


 Nothing needs universal popularity, you can be popular with one weird group and the algorithm will unite you with them.


It does however automatically put people in a media filter bubble with those most like them which can lead to some weird worldviews.",4
post54hb,richly branching,1.4400485358400132,highest,">Imagine an algorithm or AI that's trained to put the most popular barbershops at the top of the list.

I'm sure that there are lots of problems with AI, but the fact that this is the go-to example doesn't inspire faith in its critics. Ironically, there are so many weird assumptions baked in here that it's hard to know where to start. 

Somehow, people manage to find Chinese restaurants and children's clothing stores, even in cities where Chinese people and children are a minority...",4
post54hb,richly branching,1.4400485358400132,highest,"This isn't a meaningful argument against AI. It's an argument against researchers using one model and making bold assumptions about it's usefulness.  
  
They can likely create a second model for women or black individuals now that they know the issue.",2
post54hb,richly branching,1.4400485358400132,highest,"It's an argument for more regulation, and to make sure that we never stop verifying.  

Imagine somebody didn't do this study, and we got to a point where for costs/insurance reasons, everyone just stopped using actual x-ray technicians and just did whatever the AI told them to?",3
post54hb,richly branching,1.4400485358400132,highest,"This is why proper studies of diagnostic tests of any variety in medicine require multiple stages of study in multiple patient cohorts and settings. 

The whole process of clinical validation (not just developing the test) can easily take 5-10y - it takes time to enroll patients into a study, wait for the outcomes to happen, etc.

It’s one reason why anyone who says AI will be widespread in clinical medicine within less than 5y has no idea what they’re talking about.",4
post54hb,richly branching,1.4400485358400132,highest,"Its an argument against AI. We clearly are oversold on how it works and implementing it is difficult because we don't understand it. It means we shouldn't adopt it without knowing all the possible issues.


The fact that they keeping coming out with new models is a case against using them because there are so many untested unkowns. 


Its like if we had iOS 1 then iOS 5 then next year its a Linux Ubuntu distro. The shift is too great to reliably implement",3
post54hb,richly branching,1.4400485358400132,highest,"If you had a magic box into which you could insert a picture of a person's face, that instantly tests whether a person has cancer, but only 20% of positives are true, and only 20% of carriers are positive. The box is magic, ie you ""dont know all the possible issues"". And the box is wrong more often than it's right. Is that a useful machine that we should definitely use as soon as possible? To me the answer is yes, it's arguably immoral not to use it. If a consenting person gets flagged, they should go get checked by a doctor.",4
post54hb,richly branching,1.4400485358400132,highest,"This is a massive problem with science. Far too many scientists see women and non-whites as ""unnecessary variables"". The ""default white man"" is pervasive across every area of study.",2
post54hb,richly branching,1.4400485358400132,highest,"What a quintessentially 'reddit' take on things....The effectiveness of an predictive AI model is as good as the data set that its trained on.  The availability of data, especially medical data is tricky due to several factors. In this case, the Stanford team which built the chest Xray model (cheXzero) used a dataset of \~400000 chest xray images to train the model, but it seems only 666 (0.16%) of those images actually contained both diagnostic (from a radiologist) and demographic (race, age, sex) data. 

In the UWash [study ](https://www.science.org/doi/10.1126/sciadv.adq0305#sec-4)cited in this news article, their findings of AI bias are based on these 666 images which contained the necessary metadata. Its not an issue with the scientists from the Stanford [study ](https://www.nature.com/articles/s41551-022-00936-9#Sec4)\- the more data available for training, the more robust the model will be. Given the limited metadata they had to work with, taking into account demographic biases is outside the scope of their project and they used the full dataset. Its also worth noting (*only because you mention this as an issue*) that only two of the six authors on the Stanford team are white and one of them is female (the rest appear of east/south Asian origin). The UWash team highlighted an important issue with the model that demonstrates major pitfalls in the Stanford model which need to be addressed - but I think the baseless claim that the Stanford team is racist/sexist is very unfair, and its even more unfair to generalize it across scientists. 

Its also worth pointing out that the UWash study itself has ""sampling bias"" (not with malicious intent of course though; they had the same limitations as the Stanford team). Their model is trained on only the 666 images with demographic data - no one knows the demographics of the other \~400000 images used. Its difficult to tell whether their findings hold true across the entire data set simply because the necessary metadata doesn't exist. This is the core of the issue here:

Using chest Xray images as an example, medical privacy laws and patient consent can make it difficult to publish these kinds of data to public databases. And that's just the images, nevermind the demographic data. Add that to other variables that need to be controlled (eg quality of the Xray, reliability of patient health records, agreements between database administration and clinical teams etc), its tricky to get a large enough data set to robustly train a ML model while accounting for things like demographics. I'm of the opinion that consent for release of medical data should be a prerequisite and obligation for access to health care (assuming data security is robust and discrete patient identifiers are removed). Likewise, hospitals/clinics should be obliged to upload their data in free-publicly available datasets.",3
post54hb,richly branching,1.4400485358400132,highest,"This isn't a ""Reddit"" take. Go read Invisible Women. Maybe you're part of the problem.",4
post54hb,richly branching,1.4400485358400132,highest,"I mean that's just the fault of our regulations. It's so expensive to run studies that cofounding variables are never worth the risk to any company.

It also doesn't help that people really like to burry their head in the sand and pretend ""races"" aren't different enough to have very different interactions with the same drug.",3
post54hb,richly branching,1.4400485358400132,highest,Most of my peers in my life have been very left leaning. The politics in your echo chamber is causing you more suffering than you realize. Please try to get out of it and attain a more balanced view. You'll be happier and have a more clear picture of the world.,3
post54hb,richly branching,1.4400485358400132,highest,Go read Invisible Women and then tell me that again with a straight face.,4
post54hb,richly branching,1.4400485358400132,highest,"> trains it to literally discriminate just like the people who made it.

After reading the article that might be exactly what they need to do, build discrimination (as in the ability or power to see or make fine distinctions) into the model so to speak.  Reading the chest x-ray of an 80 year old white man compared to a 30 year black woman with the same model is probably not going to yield the best results.",2
post54hb,richly branching,1.4400485358400132,highest,"The upside to discovering its error is to either only use it on the sunset it is good for while giving it additional training for others areas or if that will not work, start from scratch.",2
post54hb,richly branching,1.4400485358400132,highest,"That's not really a problem with AI, though. It's a problem with our methods of training AI. 

We've had a very similar issue with automatic hand dryers. Some of the earlier hand dryers worked based on light reflectivity. Guess what - white people have more reflective skin. It refused to dry the hands of people with a critical threshold of melanin in their skin. If they tested with non-white people, they would have realized that their thresholds needed adjustment. We're dealing with something similar here. With all the attention put on racism and equity, we still keep forgetting to implement diversity in our product design.",2
post54hb,richly branching,1.4400485358400132,highest,"It's a problem across a lot of technology and science.    
    
Essentially every image recognition/analysis tool or toy I've ever encountered has had significant issues with darker skinned people.     
      
A disproportionate amount of what we know about humans is mostly from studying European descendants, and men.    
Even when it comes or animals, many studies have been limited to males, to reduce complexity and variance.  
   
We really need high quality, diverse public data sets. This is something the government should be funding. AI isn't going away, we need to find ways to make it work for everyone.   
Medical diagnostics, of all things, should not be exclusively in private hands.",2
post54hb,richly branching,1.4400485358400132,highest,"As someone who does do AI research in medical stuff,this is actually a pretty good idea. They're one of the few who could actually do it without getting hippa'd",3
post54hb,richly branching,1.4400485358400132,highest,I know of the issue in general but I'm pretty surprised race affects their reading of x-rays of all things.,2
post54hb,richly branching,1.4400485358400132,highest,This isn’t really an “AI” problem. What you are describing is *human error*,2
post54hb,richly branching,1.4400485358400132,highest,"I didn't read the study, but usually, this problem occurs due to lack of data from certain groups of people.

I assume there is simply less data available from black women, and this is usually due to the history of people of African origin, as well as their current living conditions.

We simply have less data available since these people don't visit (for many reasons like poverty) the doctor as often, or since the majority of these people live in countries where we don't have easy ways of collecting data from them.",2
post54hb,richly branching,1.4400485358400132,highest,Because they correctly trained it on the most common cases first. Of course there's always going to be outliers.,2
post54hb,richly branching,1.4400485358400132,highest,Women and blacks are outliers?,3
post5hb,richly branching,1.422454390748317,highest,How do people construe facial recognition misidentifications as evidence of the system being racist? The word has lost all its meaning thanks to articles like this.,1
post5hb,richly branching,1.422454390748317,highest,"No, it still means what it means. No one has to use facial recognition software that doesn't actually recognize faces. The people who work on this software could actually  test it to make sure it works before calling it finished. What's the functional difference between bias in the system and being apathetic about contributing to the bias?",2
post5hb,richly branching,1.422454390748317,highest,"I agree with you. I just think it's at least worth mentioning the fact that facial recognition inherently needs more light for darker skin. To implement it when there isn't yet a 0% bias rate is racist, yes (and I wouldn't want it implemented then either).",3
post5hb,richly branching,1.422454390748317,highest,"Two things:

A) Racial bias in algorithms is a very common phenomenon and needs to be addressed.

B) The police are still trying to use said algorithm EVEN THOUGH IT CAN'T TELL PEOPLE APART.",2
post5hb,richly branching,1.422454390748317,highest,">A) Racial bias in algorithms is a very common phenomenon and needs to be addressed.

Imagine just making up bullshit because you want to look progressive and then having 25+ people agree with you all based on literally nothing. Does ""racial bias"" exist in algorithms? Sure. Is it ""very common""? Not even remotely. And like most racist conspiracies on reddit this dog-whistle is an the alt-right talking point about white people being inherently smarter than minorities since a machine says so. It's sad to see this racist bullshit upvoted but not exactly surprising",3
post5hb,richly branching,1.422454390748317,highest,"Ok well.... this isn’t a hard thing to find out. I do have a computer science background, but this is doesn’t require a deep understanding of ML to get. Then there “25+ disagreeing with me”. That last half was just incoherent. How am I being racist? Buddy are you ok?",4
post5hb,richly branching,1.422454390748317,highest,"Racial bias in algorithms is extremely uncommon.

Racial bias in ML training data sets is a lot more common though, often leading to biased models.",3
post5hb,richly branching,1.422454390748317,highest,"It’s more common than you think. It isn’t always things as overt as a facial recognition program thinking all brown people look alike. And it usually isn’t intentional.

Data training sets for ai is one cause, but not the only one.",4
post5hb,richly branching,1.422454390748317,highest,I agree and I am aware.,3
post5hb,richly branching,1.422454390748317,highest,[removed],3
post5hb,richly branching,1.422454390748317,highest,Uh..... how?,4
post5hb,richly branching,1.422454390748317,highest,"It's not used as conclusive evidence, it just narrows down lists of suspects.",3
post5hb,richly branching,1.422454390748317,highest,"“racial bias in algorithms” just tells me you know nothing about algorithms.

Pattern recognition systems (which aren’t actually algorithms) (that people mislabel AI) based on racist training data can be racist, but that same system based on non-racist data won’t be racist.",3
post5hb,richly branching,1.422454390748317,highest,"You sound like you read the first chapter of a 15 year old book on java. 

AI isn’t just one thing. In this context, we mean machine learning algorithms. In others it could mean software that emulates human decision making.

And a pattern recognition system is without a doubt an algorithm. It is an extremely broad term.

Talk about proving you don’t know what you are talking about.",4
post5hb,richly branching,1.422454390748317,highest,"Because the police, electing to use this flawed and biased system, will end up having more false positives among minorities with black or brown skin. 

As such, the net effect is a racist system. 

The meaning of the word is fully sound.",2
post5hb,richly branching,1.422454390748317,highest,"I agree. It's the way in which the technology is used that is racist, not the technology itself alone (which the article seems to not clarify).",3
post5hb,richly branching,1.422454390748317,highest,"> It's the way in which the technology is used that is racist, not the technology itself alone (which the article seems to not clarify).  

From the beginning of the article:  

> But there’s two factors that need screaming above all others when it comes to the debate surrounding facial recognition. 

> One: it’s racist. 

> Two: it doesn’t even work. 

> **Technology never exists in a vacuum.**  For now, humans are still responsible for the production of new digital systems; and that means they come into being with all the biases and fallibility of their creators baked right into their code.  

Jesus fuck.  

But yeah, keep circlejerking about ""omg smh people calling everyone racist these days are debasing the super-great English language (that I don't even bother to read).""",4
post5hb,richly branching,1.422454390748317,highest,">will end up having more false positives among minorities with black or brown skin

Which means the system will be better at tracking down white people than those minorities.

Did you even consider that angle? And I'd even go as far as to assume you'd be even more angry if it was the other way round, if the system eg were best at tracking down black people. Police using tech thats only good at tracking down black people, imagine the screams about racism. But if its aimed at white people, then thats racist to non-white people too?

&#x200B;

And yes, that what makes it ridiculous that overly woke people - particuarly an american stream of politics - try to apply racism to anything, pretending its such a sensitive topic, but then cant even make a good judgement call on it.",3
post5hb,richly branching,1.422454390748317,highest,"If they use facial recognition to incorrectly prosecute people of colour, or at the very least harass them and arrest them and waste their time because the facial recognition technology popped a false positive, then it’s harming black people. If it’s accurately recognising white criminals, doing the crime — then it’s working as intended. 

What the fuck were you even thinking here? It is accurate with white people, poor white criminals getting caught? 

It’s not ‘aimed’ at white people, it’s not being used to persecute or prosecute *just white people*, fucking obviously. 

Alright, I’ll break it down. 

* The system for facial recognition is being applied to the general public. All video and photo relevant to a crime, supposedly, will be subject to this technology to identify perpetrators.

1. For white people, it’s accurate and the criminal is caught.
2. For people of colour, they get false positives, bring innocent people down to the precinct, subject them to inquiry, and possibly say ‘we have data that proves you were there!’.

Let’s not get into what’s ‘harmful’: you’re seeing how the system is discriminating between race? Yes?

That’s what makes it racist.

From there, we can better argue as to why it’s more harmful to be incorrectly identified for crimes you didn’t commit...",4
post5hb,richly branching,1.422454390748317,highest,"This entire post and thread are absolutely baffling

1.	This entire problem is a symptom of technological drawbacks where cameras can contrast objects easier against light backgrounds. Not systematic racism or “racist data”
2.	Given the above, even using the system would likely perpetuate criminal stereotypes against white people as they would be far easier to be recognized, not brown/black like everybody is implying.

y’all are dumb as hell",4
post5hb,richly branching,1.422454390748317,highest,">Which means the system will be better at tracking down white people than those minorities.

This is assuming that the cops have the integrity to acknowledge the possibility of false positives and not use this in cases involving black or brown people. Realistically it's going to put innocent minorities in prison while the increased accuracy for white people means it will (correctly) exonerate innocent white people.",4
post5hb,richly branching,1.422454390748317,highest,"So you’re telling me there are gonna be two faces that are identical in every way and the only difference between them is going to be skin color? Doesn’t seem likely.

Skin color isnt going to be a determining factor when recognizing faces.",3
post5hb,richly branching,1.422454390748317,highest,[deleted],4
post5hb,richly branching,1.422454390748317,highest,Because it's disproportionately inaccurate for nonwhite faces?,2
post5hb,richly branching,1.422454390748317,highest,"It’s not racism, it’s physics. They’re based on light, and dark people reflect less. Really light skinned black people and pale mexicans are recognized as easily as white people.

Y’all are acting like photons are racist and proving OP’s point.",3
post5hb,richly branching,1.422454390748317,highest,"Phenology was physical that didn't make it magically not racist. There's no physical reason that a camera can't pick up a black face just as well as a white one, it's a failure of the neural nets analyzing the image. You feed them a biased dataset and they cannot do anything but recreate that bias. You feed them disproportionate white faces and it will be disproportionately inaccurate on nonwhite faces.",4
post5hb,richly branching,1.422454390748317,highest,[deleted],3
post5hb,richly branching,1.422454390748317,highest,"If the AI was hugely disproportionately misidentifying white people, and this faulty AI was used by police to flag potential criminals, I have a feeling it would not be being used by the police right now.",4
post5hb,richly branching,1.422454390748317,highest,"Well nobody seems comfortable calling the tool racist, nor the tool's makers, nor the tool's users, and yet from that combination some racism magically happens so I don't know what part of the pipeline you want to attribute it to but it's not there by accident.",4
post5hb,richly branching,1.422454390748317,highest,"By the system, I mean the technology itself in isolation.",3
post5hb,richly branching,1.422454390748317,highest,"It works differently for people of different races. Insofar as a *thing* can be racist, the algorithm is racist.",4
post5hb,richly branching,1.422454390748317,highest,Confirmation bias.,2
post5hb,richly branching,1.422454390748317,highest,"Racism means something different depending on who you ask, but if you ask me the intention behind this system is irrelevant. The fact that the effect is a problem specifically for people of some ethnic group/race means it's ""racist"".

So if the system they're using misidentifies black people really easily, and that results in more black people being arrested falsely and then released, you could say that it's racist. Even if you're giving them the benefit of the doubt, and saying it was an accident.

That's how a lot of people define racism. The reason is that you can't prove someone's intentions that easily, all that matters is the effect.",2
post5hb,richly branching,1.422454390748317,highest,"Facial recognition systems can’t spot black people because they don’t reflect enough light to find their facial features. On a 2d contrast based image, especially if it’s low resolution or poorly lit their facial features simply don’t stand out enough to be detected.

Imagine if you saw four different black people on a grainy, low resolution security camera with bad lighting 20 feet from the camera. Could *you* tell who they are? Would that make you racist?

This isn’t racism and you’re just deluding the term by calling it racism. It’s a poorly designed system that we shouldn’t be using, but it’s not racist.",3
post5hb,richly branching,1.422454390748317,highest,"It's like you didn't even read my comment.

If every person only had a grainy photo as an ID, and because of that I could only identify and let through people with light skin tones, that system would be considered racist.

Even if I had no malicious intent at all, the result of my actions and the system would be that people with dark skin tones are stopped because their id can't be verified, while only people with light skin tones are let through.

There are plenty of faulty systems like this in the world that disproportionately affect people that are poor, live in a certain area, or have a certain appearance. Whether it's intentional or not is irrelevant when determining if it's discriminatory.",4
post5hb,richly branching,1.422454390748317,highest,"If a facial recognition system is significantly more likely to misidentify black people, then it's just bad design and unconscious bias by the people who fucked up when they were training the algorithm.

I a facial recognition system is so likely to misidentify black people that it's as bad as random chance *and the cops know this and insist on using it as evidence against black people anyway*, it's getting kinda racist.",2
post5hb,richly branching,1.422454390748317,highest,"Because the system has to have input from developers on how to read the data it's receiving. If the developer is racist/can't tell black and brown people apart, then the system will have the same issues. The system in and of itself isn't conscious enough to be 'racist', no, but if it was programmed by people who are (and research shows that even the most loveral hippy dippy white person has some latent racist responses) the end result is the same.",2
post5hb,richly branching,1.422454390748317,highest,"It seems more likely that misidentifications will occur more frequently with people who have darker skin simply because it's more difficult to identify facial features due to less light being reflected from their faces, therefore creating less data for the software to work with.",3
post5hb,richly branching,1.422454390748317,highest,"The problem isn’t with lighting and instead with bias and limited information in the training sets. 

It’s the same with gender of equivalently light-skinned people as well. From the article below, light-skinned men are correctly categorized each time, whereas light-skinned women are incorrectly categorized  19% of the time. The numbers only increase for darker skinned individuals, and the gender gap exists there as well. 

https://www.theverge.com/2019/1/25/18197137/amazon-rekognition-facial-recognition-bias-race-gender",4
post5hb,richly branching,1.422454390748317,highest,It's funny because this is obvious and clearly the right answer but everyone is gonna play dumb because they would rather it just be racist.,4
post5hb,richly branching,1.422454390748317,highest,People that don’t work with the tech need to stop perpetuating this outdated factoid.,4
post5hb,richly branching,1.422454390748317,highest,"Sure. And yet, here we are with smartphones that can identify you in a scarf and sunglasses",4
post5hb,richly branching,1.422454390748317,highest,"This comment shows complete ignorance for even the basic methods of AI training, and because of this ignorance attribute malice where there is none.

If the system is trained with primarily Caucasian photos, it will have difficulty with nonCaucasian faces and vice versa.

This article exists because posters like this eat it up and beg for more because it reinforces their worldview that all white people are racist, they even admit it in the post.

>research shows that even the most loveral hippy dippy white person has some latent racist responses",3
post5hb,richly branching,1.422454390748317,highest,">If the system is trained with primarily Caucasian photos, it will have difficulty with nonCaucasian faces and vice versa.

Besides that, I also wonder if the vice versa applies. It might well be that the current tech and cameras are just naturally worse at detecting non-white people, eg because black skin reflects less light.",4
post5hb,richly branching,1.422454390748317,highest,So why don't they just train it with non-Caucasian faces more before implementing it to solve the problem?,4
post5hb,richly branching,1.422454390748317,highest,"Thats not how it works at all. The System is training itself to identify whatever you want it to identify. You need good training data though. Lighter Skin will reflect more light so pictures of white people will be better training data than pictures of black people. 

If the system was programmed by the most ""racist"" people on this planet but would read faces in 3D via some kind of laser that perfectly catches every facet of a face no matter the lightning conditions then it would learn to identify black faces just as well as white faces.

 Black faces don't produce good training data. That's all.",3
post5hb,richly branching,1.422454390748317,highest,"Victim mentality, although I do think facial recognition technology should be getting any better, nor start being applied to the general populace,",2
post18hb,richly branching,1.3953958801240345,highest,Why did they accumulate so much without cashing out? You can take payments every 3 days.,1
post18hb,richly branching,1.3953958801240345,highest,Wondering the same,2
post18hb,richly branching,1.3953958801240345,highest,"Same.  I have earned well over $6k with DA and I withdraw it to pay my mortgage and things like that. If there is over $250 in there and I have a withdrawal due, I withdraw it. That's usually every 3 days.  I can't think of a good reason to leave that much money in there, it would take a fair amount of time to rack that up, certainly more than 3 days!  I trust DA as I have never had an issue working for them or being paid, ever, but I wouldn't leave $6k in there. I wouldn't leave that in PayPal.  If it's not paying something the only place for it is in a bank account earning interest ($300 a year at 5%)",3
post18hb,richly branching,1.3953958801240345,highest,My only thought was that they were logging hours that didn’t match with the work that was being produced -and/or- inputting basic minimal responses that are clearly not inline with project instructions.,4
post18hb,richly branching,1.3953958801240345,highest,"Thank you for responding with this. I have only been working with them for a couple of weeks, but your described experience has also been my own.",4
post18hb,richly branching,1.3953958801240345,highest,"Give it time. My bet is that they will “fire” you for no reason sooner or later. Probably right after a promotion and a message stating how good of a job you’re doing. It’s happening to everyone. And I’ll speak for myself when I say I carefully read all instructions for every project and was darn good at the tasks. Lost over $1200 that I had made over the past week between then and my last cash out. Just don’t quit your day job… I talked up this company to soo many people because it was really great. And it IS great while you’re still there. But me and tons of other people keep having this same thing happen, a lot of us for no reason, or I’ll speak for myself at least, and it’s only a matter of time before it happens to you too. I hope it doesn’t, but don’t be silly like I was and used this as my only source of income :(",4
post18hb,richly branching,1.3953958801240345,highest,"Hi - hope you can help me.  I was downsized Sept of last year and my unemployment benefits are about to run out.  I'd like to get gig work with DA. I setup an account and the whole nine, but it just says they have no work.  How do you start getting work with this company?",4
post18hb,richly branching,1.3953958801240345,highest,Do they provide a 1099!?,4
post18hb,richly branching,1.3953958801240345,highest,Is DA still a thing? Would I be able to do it from Aus?,4
post18hb,richly branching,1.3953958801240345,highest,How do taxes work with them? Is it a W9?,4
post18hb,richly branching,1.3953958801240345,highest,I completed my starter assessment yesterday how long they take to get back with results?,4
post18hb,richly branching,1.3953958801240345,highest,"I’m looking into doing DA, but i see alot of threads about it it being a scam, is it worth it ?",4
post18hb,richly branching,1.3953958801240345,highest,how long does it take to hear back for them after applying?,4
post18hb,richly branching,1.3953958801240345,highest,"Are you doing DA full time? Thats great that you get that much work. If you don't mind me asking, how much do you make per month?",4
post18hb,richly branching,1.3953958801240345,highest,"Yeah. This doesn’t make sense. I cashed out about $2k in my first month with DA. How long were these guys stacking cash and more importantly, why? If they were simply deactivated for poor performance then they’d be allowed to withdraw their funds. Seems like they were up to something shady to be banned completely.",2
post18hb,richly branching,1.3953958801240345,highest,"Side topic here but I've just started with Data Annotation and I'm doing the onboarding. How did you track your time? I'd like to start off on the right foot on this platform. Thanks for any help or suggestions you can give. 

![gif](giphy|AeWoyE3ZT90YM)",3
post18hb,richly branching,1.3953958801240345,highest,This sounds like victim blaming. There's nothing wrong with being owed $6k by a company. That is a normal paycheck.,2
post18hb,richly branching,1.3953958801240345,highest,"You have the option to withdraw funds every three days. In order to earn enough to have 6k you would have to work three to four weeks at top pay, full time. There is absolutely no reason to accumulate and leave so much money there. Would you allow your boss to hold your check when you could cash it out? You don't even have to transfer it to your bank, you can hold it in PayPal after cashing it out from DA.

It sounds like there is more to the story that hasn't been disclosed. The person posting this complaint ""for their friends"" made this account on the same day they posted the complaint. No answer has been made to what period of time the funds were earned in or any other details. The company is definitely known to lock out folks who have violated the code of conduct - but folks who just did subpar work still get to cash out, even if future work is locked out.",3
post18hb,richly branching,1.3953958801240345,highest,There's nothing wrong with having a large paycheck there's something illegal about company not paying it you don't really need a wall of text to understand that,4
post18hb,richly branching,1.3953958801240345,highest,"There could be some edge case reasons to not cash out regularly, but most of them involve deferring taxable income to the next year. Personally, I think the risk of something going sideways is not worth some potential small tax savings though.",4
post18hb,richly branching,1.3953958801240345,highest,It's not victim blaming. You would be an idiot to leave 6k+ with any affiliate.,3
post18hb,richly branching,1.3953958801240345,highest,"At the same time, I would like to know reasons why a person might be locked out even if the company can't address specific situations. I do work for DA and move my money regularly even if I only managed to get in 30m but money owed is money owed and if there is no rule against stacking the cash then that by itself wasn't wrong.. 6k is a lot of hours of work to leave sitting there though I wouldn't be able to afford to just leave that there. Mainly I hear people are having a good experience with the company which is why I got involved .
 So far I am pretty happy with the work myself.. but would love to know more on this.",4
post18hb,richly branching,1.3953958801240345,highest,"> You would be an idiot to leave 6k+ with any affiliate.

This is literally the definition of victim blaming. ""You would be an idiot to leave yourself so vulnerable to becoming a victim!""",4
post18hb,richly branching,1.3953958801240345,highest,Seems like you would be an idiot to complain on the internet about what other people do with their money but you do you,4
post18hb,richly branching,1.3953958801240345,highest,You are attacking the person instead of the issue at hand.,4
post18hb,richly branching,1.3953958801240345,highest,"No it isn't. When you are a freelancer, under a contract, you are running a business. You take your money owed. They are not employees, so it's their responsibility to get their pay. 


There's definitely more to the story. If you can cash out in 3 days, there's no reason leave it. These ""friends"" are definitely idiots, and most likely were scamming the company and got caught.",3
post18hb,richly branching,1.3953958801240345,highest,"I agree. Of course it makes good sense to cash out to avoid just such a happenstance that you lose access to your account.  But this is a gig job where you signed a contact to be paid money -- not microsoftrewards or some promotion game where you use a VPN or violate some other rule and they can seize your points. 

Even if OP did something that made DAT cast doubt on his billable hours, normal business practice is to issue written communication saying such and providing the employee with a mechanized option to dispute the company's account. 

Not saying DAT isn't worth taking the risk -- it's a better side gig than most and some people have made a lot of money from them. 

But it isn't remotely normal to lose access to PRIOR EARNINGS just because you are no longer employed by the place where you earned them.  So knowing this can happen is extremely useful information.  

(you don't have to be rich to let the money pile up. Sometimes people delay cashing their check for a few days just so the money doesn't burn in whole in their pocket over the weekend).",3
post18hb,richly branching,1.3953958801240345,highest,"Payments for timed projects are pending for  7 days , then 3 days to wait between withdrawals. If someone, as I did, worked for 10 hours a day on $40/h projects,  it's easily $4000.

For example:  
I worked from March 1st. First withdrawal is on March 10 for work up to March 2.  The next withdrawal is on March 13. If they cancel my account on March 13, they don't pay me for work done after March 2, that is 10 days to March 13.",2
post18hb,richly branching,1.3953958801240345,highest,"If you were working 10 hour days, 7 days a week - along with working for Telus in the same time frame - there is no way that you were able to maintain quality of work without violating the CoC to some degree. They only refuse payment for Code of Conduct violations, not for crappy work. If you were using AI to generate some of your content, farming your work out to other people, reusing material, etc etc then yeah - they refuse to pay you because you didn't uphold your end of the agreement and you didn't actually earn that money. Texas Workforce isn't gonna help you, BTW - as an independent contractor you will have to take them to small claims court to fight your nonpayment.",3
post18hb,richly branching,1.3953958801240345,highest,"Telus is a freelance job that pays $11, there is no reason to think that I would spend any time on it if I have something to do for $40/h.   I know, nobody can help me, the DAT can do anything with impunity. They hide their identity for a reason. I just tell other people what they can do other than complaining here on Reddit.  
The small court is useless: 1) I cannot serve the DAT with subpoena, as the site is registered anonymously; 2) the small court decision is impossible to enforce, as nobody takes it seriously.",4
post18hb,richly branching,1.3953958801240345,highest,My guess is they were programmers making $60 an hour so were comfortable with that.,2
post18hb,richly branching,1.3953958801240345,highest,If they were new to the site they weren't making $60/hr.,3
post18hb,richly branching,1.3953958801240345,highest,It does not say they were new accounts.,4
post18hb,richly branching,1.3953958801240345,highest,Why is anyone tracking how much a person is accumulating?  That’s their business whatever they do with their money. Anyone taking money from account that is NOT theirs is called theft and anyone monitoring ur account without permission is called invasion of privacy,2
post18hb,richly branching,1.3953958801240345,highest,"Not knowing anything about the company you're speaking of, I'll just say some folks are unbanked for whatever reasons and have to take some extra steps to move their $ around. So it kind of makes sense if they were new/new-ish and just procrastinated on that detail. But just like doordash offers dashercards I cannot fathom why any such service wouldnt have offered that as an option upon signing up. Maybe theese two will get a random paper check in the snail mail like 30 days out.",2
post18hb,richly branching,1.3953958801240345,highest,This company only pays out through PayPal. They disclose this up front.,3
post18hb,richly branching,1.3953958801240345,highest,"I can’t deal with PayPal; they’re almost the biggest scam ever. They just randomly grabbed some money that was sent to me and said that I owed it to them (which I didn’t), but would never respond back with a concrete reason why they felt I owed it to them. It was only ?30 or so, but just imagine how much bank they make by taking peoples money x however many people they get money from. They are unscrupulous and there’s no way to talk or email someone who isn’t reading or replying directly from a generic script",4
post18hb,richly branching,1.3953958801240345,highest,The payouts appear to be to PayPal.  ONE may choose to leave the funds in their PayPal account and get a PayPal debit card.  Having an old school bank account is not necessary but an option like any other linked account PayPal allows.,3
post18hb,richly branching,1.3953958801240345,highest,"No. You can't be unbanked and work DA. There's no random paper checks or snail mail. Nope. They use Paypal, and they even PAY YOU in onboarding to make sure your Paypal account is set up properly with correct email address, etc. Getting your money transferred into Paypal involves clicking a button. There's more to this story.",3
post18hb,richly branching,1.3953958801240345,highest,Because they were scamming the system and got caught,2
post18hb,richly branching,1.3953958801240345,highest,"You can cash out every 3 days ONLY for pay per task tasks. Not hourly tasks. Those pay once a week. And you can ONLY cash out every 3 days. So if you work 3 hours on a Monday, you can cash out for JUST those 3 hours the following Monday, but then have to wait a full 3 extra days before you can cash out anything you made after that Monday (as long as it’s within the 7 day period). So THATS why we accumulated so much and lost it all. Because it’s complicated. Some people have bills and need Monday, Tuesday, and Wednesday’s wages to pay those bills. So we’d have to accumulate nearly two weeks of wages without cashing out in order to cash out the full amount one may need for those bills if that makes sense. You can only cash out once every 3 days, hourly tasks are paid out every 7 days (which this money is deposited into your account). Let’s say it’s been 7 days and you get your wages from the day you worked 7 days ago, but you just cashed out yesterday. You now have to wait 3 more days to cash-out the wages you earned 7 days ago. And if you work every day with high paying hourly tasks, it adds up quickly. I lost $1256 bucks for a week and two days worth of work before getting “banned” for no reason. It’s not as easy as work, and get paid 3 days later. If you worked there, you’d understand. It’s just not as simple as it may seem or not as simple as they make it seem.",2
post18hb,richly branching,1.3953958801240345,highest,"I know how it works, I've been doing it for months. Earning 6K in that amount of time is unlikely without low quality work or a violation of CoC by account sharing. You are misrepresenting the pay - it doesn't pay ""once a week"", you can cash out the money exactly 7 days after you did the work to allow time for review of the work. I cash out twice a week, I can get money from as recent 7 days prior (and do, every week, twice a week).",3
post18hb,richly branching,1.3953958801240345,highest,"lol, no my friend, I’m not misrepresenting, I’m basing my answer on actual facts and experiences with this specific company. Your using terms like “unlikely” makes me think that you and I aren’t speaking about the same company. DA allows you to cash out every 3 days period. Hourly projects are paid out 7 days after completion. If you work on a Monday, let’s say, and go to cash out the next Monday, then you can! UNLESS you JUST cashed out on Sunday, which would then mean you now have to wait until Wednesday to cash out for the previous Mondays work. And by cash out I mean getting money into your PayPal. There’s a countdown timer and everything letting you know exactly when your last cash out was and when you can send money to your PayPal again. I’ve got screenshots if you’d like to see them. Just because you don’t agree, doesn’t make a true claim a misrepresentation. Non hourly tasks pay out ever 3 days, hourly tasks pay out every 7 days. Separately from that/ you’re only allowed to send money to your PayPal every 3 days despite the previously mentioned 3day/7day rule.",4
post18hb,richly branching,1.3953958801240345,highest,"I don’t think you understand what I’m saying lol. Or you don’t h see stand what YOUR saying. It’s a 7 day review period, allowing you access to the money you made 7 days prior. If you haven’t cashed out within 3 days before that 7 days, then you can cash out that day. Heck, you can sometimes cash out 3 times a week if you do it correctly. UNLESS your within that 3 day rule/vs the day you actually worked and it being after the 7 day review period",4
post18hb,richly branching,1.3953958801240345,highest,"Also I’m not sure who mentioned making 6k, but it sure wasn’t me friend. I pulled in between 2-4k a month when I worked for them based on my own choice to work however many hours.",4
post18hb,richly branching,1.3953958801240345,highest,Remember- just because it didn’t happen to you doesn’t mean that it didn’t happen to someone else. Glad you’ve still got a job there/ but a ton of us aren’t as lucky. And most of us unlucky folks did every single little thing right. Don’t be ignorant to common logistics -no offense😊,4
post18hb,richly branching,1.3953958801240345,highest,"I think they may have been doing something sketchy, like copy paste reviews or not being careful with details. I agree that having 6000 and not collecting is weird.",2
post18hb,richly branching,1.3953958801240345,highest,The question fails to address the complaint.,2
post18hb,richly branching,1.3953958801240345,highest,This. The story OP claims sounds suspect.,2
post18hb,richly branching,1.3953958801240345,highest,[deleted],2
post18hb,richly branching,1.3953958801240345,highest,What proof is out there that they have bad business practices other than a flood of people online bitter because they never got approved into the work flow. Im sincerely asking as I am curious.,3
post18hb,richly branching,1.3953958801240345,highest,[deleted],4
post18hb,richly branching,1.3953958801240345,highest,is that even relevant?,2
post20hb,richly branching,1.3747981746419655,highest,"I have yet to see AI replace or do any meaningful work in an enterprise environment or on an application that is more than just a simple frontend.

If you feel like the show is over, to me that suggests you are not building sites with any real features beyond basic CRUD forms or static displays.

I know this sounds shitty, but if you want your job to be more bulletproof, you need to start learning how to build applications that AI can't replicate.  AI isn't going to design, setup, and build your service bus that manages your mapping engine job scheduler which then calculates risk portfolios across Florida roof maps.",1
post20hb,richly branching,1.3747981746419655,highest,"Yeah, from my experience with AI it's just kind of like a more advanced autocomplete and helps me save time writing map functions and stuff like that...things I could easily do but consume time and energy I could be spending on more complex things.  But when it comes to understanding requirements, architecting projects, third party integrations and more complex coding it is REALLY bad.

It's a great productivity tool, but like you said, if you never find yourself needing to change it or even program from scratch you may be doing stuff that could have already been done with low/no code solutions already.

But I get that a lot of people here are doing agency work or other smaller, less functional websites that are more about producing the same thing frequently than bespoke function or complexity.  That's a valid way to earn a living and we probably will see AI eat up a lot of those jobs (though you'll still need someone who understands enough to fix when it's wrong.)",2
post20hb,richly branching,1.3747981746419655,highest,"I agree. I think AI llms, for me, it feels like having ten obedient interns in a team and get things done like 10-20 times faster. It can do simple functions that I would know how to do but would take me 10min… with ai, it takes 1min or less. 
But, when it gets too complex or a bit novel, it gets lost. 
It is sometimes not even suggesting  obvious improvements that you know as experienced developer. 
I agree it is probably able to replace those million times done job easily. 
By the way, I noticed that sometimes it’s very thorough ; I guess it’s because many people did it before. But for more abstract stuff, it is not so good. Recently it struggled with Promises",3
post20hb,richly branching,1.3747981746419655,highest,"I understand the need to downplay LLMs due to their obvious failure at handling esoteric and novel problems, but to act as if they don' t do any meaningful work is akin to having your head in the sand.  There are devs at all levels, staff-level engineers included, that have woven AI into their workflow.

It's so paradoxical to me, because there are insanely talented people on both sides of the fence and for those that flat out assume it's not helpful, it must come down to a few things.  Either their lack of commitment to the tool, there inability to prompt correctly or maybe even more obvious, their reluctance to let disruption happen to the craft they love so much.  Regardless, most of the software that the industry creates is basic CRUD applications, and frontier LLMS are MORE than capable at helping expedite that process - this goes well beyond ""basic CRUD forms"" and even includes fleshing out quality business logic.",2
post20hb,richly branching,1.3747981746419655,highest,"As a senior dev at a company with a relatively large scale software project: we use AI, but it's a slight productivity boost at best.  It simply can't handle the project in context.  It basically is just a slightly better than eslint autocorrection.

I did a hackathon recently where I tried vibe coding, and while I do think the AI helped me accomplish something that I wouldn't have been able to get done so quickly without it...  The codebase is a disaster.  Duplicated improper config all over the place, hard coded variables everywhere, nonsensical redundant architecture.  If I was at all worried about software security with this project I wouldn't be able to sleep a wink at night.  These AI can do some pretty impressive leet code assignments, but they're quite far from actually writing well structured clean code.",3
post20hb,richly branching,1.3747981746419655,highest,"I'm not downplaying it at all.  I use AI all the time to help with stuff similar to how I would use Google to search Stack Overflow.  Yes, AI can build CRUD applications to some extent.  It really depends on the amount of business logic that drives the form.  If its just a simple submit form, sure, but it really starts to fall apart once you start getting into actual logic.

I 100% know that AI is going to change the way we work, but I don't see it as a threat to actual development at this point.",3
post20hb,richly branching,1.3747981746419655,highest,"This is the problem with discussions on this subject: putting out fair criticism is met with being told you have your head in the sand or that you’re a Luddite. I’ve been using GitHub copilot, but it’s at best an elevated intellisense/visual assist suggestions tool. ChatGPT is sometimes more helpful than Google, but as broken as Google has been I still often get better results from a traditional web search.

I see a lot of marketing and hype around the future of these tools, but in today’s reality the promised features aren’t there, and as far as I can tell LLMs aren’t the road to the solutions people want the current products to be.

I’m often told “well look how fast things have progressed in the last few years” but if they knew anything about AI development they’d know that the current applications are built on decades of research and development. You just can’t argue with people who don’t work within the domain of reality.",4
post20hb,richly branching,1.3747981746419655,highest,"I've kind of lately started using it as a rubber ducky. Bouncing ideas off of, which it then searches the web for.",4
post20hb,richly branching,1.3747981746419655,highest,"I experience what you describe all the time. On larger codebases it often bungles the logic or the basic intention you're going after. It kind of makes sense though - the AI was never trained on _your_ specific problem, so unless your problem is generic (like a helper class or common dev pattern), the AI is going to do a lot more hallucinating.

As a concrete example, I see it when using CoPilot/vscode to write php docblock comments for my class methods while building out boilerplate. I would write the function signature, using a super clear and obvious name to state what it should do, likewise with parameter names (etc.), and after starting `/**`, it'll copy the docblock from a completely unrelated method (like the constructor). Makes me wonder if it read what I just wrote at all. It does this much more often in larger codebases and even just large class files with a lot of methods.

So I use it a lot like you do, surgical strikes to save time switching Windows and wading through ads and spam to look up a solution. But that being said, I never accept anything it provides at face value. I'll review every line and often rewrite half of it. 

And just from seeing and knowing every day how many hallucinations a tool like CoPilot still has, I can tell you vibe coding is going to lead to some serious tech debt in the future.

For a small throw-away utility, like a side tool you need to process some data, I'll be more lenient there and largely vibe-code. I'm still reviewing every line, just not as picky about style or best practices here.

But ultimately, _I'm_ dictating the logic and architecture, and it's just saving me time, clicks, and typing.",4
post20hb,richly branching,1.3747981746419655,highest,"I use AI all the time too, and I’m often surprised by moments where it feels like it’s reading my mind and anticipating a non obvious next move. It’s kindof spooky and I think it might do more in the future than I’m currently considering.

That said, I honestly am not seeing productivity increases because it’s become apparent that coding is a minor portion of my job. Analysis of what to do, and where to do it, is the majority of my job. How much time do other devs spend on the mechanics of coding around here?",4
post20hb,richly branching,1.3747981746419655,highest,I use it to do complex tasks but if I don't guide it then it may as well be a chicken.,4
post20hb,richly branching,1.3747981746419655,highest,"The other day I witnessed how British Rail uses AI to process delay refunds, using multiple AI agents. It wasn’t “creating” anything, but it was managing an entire workflow, making decisions based on available data and prompts that they used AI to refine. It really opened my eyes as to how AI can be used to solve real problems. 

We are doing website migrations with the assistance of AI. Think moving a 20,000 node site using 8 content types from a proprietary system to a new CMS. What used to take 80 hours now takes 8-16. 

We’re also finding that custom reporting can be enhanced with AI. With the right libraries and setup it’s incredible. You can ask the system something like, “Using historical sales data from the last 3 years and our current Q1 sales progress, create a forecast report for Q3 sales.”",4
post20hb,richly branching,1.3747981746419655,highest,"I have to agree with the person you replied to AI is near useless for coding outside of duplicating unit tests and documentation.

Software development inherently requires context - and lots of it. Something out of the box might work in a vacuum but in the context of an enterprise environment it quickly just creates a mess.

AI hasn't shown any ability to work with large context (yet) but it can one shot a really simple front end UI.

So right now it can scoop up the entry level stuff but no dev worth their salt is actually using it to write code.",3
post20hb,richly branching,1.3747981746419655,highest,"I disagree. AI won’t create your application for you, but try making it create the methods as you create the application. And the unit tests for those methods, and the infrastructure of you use IaC. Any dev willing to remain a dev, worth their salt or not, should learn how to use AI.",4
post20hb,richly branching,1.3747981746419655,highest,"> I have to agree with the person you replied to AI is near useless for coding outside of duplicating unit tests and documentation.

Not in my experience whatsoever.

> no dev worth their salt is actually using it to write code.

Git gud. It's a godsend for A and S-Tier developers. The better understanding you have of software engineering best practices, the more useful and time-saving it becomes. My code has never been of higher quality because AI frees up time to be more mindful and proactive in every step of the development process.

AI is your junior dev cranking out code, as you the architect and technical lead map out the problem domain, implementation structure and strategy.",4
post20hb,richly branching,1.3747981746419655,highest,">no dev worth their salt is actually using it to write code.
 
Gonna disagree here",4
post20hb,richly branching,1.3747981746419655,highest,"I used to have the same idea as you, that context is what AI was terrible at. That is until I tried Cursor, paired with Claude 3.7 and I was just amazed and disturbed in the same time.",4
post20hb,richly branching,1.3747981746419655,highest,"What exactly are you doing all day that involves making CRUD apps?


I simply copy paste my code templates/import libraries to do this and it's literally faster than anyone using code from LLMs.",3
post20hb,richly branching,1.3747981746419655,highest,"I agree. I’m using AI to build real apps and as long as you guide it well it can do real work. 

I made the same mistake everyone makes at first. 

Hey AI, make me instagram and expect to have a working app, then say it suck’s when it doesn’t do that. 

But if you break that down into small tasks, it will do it.",3
post20hb,richly branching,1.3747981746419655,highest,"I will say this, and this might be what you were trying to say but having deep domain knowledge is still the ONLY way to utilize AI in a professional manner.  This fact, alone, means quality devs will have to be in the loop, because no matter how efficient or fast, you need that expert intention to build quality software.

To be completely blunt, I don't see how less-than-quality devs won't be impacted. A very basic business example would be the impact on startup hiring.  If you have a few quality senior engineers who can now spit out boilerplate in a matter of minutes, why would the team ever scale up to a potential pre-LLM size?  The sad reality is, they won't and that efficiency driven by LLM may be a long-term trend within that organization.  Now, does the world need exponentially more software because if so, all devs might be good to go in the long run.",4
post20hb,richly branching,1.3747981746419655,highest,"This. LLMs will get better and do more. But if today you already feel replaceable by an AI maybe you ARE replaceable. Look. Any position has geniuses and morons, sinners and saints, humble and bold folk. Even if AI didn't exist, there is a subset of developers still in ""fake it till ya make it"" mode + who could have been replaced already. That's just how life goes. AI is an agent of change, but it didn't make change happen. That was just always part of life. 

Wake up folks. Are you replaceable by an AI bot tomorrow? Really? All your human capabilities and potential? Just like that?",2
post20hb,richly branching,1.3747981746419655,highest,"where the rubber meets the road is what the c suite executives believe and are willing to infest in (I mean invest in lol), 100,000 of thousands of tech workers are being fired for the 'ai god'.....now this is a bit premature and there will be much fallout which high priced programmers will be happy to fix for a large hourly fee of course :)

by then those ceos will have been fired for other ceos",3
post20hb,richly branching,1.3747981746419655,highest,I agree with everything with one exception. AI is actually pretty good at writing unit tests.,2
post20hb,richly branching,1.3747981746419655,highest,"I don't think the post was about that though. Of course they're not (yet) at a point where they can create complex backends or award winning designs but they do more than fine for basic gigs most web developers get. Which are things like designing a website for a local bakery or a barbershop etc.

And as a mainly backend developer, especially Claude can come up with designs I wouldn't be able to do myself if I spent a week. Couple weeks ago I was messing around and wanted to see what it could come up with for a page design for my webapp and the result made my jaw drop. It was at least as good as what a freelance designer would create for $50. And my app isn't that simple either. There were modals, quizzes, textareas and many different form elements on the page.

After seeing that I changed the way I start my new projects. I describe the page I want in detail to Claude and have it create the design for me. Then I put that design into a new route (I usually put it on 127.0.0.1/vision) and try to make mine look as good (better, if possible) than that. That way I'm also polishing up my design skills while not being completely dependent on it.",2
post20hb,richly branching,1.3747981746419655,highest,"I would agree with what you said.  I think part of the confusion I have is I have never really worked as a front-end dev or backend dev... I've always been full stack.  I have always been responsible for building everything from what the user sees and clicks down to the optimized databases and everything inbetween.

  
I know the industry shifted away from that, but it's what I've been doing for 20 some-odd years and I'm seeing the industry is shifting back that way this very moment. 

I definitely use AI to generate some basic details and designs and 100% agree it's good at doing that.",3
post20hb,richly branching,1.3747981746419655,highest,But in what way are WordPress and Shopify not already satisfying this market?,3
post20hb,richly branching,1.3747981746419655,highest,"Brother respectfully what are you talking about. 

I’ve played with Claude Sonnet 3.7 extensively 

All the designs it generates looks like they came from 2017. It’s still stuck on the flat design paradigm. 

At that point why not just get a template? Even the free ones are infinitely better than what Claude pumps out.",3
post20hb,richly branching,1.3747981746419655,highest,"> And my app isn't that simple either. There were modals, quizzes, textareas and many different form elements on the page.

There are no templates for this kind of thing. I described the business logic in detail and it returned a dashboard page that fits all my needs. Again, it's not winning any awwwards any time soon but I had it build many pages and most of them were pretty nice looking. The ones I didn't like were the ones I didn't give much attention to detail in the prompts so that could be it too. Idk, try some more detailed prompts maybe.

Though again I'm not specialized in frontend and I'm not a designer/artist. What looks great to me could be garbage to you",4
post20hb,richly branching,1.3747981746419655,highest,"Maybe you are playing with it wrong then?  Nah, couldn't possibly be your fault.  AI sucks.",4
post20hb,richly branching,1.3747981746419655,highest,"Maybe you are playing with it wrong then?  Nah, couldn't possibly be your fault.  AI sucks.",4
post20hb,richly branching,1.3747981746419655,highest,"That sounds very niche. Interesting, but niche",2
post20hb,richly branching,1.3747981746419655,highest,"Thats the whole point.  It's not a niche, it's just one example of thousands of business and enterprise apps that need to be built right now, right this very second.  These are the kinds of applications that developers need to start focusing on, and not static webapps with simple signup forms or the like.

I promise i'm not trying to be obtuse or a jerk, i'm just trying to share my viewpoint that there are literally thousands of companies and thousands of apps that AI is not going to build right now.",3
post20hb,richly branching,1.3747981746419655,highest,"100%.

It is indeed a shitty insight, but hopefully it serves as a wake up call for some people.",2
post20hb,richly branching,1.3747981746419655,highest,Yea pretty much. The only meaningful thing I’ve seen it do in enterprise is give better reasoning to laying off the terrible devs. Doesnt matter if AI code sucks if the dev code sucks as well. But all you have to do is be more than a coder.,2
post20hb,richly branching,1.3747981746419655,highest,Yes it will. That’s exactly what the hyperscalers and geospatial data brokers are selling to insurance and capital markets.,2
post20hb,richly branching,1.3747981746419655,highest,">I have yet to see AI replace or do any meaningful work in an enterprise environment or on an application that is more than just a simple frontend.

Yet. It's obviously heading that way, regardless of whether this will come by the LLM's alone or by introducing external assisting systems to handle the parts where an LLM on its own fails.",2
post20hb,richly branching,1.3747981746419655,highest,"Any real features? You building a dashboard or a website? Cuz if youre building ""real"" features it sounds like youre wasting a lot of time.",2
post20hb,richly branching,1.3747981746419655,highest,"True but it’s only a matter of time before models and apis come out that can increase the contextual awareness of the output. Currently, as many of said, it feels like autocomplete cause the tool is largely limited to looking at a single repo or service. But if they make it so you can broaden in input to include your backend etc it could get a lot better. 

I don’t think it’s coming overnight and it won’t be cheap. But it just has to be competitive with a human salary and it can completely undermine things. 

TLDR I wouldn’t want to be a junior dev right now let alone in 5 years. The job pickings are slim as is.",2
post20hb,richly branching,1.3747981746419655,highest,"Agreed, ive already started looking to change my career and my team is looking at how to most successfully phase ourselves out.  its a tough world",3
post20hb,richly branching,1.3747981746419655,highest,"I had a phone interview with OpenAI that didn’t go anywhere but I asked the recruiter “does the company have any policy around engineers coding themselves out of the job?” And they could only give me a trite “we make ai that helps people, not replace them” response. I would’ve been curious to see what the high level folks later in the interview process would’ve said but then again asking that kind of question would probably lose you the job! 🤣",4
post20hb,richly branching,1.3747981746419655,highest,"What exactly even is ""basic CRUD?"" Do you mean the final coding step after someone has figured out the project requirements, consulted all the stakeholders, determined the data models and workflows to be implemented, mapped out the how these elements will interact across multiple applications, deployed the infrastructure necessary to run it, implemented a comprehensive security policy, and described the rest in a way that a kid fresh out of school can understand so that they can implement a bit of UI around it?

I guess that's technically ""basic CRUD."" It's also something between 1% and 5% of the total work in any sort of moderately complex system.

The way I see it, talking about basic CRUD is about as useful as saying all programming is implementing some branching logic in an environment that can be described as a Turing machine. Practically everything a programmer does is going to create, read, update, and/or delete stuff, often across some sort sort of communication channel, backed by one or more data store of some sort. It's more a description of the environment than anything else. Figuring out all the things you're going to CRUD, and how all the information is going to transform and mutate in the progress is the hard part. Everything from building a website, to training an ML system, to implementing that service bus for risk portfolio calculations is going to involve these operations.

The whole AI is going to replace programmers thing seems to be largely kids fresh out of school, that don't realise that most of the ""programming"" they are doing is just menial busy work that the seniors give them so they have a chance to explore the problem domain a bit, before being given actual tasks. That and hobbyists that spent a few months learning to code, and then decided that they are actually master system architects because they managed to wire together 10 or 20 files that run a chatbot or something of the sort. 

These people have suddenly gained access to a tool that can understand the thing they're working on about as well as an expert that's never touched a particular codebase, but they don't have the context to realise that such an expert would need to spend a few months getting up to speed on everything before being confident enough to actually make any significant changes. They just see hundreds of lines getting generated, and figure that those lines are just as good as any other. It's sort of like deciding that some off-brand glue was good enough to hold structural components of a truck together, without understanding why most other people prefer to use mechanical fasteners for the job.",2
post20hb,richly branching,1.3747981746419655,highest,"what the christ is happening here

basic crud = submit a form to a post endpoint

non basic crud = tons of validation routines, business logic for dynamic drop-downs, permissions and validators for enable and disable, roles and rights management, and then all the stuff on the backend to process the result that isnt just dumping it into a database.


there is a difference, and its simple.  This is just high level from my phone because this is just too much to explain for something simple to understand",3
post20hb,richly branching,1.3747981746419655,highest,"My point is that ""basic CRUD"" isn't actually a thing that exists in a professional environment, outside of some boot camp or some trash tier off-shoring group somewhere. 

If you're in a real job doing what you define as ""basic CRUD"" then you're just working in the context of the things a lot of other people did. Just because you don't know about the other things that must happen, doesn't mean that these things don't happen, and that they won't affect the code you write. Eventually you'll have to deal with them, even if only because your ""basic CRUD"" isn't working.

You might as well talk about ""basic conditional logic"" or ""basic functions."" It's a meaningless distinction, because it's describing a tiny part of what the job entails. If you're actually doing this professionally, you simply aren't going to be doing much ""basic"" except when you're just starting out.",4
post20hb,richly branching,1.3747981746419655,highest,I spent the last 3 days fixing the fuckups of a colleague who blindly trusted AI to do his work… he’s never been a good programmer but AI has only increased his efficiency in fucking up lol,2
post20hb,richly branching,1.3747981746419655,highest,"Don't be like nokia AI is going to take most of the low level jobs, after that the middle level jobs, after that senior level jobs until a super AI computer is estabilished that can do anything. The one who owns that super computer will be a billionaire like bill gates owning microsoft in the 90s.",2
post20hb,richly branching,1.3747981746419655,highest,"i am terrified, what do you suggest I do to make sure im the billionaire?!?!?!",3
post20hb,richly branching,1.3747981746419655,highest,In a year it’s gone from useless to replicating entire applications in one shot. It’s even making games and AI agents to play said games… this is the worst it will ever be. If you think it won’t be able to crack basic maintenance and enterprise level systems soon. You are simply mistaken… most devs are just copy and paste bots from stackoverflow. It’s been the meme for the past 10 years at least.,2
post20hb,richly branching,1.3747981746419655,highest,"100% agree, man!  I actually sat down with my boss today to come up with a plan to step a phase out of 4 of our 6 developers.

After playing with cursor and 3.7 we see the value.   We expect a reduction in staff within less than 6 months.

I am stoked, my team budget is going to be so much leaner but in theory have the same productivity.


Im with you, man, AI is the shit and human devs are on the way out.",3
post20hb,richly branching,1.3747981746419655,highest,"??? 180 and changed tone. Can’t tell if you’re taking the piss. I worked fintech where we’re getting 300k a year inc bonus and options. We will definitely be replaced within a few years. Me and my team have cut all our stupid spending to prep. Good luck to all devs. But if you aren’t in the top 1 percent that are actually pushing the boundaries (researches, phds etc) your work is replicable by an AI.",4
post20hb,richly branching,1.3747981746419655,highest,Ai is doing meaningful work in our company and is at the core of what we do. However it's a block of our product and doesn't replace any devs. It just made our idea possible. Cannot go into details as it's sensitive tho.,2
post20hb,richly branching,1.3747981746419655,highest,[deleted],3
post20hb,richly branching,1.3747981746419655,highest,"Except the growth is blocked by the fact they use large language models and not true Ai. It's machine learning masquerading as Ai. 

I researched it and the easy gains are maxed out(data and brute force computing power). It's not like Moores law.",4
post20hb,richly branching,1.3747981746419655,highest,[deleted],4
post20hb,richly branching,1.3747981746419655,highest,"Sorry, but this feels like massive cope. AI will absolutely be able to replicate that, it's just not there yet. Anyone that's used Claude 3.7 will tell you that it can indeed do some insane tasks already. Combine that with copilot integration, Claude code, or cursor, and yeah... We're entering the phase where it's starting to materially impact workflows, even the complex ones. Speaking as a full stack developer at a large enterprise. We're at the opening phase right now. Give it 10 years at most (if not 5), and the entire field of development is going to be drastically different from current day. There's WAY too much money on the table for executives to not exploit this as much as they can to reduce workforce numbers and increase profit. They'll find a way to do it.",2
post20hb,richly branching,1.3747981746419655,highest,[deleted],2
post20hb,richly branching,1.3747981746419655,highest,"I definitely use it for writing tests in our Angular project, thats the truth!",3
post20hb,richly branching,1.3747981746419655,highest,tell me a feature that's not based on crud. I'll wait.,2
post20hb,richly branching,1.3747981746419655,highest,"I mean, yeah, 99.9% start with crud, but it can very quickly diverge from there with what it does with the info.  5 textboxes and a checkbox can be all it takes to kick off a calculation resource or generate complex financial reports, for example.

Not sure where you were going with this, dude.",3
post20hb,richly branching,1.3747981746419655,highest,If you're letting an AI develop blocking code on your async app then you fucked up long before,4
post20hb,richly branching,1.3747981746419655,highest,"\> AI isn't going to design, setup, and build your service bus that manages your mapping engine job scheduler which then calculates risk portfolios across Florida roof maps.

Claude can absolutely do that. And so can the new Gemini. You have no idea what you are talking about or you are just in denial.

I have been programming for basically 40 years and I think it's asinine to try to write programs without a SOTA LLM and coding agent/environment these days. Of course it still needs help and I prefer to give it my own architecture rather than let it dictate it for a lot of things,  but the best models absolutely can design (and setup whatever using tool commands or computer use). Yes it still needs help sometimes but it can do 80-95% of the work for applications as complex or more complex than the one you gave in the example.

And will continue to get better.",2
post20hb,richly branching,1.3747981746419655,highest,"Damn, you are right, I just tried claude code and it literally just replaced me and 4 other devs.  This is bonkers, we are all truly fucked.",3
post20hb,richly branching,1.3747981746419655,highest,Ha! Give it a few minutes. It’s over dude.,2
post20hb,richly branching,1.3747981746419655,highest,"Sure man, whatever helps you sleep at night",3
post20hb,richly branching,1.3747981746419655,highest,👍,4
post14hb,richly branching,1.3639907689457174,highest,"It's already happening, as he presents his outlook.
The biggest Fortune 500 companies are freezing hiring, while at the same time, increasing investments into AI agents.
As they developed strategies to replace human workers with AI agents, in everything from code writers to engineering.
Many sales positions as well as customer service Representatives.
Even Wall Street isn't immune from this. Jobs are being replaced in masses.
Why so shareholders can make even more money by saving on labor costs.
The bottom line is more important to the wealthy investors.
While all the AI companies are reaping massive investments from the ultra rich. 
The amount of money being invested is staggering, all with the ultimate intention to increase profits and reduce the labor force. 
We don't have to wait a few years for this to affect the average person, it's already started the tsunami is here. The first wave is crashing ashore. 
People like Sam Altman and Elon Musk, Jeff Bezos, companies like Meta and Tesla Amazon and Open AI are reaping the benefits, while the average worker will not have a job in two years. If you work in the majority of services industry including working for top Fortune 500 companies.",1
post14hb,richly branching,1.3639907689457174,highest,but...  who buys their product when no one has a job?,2
post14hb,richly branching,1.3639907689457174,highest,What you are missing (maybe) is that they are not thinking about what happens if every corporation does this. Instead they are just thinking about how their decisions will look on the quarterly balance sheet that goes to the board and shareholders.,3
post14hb,richly branching,1.3639907689457174,highest,"then they are not, strictly speaking, rational.

this is like all 100 customers stampeding to get into the 'short line' at the checkout. smart for one,  dumb for all.",4
post14hb,richly branching,1.3639907689457174,highest,"I think they are mostly thinking: what if my competitors do this first and we go bankrupt because we can't compete? 

What do they care about the consequences of everyone doing it if they feel they'll disappear on the shorter term if they don't do it?",4
post14hb,richly branching,1.3639907689457174,highest,"Exactly and this is called Game Theory.  “If I don’t do it, one of my competitors will and gain an advantage so I might as well do it to”. It’s precisely things like this that need to be regulated because of this psychological phenomenon and the implication",4
post14hb,richly branching,1.3639907689457174,highest,"Keep ai for scientific use. It was too early.

The problem lies in greed, abolish money first then release ai for everyone.",4
post14hb,richly branching,1.3639907689457174,highest,And probably not thinking past the next couple of quarterly earnings reports,4
post14hb,richly branching,1.3639907689457174,highest,"They will figure that out when they get there. Or at least, that’s the thought process. Right now there is an AI gold rush, and any executive arguing for anything other than aggressive pursuit of it will get axed quickly.",4
post14hb,richly branching,1.3639907689457174,highest,"Well it had to end somehow.  To be by short sighted greed seems poignant.  

See you all at the going away party",4
post14hb,richly branching,1.3639907689457174,highest,True.  The long game is not typically the domain of the greedy and the criminally insane...,4
post14hb,richly branching,1.3639907689457174,highest,"God... how I've learned to hate the ""quaterly cult.""",4
post14hb,richly branching,1.3639907689457174,highest,"THIS. The ruin of our version of capitalism comes largely from this. Capitalism itself is not evil. It’s a market competitively supplying goods and services to a demand, for a profit. But serving the corporations at the expense of the consumers and employees and state, giving corporations legal personhood, constantly trying to exceed unreasonable expectations to benefit shareholders, and managing by spreadsheet have ruined it. 
We need other metrics for success like how many employees are healthy and happy, able to survive and educate themselves, and their kids, what has been committed to the welfare of their localities, etc. Use the greed of the execs and give more tax incentives for this kind of thing and it might improve a little.",4
post14hb,richly branching,1.3639907689457174,highest,"Well that's where the credit card companies step in.


Here's how I know a.i. won't be good if it's the one making all the decisions then it should realize the easiest way to make a huge profit is cutting from the top.


What's the point of a CEO of all of the decision are made by a.i.",3
post14hb,richly branching,1.3639907689457174,highest,"“The development of modern industry, therefore, cuts from under its feet the very foundation on which the bourgeoisie produces and appropriates products. What the bourgeoisie therefore produces, above all, are its own grave diggers. Its fall and the victory of the proletariat are equally inevitable.” -Karl Marx",3
post14hb,richly branching,1.3639907689457174,highest,"Except that, theoretically, automation would allow the bourgeoisie to exist *without* a proletariat. If robots do all the work and make all the products, then the people who own the robots can have anything they want for free, and the rest of humanity can simply disappear.",4
post14hb,richly branching,1.3639907689457174,highest,"First two sentences, solid gold.  Third sentence, unwarranted optimism / millennarist fantasy.",4
post14hb,richly branching,1.3639907689457174,highest,"You just found out what Karl Marx figured before automation was called automation. [https://thenewobjectivity.com/pdf/marx.pdf](https://thenewobjectivity.com/pdf/marx.pdf) Because I like to be funny I used automation to write this summary.

>Marx argues that machinery creates a fundamental contradiction for capitalism because it simultaneously tries to reduce labor time while relying on it as the source of value. Here's how it breaks down: On one hand, capitalism, driven by competition, uses machines to make production more efficient, cutting down the amount of labor needed to produce goods. **This is good for capitalists because it lowers costs, increases productivity and increases surplus labor time**, enabling them to produce more goods for sale and increase profits. But, on the other hand, capitalism depends on labor time to measure value. **The more machines replace workers, the less labor is directly involved in making things, and the more difficult it is for capitalism to make a profit**. So, capitalism ends up in a bind: it needs to reduce labor to maximize profits, but at the same time, it relies on that same labor to generate value. This leads to overproduction, and the system becomes unstable, because the value is not being generated at the same rate by the labor that has been replaced by machines.

To be funnier, here's an AI generated podcast about it. [https://notebooklm.google.com/notebook/781b78aa-a1cf-4dd1-8a4a-8ff1096b4556/audio](https://notebooklm.google.com/notebook/781b78aa-a1cf-4dd1-8a4a-8ff1096b4556/audio)

You can do this with NotebookLM, just upload the PDF as a source and you can ask it questions and it will cite sections from your sources.",3
post14hb,richly branching,1.3639907689457174,highest,"Really funny how many people use the term ""late stage capitalism"" who also get upset about AI. Automation (reducing the absolute number of laborers total) is literally the thing that Marx says will cause a revolution and the collapse of capitalism.

""**A development of productive forces which would diminish the absolute number of labourers,** ***i.e.*****, enable the entire nation to accomplish its total production in a shorter time span, would cause a revolution**, because it would put the bulk of the population out of the running. This is another manifestation of the specific barrier of capitalist production, showing also that capitalist production is by no means an absolute form for the development of the productive forces and for the creation of wealth, but rather that at a certain point it comes into collision with this development."" - Capital, Vol 3, Ch 15

He also says this is inevitable and unavoidable due to competition:

""No capitalist ever voluntarily introduces a new method of production, no matter how much more productive it may be, and how much it may increase the rate of surplus-value, so long as it reduces the rate of profit. Yet every such new method of production cheapens the commodities. Hence, the capitalist sells them originally above their prices of production, or, perhaps, above their value. He pockets the difference between their costs of production and the market-prices of the same commodities produced at higher costs of production. He can do this, because the average labour-time required socially for the production of these latter commodities is higher than the labour-time required for the new methods of production. His method of production stands above the social average. But competition makes it general and subject to the general law. **There follows a fall in the rate of profit — perhaps first in this sphere of production, and eventually it achieves a balance with the rest — which is, therefore, wholly independent of the will of the capitalist.**"" - Capital, Vol 3, Ch 15

And how does he feel about the machinery itself?

""**It took both time and experience before the workpeople learnt to distinguish between machinery and its employment by capital, and to direct their attacks, not against the material instruments of production, but against the mode in which they are used**. The contests about wages in Manufacture, pre-suppose manufacture, and are in no sense directed against its existence. The opposition against the establishment of new manufactures, proceeds from the guilds and privileged towns, not from the workpeople."" - Capital, Vol 1, Ch 15",4
post14hb,richly branching,1.3639907689457174,highest,"I feel like I have to explain this a lot: they don't care. Companies these days only think about a quarter or three ahead. They legit do not care about the long term.

It's the MBA/corporate raider mentality and it's basically the standard amongst the managerial/c suite class in America. They've been educated to think operating ratios are like THE most important thing and it's reenforced by the investor incentive structure. You're rewarded based on quarterly performance, which means cost cutting is valued basically the same as improving the business or product and is MUCH easier to achieve.

Which should be obvious given how many of them think the US rail industry is super good (because they have really insane ratios) when in reality it's the corpse of a whale who died mid-swim and hasn't quite hit the bottom yet.",3
post14hb,richly branching,1.3639907689457174,highest,"I just had to award you not only for the very accurate description of the fundamental problem with capitalism, but for that last graf and metaphor which was solid gold -- solid gold example, solid gold analysis, brilliant metaphor which I will probably steal at some point.",4
post14hb,richly branching,1.3639907689457174,highest,They just want to see people suffering and getting dependent on them.,3
post14hb,richly branching,1.3639907689457174,highest,"The elites don't need money if the machines they command provide any labour they desire, so they don't need customers. Money will fall out of the picture.",3
post14hb,richly branching,1.3639907689457174,highest,"The rich. It is not necessary to sell products to the working class, so there is no reason why the economy cannot shift to address mostly the wealthy’s needs.",3
post14hb,richly branching,1.3639907689457174,highest,[You got it](https://youtu.be/MYB0SVTGRj4?t=203).,4
post14hb,richly branching,1.3639907689457174,highest,"you're thinking late feudal?  the consumers are the 1 percent, everyone else labours to produce wealth for them to hoard and consume?  big retooling needed to get back there, but obviously they are working on it.",4
post14hb,richly branching,1.3639907689457174,highest,I agree with your sentiment but look at civilizations throughout history - a wealthy ruling class and poor masses is the default setting.,3
post14hb,richly branching,1.3639907689457174,highest,They tend to fail in this exact fashion as well,4
post14hb,richly branching,1.3639907689457174,highest,"Only within societies which we have dubbed ""civilizations."" These structures were by no means inherent across all of humanity, nor a natural one.",4
post14hb,richly branching,1.3639907689457174,highest,"Money is exchanged for goods and services. If they have good enough AI, they don't need humans to get the things they want, and that includes buyers as well as employees.

The more clever industries will shift to automated modes of existence. Those catering to human beings will shrink and shrivel as the human being becomes increasingly destitute.

I'm sure the CEOs will cheer as productivity increases, as I'm sure the shareholders will cheer when they can replace the CEOs with far more obedient and clever AIs, ones that can invest and become shareholders as well.",3
post14hb,richly branching,1.3639907689457174,highest,"Universal income funded by the corporations, we will basically be work-free slaves.",3
post14hb,richly branching,1.3639907689457174,highest,"You guys still think money and capitalism are end goals?

They are tools to redirect power and control.

You don't need them anymore once you accumulated enough power and control to use more..direct tools.",3
post14hb,richly branching,1.3639907689457174,highest,Other corpos doing the same thing?,3
post14hb,richly branching,1.3639907689457174,highest,They’ll just sell and ship their products to wealthier countries,3
post14hb,richly branching,1.3639907689457174,highest,"its not their job to ensure poeple in general have money. their only job is to ensure adding value to share holders. 

the govt will have to figure out ways to allow people to afford food [UBI]",3
post14hb,richly branching,1.3639907689457174,highest,Not to mention the economic affect it will have in major cities. If AI truly replaces people mass layoffs will happen and high skilled workers will have to shift industries and move out of tech hubs,3
post14hb,richly branching,1.3639907689457174,highest,"Corporations don't care about that anymore. They care about how they look at the stock exchange. And that's something that has little to do with how much they sell. It's not about value anymore, it's about beautification.",3
post14hb,richly branching,1.3639907689457174,highest,"Down the line but we're going to have to live through potentially many years until society is willing to change. During the transition many, likely most, are going to just have to eat the consequences and spend their savings while the rich get massively richer. Or maybe not! Maybe everything will be fine!",3
post14hb,richly branching,1.3639907689457174,highest,"It doesn’t matter if the money is valuable. It’s about getting all of it and having more than your fellow man,  not spending it.",3
post14hb,richly branching,1.3639907689457174,highest,The government that they own.,3
post14hb,richly branching,1.3639907689457174,highest,They’ll take over the government and funnel tax money into subsidies.  They will make deals with each other hyping the deals and pump their stock. People will invest those stocks and increase the worth of the companies while taking some profit to buy the services and products of the same companies.  Your income will go down but your investments will go up until something collapses. the government will bail out those who are in charge.  Rinse repeat dystopia.,3
post14hb,richly branching,1.3639907689457174,highest,"Exactly. And ""AI Agents"" will lead to customer frustration, it's a huge opportunity for China to fill the blank with actual humans providing actual service. Tesla as a car company is mostly already dead, they just don't know it. you can get a comparable electric car from a china brand at a fraction of the price, that is why tariffs are all the talk. they aren't there to help the voters or fight China but to preserve status quo.",3
post14hb,richly branching,1.3639907689457174,highest,Perhaps AI consumers order stuff from AI producers without anything being produced and the money is just shuffled from corporation to corporation and companies manage to include a tax break.,3
post14hb,richly branching,1.3639907689457174,highest,Also wtf is the product.,3
post14hb,richly branching,1.3639907689457174,highest,"“Capitalism slits its own throat” 
-paraphrasing Marx",3
post14hb,richly branching,1.3639907689457174,highest,"You stop that right now, that’s entirely to much thought, nothing exists outside of Q1 you ignorant swine. Maaaaybe Q2 but that’s.. that’s oretty out there",3
post14hb,richly branching,1.3639907689457174,highest,Unfortunately: [other wealthy people](https://youtu.be/MYB0SVTGRj4?t=203).,3
post14hb,richly branching,1.3639907689457174,highest,"They will change HOW they profit from individuals rather than conventional money transactions. If we are talking about retail it will change what they are selling and how people are consuming it. Data which can be sold for example like social media profits immensely from 

The top companies will always always always be ahead of the curve so the new argument I see here a lot of ""what happens when nobody has money to buy things"" will always be irrelevant because to the companies who are able to adapt and adjust people will always be a commodity with or without money",3
post14hb,richly branching,1.3639907689457174,highest,They will look for government handouts,3
post14hb,richly branching,1.3639907689457174,highest,"Well, by then they will have sold out enough shares to buy things that hold value through a recession, depression, and economic collapse. Food, agriculture, real estate, water, food/water processing, energy, technology, ""defense,"" and medicine all have fundamental value. They will own and be able to defend large amounts of that.

Money is just an exchange medium, you can still leverage promises for the future. Power is power.",3
post14hb,richly branching,1.3639907689457174,highest,"That's where basic universal income comes in.

People have just forgotten that the idea is inherently capitalistic.",3
post14hb,richly branching,1.3639907689457174,highest,They sell to themselves and upper middle class whales/ DINKs that maintain jobs due to their place as PMCs or as engineers. (aka most of reddit),3
post14hb,richly branching,1.3639907689457174,highest,"I swear people always make this argument and they miss how for hundreds if not thousands of years there were peasants and kings.


Did the kings need the peasants to buy things? No. You got taxed, used and abused.


There are no jobs? You'll be sent to wars or to Mars to set up shit and die there. They'll find a way. You are not protected because at this specific moment they are after your wallet. They are just keeping the status quo until they can stop pretending you were ever in control. I love how elections keep the illusions going, as if it's not always a rich guy bought and paid for from one of two or three parties lol",3
post14hb,richly branching,1.3639907689457174,highest,"What a weird and fundamentally wrong take. 

Taxed of what, if I don't have anything? 

Medieval society wasn't some pop-culture idea of dystopia - peasants kept a large share of what they produced, so they could reinvest it into trade (either as small-scale merchants themselves or by selling their excess produce to organized merchants). Whenever this system broke (due to war, excessive taxation or natural disasters like famine or plague), this universally led to a collapse of the society in the local area (usually a violent collapse). 

Moreover, relationship between feudal and peasantry was usually regulated by charters and laws, which specified obligations of both sides of social contract.

Unironically, most medieval societies had a much better grasp of sustainability than modern emergent oligarchies.",4
post14hb,richly branching,1.3639907689457174,highest,The 20th century was a historical aberration in almost every way. We are reverting to mean.,4
post14hb,richly branching,1.3639907689457174,highest,Immigrants and foreign workers maybe? And China or India?,3
post14hb,richly branching,1.3639907689457174,highest,"I’m exec level in a huge company and can confirm. Junior to mid levels frozen as our upper management “wait and see” how we can have AI do their jobs (I live in Germany where hiring someone is essentially a life long marriage). 

It scares me because we are witnessing the death of critical thinking. These AI agents won’t push back on managements dumb and politically driven ideas. And our younger population is increasingly delegating their information synthesis to computers. 

Easier people to control and influence by those with the means.",2
post14hb,richly branching,1.3639907689457174,highest,[removed],2
post14hb,richly branching,1.3639907689457174,highest,"It won’t work but the Executives won’t ever admit they were wrong and will pretend not to understand sunk cost

As long as they can fuck over labor it’s worth the cost",3
post14hb,richly branching,1.3639907689457174,highest,[removed],4
post14hb,richly branching,1.3639907689457174,highest,"I work for a top fortune 50 company and we're still using ancient tools and software from 25 years ago, there's no way in hell they'd survive a day trying to replace people with AI. They probably couldn't even afford the AI and if they did everything would just break instantly. Our company would need to completely overhaul literally everything before AI would even be compatible with its systems and it can't afford to do that.",3
post14hb,richly branching,1.3639907689457174,highest,"I keep trying to use it because I want it to be useful to me. I want to get more done and do less work.

I actually asked it how to use its own API and it straight just made shit up. Gave me some fake instructions that looked correct 🙄.

Yeah I don't think they'll be replacing my job any time soon. I'll get plenty of work unfucking the mistakes it makes I'm sure.",3
post14hb,richly branching,1.3639907689457174,highest,"We are literally at infancy stage. Only a couple of years in. There is virtually no chance that this is as good as it gets and there will be no improvement from here on in. 

So maybe it won't happen for 10 years or 50......but it will happen at some point and the same problems will arise. Better for us to be prepared and talking about it now.",3
post14hb,richly branching,1.3639907689457174,highest,">We are literally at infancy stage. Only a couple of years in.


We are many decades into the research. There's a lot of hard work to get us to this point. What is visible may only be a few years in, but it's been going on a lot longer underneath the surface.",4
post14hb,richly branching,1.3639907689457174,highest,"We will have vastly worse problems in 50 years due to collapsing global ecosystem.  Extreme weather will be far more extreme and will have a major impact on global food supply.

Gonna get really ugly",4
post14hb,richly branching,1.3639907689457174,highest,"We're already decades in to machine learning research, we're only in the infancy (although honestly id argue we're well into) the latest hype cycle. This happens every few years in ML, it is literally taught in schools this cycle. Look up AI winter",4
post14hb,richly branching,1.3639907689457174,highest,[removed],4
post14hb,richly branching,1.3639907689457174,highest,"For now, anyways. We know intelligence is possible, so automating it is posible too. We just haven't come up with the right architecture, but every passing year we are closer. If Large language models and transformers don't pan out, that just delays the problems here presented.",3
post14hb,richly branching,1.3639907689457174,highest,"Oh, sweet summer child.

Over the last six months, we (F500) started letting go of our frontend devs because upper management realized that an architect paired with AI outperforms an architect paired with a frontend dev on every KPI imaginable. They were even offered training to transition from being an ""Angular Andy"" to someone skilled in system design, solution architecture, and the like. Less than 10% bothered with those learning paths, brushing it off as fearmongering from the suits.

Ironically, the same ones who spend four hours a day on Stack Overflow just to get their shit going, and need two hours of meetings every day so I can explain for the fifth time that week how I want my REST API structured, were the ones who thought they were absolutely indispensable. ""I don't worry, it's just a stochastic parrot"". Hilarious.

I know every dev on Reddit thinks they're the smartest mf ever, but out of the hundreds of devs I’ve had to manage so far, 80% are easily replaceable, and are getting replaced. Their actual dev skills didn't match their inflated ego at all. Like, we even did workshops showing what SOTA AI can do, and how I create a production-ready app in a fraction of the time... then those fucks accused me of staging my demonstration. Holy shit. I hope the parrot teaches them some humility.

You can also see it in the tech subs how everybody is ""it won't ever replace me"" while in the same sentence admitting their horizon just goes up to ChatGPT. So basically, they don’t know shit about AI at all except chatting with some mainstream chatbot, but think they have some kind of authority on the topic. This is going to be a rude awakening for some.

Meta stopping hiring mid-level engineers and us letting them go is just the beginning. But even news like that get brushed off like, ""Meta doesn’t know what it’s doing. They’ll hire them again next year"". Mindblowing cognitive dissonance... hallucinations worse than an open-source LLM running on a Raspberry Pi. But at least the LLM is capable of learning.

I realized my professional days were numbered back when the transformer paper was published. I was reading it with some colleagues, and all five of us in that room instantly knew what this paper meant (or at least we had an idea... being 100% sure of it came in 2020 after the GPT-3 paper dropped). That was long before anyone even knew what an LLM was... seven years ago. Those exact frontend devs who aren’t with us anymore were the ones laughing the loudest at my ""fear of parrots"".

Well, thanks to my paranoia, I have absolutely no problem with getting replaced in 3–5 years or whenever. Finally, I’ll have time to do whatever I want and pursue some of my hobbies. Perhaps I’ll even keep some pet parrots.",3
post14hb,richly branching,1.3639907689457174,highest,"Custom OpenAI solutions with datasources configured and memory systems, are whats doing the heavy lifting, they can replace an awful lot of stuff with it",3
post14hb,richly branching,1.3639907689457174,highest,[removed],4
post14hb,richly branching,1.3639907689457174,highest,"Any serious company looking into AI for their future is developing their own customised AI systems, they aren't using off the shelf solutions.

I think its really important for people to know that, because all they have read are news articles saying how they are firing employees and using ChatGPT which is generally not the case except for the companies doing it for the AI buzz words.

In other words, there are employees working right now on automating jobs, its just a matter of time until they are complete, they don't have to wait for ChatGPT to do it for them.",3
post14hb,richly branching,1.3639907689457174,highest,"What? What python web app are you talking about that costs too much money? 

I feel like people who have this opinion should really read about the frontier of research - people who are aware of what is on the frontier have a VERY different opinion than this. I don't mean me. I mean research scientists, ethicists, economists etc.

That's not to say that they all agree with what will happen, but the idea that these models are not capable and not getting rapidly better is inexistent in those discussions. 

Look up o3, then look up frontier math, swebench, arcagi etc. if you don't know what any of these things mean, ask an llm that can search the Internet because most of this is too new for it to be in the training data. Swebench and arc agi excluded, but definitely the interplay between them all.

Long story short, shit is getting very very real.",3
post14hb,richly branching,1.3639907689457174,highest,[removed],4
post14hb,richly branching,1.3639907689457174,highest,"? It's useful to have some context here. AI code assist absolutely does work and does increase productivity.  Will it completely replace mid levels this year? No. Will it allow one mid level so the job of 1.3 mid levels? Probably. 

Also keep in mind chatGPT was released in late 2022. LLM really didn't explode until mid 2023.

We're about 2 years in.. it's reasonable to think that in another 2-5 years the world will be very very different. 

At this point I'm more worried about AI turning our world into a dystopian corptpcracy than I am about climate change.",3
post14hb,richly branching,1.3639907689457174,highest,"This will backfire so horribly that it would be hilarious if it wasn't so serious. Imagine creating almost overnight a new class of millions of unemployed people, used to having a job and living comfortably and suddenly destitute. 


It will be the french revolution all over again.",2
post14hb,richly branching,1.3639907689457174,highest,"Tbh, maybe this will just speed it up so we don't have to watch another 40 years of slow decline where people barely notice.",3
post14hb,richly branching,1.3639907689457174,highest,"If it happens slowly enough maybe the system will balance itself out with the demographic decline, I'm not sure what would happen in that case.",4
post14hb,richly branching,1.3639907689457174,highest,"Don’t worry, they are developing armed AI managed drone swarms to manage that future problem.",3
post14hb,richly branching,1.3639907689457174,highest,"I wish i could just laugh at that. However, it doesn't matter how bloody it gets, in the end, numbers do matter.",4
post14hb,richly branching,1.3639907689457174,highest,"I keep thinking about the Butlerian Jihad.  ""Thou shalt not make a machine in the image of a man's mind.""  Herbert has his bizarre aspects but he was weirdly prescient \[joke intended\] in some ways.",3
post14hb,richly branching,1.3639907689457174,highest,"Isn't it possible that the hiring freezes have more to do with global macroeconomic trends?

Like the higher interest rate environment pushing investors back to bonds, and relatively low investor confidence forcing businesses to consolidate and put off larger hiring plans because there's actually *less* appetite for risky investments than in the past few years.",2
post14hb,richly branching,1.3639907689457174,highest,They’re not freezing hiring because of AI. The fearmongering is starting to sound like a broken record…,2
post14hb,richly branching,1.3639907689457174,highest,They’ve got nothing new. I’ve been reading the same frantic screeds here in r/technology for over three years now,3
post14hb,richly branching,1.3639907689457174,highest,[removed],3
post14hb,richly branching,1.3639907689457174,highest,Because they didn’t over-hire during the pandemic like tech companies did.,4
post14hb,richly branching,1.3639907689457174,highest,"Replace the executives. This means the disenfranchised will have to take up entrepreneurship on their own, also using AI to cut down on start up costs. It’s not ideal, but there’s not much else the lower and middle class can do.",2
post14hb,richly branching,1.3639907689457174,highest,"Dotcom bubble 2.0 is going to come when investors start noticing that adding AI into everything doesn't actually increase sales or revenue, once the stock sell off starts it won't stop.",2
post14hb,richly branching,1.3639907689457174,highest,[removed],3
post14hb,richly branching,1.3639907689457174,highest,"Just because it has a large user base doesn't mean it's currently generating profit, while it's generating revenue, unless you turn profit you can't pay your shareholders dividends in which they expect.

At some point they will start selling their stocks / shares to invest into other things that are turning profit.",4
post14hb,richly branching,1.3639907689457174,highest,Is that why he works for a company to profit from the process.,2
post14hb,richly branching,1.3639907689457174,highest,"""Once men turned their thinking over to machines in the hope this would set them free. But that only permitted **other men with machines** to enslave them. """,2
post14hb,richly branching,1.3639907689457174,highest,Whose the average worker ?,2
post14hb,richly branching,1.3639907689457174,highest,total scare mongering. Please reread this post in 5 years and see if i was wrong.,2
post14hb,richly branching,1.3639907689457174,highest,Remindme! 2 years,2
post14hb,richly branching,1.3639907689457174,highest,"> developed strategies to replace human workers with AI agents

Please, name one company where such strategy has actually worked.",2
post14hb,richly branching,1.3639907689457174,highest,They’re not freezing hiring because of AI.,2
post14hb,richly branching,1.3639907689457174,highest,"At a certain level it almost feels like being a US *citizen* is sort of pointless. It only serves you if you’re in the ownership class, being a regular citizen it almost feels like these entities are actively spiteful of your existence. 

As a loose example, I got a doughnut from Dunkin (formerly known as Dunkin DONUTS) and it was so fucking dry and stale and had basically a single drop of frosting spread into a micron-thin veneer. Biting into it felt like I was biting into the middle finger of the board of directors. Like they’re mad at me for having the audacity to even request a fucking doughnut before I give them any money, and I should have just given them that money for nothing.",2
post14hb,richly branching,1.3639907689457174,highest,How do we know that this is putting people out of work? Unemployment went down in December. https://www.cnbc.com/amp/2025/01/10/jobs-report-december-2024.html,2
post14hb,richly branching,1.3639907689457174,highest,"It looks like you shared an AMP link. These should load faster, but AMP is controversial because of [concerns over privacy and the Open Web](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot).

Maybe check out **the canonical page** instead: **[https://www.cnbc.com/2025/01/10/jobs-report-december-2024.html](https://www.cnbc.com/2025/01/10/jobs-report-december-2024.html)**

*****

 ^(I'm a bot | )[^(Why & About)](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot)^( | )[^(Summon: u/AmputatorBot)](https://www.reddit.com/r/AmputatorBot/comments/cchly3/you_can_now_summon_amputatorbot/)",3
post28hb,richly branching,1.3436120224105876,highest,"> So first let's look at what happened so far, let's use the US as an example. 50 or 60 years ago the middle class in the US was actually bigger than it is today. Since then income inequality has significantly increased. 

This is where you lose me.  The ""middle class"" is a wholly invented construct.  It developed as a way to describe the people who were not rich but also not poor, but also not working class.  It's an inexact classification with little utility.

Income inequality has risen, yes, and the ""middle class"" has shrunk in the United States.  Worldwide, poverty has plummeted as well. As much of that is literally true, however, it's because the middle class are becoming the upper class in the United States and we're finally addressing third-world poverty.  Clearly, the rise in wealth inequality is not making any  of those things *worse*, so why are you bringing it up?

> But so that bring me to my main point, which is that technological advancement will most likely relatively soon reach a critical threshold, which will cause most human labor to lose its value, not just low-level labor.  If we consider how much technology has progressed in just the last 10-20 years, if we consider how rapidly AI has progressed in just the last few years, then we can only dream about how hyper-advanced society will be in say 25 years of 50 years.

This argument crops up every single time a new technology hits the market.  In case you missed it, LLMs are *not good at what they do* in a lot of ways.  It's not on track to replace much of anything given how relatively stagnant the whole thing is.  Given the hallucinations and what have you, we're a ways from generative AI, and even that won't be ready for prime time on release.

Microsoft Excel didn't make accountants redundant.  ATMs didn't kill the bank worker.  The luddites have never been correct.

> But once AI reaches a certain point, the capitalist class will have no more use for the vast majority of the human population, except for a tiny minority of exceptionally gifted, exceptionally intelligent and exceptionally motivated group of extremely high-level workers who AI and automation cannot yet replace.

We're all the capitalist class, friend.  Capitalism won.  The world has never been more prosperous, and its people more better off, than it has under capitalism - especially following the fall of the Soviet Union.  

We're all capitalists.  We have come to the understanding that markets are the best way to distribute goods, that supply is the primary economic driver, that economic freedom is as important as any other.

The *most likely* worse case scenario is that AI displaces a nontrivial number of jobs and the people it replaces do something else, just like they have every other time some seismic technological advancement occurred.  It's highly unlikely that this would occur, either.",1
post28hb,richly branching,1.3436120224105876,highest,"Great answer. Small nitpick and a comment. 

I’m a dev. I own a dev company. We weren’t hiring at breakneck pace to begin with —I look for real talent, and that’s rare—but the most meaningful difference I’ve observed since LLMs hit the scene is that our releases are massively more frequent. We’re shipping product like never before. 

I can’t recall the nitpick, but my impression is that this is in fact a hugely transformational technology in my field, and yet it has caused us to fire nobody. Everyone is 100x more productive, and we get the dopamine hit of seeing ideas become reality at an incredible pace.

Plus, we no longer have to do the drudgery of documenting product, writing tests, etc. nobody wanted to do that before, and now we don’t have to. 

This is supposedly the end of dev jobs, and yet I feel like we’re in a golden age.",2
post28hb,richly branching,1.3436120224105876,highest,"I'm deeply, deeply skeptical of AI's utility, but I can recognize that it does *some* things well.  I just feel like we're talking about Microsoft Excel putting accountants out of business again.",3
post28hb,richly branching,1.3436120224105876,highest,"Just in case it's not clear.  I completely agree with you.  I just think you underestimate the impact of this tech.  But even if it's 100x as impactful as you imagine it to be -- and it is -- it will still mean we're all much more productive, and far better off.",4
post28hb,richly branching,1.3436120224105876,highest,"My point is that even is excel was as transformational as AI, which it isn't, it still wouldn't put accountants out of work.

Man, if you could see how we work now, you'd lose much of your skepticism and probably change how you work as well. This paradigm works for almost every kind of written work. I also use it for planning, administration, contract law, and marketing. It's life-changing.

Things haven't changed as much in 25 years as they have in the last 12 months.",4
post28hb,richly branching,1.3436120224105876,highest,"This happened before when the compiler came out. Compilers allowed people to code at incredible speeds compared to before, and everyone thought their jobs would be gone since one person can now do the work of many. Just created new demand since they can now release a lot more and take on many more projects. Sounds like the same is happening now. Glad to hear your experience is similar.",3
post28hb,richly branching,1.3436120224105876,highest,"People keep saying ""AI will create as many jobs as it kills"" but they can't actually say what those new jobs will be lol.",2
post28hb,richly branching,1.3436120224105876,highest,"Middle class mostly becoming upper class is false. We are seeing a larger and larger percentage of the US population (anyway) with a smaller percentage of total wealth. 

You are repeating propaganda, not actual facts.

Actually, this whole post is basically every capitalist propaganda trope rolled into one.",2
post28hb,richly branching,1.3436120224105876,highest,"> Middle class mostly becoming upper class is false. 

[Sorry, you're wrong] (https://imgur.com/a/EXKtFYz).

> Actually, this whole post is basically every capitalist propaganda trope rolled into one.

I mean, it's not propaganda to correctly note that we're better off under capitalism.  It's just facts.",3
post28hb,richly branching,1.3436120224105876,highest,"Try actual studies and data, instead of a random picture on the internet:

https://www.statista.com/statistics/203961/wealth-distribution-for-the-us/

https://www.pewresearch.org/social-trends/2020/01/09/trends-in-income-and-wealth-inequality/

https://www.cbo.gov/publication/60807#:~:text=Between%201989%20and%202022%2C%20the,distribution%20increased%20by%20285%20percent.",4
post28hb,richly branching,1.3436120224105876,highest,Wake me up when past performance becomes a guarantee of future success.,4
post28hb,richly branching,1.3436120224105876,highest,"the capitalist class is defined by an economic relationship, not by your existence within a capitalist society that ""has never been more prosperous"" (by capitalists' own definitions, maybe)

""economic freedom"" is nothing more than the ""freedom"" given to capitalists to rape the planet and dominate the rest of us",2
post28hb,richly branching,1.3436120224105876,highest,Last I checked not like any of the other non capitalist systems did any better,3
post28hb,richly branching,1.3436120224105876,highest,"its a question of whether or not you believe that you as an individual have the inherent worth to demand an equal say and share in your society

everything ""works"".  slavery ""works"".  the question is who is it working for",4
post28hb,richly branching,1.3436120224105876,highest,"> the capitalist class is defined by an economic relationship, not by your existence within a capitalist society that ""has never been more prosperous"" (by capitalists' own definitions, maybe)

No.  We're not all Marxists, sorry.  The ""capitalist class"" are all of us.  We are all capitalists.  We rely on the advancement of capital both for our own livelihoods, but for the world around us to operate.  

The people who tell you they are not part of the capitalist class just haven't realized it yet.

> ""economic freedom"" is nothing more than the ""freedom"" given to capitalists to rape the planet and dominate the rest of us

Ah, yes, the fact that I have the ability, if I so choose, to open my own business, work for myself, etc., it's all to serve those evil capitalists trying to actually dominate me.

If ""we'll largely leave you alone"" is domination, then thank you sir, can I have another?",3
post28hb,richly branching,1.3436120224105876,highest,"then what does the ""capitalist class"" even mean; if you're taking capitalist class to mean anyone that lives within a capitalist society then the term ceases to have any real descriptive meaning

a class can only mean something by its relation to something else.  that's what classes define: a hierarchy, social stratification.  if everybody is in a ""class"", then it isn't a class.

its like saying ""everything is a base"".  a base is only defined by its opposition to an acid.  saying ""everything is a base"" makes no sense, its depriving the term of its intended meaning.

if you're starting your own business, then you're trying to become a capitalist, you're trying to join the class that dominates the classes below them",4
post28hb,richly branching,1.3436120224105876,highest,"Yeah I was confused by the “capitalist class” name. We’re all living in a capitalist system. If by “capitalist class” he means “upper class”, just say that.",4
post28hb,richly branching,1.3436120224105876,highest,"The average American owns nothing. Not their home, not their labor, etc.

The majority of people are not capitalist. They own almost no private capital. The majority of people are the exploited workforce that capitalism relies on.",4
post28hb,richly branching,1.3436120224105876,highest,"Is ""capitalist class"" a meaningful classification if we all fall within that classification?",4
post28hb,richly branching,1.3436120224105876,highest,"It’s not by capitalist definition. It’s by definition of metrics as poverty rate, real household income, access to energy and electricity, life expectancy and many more.",3
post28hb,richly branching,1.3436120224105876,highest,"""poverty rate"" is an arbitrary measure, it can be set at whatever level its measurers prefer.  what is ""poverty""?  is there an objective definition?

income ""rises"" because production increases over time; access to goods increases.  relative incomes do not rise over time, they actually fall.  people get smaller and smaller shares of the pie over time

access to electricity and life expectancy are measures of development, of technological progress.  you can see development occur in socialist states and also see huge increases in life expectancy.",4
post28hb,richly branching,1.3436120224105876,highest,"Funny definition of capitalism you've got there.

I suppose everyone living in feudal times was a Lord, too?",2
post28hb,richly branching,1.3436120224105876,highest,"Well, part of the middle class is becoming the upper class, sure. That's what I said in my OP as well. But another part of the middle class is becoming the new lower class. 

But my point is that as AI and technology advances at an ever faster rate, soon AI will also be able to replace upper class workers like engineers, architects, doctors etc. The reason why some middle class people have moved into the lower class because their labor no longer has much value due to automation. But for now, new upper class jobs have also been created. 

But what do we do when AI and technology become so advanced that even engineers, and doctors and bankers and marketing specialists and whatever can be replaced by AI systems, robots or other technology? 

So once we reach a certain technological threshold for the first time we would not only see a shrinking of the middle class but also shrinking of the upper class. 

And no, we're not all capitalists. Many of us, especially those of us in the West, for now, benefit from capitalism to some extent, sure. 

But what do you do once the owners of the means of production have no more use for the vast majority of people, because AI and robots are way more effecient at every economic tasks those people could do? At that point, are you also gonna benefit from the system of capitalism if your labor has no more economic value?",2
post28hb,richly branching,1.3436120224105876,highest,"> Well, part of the middle class is becoming the upper class, sure. That's what I said in my OP as well. But another part of the middle class is becoming the new lower class. 

The data doesn't bear that out.  There is no increase in the middle class moving to the lower, statistically.

> But my point is that as AI and technology advances at an ever faster rate, soon AI will also be able to replace upper class workers like engineers, architects, doctors etc. 

Yeah, I don't buy it.  Like I said, we can't get it to count numbers right.  Even if the enterprise-level models are superior, LLMs aren't going to pull this off anytime soon and in the off chance that we start seeing some impacts, there's no reason to believe this time will be different.

> And no, we're not all capitalists. Many of us, especially those of us in the West, for now, benefit from capitalism to some extent, sure. 

More than benefit, we are the capitalists.

> But what do you do once the owners of the means of production have no more use for the vast majority of people, because AI and robots are way more effecient at every economic tasks those people could do? At that point, are you also gonna benefit from the system of capitalism if your labor has no more economic value?

By selling your labor somewhere that it's valued.

The same way we did every single other time.",3
post7lb,poorly branching,0.111396923154935,lowest,"Interesting work! Can it be used for multilingual bias probing (for example, to evaluate gender bias in low-resource languages)?",1
post7lb,poorly branching,0.111396923154935,lowest,"Thanks. Right now it's English only, but the idea is to have the project be easily extensible with new probes, and they can be in arbitrary languages. I would love to include some multilingual evaluations in the future, but it is more or less limited by my capacity.",2
post16lb,poorly branching,0.1674355182091233,lowest,"You as a photographer will have to learn how to use those tools. 

People have been saying that AI will replace me as an IT guy. That's not going to happen anytime soon. Same with you. 

All industries are changing. We have to adapt.",1
post16lb,poorly branching,0.1674355182091233,lowest,"If anything it'll make IT more interesting by streamlining some of the boring and repetitive stuff so I can focus on the hands-on work and complex, interesting issues that I am going into the field for. Hopefully it won't affect the barriers or entry too badly though, because those of us at the bottom are having enough trouble getting hired.",2
post28lb,poorly branching,0.1674355182091233,lowest,"You as a photographer will have to learn how to use those tools. 

People have been saying that AI will replace me as an IT guy. That's not going to happen anytime soon. Same with you. 

All industries are changing. We have to adapt.",1
post28lb,poorly branching,0.1674355182091233,lowest,"If anything it'll make IT more interesting by streamlining some of the boring and repetitive stuff so I can focus on the hands-on work and complex, interesting issues that I am going into the field for. Hopefully it won't affect the barriers or entry too badly though, because those of us at the bottom are having enough trouble getting hired.",2
post12lb,poorly branching,0.2732746911530488,lowest,"I think you’re looking at it the wrong way. Will AI change skills you need? Yes. When was the last time you thought about your ability to make candles now that you have electricity ? You use a computer but probably don’t have a clue how to build a motherboard. 

AI is the new electricity - does it benefit us to understand the underlying workings versus fully offloading knowledge ? Of course, but only to a certain extent - technology brings the need for more specialization - so the question is which way do you want to specialize? Towards understanding the technical underpinnings or towards working with specialists who understand when needed while you advance other areas of knowledge?


-PhD in educational sciences here 😊",1
post12lb,poorly branching,0.2732746911530488,lowest,"You make a good argument, but I’m going to argue that electricity is not intelligent. These new tools are distinct from any prior tools humans have developed. We need to be cognizant of the potential dangers.

In the same way that LLMs can be extraordinary teaching and research aids, they can and will also result in the offloading of human intelligence.

Just anecdotally, recently I’ve been having debates with people on these subjects where the other person is using the LLMs to make their arguments. In many of these instances, it is clear that the human using the LLM does not understand the arguments.",2
post12lb,poorly branching,0.2732746911530488,lowest,"I  am also not arguing that electricity is intelligent - that would be… odd. My point is that it is coming no matter what and it’s going to be ubiquitous. Sometimes a technology comes along that changes everything, like electricity. AI will be that only more. In many ways we can barely even imagine at this stage. Indeed if we’re not prepared for it (which arguably we are not) it can be dangerous, the discomfort the transformation will bring will displace people … for example, if asked what a computer is you’d likely first think of a machine with software. Not so long ago it was Judy in the next office who crunches the numbers. 

AI can make us lazy, just like computers or the argument OP made about the calculator because there’s a possibility of distribution of cognitive load. How we think about it and approach it is important, but it’s not an impossible riddle.

I’m not saying there aren’t risks - if you don’t know the dangers of electricity you can easily electrocute yourself. We’re not going to be able to stop everyone from electrocuting themselves. But I’m also not for throwing the baby out with the bathwater.",3
post23lb,poorly branching,0.2735108914734334,lowest,"Feminists often cite a study showing a bias for resumes with male names, but what you never hear about is that experiment was redone showing the opposite.  

Try googling what the pay gap actually measures.  The facts will be buried under a mountain of feminist misinformation.    

Try finding actual reported sexual assault rates of college students.   Same thing, actual data is again buried under a mountain of feminist propaganda.   

It’s the same with convictions rates and so much other information:   Feminist and woke propaganda is prioritized while relevant facts are downplayed and hidden.     (Try googling the actual conviction rate for rape.   You won’t find it, but you’ll find plenty of misinformation about it propagated by RAINN)

There’s a bias all right, the bias is to promote cherry picked information and propaganda favorable to the feminist cause.

Whether it’s evaluating resumes or prioritizing internet search results, what we call AI is really just programming and will reflect any bias of the programmer.",1
post23lb,poorly branching,0.2735108914734334,lowest,"Pretty sure there's been studies done where recruiters were asked to pick from totally anonymized resumes (the hypothesis being it would result in women being picked more often than usual because now the recruiters could not discriminate against women based on their names) and it actually resulted in men being picked more often than usual. So because the studies found women get preferential hiring on the basis of their feminine names, the recommendation is of course then to not anonymize the resumes.

tl;dr: they set out to find sexism, find that there *is* sexism but it's the *desirable* kind of sexism according to their agenda, so everything's business as usual.",2
post23lb,poorly branching,0.2735108914734334,lowest,"Isn't AI trained on large amounts of real world data, so whatever biases are present in that data set will be present in the resulting AI condensed algorithms?

If women represent the squeakiest wheel, then they will be over-represented in complaints, which would bias the data because it only represents complaints and not applause (which usually is never presented as people rarely call up to applaud something that is working well, only when it isn't).",2
post23lb,poorly branching,0.2735108914734334,lowest,Pay gap is a conspiracy theory that was debunked in the 90's.,2
post10lb,poorly branching,0.3070846261941389,lowest,"reading through that article it sounds like they must be discriminating against hiring men. Which is fine; while the STEM gender gap continues. I'm curious if places like this have plans to remediate the male hiring gender bias they'll have firmly ingrained in their work culture when STEM gender gaps level out, as they're slowly but steadily doing though.",1
post10lb,poorly branching,0.3070846261941389,lowest,"I guess you didn't read it that closely:

""Women still make up only one-third of the global scientific community, with the percentage stagnating over the past decade, according to a 2024 report by UNESCO (United Nations Educational, Scientific and Cultural Organization). In some countries, less than 10 per cent of researchers are women. 

They hold just 22 per cent of STEM jobs in G20 countries, and only one in 10 ascend to leadership positions.""

What do you want to bet it's filled with women who couldn't get jobs elsewhere?

Regardless, I love this article. It reminds people that, no, DEI is not reverse discrimination. It can't be because, unlike what people are whining about, that facts say that women and minorities are still underrepresented pretty much everywhere that isn't minimum wage adjacent.",2
post10lb,poorly branching,0.3070846261941389,lowest,"I specifically mentioned that woman were still highly underrepresented in STEM fields as the article said, that being said 15 years ago that number was less than 5%, and the fact that it's up to 22% now, while great means that in entry level STEM position women are being hired at over 2:1 ratios compared to men. Which is fine for now, but it's going to lead to a situation in another 10 years where the gender roles are completely reversed and only men will need to be hired in entry level positions, it's turned into a see saw situation. It's currently much easier for women to get stem jobs than men so I believe asking what policies institutes that are primarily hiring woman have to even things out in a few years when it's a woman dominated field is reasonable.",3
post21lb,poorly branching,0.3187790113905335,lowest,"Machine learning algorithms need lots of labeled data.  Essentially a computer is shown a picture of a person and the computer makes a guess whether it's a person or not.  Then the computer is told the correct answer and the computer takes notes on what it got right and wrong and then adjusts itself.  Sometimes this data is biased, such as being shown way more white people than black people.  Because the computer primarily sees white people, it begins to associate white humanoid with person and black with not person in a more extreme case and not sure in a less extreme case.  There was a really bad example of this a while ago where an AI started labeling black people as gorillas.  Because the people it had primarily seen where white and the things that were dark skinned and humanoid where predominately gorillas not black people.

&#x200B;

With kinect specifically it may be a different problem that has a similar cause.  Kinect uses an infrared camera and different skin tones (especially darker skin vs lighter skin) looks different in infrared so the more basic software wasn't capable of recognizing what it was seeing as a human (this actually has happened with hands free sinks before).  In teams that are predominately white, it may not have come up during testing (engineering teams are often their own first tests) and they may not have thought about it.",1
post21lb,poorly branching,0.3187790113905335,lowest,Thank you!,2
post21lb,poorly branching,0.3187790113905335,lowest,This has always seemed a weak explanation to me. You don't train a machine learning algorithm on a training set of the half dozen guys who developed it.,2
post22lb,poorly branching,0.3187790113905335,lowest,"Machine learning algorithms need lots of labeled data.  Essentially a computer is shown a picture of a person and the computer makes a guess whether it's a person or not.  Then the computer is told the correct answer and the computer takes notes on what it got right and wrong and then adjusts itself.  Sometimes this data is biased, such as being shown way more white people than black people.  Because the computer primarily sees white people, it begins to associate white humanoid with person and black with not person in a more extreme case and not sure in a less extreme case.  There was a really bad example of this a while ago where an AI started labeling black people as gorillas.  Because the people it had primarily seen where white and the things that were dark skinned and humanoid where predominately gorillas not black people.

&#x200B;

With kinect specifically it may be a different problem that has a similar cause.  Kinect uses an infrared camera and different skin tones (especially darker skin vs lighter skin) looks different in infrared so the more basic software wasn't capable of recognizing what it was seeing as a human (this actually has happened with hands free sinks before).  In teams that are predominately white, it may not have come up during testing (engineering teams are often their own first tests) and they may not have thought about it.",1
post22lb,poorly branching,0.3187790113905335,lowest,Thank you!,2
post22lb,poorly branching,0.3187790113905335,lowest,This has always seemed a weak explanation to me. You don't train a machine learning algorithm on a training set of the half dozen guys who developed it.,2
post25lb,poorly branching,0.3285035774153585,lowest,What a strange project. Were you given this project title or did you choose it?,1
post25lb,poorly branching,0.3285035774153585,lowest,"It's not strange lol, the earliest studies of AI implementation have already found that certain biases are built-in unless specifically accounted for.

For example, while selecting top candidates during a hiring process, Amazon’s automated resume screening system discriminated against women. The data used to train the recruitment model was informed by resume samples from a 10-year period, where women were underrepresented. The resume screening model thus used “linguistic signals” associated with successful male candidates.

Basically because AI draws from the past to find patterns relevant to its programming, it's also drawing all the biases from the past. So if an AI program notices that women are not represented in the past hires, it doesn't understand the context as to why. It just assumes that women must not be good at the job and gives applications with male-sounding names more points automatically. This type of bias extends to things like race, language, etc.",2
post25lb,poorly branching,0.3285035774153585,lowest,Why is it strange ? Yes I picked it myself,2
post25lb,poorly branching,0.3285035774153585,lowest,[removed],2
post26lb,poorly branching,0.3546733559933284,lowest,"From the research paper, the old strategy was to train the AI and then brainwash it afterwards. Now it's possible to brainwash the AI during the training phase.",1
post26lb,poorly branching,0.3546733559933284,lowest,What do you mean by brainwashing?,2
post26lb,poorly branching,0.3546733559933284,lowest,"They're finding new and creative ways to correct the AI's ""perceptions"" until it matches our collective prejudices.",3
