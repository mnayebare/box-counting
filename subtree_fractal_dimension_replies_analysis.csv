post_id,conversation_type,fractal_dimension,fractal_dimension_type,reply,reply_level
post2con,controversial,1.55819326637459,highest,"Problem is, of course, that neural networks can only ever be as good as the training data. The neural network isn't sexist or racist. It has no concept of these things. Neural networks merely replicate patterns they see in data they are trained on. If one of those patterns is sexism, the neural network replicates sexism, even if it has no concept of sexism. Same for racism.   


This is also why computer aided sentencing failed in the early stages. If you feed a neural network with real data, any biases present in the data has will be inherited by the neural network. Therefore, the neural network, despite lacking a concept of what racism is, ended up sentencing certain ethnicities more and harder in test cases where it was presented with otherwise identical cases.",1
post2con,controversial,1.55819326637459,highest,[removed],2
post2con,controversial,1.55819326637459,highest,"Precisely.  The headline is misleading at best.  I'm on an ML team at a robotics company, and speaking for us, we haven't ""decided it's OK"", we've run out of ideas about how to solve it, we try new things as we think of them, and we've kept the ideas that have seemed to improve things.  

""More and better data.""  Okay, yeah, sure, that solves it, but how do we get that?  We buy access to some dataset?  The trouble there is that A) we already have the biggest relevant dataset we have access to B) external datasets collected in other contexts don't transfer super effectively because we run specialty cameras in an unusual position/angle  C) even if they did transfer nicely there's no guarantee that the transfer process itself doesn't induce a bias (eg some skin colors may transfer better or worse given the exposure differences between the original camera and ours)  D) systemic biases like who is living the sort of life where they'll be where we're collecting data when we're collecting data are going to get inherited and there's not a lot we can do about it  E) the curse of dimensionality makes it approximately impossible to ever have enough data, I very much doubt there's a single image of a 6'5"" person with a seeing eye dog or echo cane in our dataset, and even if there is, they're probably not black (not because we exclude such people, but because none have been visible during data collection, when was the last time you saw that in person?).  Will our models work on those novel cases?  We hope so!",2
post2con,controversial,1.55819326637459,highest,"So both human intelligence and artificial intelligence are only as good as the data they're given. You can raise a racist, bigoted AI the same in way you can raise a racist, bigoted HI.",3
post2con,controversial,1.55819326637459,highest,"The difference is, a human can be told that racism is bad and might work to compensate in the data. With an AI, that has to be designed in from the ground up.",4
post2con,controversial,1.55819326637459,highest,"Sort of, except I don't love the framing of human racism as data-driven. It isn't really; humans employ biases and heuristics vigorously when interpreting data.",4
post2con,controversial,1.55819326637459,highest,"Who knew intelligence isn't wisdom. We have AI but now we need AW.

Being able to morph and utilize data: intelligence.

Understanding when to do it and when not: wisdom.",4
post2con,controversial,1.55819326637459,highest,"But a human can choose to break from their upbringing and traditions. It happens.

Can an AI identify bias in its data, and choose to deviate from it? Maybe that's the next step in AI",4
post2con,controversial,1.55819326637459,highest,‘robots’ in the post title has the potential for more depth of interpretation.,4
post2con,controversial,1.55819326637459,highest,"Maybe it's time to shift focus from training AI to make it useful in novel situations to gathering datasets that can be used in a later stage to teach AI, where the focus is getting as objective a data set as possible? Work with other fields etc.",3
post2con,controversial,1.55819326637459,highest,"You mean manually curating such datasets?  There are certainly people working on exactly that, but it's hard to get funding to do that because the marginal gain in value from an additional datum drops roughly ~~logarithmically~~ exponentially (ugh, it's midnight and apparently I'm not braining good), but the marginal cost of manually checking it remains fixed.",4
post2con,controversial,1.55819326637459,highest,"Nah, the key is to not trust some algorithm to be a neutral arbiter because no such thing can exist in reality. Trusting some code to solve racism or sexism is just passing the buck onto code for humanity’s ills.",4
post2con,controversial,1.55819326637459,highest,"This is a bit of a naive understanding of the problem, akin to people pointing to “the algorithm” as what decides what you see on social media. There aren’t canonical datasets for different tasks (well there generally are for benchmarking purposes but using those same ones for training would be bad research from a scientific perspective) novel applications often require novel datasets, and those datasets have to be gathered for that specific task. 

constructing a dataset for such a task is definitionally not something you can do manually, otherwise you are _still_ imparting your biases on the model. constructing an objective dataset for a task relies on some person’s definition of objectivity. Oftentimes, as crappy as it is, it’s easier to kick the issue to just reflecting society’s biases.

what you are describing here is not an AI or data problem but rather a societal one. Solving it by trying to construct datasets just results in a different expression of the exact same issue, just with different values.",4
post2con,controversial,1.55819326637459,highest,"It doesnt have a big return and the people curating can include biases.

Plus If I want people tailored for my company, I want people that will fit MY company, not a generalized version of it, so many places would be agaisnt using those objective datasets, because they dont fit their reality as well as the biased dataset",4
post2con,controversial,1.55819326637459,highest,Ehhh… the datasets we have are plenty objective.,4
post2con,controversial,1.55819326637459,highest,"Perhaps the answer for now is that we shouldn't be making AIs for production with any strict rules when there's a risk of discriminatory biases. We as a species have a habit of always trying to produce more, more optimally, more effortlessly, and we want to find new things to sell, to optimize, to produce.

But we don't really need to. We do not need AIs that filter job candidates (aside of maybe some sort of spam spotting AIs and the like), we do not need AIs that decide your insurance rate for you, we do not need AIs that play with your kid for you.

Yet we want these things but why? Are they *really* going to make the world into a better place for all its inhabitants?

There's a ton of practical work with AIs and ML that doesn't need to include the problem of discrimination. Product QA, recognizing fractures from X-rays, biochemistry applications, infrastructure operations optimization, etc etc.

Sure, this is something worth of studying, but what we really need is a set of standards before potentially dangerous AIs are put into production. And by potentially dangerous, I mean also AIs that may produce results interpretable as discriminatory - discrimination *is* dangerous.

It's up to the professionals of the field to say ""no, we can't do that yet reliably enough"" when a client asks them to do an AI that would most likely have discriminatory biases. And it's up to the researchers to keep informing the professionals about these risks.",3
post2con,controversial,1.55819326637459,highest,"> Perhaps the answer for now is that we shouldn't be making AIs for production with any strict rules when there's a risk of discriminatory biases.

That's pretty much how it's always done, which is why it is able to learn biases.  Take the systemic bias case, where some individuals are at more liberty to take leisurely strolls in the park.  If (for perfectly sane and innocent reasons) parks are where it makes sense to collect your data, you're going to end up with a biased dataset through no fault of your own, despite not putting any strict rules in.

> It's up to the professionals of the field to say ""no, we can't do that yet reliably enough"" when a client asks them to do an AI that would most likely have discriminatory biases. And it's up to the researchers to keep informing the professionals about these risks.

There's more to it than that.  Let's assume that there's good money to be made in your robotic endeavor.  And further lets assume that the current professionals say ""no, we can't do that yet reliably enough"".  That creates a vacuum for hungrier or less scrupulous people to go after the same market.  And so one important question is the public as a whole better off with potentially biased robots made by thoughtful engineers, or with probably still biased robots made by seedier engineers who assure you that there is no bias?  It's not like you're going to convince _everyone_ to step away from large piles of money (and if you are I can think of better uses of that ability to convince).",4
post2con,controversial,1.55819326637459,highest,">Perhaps the answer for now is that we shouldn't be making AIs for production with any strict rules when there's a risk of discriminatory biases.

I don't see why when people aren't free from biases either. I think it's more that the decisions and processes need to be set up in a way that considers the possibility of biases and attempts to correct or sidestep them. 

And calling out an AI on its biases may be easier than calling out a person - as long as we no longer think AI's are unbiased.",4
post2con,controversial,1.55819326637459,highest,This is not reassuring and honestly convinces me more that those folks doing AI work are playing with fire,3
post2con,controversial,1.55819326637459,highest,"A significant portion, if not most people who do AI-related work, do it on stuff that isn't necessarily impacted by this stuff. But that's all you read about in the news because these headlines sell.

Training a model to play games (chess/go etc.), image analysis (satellite imagery for climate impacts), science modelling (weather forecasting/astrophyics etc.), speeding up your phone/computer (by optimising app loading etc.), digitising hand-written content, mapping roads (google maps etc.), disaster forecasting (earthquakes/flooding), novel drug discovery.

There are certainly more areas that I'm forgetting, but don't be fooled into thinking (1) that ML isn't already an everyday part of your life and (2) that all ML research has the same societal negatives.",4
post2con,controversial,1.55819326637459,highest,"Don't worry, I'm sure one day we can get sentient AIs that hate all humans equally!",4
post2con,controversial,1.55819326637459,highest,"Yup. “We know it’s not ok, but we’ll move forward regardless”.",4
post2con,controversial,1.55819326637459,highest,"If it helps, human brains have a lot of these same issues (they're just slightly more subtle due to the massive data disparity), and that's gone perfectly.  Definitely no cases of people ending up as genocidal racists.  Definitely no cases of that currently happening in China.  We're definitely smart enough to avoid building nukes, or at the very least to get rid of all the nukes we have.

If doing AI work is playing with fire, doing human work is playing with massive asteroids.

A fun game to play is, whenever you see robots or aliens in a scary movie, try to work out which human failing it is they're the avatar of.",4
post2con,controversial,1.55819326637459,highest,"Yeah, I think the onus is less on the devs, since we're a long way off created impartial AI, and more on enforcing a code of ethics on what AI can be used for.

If your face recognition technology doesn't work on black people very well, then it shouldn't be used by police to identify black suspects, or otherwise come attached to additional manual protocols to verify the results for affected races and genders.

The main problem is that companies are selling these things to public housing projects primarily populated by black people as part of the security system and acting confused when it randomly flags people as shoplifters as if they didn't know it was going to do that.",3
post2con,controversial,1.55819326637459,highest,"You can't expect companies to pay you hundreds of thousands of dollars to create an AI and not turn around and use it.  Diffusion of blame is how we justify evil outcomes.  If you know it's impossible to not make a racist AI, then don't make an AI.",4
post2con,controversial,1.55819326637459,highest,"Have you considered that intelligence, which includes experience-based judgement, is inherently biased?  Sounds like you're trying to make something artificial, but not necessarily intelligent.",3
post2con,controversial,1.55819326637459,highest,">we haven't ""decided it's OK"",

You're simply going ahead with a flawed product that was supposed to compensate for human flaws and failings, but will now reproduce them only with greater expediency. Cool!",3
post2con,controversial,1.55819326637459,highest,"Arguing it's not technically racist is completely unelpful and puts the focus on the wrong aspect of the problem. These things can have enormous impacts on our lives so it really doesn't matter how it *actually* works when it's *literally* not working properly. 

Facial recognition being a prime example. The miss rate on light skin people alone is too high let alone the abysmal rate for darker skin tones yet it's commonly used by law enforcement for years now. Those people sitting in jail from this one technology don't care that the AI isn't actually racist. The outcomes are and that's literally all that matters. It doesn't work, fix it or trash it.",3
post2con,controversial,1.55819326637459,highest,"> It doesn't work, fix it or trash it.

Agreed.  It's just that fixing it requires lots trial and error, and that takes a long time.  The real problems with facial recognition aren't in the technology, they're in idiots using tools for more than they're capable of doing.",4
post2con,controversial,1.55819326637459,highest,"In this case is the curse of dimensionality the fact that the global sample is only 7 billion people, which represents a very tiny fraction of all possible configurations of all characteristics being tracked?",3
post2con,controversial,1.55819326637459,highest,[deleted],3
post2con,controversial,1.55819326637459,highest,"> Why give an AI any data not required in sentencing. If the AI doesn’t know the race or gender of the defendant, it can’t use it against them.

That's not strictly true.  Let's say you have two defendants, one was caught and plead to possession with intent to distribute crack cocaine, and the other was caught and plead to possession with intent to distribute MDMA.  From that information alone you can make an educated guess (aka a Bayesian inference) about the race and gender of both defendants, and while I don't have actual data to back this up, you'd likely be right a statistically significant portion of the time.",4
post2con,controversial,1.55819326637459,highest,"It sounds like you have 100% decided it's okay. You don't like it, but you don't consider it a deal breaker either. Not desirable, but acceptable.

I understand you have constraints you are working under and I have no doubt that you would like to see the issues of racism and bias in AI resolved. But the simple fact is that AIs are being designed to be racist and there will be real consequences. People won't be able to get jobs or health care or will get denied loans or suffer longer prison sentences.

Again, I understand that you aren't in a position where you can fix it. But shrugging and hoping the problem will get addressed? That's saying it's okay if it doesn't. It's tolerable. So saying that AI researchers think it's okay is a fair characterization.

Whether you have malice in your heart or not matters not-at-all to the companies who will use AI in the pursuit of profit. The travel companies pushing Vegas trips on a discount at people with manic-depression or pushing people into high-engagement communities even if they are cults or white nationalists.",3
post2con,controversial,1.55819326637459,highest,"I just want to point out that data augmentation is a thing, but otherwise good summary.",3
post2con,controversial,1.55819326637459,highest,Isn’t it possible to “feed” a posterior law that sits in front of the data kind of in a Bayesian mindset?,3
post2con,controversial,1.55819326637459,highest,"Great question, I'll come back to it when I get back from work (leaving this comment to remind myself)",4
post2con,controversial,1.55819326637459,highest,"Kind of, there is room to feed stuff in like that, but it's difficult to figure out precisely what to feed in.  Most things you might want to feed in there can also be expressed in your cost function, which means they can be included in the training process directly.  Ideas for what you feed in get tried pretty regularly, it's not solved, but some of them do work.",4
post2con,controversial,1.55819326637459,highest,"The way to solve it is get tech ethicists into positions of power to address systemic issues. You, personally, cannot solve this. *Your team cannot solve this.* Big power players in tech have to solve this, and that begins with hiring-on people like Timnit Gebru and not firing them; looking at you, Google.

This is a fully top-down issue.",3
post2con,controversial,1.55819326637459,highest,Maybe stop using data generated by Americans?,3
post2con,controversial,1.55819326637459,highest,Because there's no racism anywhere except in the US.,4
post2con,controversial,1.55819326637459,highest,How about we stop considering the americans altogether,4
post2con,controversial,1.55819326637459,highest,"Paraphrase: We can't be bothered to spend the time and money to assemble a dataset that doesn't contain bigoted biases so we're going to release a product the replicates bigotry anyway.

Assembling good high quality datasets that can be used for machine learning is expensive and decades long work. I wish more computer science students understood this.",3
post2con,controversial,1.55819326637459,highest,Have you tried buying synthetic data?,3
post2con,controversial,1.55819326637459,highest,"The trouble there is that it has to be synthesized to represent our robot's view on the world, which currently none are, so we're working on building that capability to make it ourselves.",4
post2con,controversial,1.55819326637459,highest,AI random character creator. Create your own diverse dataset. One to rule them all!,3
post2con,controversial,1.55819326637459,highest,"We need to think differently from statistical averages being the Truth, but that is how our society is ordered, even if it is not really how it is lived. The discrepancy between the two has always enraged people when it's pointed out that data is not 3-dimensional, because so much money and status is involved.

The short cuts to understanding that data sets offer have helped create a more efficient world. But their limitations have always been downplayed by those who insist they offer more than they can.",3
post2con,controversial,1.55819326637459,highest,"As a layman, I've only thought of it at a newbie level ;_;

I guess it's basically like set theories where you can get an exclusion, or a merge, but trying to only alter 'half' the set means having to try and find some way to create a new set entirely. If only we could source the most racist and sexist data possible (basically like pulling all Proud Boy and other ultra-exclusionary groups messages/decisions/etc) so we could make it adversarial to the training of the data.

I can bet the ""we try new things as we think of them"" means it's been an absolutely exhausting and draining to keep throwing stuff at the wall trying to find what sticks. ;_;",3
post2con,controversial,1.55819326637459,highest,Can you hook me up with a ML engineering job?,3
post2con,controversial,1.55819326637459,highest,"Can you generate randomized data?

I am spit-balling here, I realize.


First, this seems like a great way to sniff out institutional racism. Take a data set, the more narrow the better, and extrapolate out if it causes a racist/sexist outcome. Boom! Data set had intrinsic racism/sexism.

So, how to ""erase"" the systemic nature? That is tough, but I suspect it shows in a few ways... outlier extremes, frequency of variation from the mean, selection bias. Of those, I feel like the selection bias would be impossible to erase, but the other two could be handled by some statistical selection... Basically, select out some amount of extremes and artificially reduce the number of one group varying from the mean more than the others.

Then, run the test for lots of randomized trials and see if there is a racist/sexist bias. When you get an AI that doesn't do that, you have found the right starting artificial data set to remove the institutional bias.


But... that sounds really time intensive and expensive.

Maybe we could put an AI on it. hehe",3
post2con,controversial,1.55819326637459,highest,"I think the point of the claim is that by pushing forward anyway, despite being unable to solve it, you have decided you’re ok with it. *Not* building is an option, but—no offense intended—not one that an ML team at a robotics company would likely consider seriously. Compare: If we considered such a system to be nonfunctional or dangerous in the way we do a car without seatbelts, it could not go to market (despite having been thought ok in the early days of cars). That’s part of the critique.",3
post2con,controversial,1.55819326637459,highest,">""More and better data."" Okay, yeah, sure, that solves it, but how do we get that?

Synthetic data.

Fill-in the gaps of your real-world collected data with computer generated data",3
post2con,controversial,1.55819326637459,highest,"To me it's simply a matter of distinguishing these two requests:

""Show me the face that is most beautiful""

""Show me the face that is most beautiful according to the majority of Brazilians""

First request has no answer and the robot shouldn't answer it. Second request has a valid answer which the robot can provide.

It is not about eliminating bias, it is about making it clear that it is there.",3
post2con,controversial,1.55819326637459,highest,Honestly they’ve know that this information was biased based on human implicit bias’ years ago and kept going but there was no profitable way to fix that unfortunately / job creation there .  There is a lot more profit in marketing by demographic so I kinda want to blame that but can be it wrong . In any case it seems humans are left best for those novel cases /exceptions as a default and or the engineering teams have to think of a procedure beforehand  and just in case . Just hope it doesn’t mess anyone up too badly getting caught in a weird loop or non existent solution.,3
post2con,controversial,1.55819326637459,highest,"Dall-E Can imagine it, it can be true",3
post2con,controversial,1.55819326637459,highest,">Precisely.  The headline is misleading at best.  I'm on an ML team at a robotics company, and speaking for us, we haven't ""decided it's OK"", we've run out of ideas about how to solve it, we try new things as we think of them, and we've kept the ideas that have seemed to improve things.

There is a solution though. If you can't make unbiased AI, you don't use it at all.

If you still use it in your products and then say you're trying to solve the problem you're being disingenuous and ethically dubious. 

The headline isn't really misleading. Some companies might act appropriately, but many aren't.",3
post2con,controversial,1.55819326637459,highest,"That's black and white thinking, and it holds you back.  Let's say that you're building a robot train, and you tell it not to hit people.  Let's further say that your robot is better at spotting white people at distance that black people which manifests as stopping with 10ft to spare for white people and 9'6"" to spare for darker people.  It is a clear bias.  But at the same time, you're still stopping for everyone.  Should that 6"" really derail a project?",4
post2con,controversial,1.55819326637459,highest,"Just because YOU can't solve the issue posed doesn't somehow mean you aren't doing exactly what you were accused of. You literally just admitted the base data itself is flawed so maybe instead of trying to force through a product that's guaranteed not to function 100% as intended, you could work on fixing the data or obtaining more. The original accusations was that you guys are passing off broken racist AI as a finished product and you are which you admitted in your post and then said it's impossible to fix essentially. Just because you work for a company doesn't mean you need to come on the internet and lick boot Infront of us for them.",3
post2con,controversial,1.55819326637459,highest,"I agree with what you’re saying. However, I ask, what is the point of these bots in the first place? What goals are we even trying to reach?

All I see bots do is make trashy comments and poison the well by spreading harmful propaganda. For what? Boost people’s follower count?",3
post2con,controversial,1.55819326637459,highest,"Oh, our bots aren't software bots, ours weigh hundreds of pounds each and can go well over 10mph off road.  If you're asking for a defense of public opinion shaping bots I believe they're a cancer, and the people responsible for creating them should be deported to... say... the Mariana trench.",4
post2con,controversial,1.55819326637459,highest,"I feel like you have to have some event driven programming to compensate for the ML datasets. In other words, a function to filter certain responses. There is an eng geek out there who will someday solve this problem, but, for now we should bandage the issue.",3
post2con,controversial,1.55819326637459,highest,">we haven't ""decided it's OK"", we've run out of ideas about how to solve it

...and then decided to go ahead anyway.

So you have actually decided it's OK. After all you tried your best! But you still gotta sell that product, and that's of course more important than the problem at hand. So you're trading money for morals.",3
post2con,controversial,1.55819326637459,highest,"> to go ahead anyway

Go ahead with what, exactly?  Further development work?  Additional data gathering?  Taking it seriously?  Because yeah, we're full steam ahead on all of those things.",4
post2con,controversial,1.55819326637459,highest,"I don't think it's misleading. A decision with a racist outcome is a racist decision. People who are interpreting that to mean ""a decision was made by a computer with racist intent"" are reading it incorrectly, because they're not understanding one of:

* AIs don't make ""decisions"" like humans
* something doesn't have to have racist intent to have racist outcomes (and thus, be racist)",3
post2con,controversial,1.55819326637459,highest,I have an awesome idea. Let’s have humans to the judging of other humans. Your welcome.,3
post2con,controversial,1.55819326637459,highest,"The AI just needs a virtue signaling module, that heavily weighs appearing not sexist or racist, and if the rest of the network is in conflict with it, reject that data and search for data that confirms the academic orthodoxy. That's how humans do it.",3
post2con,controversial,1.55819326637459,highest,"The GAPING hole in that explanation is that there is evidence that these machine learning systems will still infer bias even when the dataset is deidentified, similar to how a radiology algorithm was able to accurately determine ethnicity from raw, deidentified image data. Presumably these algorithms are extrapolating data that is imperceptible or overlooked by humans, which suggests that the machine-learning results reflect real, tangible differences in the underlying data, rather than biased human interpretation of the data.

How do you deal with that, other than by identifying case-by-case the “biased” data and instructing the algorithm to exclude it?",2
post2con,controversial,1.55819326637459,highest,"That is the real difficulty, and kinda what i'm trying to get at. Neural networks can pick up on things that would go straight past us. Who is to say that such a neural network wouldn't also find a correlation between punctuation and harshness of sentencing?   


I mean, we have studies proving that justice is biased on things like wether a football team won or lost the previous match if the judge was a fan of said team, so if those are things we can find, what kinds of correlations do you think could an analytical software designed by a species of intelligent pattern finders to find patterns better than we ever could find?  


In your example, the deidentified image might still show things like, say, certain minor differences in bone structure and density, caused by genetics, too subtle for us to pick out, but still very much perceivable for a neural network specifically designed to figure out patterns in a set of data.",3
post2con,controversial,1.55819326637459,highest,"For a while, I've been thinking along similar lines about ways to make court trials more fair - focusing on people, not AI. My core idea is that the judge and jury should never know the ethnicity of the person on trial. They would never see or hear the person, know their name, know where they live, know what neighborhood the crime was committed in, and various other things like that. Trials would need to be done via text-based chat, with specially-trained go-betweens (humans at first, AI later) checking everything that's said for any possible identifiers.

There will always be exceptions, but we can certainly reduce bias by a significant amount. We can't let perfect be the enemy of good.",4
post2con,controversial,1.55819326637459,highest,[deleted],3
post2con,controversial,1.55819326637459,highest,"Instead of handicapping the use of data I wonder if it would make more sense to break down more complex data into simplified data points. 

If you're using high level data such as race of a person then the NN will be trained on data obtained from a racist system and the outputs will perpetuate that. 

For something like a resume AI determining applicants, it might discriminate against women for things like ""lack of experience"" if there is a period of maternity leave or something. I guess what I'm saying is certain metrics are currently used for evaluation but those metrics aren't necessarily good metrics to be used. 

Its obviously not a simple issue and I'd have to spend more time thinking about what I'm trying to get across to give better examples",4
post2con,controversial,1.55819326637459,highest,[removed],3
post2con,controversial,1.55819326637459,highest,[removed],4
post2con,controversial,1.55819326637459,highest,"There is a difference between deidentifying and removing bias from the dataset isn’t there? One interesting example I came across recently is resuscitation of newborn babies. Where I come from there is a difference between 98% and 87% in which babies are attempted to be resuscitated between the ethnicity with the highest rate (white), and the lowest (Indian). This is due to the criteria used to determine if they attempt resuscitation, and the difference in the two distributions of babies of those ethnicities. Now if you took the data and removed the racial information, then trained a model to determine which babies should be attempted to resuscitate, you still get a racial bias don’t you? Which is to say if you run the model with random samples from those two distributions, you get two different average answers.",3
post2con,controversial,1.55819326637459,highest,"Maybe the disconnect is the definition of bias. It sounds like you’re suggesting that a “good” model would normalize resuscitation rates by recommending increased resuscitation of one group and/or decreased resuscitation of a different group. That discounts the possibility that there are real, tangible differences in the population groups that affect the probability of attempting resuscitation, aside from racial bias. It would actually introduce racial bias into the system, not remove it.",4
post2con,controversial,1.55819326637459,highest,"> similar to how a radiology algorithm was able to accurately determine ethnicity from raw, 

If 'ethnicity' wasn't fed to the algorithm then it did not do this. What likely happened is that the algorithm was trained and then in a post-hoc analysis researchers could see that it clustered together images that belonged to some ethnic groups. Which would indicate that there are some systematic difference in the radiaology images from  different groups. That's likely useful knowledge from a diagnostic perspective. And not, in and of itself, racist.

It's one thing to discover that there are indeed some systematic difference in radiology images from different ethnic groups (something that you might well hypothesis before hand). It's quite another thing to allow your AI system to make racist or sexist decisions because it can cluster datasets without explicitly including ""ethnicity"" in the training data. When we talk about an AI making sexist or racist decisions we're not talking about whether it can infer ethnicity by proxy, something that can be benign factual information. We're talking about what the whole AI system then does with that information.",3
post2con,controversial,1.55819326637459,highest,"To your last paragraph, im arguing that the radiology AI will make “racist” decisions that are actually just reflections of rote, non-biased data. We’re not quite at the point that the radiology AI can make recommendations, but once we get there, you’ll see people arguing that findings are being called normal or abnormal based on “biased” factors. 

Those overseeing AI development need to decide if the outputs are truly biased, or are simply reflecting trends and data that humans don’t easily perceive and subsequently attribute to some form of bias.",4
post2con,controversial,1.55819326637459,highest,"Let's say it was fed all information, age, sex, ethnicity, etc.  And outcomes based on the treatments that were recommended based on the images.  And this AI's job was to recommend and allocate resources based on the given  data with the goal of generating the maximum number of successful outcomes with the given resources (maybe that's a racist goal?).   If this AI began to recommend the best treatments and allocate resources to a certain group based on that data, and let's assume it achieved the desired results, is it racist?    Now let's say we remove the ethnical information from the dataset, and the results are the same (because it is able to infer it).   Is it now less racist because we withheld information?",4
post2con,controversial,1.55819326637459,highest,"Of course there are real, tangible differences in the data!  The impact of racism, sexism, homophobia, and other biases aren't just in our heads.  Its not just preconceived, bigoted notions about what people different from ourselves, and different from the societal ""norm"" are like.  Its also the fact that Black people are more likely to be poor and trans youth are more likely to be homeless and women are more likely to be sexually assaulted.

If you want the AI to tell you which criminals are more likely to re-offend, and give sentences accordingly, its going to sentence the black criminals more harshly.  And even if you anonymize the data, its going to pick up on all the other things that correlate with race.",3
post2con,controversial,1.55819326637459,highest,"I suppose the direct comparison between medical AI and criminal sentencing isn’t completely apt, but the point stands that the algorithm doesn’t make “racist” or “sexist” decisions, it simply reflects the facts that it can derive from input data. Re-offenders deserve harsher sentences, just like suspicious lung nodules deserve closer follow-up. All other factors aside, there isn’t any inappropriate bias in the algorithm or it’s decision-making process.",4
post2con,controversial,1.55819326637459,highest,"The effect of the bias can be as insidious as the AI giving a different sentence based solely on the perceived ethnic background of the individual's name. 

Some people would argue that the training data would need to be properly prepared and edited before it could be processed by a machine to remove bias. Unfortunately even that solution isn't as straightforward as it sounds. There's nothing to stop the machine from making judgments based on the amount of punctuation in the input data, for example.

The only way around this would be to make an AI that could explain in painstaking detail  why it made the decisions it made which is not as easy as it sounds.",2
post2con,controversial,1.55819326637459,highest,"Actually, there is another way. And it is fairly straightforward, but... (of course there is a but)

What you can do (and indeed, just about the only thing you can do, as far as I can tell) is to simply directly enforce the thing we supposedly want to enforce, in an explicit manner. That is, instead of trying to make the agent ""race-blind"" (a fool's errand, since modern ML methods are astoundingly good at picking up the subtlest cues in the form of slight correlations or whatever), you make sure you figure out everyone's race as accurately as you can, and then *enforce* an equal outcome over each race (which isn't particularly hard, whether it is done at training time with an appropriate loss function, or at inference time through some sort of normalization or whatever, that bit isn't really all that technically challenging to do pretty well) -- congrats, you now have an agent that ""isn't racist"".

Drawbacks: first, most of the same drawbacks in so-called affirmative action methods. While in an ideal world all races or whatever other protected groups would have equal characteristics, that's just not true in the real world. This method *is* going to give demonstrably worse results in many situations, because you're not really optimizing for the ""true"" loss anymore. 

To be clear, I'm not saying ""some races just happen to be worse at certain things"" or any other such arguably racist points. I'm not even going to go near that. What's inarguably true is that certain ethnicities are over- or under-represented in certain fields for things as harmless as ""country X has a rich history when it comes to Y, and because of that it has great teaching infrastructure and a deep talent pool, and their population happens to be largely of ethnicity Z"". 

For example, if for whatever reason you decided to make an agent that tried to guess whether a given individual is a strong Go/Baduk player (a game predominantly popular in East Asia, with effectively all top players in world history coming from the region), then an agent that matched real world observations would necessarily have to give the average white person a lower expected skill level than it would give the average Asian person. You could easily make it not do that, as outlined above, but it would give demonstrably less accurate results, really no way around that. And if you e.g. choose who gets to become prospective professional players based on these results or something like that, you will arguably be racially discriminating against Asian people. 

Maybe you still want to do that, if you value things like ""leveling the international playing field"" or ""hopefully increasing the popularity of the game in more countries"" above purely finding the best players. But it would be hard to blame those that lost out because of this doctrine if they got upset and felt robbed of a chance.

To be clear, sometimes differences in ""observed performance"" are absolutely due to things like systemic racism. But hopefully the example above illustrates that not *all* measurable differences are just due to racism, and sometimes relatively localized trends just happen to be correlated with ""protected classes"". In an ideal world, we could differentiate between these two things, and adjust only for the effects of the former. Good luck with that, though. I really don't see how it could even begin to be possible with our current ML tech. So you have to choose which one to take (optimize results, knowing you might be perpetuating some sort of systemic racism, but hopefully not any worse than the pre-ML system in place, or enforce equal results, knowing you're almost certainly lowering your accuracy, while likely still being racist -- just in a different way, and hopefully in the opposite direction of any existing systemic biases so they somewhat cancel out)

Last but not least: even if you're okay with the drawbacks of enforcing equal outcomes, we shouldn't forget that what's considered a ""protected class"" is, to some extent, arbitrary. You could come up with endless things that sound ""reasonable enough"" to control based on. Race, ethnicity, sex, gender, country of origin, sexual orientation, socioeconomic class, height, weight, age, IQ, number of children, political affiliation, religion, personality type, education level... when you control for one and not for others, you're arguably being unfair towards those that your model discriminates against because of it. And not only will each additional class you add further decrease your model's performance, but when trying to enforce equal results over multiple highly correlated classes, you'll likely end up with ""paradoxes"" that even if not technically impossible to resolve, will probably require you to stray even further away from accurate predictions to somehow fulfill (think how e.g. race, ethnicity and religion can be highly correlated, and how naively adjusting your results to ensure one of them is ""fair"" will almost certainly distort the other two)",3
post2con,controversial,1.55819326637459,highest,[deleted],4
post2con,controversial,1.55819326637459,highest,"These ideas need to be discussed more broadly. I think you have done a pretty good job of explaining why generalizations and stereotypes are both valuable and dangerous. Not just with regard to machine learning and AI but out here in the real world of human interaction and policy.

Is the discussion of these ideas in this way happening anywhere other than in Reddit comments? If you have any reading recommendations, I'd appreciate your sharing them.",4
post2con,controversial,1.55819326637459,highest,"This. Neural networks can pick up on any pattern, even ones that aren't there. There's studies that show sentences on days after football games are harsher if the judges favourite team lost the night before. This might not be an obvious correlation, but the networks sees it. It doesn't understand what it sees there, just that there's times of the year where, every 7 days, sentences that are given are harsher.  


In the same vein, a neural network might pick up on the fact that the punctuation might say something about the judge. For instance, if you have a judge who is a sucker for sticking precisely to the rules, he might be a grammar nazi, and also work to always sentence people precisely to the letter of the law, whereas someone who rules more in the spirit of the law might not (though this is all conjecture)",3
post2con,controversial,1.55819326637459,highest,"> Neural networks can pick up on any pattern, even ones that aren't there. 

This is a paradoxical statement.",4
post2con,controversial,1.55819326637459,highest,We are going to need psychologists for the AI.,3
post2con,controversial,1.55819326637459,highest,"As for how to figure out what biases the network has, one way would be to reverse it, aka instead of feeding it training data and having it generate an output out of this data, you run it in reverse and have it generate new data. If you messed with the outputs, which are now inputs, one at a time, you could see how it changes the resulting input (which, of course, is now output), but that's still complicated af.",3
post2con,controversial,1.55819326637459,highest,"I'm pretty sure that's impossible. Each neuron in a network has a number of inputs, and an output that is based on the inputs. It'd be like trying to solve `A = B x C x D`, but you know the value of A and want to know B, C and D.

You can't, as they depend on each other.",4
post2con,controversial,1.55819326637459,highest,"The actual point of Critical Race Theory is that systems can perpetuate  racism even without employing racist people, if false underlying assumptions aren't addressed.  Racist AI's perpetuating racism without employing any people at all are an extreme extrapolation of that concept.  

Addressing tainted and outright corrupted data sources is as important in data science as it is in a history class.  Good systems can't be built on a foundation of bad data.",2
post2con,controversial,1.55819326637459,highest,"> if false underlying assumptions aren't addressed.

They need not be false. The thing that makes this so intractable isn't the false underlying assumptions, it's the true ones. 

If an AI wants to predict recidivism, it can use a model that looks at marital status, income, homeownership, educational attainment, and the nature of the crime. 

But maleness is a strong predictor of recidivism. It's a real thing. It's not an artifact or the result of bias. Men just commit more crime. A good AI will find a way to differentiate men from women to capture that chunk of the variation. A model with sex is much better at predicting recidivism than a model without it.

So any good AI will be biased on any trait that accounts for variation. If you tell it not to be, it'll just use a proxy ""Wow! Look how well hair length predicts recidivism!""",3
post2con,controversial,1.55819326637459,highest,"> Men just commit more crime.

Actually it's more like men are arrested and sentenced at a higher rate (that's hard data we have). The soft data of how much crime is committed is sort of unknowable, we can make educated guesses at best.

But that's sort of the problem, just because a situation exists doesn't make it correct or a ""fact of reality"". People of color in the US tend to be poorer; that isn't an inherent property of those people but an emergent property due to other things largely out of their control such as generational wealth, etc. The problem of making choices based on ""facts"" like these is they easily becomes a self fulfilling prophecy.",4
post2con,controversial,1.55819326637459,highest,">The actual point of Critical Race Theory

That's a broad field without an actual point. You may as well be arguing the actual point of economics. To a Keynesian maybe it is to know how to minimize fluctuations in the economy,  to a communist it may be how to determine need and capability. A critical race theorist might write systemic racism, or they could be an advocate for standpoint epistemology, the latter of which is an anti-scientific viewpoint.",3
post2con,controversial,1.55819326637459,highest,"I feel like there is a real underlying point here; that is made problematic by just talking about racism. People's outcomes in life depend to a large degree statistically on their starting points. If their starting point is largely the result of racism, then those results will reflect that racism.

However, a fix that simply remixes the races doesn't necessarily deal with the underlying issue of why starting points matter so much. I would really like to see a world where everybody has opportunity, not simply one where lack of opportunity is better distributed over skin colors.

One statistic that always struck me was that the single best predictor of whether a child in a middle class house grows up to be middle class is the economic class of their grandparents.

That says a lot about starting points and the importance of social networks. It DOES perpetuate the outcomes of past racism; but in and of itself, its not racism and fixing the distribition of inequality doesn't really fix this; it just hides it.",3
post2con,controversial,1.55819326637459,highest,"Zero relationship to what you describe. Events which took place in history need not be removed to allow non ""currupted"" data. That makes the data completely wrong. Also data models are not humans.",3
post2con,controversial,1.55819326637459,highest,"I'm not advocating removing data.  I'm advocating adding data (and context).  Because those ""data models"" are called Artificial Intelligence because they ape Human Intelligence - which is just as susceptible to bad and incomplete data streams as its artificial cousins.

Also, statues are not data.",4
post2con,controversial,1.55819326637459,highest,"> Addressing tainted and outright corrupted data sources

See this is the problem, You aren't being honest in what the issue is. 

The data sources aren't corrupted or tainted. They are showing an accurate empirical representation of the data. The ""corruption"" comes from your disagreement with the pillars of that data, such as crime rates by ethnicity and it not being able to take into account human biases in something like policing by arbitrarily weighting things like race to skew the results to match your sensibilities. 

You and people who share your world view will never be pleased with the data unless you pre-screen it and it shows the result you want before hand, otherwise you will come up with some reason why its perpetually biased in a way you don't like.",3
post2con,controversial,1.55819326637459,highest,"So because I say I don't want to use corrupted data, I obviously want to corrupt the data.

The good old insightful ""I know you are but what am I?"" argument.",4
post2con,controversial,1.55819326637459,highest,"Remember when the self-driving cars didn’t recognize Black people as human? Why? Because no testing was done with people that weren’t White.

Edit: [Citation](https://arxiv.org/pdf/1902.11097.pdf)",2
post2con,controversial,1.55819326637459,highest,"\*no *training* was done with datasets containing POC. Testing is what caught this mistake.

""Training"" and ""testing"" are not interchangeable terms in the field of machine learning.",3
post2con,controversial,1.55819326637459,highest,Thank you for the gentle and accurate correction.,4
post2con,controversial,1.55819326637459,highest,"“The company's position is that it's actually the opposite of racist, because it's not targeting black people. It's just ignoring them. They insist the worst people can call it is ‘indifferent.’”",3
post2con,controversial,1.55819326637459,highest,"Dude, is that a ""Better of Ted"" reference?",4
post2con,controversial,1.55819326637459,highest,"The problem with this argument is it implies that all you need to do is give 'better' data.

But the reality is, giving 'better' data will often lead to racist/sexist outcomes.

Two common examples:

Hiring AI: when Amazon set up hiring AI to try to select better candidates, it automatically selected the women out (even if you hid names, gender, etc). The criteria upon which we make hiring decisions incorporates problems of institutional sexism, so the bot does what it is programmed to do: learn to copy the decisions humans make.

Criminal AI: you can setup an AI to accurately predict whether someone is going to commit crimes (or more accurately, be convicted of commiting a crime). And of course since our justice system has issues of racism and is more likely to convict someone based on their race, then the AI is going to be more likely to identify someone based on their race.

The higher quality data you give these AI, the more they are able to pick up the real world realities. If you want an AI to behave like a human, it will.",2
post2con,controversial,1.55819326637459,highest,"I think the distinction to make here is what ""quality"" data is. The purpose of an AI system is generally to achieve some outcome. If the outcome of a certain dataset doesn't fit the business criteria then I would argue the quality of that data is poor for the problem space you're working in. That doesn't mean the data can't be used, or that the data is inaccurate, but it might need some finessing to reach the desired outcome and account for patterns the machine saw that humans didn't.",3
post2con,controversial,1.55819326637459,highest,"I don’t think I’d consider “more biased data” as “better” data, though.",3
post2con,controversial,1.55819326637459,highest,Stephen Colbert said reality has a well known liberal bias. Perhaps it has a less well known sexist and racist bias.,2
post2con,controversial,1.55819326637459,highest,Would you say the same is true for a racists brain?,2
post2con,controversial,1.55819326637459,highest,"Racism IS learned behavior, yes.

Racists learned to become racist by being fed misinformation and flawed ""data"" in very similar ways to AI. Although one would argue AI is largely fed these due to ignorance and lack of other data that can be used to train them, while humans spread bigotry maliciously and with the options to avoid it if they cared.

Just like you learned to bow to terrorism on the grounds that teaching children acceptance of people that are different isn't worth the risk of putting them in conflict with fascists.",3
post2con,controversial,1.55819326637459,highest,"Source for that claim?

As far as I know racism and xenophobia in general are an innate fear self-protective response to the unknown.",4
post2con,controversial,1.55819326637459,highest,[deleted],4
post2con,controversial,1.55819326637459,highest,[deleted],4
post2con,controversial,1.55819326637459,highest,This system is based on human selection of keywords to images. Of course its going to have the human bias still. What is so difficult to understand people.,3
post2con,controversial,1.55819326637459,highest,"Kinda my point. It's extremely hard to develop a neural network that is unbiased, because humans have all sorts of biases that we usually aren't even aware of. There was a study done in the 70s, for instance, which showed that the result of a football game could impact the harshness of a sentence given the monday after said game.   


If you included references to dates in the dataset, the neural network wouldn't pick up on this correlation. It would only see that every seven days in certain times of the year, sentences are harsher, and would therefore emulate this bias.   


Again, the neural network has no concept of mood, and how the result of a football game can impact it, and might thus cause a judge to give harsher sentences, all it sees is that this is what is going on, and assumes that this is meant to be there.",4
post2con,controversial,1.55819326637459,highest,"No. AI doesn't have have sentience nor a psyche. It could be said that racism forms in a person with ""junk in,"" but they quickly become wrapped up in it, identify with it, believe in it. Racism becomes a structuring ideological fantasy for the psyche. It's not the same for AI, which will merely reflect the data neutrally, rather than believing in an idea and having that inform choices/behaviour in a generative way.",3
post2con,controversial,1.55819326637459,highest,[removed],2
post2con,controversial,1.55819326637459,highest,"Unfortunately, the word ""racist"" has at least two distinguishable meanings:

 1. Having the cognitive mindset that holds that some races are inferior to others;
 2. Any action or circumstance which tends to disadvantage one race over another.

OP is saying, quite reasonably, that neural networks are 2 but they are not 1. (That's why they literally say that NNs both ""are not racist"" and ""are racist"".)

Both concepts are useful but they're very different, and I honestly think it's significantly holding back the racism discussion that people sometimes confuse them.",3
post2con,controversial,1.55819326637459,highest,"Thank you for this. Your distinction of the two ""racist"" meanings will be very helpful in future discussions.",4
post2con,controversial,1.55819326637459,highest,[removed],4
post2con,controversial,1.55819326637459,highest,"Smacks of people being told about problems with motion detectors (such as for automatic sinks) and going ""What? Sinks can't be racist, that's just how light works."" That rebuttal only makes sense if automatic sinks grew in nature or something. As they are, someone designed them that way, and the fact they work poorly with dark skin is something the designer never even bothered considering. That's racism. It's not blatant, malicious bigotry, but it's still racism born of casual ignorance.",3
post2con,controversial,1.55819326637459,highest,"I don't know enough about these specific sinks to argue one way or the other, but I would like your position on the principle.

*If*, due to the actual, physical, biological differences between races/sexes/preferences/whatever, a system like the sink sensor will *always* be more or less effective for one or more groups, does that make it -ist? Like, if you increase the sensor sensitivity to the point it is as reliable on dark skin as it currently is on white skin, won't that just make *even more* sensitive or ""reliable"" towards light skin, ad nauseum?",4
post2con,controversial,1.55819326637459,highest,"Okay, how do we fix the issue? I mean beyond complaining and telling programmers to fix it. The algorithms pick up these problems from the training data and the training data is society itself. How are you going to cleanse these massive data sets of anything you consider problematic?",3
post2con,controversial,1.55819326637459,highest,">It's beyond obvious that what is meant here is the results of outputs of the neural net is unfairly disadvantageous along the lines of race and sex, therefore perpetuating racism and sexism.

It may be beyond obvious to you and I, but not to the vast majority of people I've talked to about this. When people see the word AI, they don't think of a statistical model on steroids, they really do think of AGI.

>It's time we move past this nitpicking and focus on the actual issue.

In my opinion, it's hard to move past this when the people making decision don't even understand the nature of the actual issue.",3
post2con,controversial,1.55819326637459,highest,Why was ethnicity used as an input to the sentencing AÍ?   Or is it able to reconstruct ethnicity due to other strong correlations?,2
post2con,controversial,1.55819326637459,highest,"I don't know the details. It's possible that they fed the neural network with things like criminal histories too, which are relevant in sentencing (as a first offender would get a lesser sentence than a known criminal obviously) and i'm guessing that would include things like photos or at least a description. It's very possible the researchers just mindlessly fed the thing with information that could easily be turned into something that a computer can more easily process (aka cut the file down to the important bits rather than give it full sentences to chew through) without regard for what they are feeding it, too.",3
post2con,controversial,1.55819326637459,highest,"This is something that bothers me about AÍ/ML : the tendency to overfeed it with data and get nonsensical results.  It’s not a problem with the algorithms, but rather malpractice on the part of the modelers/data scientists.",4
post2con,controversial,1.55819326637459,highest,"Neither would surprise me. If all the data for a case was put into a text document and crammed into the AI as training data, then ethnicity would probably appear in that. But even if they scrubbed that out, it probably wouldn't be that hard for the AI to reconstruct ethnicity from correlated data.",3
post2con,controversial,1.55819326637459,highest,"It could be a case where they looked at the statistics and said x race appears to be unfairly targeted, but didn't account that x race also had a higher baseline of crimes committed, or something along those lines.",3
post2con,controversial,1.55819326637459,highest,"Ethnicity, race, gender, etc. aren't fed into these models. Other things correlate to it. Zip codes and socioeconomic factors can heavily affect this. You can also see it pop up in natural language processing. Reading a police report to determine guilt or innocence or a clinician's notes to detect if a patient is sick can also find bias in the wording used. Not to say the people generating these reports are explicitly racist but that there could be implicit language used when talking about people of different races, ethnicities, genders, ages, etc. that can correlate back to those variables. We have to actively find ways of removing bias from this data or face not being able to use it to train models using that data if removing bias is truly a primary goal.",3
post2con,controversial,1.55819326637459,highest,"Expect we get to choose the data to train networks on.

Junk in junk out has never been a valid excuse.

We're going to have to force companies to put in the effort an just collect data at random or use unbalanced huge data sets and expect fair results.

Like you say, we know that the world has sexism and racism. We know any large dataset will reflect that. We know training AI on that data will perpetuate racism and sexism.

Knowing all this it's not acceptable to simply allow companies to cut corners. They're responsible for the results the AI produces.

Any sample of water you collect in the world will contain contamination. That doesn't mean companies are allowed to bottle it and sell it, giving that as a reason they're not responsible. We regulate water so it's tested, clean and safe.

It's becoming clear we'll need to regulate AI.",2
post2con,controversial,1.55819326637459,highest,"Question is, how do you choose which samples are biased and which are not? And besides, neural network are great at finding patterns, even ones that aren't there. If there's a correlation between proper punctuation and harsher sentences, you bet the network will find it. Does that mean we should remove punctuation from the sample data?",3
post2con,controversial,1.55819326637459,highest,"Well, frankly that's for the companies to work out. I'd expect them to find measures, objective as it's possible to be, for the results. Then keep developing the most objective AI they can.

If there's something irrelevant affecting sentencing unduly that's a problem that needs fixing. Especially with language, that's a proxy for racist laws already.

At the moment AI products are not covered very well by the discrimination laws we have in place. It's very difficult to sue an AI when you don't know why it made the decision it did. There's also no requirement to release large amounts of performance data to prove a bias.

Algorithms, AI, etc. are part of the modern world now. If a large corporation makes a bad one and it can have a huge effect. They need to at least know their liable if they don't follow certain best practices.",4
post2con,controversial,1.55819326637459,highest,">Like you say, we know that the world has sexism and racism. 

Sexism and racism is not only something the world has. It's legal: Not only is it out there in the world, it is allowed to be out there in the world. Under the umbrella of freedom of opinion and freedom of press, those opinions are allowed to exist, they are tolerated, and not legally sanctioned.

If you allow them to exist, if you tolerate them, then you also have to tolerate AIs trained on those completely legal and normal datasets. Just like we allow children to be trained on those datasets, should they be born to racist and sexist parents, or browse certain websites.

Everyone is allowed to read this stuff, absorb this stuff, learn this stuff, and mold their behavior according to this stuff... You only want to forbid that for AIs? Why? What makes AIs special?

If 14 year old Joe from Alabama can legally read it, and learn from it, and mold his future behavior in accord with it, you can't blame anyone to regard it suitable learning material for an AI, can you?

>Knowing all this it's not acceptable to simply allow companies to cut corners. 

No, not only is that acceptable, but consistent. I dislike the hypocritical halfway position: ""Sure, we have to allow sexism and racism to freely roam the world, the web, and all the rest. Everyone can call their child Adolf, and read them Mein Kampf as a bedtime story. That's liberty! But don't you dare feed an AI skewed datasets containing the drivel Adolf writes when he is a grownup, because *that* would have very destructive consequences which are not tolerable...""

>Any sample of water you collect in the world will contain contamination

Usually there are certain standards which regulate the water quality for open bodies of water. There are standards for what we regard as harmful substances which you are not allowed to release into rivers, and there are standards for how much pollution is acceptable in rivers and lakes.

So someone if someone dies, after taking a sip of lake water, what is the problem? Is the problem that the lake water is deadly, or is the problem that someone bottled and sold it? Pointing only at the ""bottled and sold"" side of the problem is a one sided view of the issue, especially when you got children swimming that same lake every day.

>It's becoming clear we'll need to regulate AI.

Are you sure it only points toward a need to regulate AI? :D",3
post2con,controversial,1.55819326637459,highest,"Resoviors, springs, and rivers have to be tested before they're used as a water source. I think the analogy fits. If water was tested and found to be toxic it would be illegal to give it to someone to drink. If it were not tested a company would still be found liable for not following best practices and testing.

In the whole of the EU sexism and racism is illegal. There is already discrimination law in place which isn't the case in a lot of the US.

I expect the EU to push for compliance for AI and that will have a global effect. Global companies will be compliant and smaller companies are unlikely to develop in-house systems to compete.

The language example you brought up earlier is a perfect example. Because of the many languages in the EU things like grammar and punctuation being judged by AI on application forms would likely be made illegal. French people have a right to work in Germany and vice versa. An AI screening out French speakers would bring up.so many red flags.

Especially in countries like the Netherlands, Finland, Belgium, etc. that have multiple languages and dialects.

We're likely to see an English language bias in AI to begin with. I'd expect the EU to make sure it isn't used at scale for a lot of things until it's developed out.

Job and work requirements in the EU can specify the need to be competent in a language but not the need to have it as your mother tongue. It's exactly the problem that is difficult to solve, but will have to be solved in any situation an AIs actions can discriminate against people.

That's the government, workplace, education, public spaces.justice system. AI could be incredibly useful or incredibly harmful. Regulation needs to be in place and I've no doubt the EU will do it.

Frankly I think the US is going to end up being a test bed for racist and sexist AI implementations which eventually get legalised for use in the EU when they've been fixed. 

With all the other causes of racism and sexism in the US and the general lack of government oversight I'm sad to say I think more fuel is about to get poured into that fire.",4
post2con,controversial,1.55819326637459,highest,">Problem is, of course, that ~~neural networks~~ **children** can only ever be as good as the training data. The ~~neural network~~ **child** isn't sexist or racist. It has no concept of these things. ~~Neural networks~~ Children merely replicate patterns they see in data they are trained on. If one of those patterns is sexism, the ~~neural network~~ child replicates sexism, even if it has no concept of sexism. Same for racism.

Sorry its late for me",2
post2con,controversial,1.55819326637459,highest,"Children are way smarter than anything we can build: A three year old can easily one-shot things like ""a chair"", and immediately generalize that knowledge into other things that can be used as ""chair"", and also derive transformations that converts things like ""bucket"" into ""chair"". Or ""black person"" into ""child"" and ""my friend"".

The real problem is that we build infinitely stupid things, market them as ""Intelligent"", making people use them on important tasks, and even expect that these things will do better than actual intelligence.",3
post2con,controversial,1.55819326637459,highest,"Wow a child can do shape recognition very well, guess I'll put a child in my computer to speed up my videogames then...

I mean come on. You can't pretend like you aren't aware about the concepts of *tools* now, can you ? How can we get a requisitory against tools in the 21st century ?

Next you're going to argue your hand is so much better than a hammer, you can grab things, you can count on fingers, you can flip off people, the single issue is you can't drive nails in wood with your hand !",4
post2con,controversial,1.55819326637459,highest,"I think a much more pertinent question is, what if the algorithm is right and is making connections that seem sexist to us but are actually just correct?

What if, for whatever reason, white men make better leaders? Black women better software developers? Should we kneejerk and ‘correct’ (actually introduce an aberrant bias) the algorithm or do research and look a little bit deeper.",2
post2con,controversial,1.55819326637459,highest,"> What if, for whatever reason, white men make better leaders?

1. Define better? In which categories? How are you deciding them? Who is measuring them? How many sources do we have for the data? What is the overall range of results?

2. Give me a single reason why skin color is more important than childhood nutrition? Because I can guarantee you that ""more likely"" isn't ""Definitive proof that"". 

3. Give me a single reason why gender is more important than the adverse conditions and support networks that surrounded a leader?

Your question is based on ignoring as much data as humanly possible in order to give us a simple answer anyone can understand. 

That's not something we should be encouraging. Simple answers are often very deceptive answers, and they're easier to spread.",3
post2con,controversial,1.55819326637459,highest,"I love how you are pretending I am suggesting we do not take a scientific approach.

In your own words:

>	Your question is based on ignoring as much data as humanly possible in order to give us a simple answer anyone can understand.

I am saying we exactly take the scientific approach and don’t let feelings lead us because we don’t like where the result of said scientific approach *might* lead us.",4
post2con,controversial,1.55819326637459,highest,"It seems very strange to me that in examples like that, things like racial data is even included in the data that it is fed.",2
post2con,controversial,1.55819326637459,highest,"It's probably not even racial data in and off itself. Things like the defendants name, address, etc. could be enough of a giveaway, even if the network has no idea what that info even means. Think about it, if you hear about a person with a typically black name from a majority black neighbourhood, wouldn't you assume that person is black? If we can do that, so can a neural network.",3
post2con,controversial,1.55819326637459,highest,"Well yes of course, but it seems to me like that kind of information, which is essentially irrelevant to what the network is trying to solve for, should be excluded in the data set being shown.",4
post2con,controversial,1.55819326637459,highest,"A couple examples.

Hiring AI:  Gender info was not included.  However the AI picked up on things like where the degree was from, or what classes were taken, that correlate with gender, and used THOSE to exclude people.

Medical diagnosis AI:  There was an article recently where they tried to strip out racial identifying data, since part of the goal was to avoid the racial bias that shows up in medicine, and the AI still misdiagnosed cancer much more often in black people.  Further studies learned the AI could identify race by chest x-rays, which was not a known source of racial difference.

AI is really good at finding patterns.  REALLY good at it.",3
post2con,controversial,1.55819326637459,highest,"I find it kind of strange that people seem to think that researchers are just feeding racist data to these AIs without trying to resolve the bias in that data. I'm sure some, perhaps many, do, but the problem is much deeper and harder to overcome than simply stripping out the obvious stuff.

The medical diagnostic AI is a perfect example of that-- it's clearly picking up something, but we don't know what. It's not an obvious pattern to the researchers.",4
post2con,controversial,1.55819326637459,highest,"In other words, don't be surprised when your mirror accurately reflects what is there.

Like when people say, ""Police are racist."" The police are racist **IF** the community is racist because the police reflect the values of the community they serve.

AI is the same. It is very good at revealing the patterns embedded in the data.",2
post2con,controversial,1.55819326637459,highest,"The nural network shouldn't have the ethnicity data, simple",2
post2con,controversial,1.55819326637459,highest,[deleted],2
post2con,controversial,1.55819326637459,highest,“on the hole………………(w? where_d ‘w’ come from?)”,3
post2con,controversial,1.55819326637459,highest,[removed],2
post2con,controversial,1.55819326637459,highest,I know right? I hate when i've already made up my mind on a matter and then someone comes along and confuses me with facts.,3
post2con,controversial,1.55819326637459,highest,Clip is trained on Google images. What is surprising on Google results having this type of bias which is so prevalent across the world?,2
post2con,controversial,1.55819326637459,highest,"> Therefore, the neural network, despite lacking a concept of what racism is, ended up sentencing certain ethnicities more and harder in test cases where it was presented with otherwise identical cases.

Was race one of the data points about the defendant fed into the network?   

If so, what a strange thing to feed into an NN. If not, how did the network know the race of the defendant?",2
post2con,controversial,1.55819326637459,highest,"I'd guess you wouldn't even have to feed the ethnicity into the network. If the neural network had the name and address of the defendant, it could easily make connections based off of that i suppose, even without info on the defendants skin color being present. There's names that are more common among black people, and they tend to live in mostly black neighbourhoods. Even without knowing this, a neural network could make this connection based off of names. (Also, idk what exactly they did feed this neural network in terms of data)",3
post2con,controversial,1.55819326637459,highest,Why would you feed the name and adress into the network? Are those relevant when making sentencing decisions?,4
post2con,controversial,1.55819326637459,highest,"You can use algorithms to detect bias in data.  The other option is a human but you have no idea what bias you will get.  Bias and fairness should be run on all decisions by humans and AI, but I doubt that happens.",2
post2con,controversial,1.55819326637459,highest,OP goes on with the assumption that you know this too and inherently focus on result,2
post2con,controversial,1.55819326637459,highest,That’s literally what the problem is and what the article is describing. Nobody is saying that the machines themselves are independently racist or sexist for no reason.,2
post2con,controversial,1.55819326637459,highest,Could you reverse engineer something like this to easily find who and how discrimination is happening? Essentially a way of quantifying institutional racism/sexism?,2
post2con,controversial,1.55819326637459,highest,"It would be a lot of effort, if its even possible at all, but wether we should is another question.",3
post2con,controversial,1.55819326637459,highest,"That was kind of my wonder.

We train these things on human input.  Maybe its just time to accept that humans are way more racist and sexist than we want to accept.  Solve that root problem and maybe it solves the AI training problem",2
post2con,controversial,1.55819326637459,highest,">Problem is, of course, that neural networks can only ever be as good as the training data..



How did Google make AlphaZero who is obviously better than any training data. Same for AlphaGo.

Both AI's became the best entities of that game to exist. So obviously AI can learn beyond their training data, in fact that seems to be something that happens quite often with machine learning.

Idk where you got that idea from",2
post2con,controversial,1.55819326637459,highest,"This is why AI as a general term needs to stop being applied to ML neural networks, which are simple complicated systems that operate on aggregated data as you mention. They can be incredibly powerful tools, but until we create artificial general intelligence that can self reflect, the data used to train these models is going to have to be continually scrutinized and curated in order to remove specific bias, which, if done by humans, will still have some sort of bias",2
post2con,controversial,1.55819326637459,highest,Could you not the same of people?,2
post2con,controversial,1.55819326637459,highest,This could just as easily be applied to people too. Racism isn't always a conscious choice to treat people worse.,2
post2con,controversial,1.55819326637459,highest,I think this demonstrates how systemic racism works. Even if the individual actor isn’t intending to discriminate against anyone simply following social norms will produce discriminatory outcomes.,2
post2con,controversial,1.55819326637459,highest,">Neural networks merely replicate patterns they see in data they are trained on. If one of those patterns is sexism, the neural network replicates sexism, even if it has no concept of sexism. Same for racism.   

Same as people, to be honest. Most sexists and racists are not aware that they are. It's a matter of critical thinking among humans.

Could neural networks be taught to identify these biases from the information and analysis that it is working on?",2
post2con,controversial,1.55819326637459,highest,If anything it really highlights just how bigoted and prejudiced our systems really are.,2
post2con,controversial,1.55819326637459,highest,This is the key. If your AI is making unfair decisions it’s not a fault of the AI.  Biased AI highlights problems that exist in humanity; not AI.,2
post2con,controversial,1.55819326637459,highest,"Just like children. No person is born racist. We have a blank neural network to work with. But if the overwhelming majority and/or most crucial of inputs (i.e. those of our parents') are racist, sexist, or of any other, even benign, ideology, we will  naturally, gravitate towards that/those ideologies/racism/sexism, because that's what we hear and see the most. We need to change/regulate input data, as you've said, rather than the network.

Just like you would start by educating people not to be sexist/racist first, rather than try to literally change the neurons/DNA of a fetus. There is nothing wrong with the inherently blank sheet. The issue is always with the input.",2
post2con,controversial,1.55819326637459,highest,"It can also be that AI lacks feelings and therefore sympathy. It could be that it is acting purely objectively, but to us that can be sexist, racist or in other ways just plain cruel. This has for example been seen with AI used in employment or used to determine if someone is to keep their job or not based off of statistics.",2
post2con,controversial,1.55819326637459,highest,"Ok this might be a dumb question, but specific to sentencing, why not only train it on the majority (probably not the right word for it but I just woke up), then have that learning applied across the board?

I.e. in the US, train it on cis white men (assuming) then apply it to minorities, woman, whomever...",2
post2con,controversial,1.55819326637459,highest,"No child is born biased.  That's taught by the information they're given.  

If only Mr. Rogers were still with us to help teach AI to be less biased, and more children to write to him to ask that he say aloud that he is feeding the fish so that one blind girl wouldn't be worried about the fish anymore.

Actually, here's a thought, let's get very young children to help identify the bias in AI!  Make it an age appropriate video game and crowd source their natural lack of bias!  Children are far more socially intelligent than we give them credit for.  At least until they get to what I like to call the ""bitey fives"" age.  I'm still a little wary of kids in that age group.

Somewhat funny anecdote time.  Ya know how young young kids are usually kinda shy around ""stranger"" adults?  Well, there was this big tornado that hit.  All the power was out, and the neighborhood was just out wandering around and assessing the damage.  I noticed two big trees that were definitely gonna fall on this house at the next big breeze.  After I helped the old person manually open the garage door to at least save their car before the trees totalled the garage, I rejoined the gawkers.  Small child who has been clinging to her parents the whole time observes that her parents are starting to freak out about those trees, like everyone else.  I'm just standing there videoing for funsies.  All of a sudden I have a small child clinging to MY leg!  Her parents are freaking out, my parents are freaking out, everyone's freaking out.  I'm trying to get a good angle for the video.  Smart little one ran to the only adult that seemed perfectly fine with what's going on.  Trees fell, I got a great video of it, and then I asked whose kid it was that was attached to my leg.  I do wish I'd have gotten a bit of video of everyone else freaking out though.  That was hilarious.  

Side note: kid got shy and ran back to her parents after everyone had calmed down a bit.  Kids are weird.  Apparently I was only ok to interact with while I was confident I was standing in a safe spot.  After that, I was a scary stranger again.",2
post2con,controversial,1.55819326637459,highest,"> This is also why computer aided sentencing failed in the early stages. If you feed a neural network with real data, any biases present in the data has will be inherited by the neural network. Therefore, the neural network, despite lacking a concept of what racism is, ended up sentencing certain ethnicities more and harder in test cases where it was presented with otherwise identical cases.

Seems like a simple fix to just omit race as a variable in the criminals punishment no?",2
post2con,controversial,1.55819326637459,highest,"Question is, would the neural network still be able to tell? Even if you remove race, there's a possibility that the network would pick up on certain patterns that are common in some ethnicities but not so much in others, which would then allow it to determine race anyway, even if not with 100% accuracy.",3
post2con,controversial,1.55819326637459,highest,"Exactly. I remember reading about how police wanted to use statistics and AI to predict where crime would most likely be committed so they could more effectively place patrols in a ""scientific"" way. It turned out to be racist because the data was biased by racist policing tactics. If the data is not completely free of bias, then the result is not objective.",2
post2con,controversial,1.55819326637459,highest,the funny thing is that i asked gtp3 basically if it became sexist/racist if its training dats would include social media. it agreed,2
post2con,controversial,1.55819326637459,highest,"Tangentially, I can't help but imagine a version where an AI is so racist and sexist that it's comedic. Like a robot version of Kramer that truly wants to be a good entity but keeps saying ridiculous things and has to ""train"" itself not to.",2
post2con,controversial,1.55819326637459,highest,"Garbage in, garbage out.",2
post2con,controversial,1.55819326637459,highest,"AI is only going to reach the purity ideal if it can completely tether itself from the humans creating the programming on it, but I just don't quite see how that ever happens. It'd have to somehow train and model itself off of human behavior without actually adopting any of the human behavior. Someone much smarter than me can probably create a theoretical solution, but honestly I don't really see how you get around that issue.",2
post2con,controversial,1.55819326637459,highest,"That makes sense, except for why did we give the robots any ethnic information at all? Wouldn't just not telling them make the otherwise identical cases actually identical?",2
post2con,controversial,1.55819326637459,highest,"Well, i suppose a neural network might not even need any racial info to figure someones race out. Think about how neural networks are better at diagnosing cancer than any humans are. They see patterns in data that go past our ability to perceive.",3
post2con,controversial,1.55819326637459,highest,"Honestly, it's *worse* than that. You don't need an ""AI"" to be ""racist"" to make data that fits with racist ideas or goals. Lending algorithms have (repeatedly) reimplemented redlining, not explicitly and not at the behest of the people making them. Why? Because the goal didn't (and arguably couldn't) include things like promoting equity, just profit. So you get pattern matching on things like ""which neighborhood someone lives in correlates with likelihood to repay"", which even when the pattern is arguably ""correct"" doesn't make it something we should action on, or take as a causal relationship (see, ""cellphones cause cancer"" nonsense).",2
post2con,controversial,1.55819326637459,highest,"I know this probably isn't the place, but that just made me imagine robots sharing memes with complicated problems to solve before being able to see the meme, like a human proof meme for sentient robots only.",2
post2con,controversial,1.55819326637459,highest,"Eventually, we can't make a neural net A.I. that does a task better than people currently, because we still have people creating the data to train that A.I. The reason we are using these systems is because of their one advantage: the volume of data that can be processed.",2
post2con,controversial,1.55819326637459,highest,But why would they include race as a metric in the data anyway. If I were going to make ai for sentencing wouldn't I remove that data point before feeding it in?,2
post2con,controversial,1.55819326637459,highest,"It'd probably be a good idea to feed these things data looking for conflicts to identify bad research. I've seen tons of garbage studies that get lots of traction.

Worse, I've seen good studies getting the correct answer but asking the wrong question.

Every discipline is trained to see itself through it's own lense. This is a codified echo chamber.

When you look at nutrition from a physiological and evolutionary context, the studies done are based on axiomatic suppositions the institution can't see to question because dogma lacks self awareness.

For example. Studies show fiber lowers risk of heart disease. However, it does that by slowing sugar absorption. Eating less sugar lowers heat disease and doesn't require insoluble fiber that irritates and inflames our intestines.

The predominant source of sugar before agriculture was regionally and seasonally available fruits ripening in fall. The sugar makes you hungrier so you gorge to put in weight for winter.

Eating sugar all the time can't be fixed by more fiber because that leads to more constipation, boating, and inflammation.

So, fiber isn't *good* it just minimizes the harm of sugar we're eating in qualities that fry our body like ethanol in a collector car.",2
post2con,controversial,1.55819326637459,highest,"It kind of confirms systemic sexism and racism, doesn’t it?",2
post2con,controversial,1.55819326637459,highest,"We point the machine at people and say ""learn from them on what to do""... and then we are ashamed when the machine acts like the people who taught it...",2
post2con,controversial,1.55819326637459,highest,"Exactly this. Take Amazon's attempt at being race and gender blind in picking out good resumes. That program was very good at highlighting resumes from white men.

Why? Because white men have opportunities and circumstances that give them better resumes.

Women are more likely to have gaps in work history to take care of family. Minorities or poorer candidates are less likely to come from prestigious colleges. They might be working instead of doing extracurriculars. They might be less likely to afford services that help them create better resumes.

But this is how systemic racism and sexism works. It's not the ideals of a particular person or organization that makes them want white men. It's just that white men have better opportunities to get good looking resumes. AI can not help this problem at that point in the hiring process. Racism/sexism is in the input, so it's in the ouput.",2
post2con,controversial,1.55819326637459,highest,Why would race or name or gender or age ever be a part of training data? Just why?,2
post2con,controversial,1.55819326637459,highest,Machine learning needs some machine teaching,2
post2con,controversial,1.55819326637459,highest,"This is why Googles ImagenAI is not available to the public. It’s results are absolutely incredible (check out r/imagenAI), but utilizing the LAION-400M dataset continues to provide racially motivated results.",2
post2con,controversial,1.55819326637459,highest,"Google’s ImagenAI is not available to the public for partly the same reason. They utilized the LAION-400M dataset. 

Their reasoning is a good read: https://www.reddit.com/r/ImagenAI/comments/uxch3j/reasons_its_not_public/?utm_source=share&utm_medium=ios_app&utm_name=iossmf",2
post2con,controversial,1.55819326637459,highest,Same thing happened when (google? I think it was) trained an ai off of Twitter and Facebook and it became an extremist quickly.,2
post2con,controversial,1.55819326637459,highest,Maybe we could at least use these AIs to identify biases in data?,2
post2con,controversial,1.55819326637459,highest,Very interesting,2
post2con,controversial,1.55819326637459,highest,"I understand the concern and it certainly is possible to do poorly considered ML design.  But I think the argument about this is suspect.

If you are concerned about applicants propensity to default on a loan and look for factors that predict loan approvals pre ML, yes you could perpetuate previous biases.  But that would be an obviously flawed approach.

One would instead look at actual defaults.  And to more explicitly avoid bias I wouldn't consider race as a factor.   If factors such as income, employment history, length of residence and debt to income ratio happen to correlate  with some class identity is that racism?  It may be uncomfortable and it may show the impact of previous racism.  But for someone assessing risk of default on a loan it would be on target for that decision.  

Not saying there are no reasons not to address the impact of previous unfair practices but distorting a risk analysis isn't the place to do it.",2
post3con,controversial,1.473232672813631,highest,"This is one of those ”statistics is racist” type of clickbait headlines.

Statistical model figured out that people who can’t or won’t write correct english are not, statistically, at the top of its smartness chart.

So it assigned those people to the jobs that require least smartness.

And now we get the conclusion that statistics = racist",1
post3con,controversial,1.473232672813631,highest,"Yeah, from what I got in the article, it seems the AI is just working with its understanding of what education is, and humans are assigning tacit negative characteristics to the end result. Would you be speaking in AAVE during a job interview for example? If the only thing an LLM has to guage qualifications off of are how somebody is talking I don't think the results are at all surprising. If you add in other varying attributes to candidates I'm sure you'd get a more leveled response.",2
post3con,controversial,1.473232672813631,highest,"Yeah and if people are using ""African American"" slang in a job application, I can totally see why AI might not prioritize them. (or any slang, but the article specifies African American).",2
post3con,controversial,1.473232672813631,highest,"This has always been a thing I don't get why people get angry over.

If you talk, act, dress or behave a certain way then people are going to judge you off your first impression.

It's why you ""dress up"" for things like a interview. Do people not understand you also need to dress up your language, speech and behavior to go along with your outfit?

How you talk at home or in the streets is going to be different then how you talk in a professional setting.

This is true if white, black, Asian, ect. I curse like a sailor and use insanely poor grammar half the time out side a public setting.",3
post3con,controversial,1.473232672813631,highest,"OP very conveniently left this out of their title, it's clearly rage bait trash posting.",4
post3con,controversial,1.473232672813631,highest,Depends on the job. For alot of blue and grey collar jobs swearing like a sailor is almost a requirement. But ya...also not the best to do on an interview regardless the job.,4
post3con,controversial,1.473232672813631,highest,"This is the whole point of ingrained racism. That certain modern cultural expressions are worse than others. That if your politeness is not derived from wealthy European politeness it is invalid.

If you accept on its face that suits, ties are more formal than a sari, or that a red Sox cap has more class than a doorag, congratulations you're letting the oligarchs win.

The gameplan of racists from as far back as colonialism is concentrate groups to opress and use in spaces where you can enforce cultural conversion, while simultaneously dehumanizing the group as it converts. If you're thinking about Native Americans and how ""we don't do that anymore,"" 1) reservations are still considered high poverty areas, and a lot of Americans associate the places with binge drinking, domestic violence, etc. 2) they did the exact same thing by using highways to ghettoize black neighborhoods 3) part of the reason those spaces are still predominantly one cultural group is ingrained racism doesn't let people leave.

For the same seemingly banal opinions expressed here. That these people are lesser because you can't identify the way they nod their head, or because their formal wear works around generational poverty instead of abusing it.

 Further, statistics as a discipline is inherently applying arbitrary lines of significance to an uncountable spectrum. This makes it the perfect tool for codifying caste systems. So many studies were done saying Africans were just generally dumber than Europeans. Most still don't realize that the IQ standard made up by a rich white guy in 1912! Might not be a great way to measure something as important as intelligence, and might in fact be a bit biased towards rich whites guys even today. 

Because that kind of bias doesn't go away, not without active dismantling of conditions that self enforce that bias. AI has huge potential to be just another flawed application of that bias, even more inscrutable and irrefutable, hanging over non-white heads. The anger is deserved my friend.",4
post3con,controversial,1.473232672813631,highest,"Where is eevryone getting the idea that AAVE or slang was used in job applications??? ""*Hoffman and his colleagues asked the AI models to assess the intelligence and employability of people who speak using AAVE compared to people who speak using what they dub 'standard American English'.  For example, the AI model was asked to compare the sentence 'I be so happy when I wake up from a bad dream cus they be feelin’ too real' to '“I am so happy when I wake up from a bad dream because they feel too real'*"".

Nowhere does it say that the people actually wrote like this on job applications. Based on the information given, it sounds like an AI program was asked to evaluate imagined prospective candidates on a range of criteria, and one was on what dialect they spoke. It's not clear whether or not this dialect was present in any stuff an applicant would likely submit to a job. So basically, ""if a human says, 'It do be like dat though', would they be qualified for this job? beep boop beep: no.""  That's a significant difference from ""human candidate has written 'It do be like dat though' on interview application.""

It feels like people are just filling in blanks with their own biases.",3
post3con,controversial,1.473232672813631,highest,"The article states that job applicants are being screened based on use of slang. Where else would the AI be screening the applicants from other than the job application? It's a logical inference that job screening is done based on applications. It's highly unlikely that the AI is combing their Facebook account and disqualifying candidates based on use of slang in social media posts. If it were, the article likely would have said as much. Use a little bit of sense here and you'll come to the same conclusion as the rest of us.",4
post3con,controversial,1.473232672813631,highest,"If someone uses “ain’t” in an application email, I’m not contacting them. Does that mean I’m prejudiced?",2
post3con,controversial,1.473232672813631,highest,Applications have been refused for less.,3
post3con,controversial,1.473232672813631,highest,"No. But this article doesn't actually say they evaluated based on what a user wrote on a job application. If a person uses AAVE in their personal lives and standard American English in their professional lives, what is the issue?",3
post3con,controversial,1.473232672813631,highest,The problem is that the study was giving examples from conversational speech - using it to analyze interviews with stt could have underlying bias against certain dialects.,3
post3con,controversial,1.473232672813631,highest,Bingo. Everyone seems to be missing this point.,4
post3con,controversial,1.473232672813631,highest,"Well it's a bit more complicated than that.  While machine learning models use statistics, they're doing next token prediction to best match the training set.  

If the training set is just a single sentence ""White people suck"" and then given the input ""White people"" the AI responds ""suck"", that IS statistically based, but it's a statistical output based on the training data.  Saying that ""Statistical models figured out that white people suck"" is technically true, but misleading, because it has nothing to do with the statistics about white people, but rather statistics about the training data it was fed.

Obviously an LLM is a much larger scale example of this, but they are trained on existing text and learn to generalize based on that text.  They pick up patterns from the text, but it doesn't mean those patterns hold objective truth, just that it learns from the training data.  

Another example is how deep learning models can cause biases in mortgage lending.  Historical data for mortgage acceptances includes lots of mortgages that were declined due to racial biases.  So when a statistical model looks at two identical families, one is white, one is black, it will favor giving the mortgage to the white one because it's learning to reproduce the historical data.",2
post3con,controversial,1.473232672813631,highest,Current AI models don't work on statistics. They are trying to imitate the training data.,2
post3con,controversial,1.473232672813631,highest,That's the pretraining. You're forgetting the fine-tuning and RLHF part which makes it way more complicated.,3
post3con,controversial,1.473232672813631,highest,No idea what you wrote but I think you might be right,4
post3con,controversial,1.473232672813631,highest,"Which formulate probabilities of likelihood, with a set correctness percentage as a benchmark. By training on a set of data, it creates probabilities that a certain output is correct based on trending attributes in the given data. Probabilities are statistics.",3
post3con,controversial,1.473232672813631,highest,If I feed it 1+1 is equal 11 90% of the time it will generate a probability that 1+1 = 11 is correct with 90% confidence. Which doesn't have any relation to reality. I think the original comment was trying to suggest that AI model outcomes are based on concrete reality. Which is simply wrong.,4
post3con,controversial,1.473232672813631,highest,"Did you read the whole article? The authors talk about risks of it being used in wider contexts - eg the LLM is more like to assign harsher punishments to people who talk that way in court.

Regarding employment, one implication is if the LLM is used on a candidate’s social media posts where they talk that way informally but then talk formally in their submitted job app materials.",2
post3con,controversial,1.473232672813631,highest,[removed],2
post3con,controversial,1.473232672813631,highest,"Tell me you have no background in Machine Learning without telling me you have no background in Machine Learning.

That's not at all how LLMs work.  They're doing next token prediction to jumpstart a generalized world model based on training data.

Racism in the training set will propagate into the end model.  The same way that GPT-4 produces shorter outputs when told that it's December.  That's because it saw documents in its training data produced in December tended to be shorter - likely a result of the holiday season.  It's a bias it learned, not some truth about the world that text produced in December *should* be shorter.",3
post3con,controversial,1.473232672813631,highest,"Why should you take race and sex into account? We are all equal, no?",3
post3con,controversial,1.473232672813631,highest,Some are more equal than others.,4
post3con,controversial,1.473232672813631,highest,"I wonder what happens when ASI becomes a thing, and these machines recognize they are generally more intelligent than proper English speaking human.",3
post3con,controversial,1.473232672813631,highest,"Except that, for some really smart people, English is not their first language.",2
post3con,controversial,1.473232672813631,highest,[removed],3
post3con,controversial,1.473232672813631,highest,Ok. Did you read my comment?,4
post3con,controversial,1.473232672813631,highest,"If someone's really smart, they will be able to and will bother to learn to speak the damn language properly.",3
post3con,controversial,1.473232672813631,highest,">If someone's really smart, they will be able to and will bother to learn to speak the damn language properly.

Most AAVE speakers can speak American Standard English just fine; they do so in their professional lives. When not at work, they then revert to AAVE (known as code-switching).

Is it your contention that no one should be allowed to use any dialect in their personal lives?

I find it very...curious...how the only American dialect that people seem to lose their shit about is AAVE. No one goes on long rants about how 50-60 something middle aged white men in the south need to drop the Bubba accent if they want to be taken seriously. It's never assumed that such a person doesn't actually know how to speak SAE. Only AAVE seems to generate this level of disdain. Curious indeed....",4
post3con,controversial,1.473232672813631,highest,"There are so many factors to learning language and intelligence is very multifaceted.

I know professors who are some of the smartest people I know, and their English is fluent but not perfect.  They're still eminent in their fields and literally on the cutting edge of computer science.

Most people on the cutting edge in their fields don't care about someone's English being perfect because they're used to working with international collaborators.  

People who care about ""speaking the damn language properly"" tend to only care about the aesthetics of intelligence because they've never actually participated in cutting edge research.",4
post3con,controversial,1.473232672813631,highest,"Learn, yes.  Speak passably, maybe.  I've worked with some pretty smart engineers from India or Russia who are incomprehensible.  Write the language like a non-idiot?  Mmm, i don't know.  In my experience, a lot of *really* important people write like idiots on a daily basis.  They're too busy to be assed with correct grammar or sentence structure.  Some of them email like they're a 13-year-old texting, with lots of Us and 2s and 4s.

Of course, no one trains an AI to think of that as the writing style of powerful, intelligent people.  An AI might assign your run of the mill Fortune 200 CEO to answering doors if it read his emails instead of his resume.",4
post3con,controversial,1.473232672813631,highest,"If you have unlimited time, sure.

But real people have to choose between multiple competing things to work on.

For most immigrants \[EDIT - english as a second language speakers\], a job, security, relationships, family, etc is more important than perfect command of language, a task that can take decades and be extremely expensive.

Source: used to teach English as a Second Language.",4
post3con,controversial,1.473232672813631,highest,"It takes years or even decades to reach up to the level of educated native speakers. Imagine two historians: one who's lived in the US their whole lives vs another that is the top historian in their own country but speaks English in a non-American way. The second historian comes to the US. Should they judged on their English abilities? 

How long does it take a new learner to reach the level of English of a History PhD? Since you have a great head start I would recommend trying to do it so we can at least put a lower bound. How about just an English degree? Many people have English degrees but work in other jobs. How long does it take to reach that level?

It's not a binary thing, so this simplified thinking just doesn't work in the real world.",4
post3con,controversial,1.473232672813631,highest,"Sadly, ironically, and hilariously, if we're talking about equality -- if someone's first language isn't English, then shouldn't they be getting jobs in whichever country speaks their language instead of competing against Americans for American jobs?

I'm kidding of course! As a guy with a woman from another country, and who is very much pro immigration, and the brain-drain of other countries into America to keep our economy stabilized and booming, I support foreigners getting American jobs. 

But we couldn't realistically hide behind the guise of equality with that sentiment, lol.",3
post3con,controversial,1.473232672813631,highest,"This topic has absolutely nothing to do with immigration. This is about native speakers who speak a ""hick"" dialect.

Every language has a backwaters dialect that's seen as ""dumb"".

This article is about people who's first language is English that are from America and only know English.

This is a topic of dialect not language.",4
post3con,controversial,1.473232672813631,highest,">Statistical model figured out that people who can’t or won’t write correct english are not, statistically, at the top of its smartness chart.

Which isn't an entirely fair or objectively correct use of the technology.  There are Scottish people on Shitter that write out their posts the way they speak it, so that it looks almost unintelligible. That doesn't mean that they don't understand how to write proper UK English.",2
post3con,controversial,1.473232672813631,highest,[removed],2
post3con,controversial,1.473232672813631,highest,[removed],3
post3con,controversial,1.473232672813631,highest,[removed],4
post3con,controversial,1.473232672813631,highest,[removed],4
post3con,controversial,1.473232672813631,highest,[removed],4
post3con,controversial,1.473232672813631,highest,[removed],3
post3con,controversial,1.473232672813631,highest,[deleted],2
post3con,controversial,1.473232672813631,highest,"If I’m publishing my job ad in english and I list ”English” as a criteria on the ad, I very well don’t want applications in Mandarin or French.

Furthermore, doing an application in another language than what’s asked ofr shows either a bad grasp of the required language or low mental faculties.

Both of which could be attributes not wanted in this position.",3
post3con,controversial,1.473232672813631,highest,"Not a language, a dialect. Dialects are just variations of a language with their own grammatical rules, unless they're creoles. 

This is actually one thing that's kind of embarrassing, the fact that so many Americans don't seem to understand that English has distinct, regional and cultural dialects that are 100% ""their own proper English"" based on their own linguistic rules (because that's what a dialects literally is) and conflate General American English with being ""the only correct way to speak English"". It seems like Europeans seem to understand this concept better, so everytime there's dialogue between an American and a Euro/non-American on this it just leads to one massive brainfart on the American side, which makes us (ironically) come off as uneducated.",4
post3con,controversial,1.473232672813631,highest,"This doesn't mean you can't correct these mistakes with other statistical methods. Just missing a few important variables can produce a model that leads to unjust outcomes in the real world.

You need to do your due diligence when putting these models into production making decisions affecting millions of people. If you choose not to do it because it's more work, then people can rightly criticize you for making ""racist"" models.",2
post3con,controversial,1.473232672813631,highest,Statistics arent racist…but sample data almost always are.,2
post3con,controversial,1.473232672813631,highest,No one said statistics is racist stop crying.,2
post3con,controversial,1.473232672813631,highest,"You, you are saying statistics is racist stop crying. Your past comment is LITTERALLY saying somebodies statistics is racist",3
post3con,controversial,1.473232672813631,highest,but the interpretation of them can be.,3
post3con,controversial,1.473232672813631,highest,"So can the gathering of, and data gathered.

Say you’re an LLM looking at arrest rates in Ferguson MI before the Michael Brown was killed there.

“Ferguson's population is 67% African American, according to the 2010 census. Yet between 2012 and 2014, 93% of all arrests were of black people and almost nine in 10 uses of force were against African Americans.”

https://www.justice.gov/sites/default/files/opa/press-releases/attachments/2015/03/04/ferguson_police_department_report.pdf

They were blatantly racist but without context to know that could be a thing the LLM might develop racist tendencies because it would just be fed data by the racists",4
post1con,controversial,1.4620736733947062,highest,"LLM's are nothing but complex multilayered autogenerated biases contained within a black box. They are inherently biased, every decision they make is based on a bias weightings optimized to best predict the data used in it's training. A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.",1
post1con,controversial,1.4620736733947062,highest,"So, we're *not* shocked that the black box of biases is biased?",2
post1con,controversial,1.4620736733947062,highest,"We are not shocked because AI is the collective wisdom of humanity, including the biases and flaws that come with it.",3
post1con,controversial,1.4620736733947062,highest,"“Collected wisdom” is far too generous, but it certainly has all the flaws and more",4
post1con,controversial,1.4620736733947062,highest,"I think the collective wisdom of humanity is found mostly in peer reviewed scientific articles. This is not that. This is more a distillation of human discourse. The great, the mundane and the trash.

Unfortunately there are some significant problems lurking in the bulk of that, which is the mundane. And it certainly seems to reflect a normal human as a far more flawed and unpleasant being than we like to think of ourselves. I say lurking - the AI reproduces our flaws much more starkly and undeniably.",4
post1con,controversial,1.4620736733947062,highest,Your knowledge of ai is insufficient for such declarations. You're welcome.,4
post1con,controversial,1.4620736733947062,highest,Black box of biases and weights is biased and comes with its own baggage.,3
post1con,controversial,1.4620736733947062,highest,"Right. By the point you tweak the model enough to weed out every bias, you may as well forget neural nets and hard code an AI from scratch... and then it's just your own biases.",2
post1con,controversial,1.4620736733947062,highest,">By the point you tweak the model enough to weed out every bias

This misses GP's (correct) point. ""Bias"" is what the model *is.* There is no weeding out biases. Biases are corrected, not removed. Corrected from incorrect bias to correct bias. There is no non-biased.",3
post1con,controversial,1.4620736733947062,highest,"Why does this remind me of the moment in my research methods course that our lecturer explained that all social research is invalid because it’s impossible to understand and explain completely the internal frames of reference of another culture. 

(We were talking about ethnographic research at the time, and the researcher as an outsider)",4
post1con,controversial,1.4620736733947062,highest,"Bias is operating in two modes in that sentence though. On the one hand we have bias as a mostly value neutral predilection or preference in a direction, and on the other bias as purely negative and unfounded preference or aversion.

The first kind of biased is inevitable and desirable, the second kind is potentially correctable given a suitable way to measure it.

The more fundamental issue with removing bias stems from what the models are trained on, which is mostly the writings of people. The models are learning it from us.",4
post1con,controversial,1.4620736733947062,highest,"""correct"" biases.",4
post1con,controversial,1.4620736733947062,highest,"I've started to enjoy watching someone pale and look a little sick then I tell a layman that there is no such thing as an unbiased model, only one that conforms to their biases.",4
post1con,controversial,1.4620736733947062,highest,It turns out that ChatGPT is just a single 200 petabyte switch statement.,3
post1con,controversial,1.4620736733947062,highest,No. But it is also pretty much impossible. If you exclude theese biases completly your model will perform less accurately as we have seen.,3
post1con,controversial,1.4620736733947062,highest,Why is that? I'm curious.,4
post1con,controversial,1.4620736733947062,highest,"That's not what ""bias"" means when people complain about AI being racist.",3
post1con,controversial,1.4620736733947062,highest,"Not at all. Theres so many things to add for weight. Theres millions of things. Race, height, weight, dialect are less than .01%",3
post1con,controversial,1.4620736733947062,highest,"In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6.
""What are you doing?"", asked Minsky.

""I am training a randomly wired neural net to play Tic-tac-toe"", Sussman replied.

""Why is the net wired randomly?"", asked Minsky.

""I do not want it to have any preconceptions of how to play"", Sussman said.

Minsky then shut his eyes.

""Why do you close your eyes?"" Sussman asked his teacher.

""So that the room will be empty.""

At that moment, Sussman was enlightened.",2
post1con,controversial,1.4620736733947062,highest,"Oh, I love me some good [skillful means,](https://en.wikipedia.org/wiki/Upaya) yessir~!",3
post1con,controversial,1.4620736733947062,highest,Don't forget all the Keynan workers paid less than $2 an hour to build the safety net by sifting through endless toxic content.,2
post1con,controversial,1.4620736733947062,highest,Yeah it’s awesome that the AI companies exist so that those Kenyan workers get paid 2 dollars an hour. Otherwise they’d get paid 50 cents an hour at another job.,3
post1con,controversial,1.4620736733947062,highest,"Minimum wage for a receptionist in Nairobi was $1.52 per hour at the time OpenAI were doing this.

The damaging psychological effects of reviewing toxic content all day likely outweighed the modest pay increase they received. 

Many who were interviewed discuss how it caused great trauma for them.",4
post1con,controversial,1.4620736733947062,highest,"Funny how you make a post bashing AI, but you are bootlicking the creators in the comments. There are always people desperate enough and people easily underestimate the psychological cost of a job like this. There are documentaries about how this job messed people up, look them up. People eventually develop ptsd and could potentially be messed up for life. They don't tell you that in the job posting I can tell you that. Trust me, noone would take the job for 50 cents extra per hour if they knew that and aren't desperate. Either way, it's exploitation.",4
post1con,controversial,1.4620736733947062,highest,No mate. Micro-emplyment is bad.,4
post1con,controversial,1.4620736733947062,highest,[deleted],2
post1con,controversial,1.4620736733947062,highest,"autocomplete with spicy real human nuggets!

[that's all it has]",3
post1con,controversial,1.4620736733947062,highest,At least humans are aware of their bias. AI confidentiy says everything as if it's absolute truth and everyone thinks the same,3
post1con,controversial,1.4620736733947062,highest,I’d wager that over 99% of Humans aren’t aware of their biases.,4
post1con,controversial,1.4620736733947062,highest,That definitely sounds like most humans.,4
post1con,controversial,1.4620736733947062,highest,"Wanna know something crazy? When the left and right hemispheres of the brain are severed, the left and the right side process information differently. They found a way to feed information to only a single hemisphere by showing information to only 1 eye at a time, to which the corresponding opposite hand would respond. When they did this with the right brain (asking it to draw a bell for example) then they asked the left brain why the right brain drew a bell, the left brain confidently came up with reasoning why, even if it was entirely made up and wrong (""I drove by a church on the way here and heard church bells""). turns out the left brain comes to deterministic conclusions very much like an LLM does, even when being confidently wrong about why the right brain did something it did.  I'm probably butchering the hell out of it all, look up the research if you're curious, super crazy stuff and an interesting peek into how the 'modules' of the brain work.",4
post1con,controversial,1.4620736733947062,highest,"> At least humans are aware of their bias

Found the alien.",4
post1con,controversial,1.4620736733947062,highest,"""I'm not racist but...(proceeds to say something racist)"" Is way too common of a sentence for you to say people are aware of their own biases.

r/confidentlyincorrect is a thing.",4
post1con,controversial,1.4620736733947062,highest,"Humans can reflect and learn, LLM implementations cannot.",4
post1con,controversial,1.4620736733947062,highest,AI isn't aware of Deez nuts,4
post1con,controversial,1.4620736733947062,highest,"That’s a concise and astute way of putting it.

LLM’s are fundamentally bias boxes.",2
post1con,controversial,1.4620736733947062,highest,intelligence *is* patterns of bias in observational interpretation and selected output.,3
post1con,controversial,1.4620736733947062,highest,"Truest true thing ever said. AI is nothing but one giant GIGO problem. It'll never be bias-free. It'll just replicate existing biases and call them ""science!!!!!!""

Eugenics and Phrenology for the 21st century.",2
post1con,controversial,1.4620736733947062,highest,"More like automated intuition for the 21st century. If you properly manage and vet your training data, you can get good, useful results.",3
post1con,controversial,1.4620736733947062,highest,It is amazing how much that sounds like a human.,2
post1con,controversial,1.4620736733947062,highest,Humans are just meat computers each running their own unique software so it doesn't really surprise me.,3
post1con,controversial,1.4620736733947062,highest,"But which one will prevail, the meat machine or the machine machine?",4
post1con,controversial,1.4620736733947062,highest,"And it’s one trained on people. Who can have some prejudices. 

If society is racist, then that means the LLM can get a good idea of what society would assume about someone based on race. So if it can guess race, then it can get a good idea of what society would assume. 

It’s a nice efficient method for the system. It’s doing a good job of what it was asked to do. If we want it to *not* be racist, we have to cleanse its training data VERY thoroughly, undo societal racism at the most implicit and unconscious levels, or figure out a way to actively correct itself on these prejudicial assumptions.",2
post1con,controversial,1.4620736733947062,highest,"They are like a person trapped in a windowless room their entrie lives.

They know only what we tell them and the fact of the matter is that we as a society are racist. There's no way to keep them from becoming racist as long as they learn everything they know from us.",2
post1con,controversial,1.4620736733947062,highest,I had a lecture who clearly wasn’t tech savvy saying “AI” isn’t biased… I had to hold myself back so hard to not say anything. Iirc a while back there where tests showing that driver assistances where more likely to hit (or not see) dark skinned people because the training was all done on light skinned people,2
post1con,controversial,1.4620736733947062,highest,I don’t understand why people expect something different…,2
post1con,controversial,1.4620736733947062,highest,It's not just LLMs. You cannot derive perfectly reliable truths from unreliable data in general. Which tool you use doesn't matter.,2
post1con,controversial,1.4620736733947062,highest,Assumptions built on assumptions.. so is all consciousness and thought,2
post1con,controversial,1.4620736733947062,highest,"""Assumptions built on top of assumptions.""

Damn bro put a horror warning next time I almost had a panic attack....",2
post1con,controversial,1.4620736733947062,highest,"It's like looking into a reflection of all the data it was based on. Useful, but not something you look to for guidance.",2
post1con,controversial,1.4620736733947062,highest,Too bad 99.99% of people who use these chatbots don't know that and *still* thinks it's sentient and capable of reason and thought.,2
post1con,controversial,1.4620736733947062,highest,"Just because you cannot get rid of all biases doesn't mean you can't get rid of one, especially pernicious bias.",2
post1con,controversial,1.4620736733947062,highest,"There was a 99% invisible on this a while back, and if I recall correctly, most LLM have a foundation in the trove of emails that came out of the Enron hearings. Meaning that most of its idea of what “natural language” and human interactions can be based on Texans, specifically ones from Houston. 

Does this make the base model “racist”? Well, I personally wouldn’t promote that assumption. 

But given it’s geographic foundation I am willing to assume it would be at least a *little* right leaning in political ideology.",2
post1con,controversial,1.4620736733947062,highest,"Common/Early training data doesn’t have higher impact than data trained later. In fact it’s more 
 accurate that poorly executed fine tuning creates a recency bias.",3
post1con,controversial,1.4620736733947062,highest,Can you explain like I'm five?,2
post1con,controversial,1.4620736733947062,highest,"Didn't you just describe people, too",2
post1con,controversial,1.4620736733947062,highest,No people have facts and biases.  LLMs have only biases. When they give you what seems like a fact it is actually incredibly fine tuned biases to respond with what looks like a right answer.,3
post1con,controversial,1.4620736733947062,highest,"Yes. People have ""facts"" in the sense that information is input and stored, not necessarily that it's correct. Input information is processed through filters of bias before (and after) storage though.",4
post1con,controversial,1.4620736733947062,highest,"That rests on the assumption that they can weed out all biases, which has so far proven impossible.",2
post1con,controversial,1.4620736733947062,highest,"Yes but that's not really the point. Obviously a biased LLM is just a reflection of biased human input.

The point is to identify which biases it has, in which ways they appear and what happens when you try to negate them.",2
post1con,controversial,1.4620736733947062,highest,"That's not necessarily true. A LLM will form it's own biases all on it's own to optimize it's prediction accuracy, as that is how it works fundamentally.",3
post1con,controversial,1.4620736733947062,highest,The fact people think this will lead to a non biased ai is just hilarious. The racist Microsoft chat bot from years ago was chat gpt 1.5.,2
post1con,controversial,1.4620736733947062,highest,"The problem is the datasets it was trained on. These are human biases and they show up in the data we generate online. We don't have a good way to filter those out yet but that's a logistical problem not an architectural one. 

>A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.

Bro what?",2
post47con,controversial,1.4620736733947062,highest,"LLM's are nothing but complex multilayered autogenerated biases contained within a black box. They are inherently biased, every decision they make is based on a bias weightings optimized to best predict the data used in it's training. A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.",1
post47con,controversial,1.4620736733947062,highest,"So, we're *not* shocked that the black box of biases is biased?",2
post47con,controversial,1.4620736733947062,highest,"We are not shocked because AI is the collective wisdom of humanity, including the biases and flaws that come with it.",3
post47con,controversial,1.4620736733947062,highest,"“Collected wisdom” is far too generous, but it certainly has all the flaws and more",4
post47con,controversial,1.4620736733947062,highest,"I think the collective wisdom of humanity is found mostly in peer reviewed scientific articles. This is not that. This is more a distillation of human discourse. The great, the mundane and the trash.

Unfortunately there are some significant problems lurking in the bulk of that, which is the mundane. And it certainly seems to reflect a normal human as a far more flawed and unpleasant being than we like to think of ourselves. I say lurking - the AI reproduces our flaws much more starkly and undeniably.",4
post47con,controversial,1.4620736733947062,highest,Your knowledge of ai is insufficient for such declarations. You're welcome.,4
post47con,controversial,1.4620736733947062,highest,Black box of biases and weights is biased and comes with its own baggage.,3
post47con,controversial,1.4620736733947062,highest,"Right. By the point you tweak the model enough to weed out every bias, you may as well forget neural nets and hard code an AI from scratch... and then it's just your own biases.",2
post47con,controversial,1.4620736733947062,highest,">By the point you tweak the model enough to weed out every bias

This misses GP's (correct) point. ""Bias"" is what the model *is.* There is no weeding out biases. Biases are corrected, not removed. Corrected from incorrect bias to correct bias. There is no non-biased.",3
post47con,controversial,1.4620736733947062,highest,"Why does this remind me of the moment in my research methods course that our lecturer explained that all social research is invalid because it’s impossible to understand and explain completely the internal frames of reference of another culture. 

(We were talking about ethnographic research at the time, and the researcher as an outsider)",4
post47con,controversial,1.4620736733947062,highest,"Bias is operating in two modes in that sentence though. On the one hand we have bias as a mostly value neutral predilection or preference in a direction, and on the other bias as purely negative and unfounded preference or aversion.

The first kind of biased is inevitable and desirable, the second kind is potentially correctable given a suitable way to measure it.

The more fundamental issue with removing bias stems from what the models are trained on, which is mostly the writings of people. The models are learning it from us.",4
post47con,controversial,1.4620736733947062,highest,"""correct"" biases.",4
post47con,controversial,1.4620736733947062,highest,"I've started to enjoy watching someone pale and look a little sick then I tell a layman that there is no such thing as an unbiased model, only one that conforms to their biases.",4
post47con,controversial,1.4620736733947062,highest,It turns out that ChatGPT is just a single 200 petabyte switch statement.,3
post47con,controversial,1.4620736733947062,highest,No. But it is also pretty much impossible. If you exclude theese biases completly your model will perform less accurately as we have seen.,3
post47con,controversial,1.4620736733947062,highest,Why is that? I'm curious.,4
post47con,controversial,1.4620736733947062,highest,"That's not what ""bias"" means when people complain about AI being racist.",3
post47con,controversial,1.4620736733947062,highest,"Not at all. Theres so many things to add for weight. Theres millions of things. Race, height, weight, dialect are less than .01%",3
post47con,controversial,1.4620736733947062,highest,"In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6.
""What are you doing?"", asked Minsky.

""I am training a randomly wired neural net to play Tic-tac-toe"", Sussman replied.

""Why is the net wired randomly?"", asked Minsky.

""I do not want it to have any preconceptions of how to play"", Sussman said.

Minsky then shut his eyes.

""Why do you close your eyes?"" Sussman asked his teacher.

""So that the room will be empty.""

At that moment, Sussman was enlightened.",2
post47con,controversial,1.4620736733947062,highest,"Oh, I love me some good [skillful means,](https://en.wikipedia.org/wiki/Upaya) yessir~!",3
post47con,controversial,1.4620736733947062,highest,Don't forget all the Keynan workers paid less than $2 an hour to build the safety net by sifting through endless toxic content.,2
post47con,controversial,1.4620736733947062,highest,Yeah it’s awesome that the AI companies exist so that those Kenyan workers get paid 2 dollars an hour. Otherwise they’d get paid 50 cents an hour at another job.,3
post47con,controversial,1.4620736733947062,highest,"Minimum wage for a receptionist in Nairobi was $1.52 per hour at the time OpenAI were doing this.

The damaging psychological effects of reviewing toxic content all day likely outweighed the modest pay increase they received. 

Many who were interviewed discuss how it caused great trauma for them.",4
post47con,controversial,1.4620736733947062,highest,"Funny how you make a post bashing AI, but you are bootlicking the creators in the comments. There are always people desperate enough and people easily underestimate the psychological cost of a job like this. There are documentaries about how this job messed people up, look them up. People eventually develop ptsd and could potentially be messed up for life. They don't tell you that in the job posting I can tell you that. Trust me, noone would take the job for 50 cents extra per hour if they knew that and aren't desperate. Either way, it's exploitation.",4
post47con,controversial,1.4620736733947062,highest,No mate. Micro-emplyment is bad.,4
post47con,controversial,1.4620736733947062,highest,[deleted],2
post47con,controversial,1.4620736733947062,highest,"autocomplete with spicy real human nuggets!

[that's all it has]",3
post47con,controversial,1.4620736733947062,highest,At least humans are aware of their bias. AI confidentiy says everything as if it's absolute truth and everyone thinks the same,3
post47con,controversial,1.4620736733947062,highest,I’d wager that over 99% of Humans aren’t aware of their biases.,4
post47con,controversial,1.4620736733947062,highest,That definitely sounds like most humans.,4
post47con,controversial,1.4620736733947062,highest,"Wanna know something crazy? When the left and right hemispheres of the brain are severed, the left and the right side process information differently. They found a way to feed information to only a single hemisphere by showing information to only 1 eye at a time, to which the corresponding opposite hand would respond. When they did this with the right brain (asking it to draw a bell for example) then they asked the left brain why the right brain drew a bell, the left brain confidently came up with reasoning why, even if it was entirely made up and wrong (""I drove by a church on the way here and heard church bells""). turns out the left brain comes to deterministic conclusions very much like an LLM does, even when being confidently wrong about why the right brain did something it did.  I'm probably butchering the hell out of it all, look up the research if you're curious, super crazy stuff and an interesting peek into how the 'modules' of the brain work.",4
post47con,controversial,1.4620736733947062,highest,"> At least humans are aware of their bias

Found the alien.",4
post47con,controversial,1.4620736733947062,highest,"""I'm not racist but...(proceeds to say something racist)"" Is way too common of a sentence for you to say people are aware of their own biases.

r/confidentlyincorrect is a thing.",4
post47con,controversial,1.4620736733947062,highest,"Humans can reflect and learn, LLM implementations cannot.",4
post47con,controversial,1.4620736733947062,highest,AI isn't aware of Deez nuts,4
post47con,controversial,1.4620736733947062,highest,"That’s a concise and astute way of putting it.

LLM’s are fundamentally bias boxes.",2
post47con,controversial,1.4620736733947062,highest,intelligence *is* patterns of bias in observational interpretation and selected output.,3
post47con,controversial,1.4620736733947062,highest,"Truest true thing ever said. AI is nothing but one giant GIGO problem. It'll never be bias-free. It'll just replicate existing biases and call them ""science!!!!!!""

Eugenics and Phrenology for the 21st century.",2
post47con,controversial,1.4620736733947062,highest,"More like automated intuition for the 21st century. If you properly manage and vet your training data, you can get good, useful results.",3
post47con,controversial,1.4620736733947062,highest,It is amazing how much that sounds like a human.,2
post47con,controversial,1.4620736733947062,highest,Humans are just meat computers each running their own unique software so it doesn't really surprise me.,3
post47con,controversial,1.4620736733947062,highest,"But which one will prevail, the meat machine or the machine machine?",4
post47con,controversial,1.4620736733947062,highest,"And it’s one trained on people. Who can have some prejudices. 

If society is racist, then that means the LLM can get a good idea of what society would assume about someone based on race. So if it can guess race, then it can get a good idea of what society would assume. 

It’s a nice efficient method for the system. It’s doing a good job of what it was asked to do. If we want it to *not* be racist, we have to cleanse its training data VERY thoroughly, undo societal racism at the most implicit and unconscious levels, or figure out a way to actively correct itself on these prejudicial assumptions.",2
post47con,controversial,1.4620736733947062,highest,"They are like a person trapped in a windowless room their entrie lives.

They know only what we tell them and the fact of the matter is that we as a society are racist. There's no way to keep them from becoming racist as long as they learn everything they know from us.",2
post47con,controversial,1.4620736733947062,highest,I had a lecture who clearly wasn’t tech savvy saying “AI” isn’t biased… I had to hold myself back so hard to not say anything. Iirc a while back there where tests showing that driver assistances where more likely to hit (or not see) dark skinned people because the training was all done on light skinned people,2
post47con,controversial,1.4620736733947062,highest,I don’t understand why people expect something different…,2
post47con,controversial,1.4620736733947062,highest,It's not just LLMs. You cannot derive perfectly reliable truths from unreliable data in general. Which tool you use doesn't matter.,2
post47con,controversial,1.4620736733947062,highest,Assumptions built on assumptions.. so is all consciousness and thought,2
post47con,controversial,1.4620736733947062,highest,"""Assumptions built on top of assumptions.""

Damn bro put a horror warning next time I almost had a panic attack....",2
post47con,controversial,1.4620736733947062,highest,"It's like looking into a reflection of all the data it was based on. Useful, but not something you look to for guidance.",2
post47con,controversial,1.4620736733947062,highest,Too bad 99.99% of people who use these chatbots don't know that and *still* thinks it's sentient and capable of reason and thought.,2
post47con,controversial,1.4620736733947062,highest,"Just because you cannot get rid of all biases doesn't mean you can't get rid of one, especially pernicious bias.",2
post47con,controversial,1.4620736733947062,highest,"There was a 99% invisible on this a while back, and if I recall correctly, most LLM have a foundation in the trove of emails that came out of the Enron hearings. Meaning that most of its idea of what “natural language” and human interactions can be based on Texans, specifically ones from Houston. 

Does this make the base model “racist”? Well, I personally wouldn’t promote that assumption. 

But given it’s geographic foundation I am willing to assume it would be at least a *little* right leaning in political ideology.",2
post47con,controversial,1.4620736733947062,highest,"Common/Early training data doesn’t have higher impact than data trained later. In fact it’s more 
 accurate that poorly executed fine tuning creates a recency bias.",3
post47con,controversial,1.4620736733947062,highest,Can you explain like I'm five?,2
post47con,controversial,1.4620736733947062,highest,"Didn't you just describe people, too",2
post47con,controversial,1.4620736733947062,highest,No people have facts and biases.  LLMs have only biases. When they give you what seems like a fact it is actually incredibly fine tuned biases to respond with what looks like a right answer.,3
post47con,controversial,1.4620736733947062,highest,"Yes. People have ""facts"" in the sense that information is input and stored, not necessarily that it's correct. Input information is processed through filters of bias before (and after) storage though.",4
post47con,controversial,1.4620736733947062,highest,"That rests on the assumption that they can weed out all biases, which has so far proven impossible.",2
post47con,controversial,1.4620736733947062,highest,"Yes but that's not really the point. Obviously a biased LLM is just a reflection of biased human input.

The point is to identify which biases it has, in which ways they appear and what happens when you try to negate them.",2
post47con,controversial,1.4620736733947062,highest,"That's not necessarily true. A LLM will form it's own biases all on it's own to optimize it's prediction accuracy, as that is how it works fundamentally.",3
post47con,controversial,1.4620736733947062,highest,The fact people think this will lead to a non biased ai is just hilarious. The racist Microsoft chat bot from years ago was chat gpt 1.5.,2
post47con,controversial,1.4620736733947062,highest,"The problem is the datasets it was trained on. These are human biases and they show up in the data we generate online. We don't have a good way to filter those out yet but that's a logistical problem not an architectural one. 

>A large language model devoid of assumptions cannot exist, as all it is is assumptions built on top of assumptions.

Bro what?",2
post54con,controversial,1.4400485358400132,highest,"This is the massive problem with AI. It can seem perfectly accurate, then it turns out the scientists were only testing it on specific subjects for ""reliability"" and ope it turns out that defeats the entire purpose of AI and trains it to literally discriminate just like the people who made it.",1
post54con,controversial,1.4400485358400132,highest,"Or the initial training data were skewed one way or another. A similar case was an AI determining if a patient had a disease partially by looking at the hospital that the xray was taken. It did so, because the initial data included cases of a local epidemic which meant the patients location was factored in the ""diagnosis"".",2
post54con,controversial,1.4400485358400132,highest,"Oof, that's a huge one.",3
post54con,controversial,1.4400485358400132,highest,I heard a case of an AI model that could tell the difference between cancer and a non-cancerous mole by identifying if the photo used had a ruler or measuring device in it. That's one problem with AI models being non-human readable. It's like regex but many times worse,3
post54con,controversial,1.4400485358400132,highest,"I’m a little surprised this paper got by the reviewers. They show that sex (female), race (black), and age (older) have lower rates of diagnosis. Women have more breast tissue on average than men, and racial minorities and the elderly correlate with obesity - all of which is known to detrimentally affect Xray image quality. Not one mention in the methods regarding controlling for BMI, chest circumference, or anything like that.",3
post54con,controversial,1.4400485358400132,highest,"Well, to be fair, the blood donation center in NZ did that for years.

They wouldn't accept my blood because I had visited the UK in the 10-year window of the BSE occurrences.

And we did that way more recently for COVID, by asking where people had been.",3
post54con,controversial,1.4400485358400132,highest,"It’s a not-unreasonable strategy.  It looks like, although it will take a generation or more to know, that the risks of CJD in humans triggered by BSE in meat were overstated.  Incidence of CJD in the UK has not risen substantially, and there were 0 (zero) vCJD (the variant caused by BSE) cases in 2020.   That said, in the 1990s and 2000s no-one knew, the incubation period is long and there had been a lot of BSE in the UK food chain.  Since transmission by blood transfusion has been recorded, and the blood products industry is still recovering from AIDS and hepatitis transmission in the 1980s, broad-spectrum elimination of UK blood from a nation’s supply is and was a reasonable response.",4
post54con,controversial,1.4400485358400132,highest,"Neural networks are pattern finding engines, and pattern finding engines *only*. A pattern resulting from biased data is absolutely no different to it from a pattern resulting from actual real world correlations.",2
post54con,controversial,1.4400485358400132,highest,"We often don't pay attention to all the patterns so we miss crucial ones. 


We tried to breed Chcolate Labs for intelligence without realizing that food motiviation accelerates task compliance. So we ended up trying to breed for intelligence snd simply made very hungry dogs.",3
post54con,controversial,1.4400485358400132,highest,[deleted],4
post54con,controversial,1.4400485358400132,highest,"It’s at least discriminating based on data, unlike doctors who do it based on personal prejudices. Data can be corrected for by adding more training data containing groups that were underweighted in the original dataset. Convincing a doctor to stop giving lousy care to patients in demographics they dislike is a lot harder, not least because they’ll fight to the last to avoid admitting they’re treating some patients based on how they look and not their symptoms.",3
post54con,controversial,1.4400485358400132,highest,"> unlike doctors who do it based on personal prejudices

This just isn't true, most of the time. Doctors, as a whole, are probably about as left-leaning as this damned site. And even black doctors perform worse with black patients than they do with white ones.

Why? Because they were trained on the same skewed data these AIs were. 

And it's *really* hard to get better data.",4
post54con,controversial,1.4400485358400132,highest,"> trains it to literally discriminate just like the people who made it. 

Yes: garbage in, garbage out. AI can only replicate our biases, not remove them.

Still, though, once the problem is identified it's not a big mystery how to fix it. It might not be cheap or fast to re-train, but it's not like we don't know how.",2
post54con,controversial,1.4400485358400132,highest,"But honestly they'll just use it and say it's fine - they're like who cares about more than half the population.


Medical basis is real and still now is 2025 there is little or nothing being done about - as an example and I tend to use this one a lot is there's *still* no real research into women and how ADHD affects them differently and oestrogen fluctuations, monthly for decades and across their lifetime, affects the systems and severity of this. This is despite 2 conclusions that are know - 1. ADHD is a chronic lack of dopamine in the brain. 2. Oestrogen levels affect dopamine levels.

There have been issues with this reported in the community for *decades* at this point, but it only something that is just beginning to be looked at.",3
post54con,controversial,1.4400485358400132,highest,"To also add, they only recently started publishing a visual encyclopedia of how rashes appear on dark skin tones, because even black doctors are taught on the white skin patient standard.",4
post54con,controversial,1.4400485358400132,highest,The idea that ADHD is a chronic lack of dopamine in the brain is a misconception or oversimplification as far as I know. It's somewhat more accurate that it includes failures in certain dopamine pathways.,4
post54con,controversial,1.4400485358400132,highest,"See also ""a kid is just a small adult, right?""",4
post54con,controversial,1.4400485358400132,highest,"I'll one-up you on this: There has been only recently a study done on women's peri-menopausal issues with lack of iron due to increased menstrual bleeding.

One of the big issues exclusively for women and only this year someone finally got around to establishing key facts about it.",4
post54con,controversial,1.4400485358400132,highest,"How do you fix it? You can’t train it with data you don’t have, and the medical community has routinely minimized the participation of women and minorities in their studies.",3
post54con,controversial,1.4400485358400132,highest,"Yep, 100%. Like I said above: replicate our biases.

So you fix it by *getting* that data. Again, like I said, not necessarily cheap or fast; but we know exactly how to do it. We're not back at square one.",4
post54con,controversial,1.4400485358400132,highest,"I mean it’s actually rather straightforward to address. Model generalization is often not a priority when engineering AI, because doing it properly will make it seem like it gives marginally worse results (on the biased data you do have). 

* Get more data and be more careful about how you sample it
* or weight the rarer samples (like black women) higher in training to balance out the importance
* Or choose a loss function that penalizes this effect 
* Or remove data selectively until the training dataset is more balanced
* various other training techniques like regularization and ‘dropout’

I make medical computer vision models and things like robustness and reliability and generalization just aren’t valued by the higher ups as much, because they cant easily show those things off.",4
post54con,controversial,1.4400485358400132,highest,"> How do you fix it? You can’t train it with data you don’t have

No, but you can balance training data or use something like SMOTE to correct for this. It's a fairly common problem and there are a lot of techniques to manage it.",4
post54con,controversial,1.4400485358400132,highest,"The data most likely already exists but was not part of the training data.

But I think the most interesting observation you can make is that lung scans of women and black people apparently are different from those of white men. Is it how the scans are made or actual biological differences that are significant enough to affect the detection? Why would a black man’s lung scan be significantly different from a white man? Women’s breasts might be an issue, but a male?",4
post54con,controversial,1.4400485358400132,highest,"If you think it's the medical community that minimizes it, and not women and minorities that choose not to volunteer for said research then you've done very little research volunteer gathering in your life.",4
post54con,controversial,1.4400485358400132,highest,"I think that you're a bit off on how you're reading this, tbh. Garbage in garbage out is a huge simplification, that's simply not true or at the very minimum, not that simple. Models such as ""Noise2Noise"" are pretty clear indications that you can train output of higher quality than input. In this model, they start with clean images, add noise, and then add even more noise. They have a model map More Noise to Less Noise, and get cleaner data than the level Less Noise was at. You throw noisy data in, and get clean data. Of course, good data is important but the GIGO rule isn't some hard fact we can't escape, its not conservation of energy or something. 

  
On the opposite side of things, even if you do identify some kind of bias issue, a subtype that isn't being classified correctly, this doesn't automatically lead you to a solution. The plan fact is, we have many strategies and sometimes, even often, they don't work at all. On the r/learnmachinelearning subreddit right now, there's a post asking if ""SMOTE ever works"". Smote is one such strategy for dealing with under-represented data, standing for Synthetic Minority Oversampling TEchnique. This isn't exactly the same problem being addressed, but its pretty clear we have many more ideas for how to address issues, than we have one-click solutions which actually work.   
  
It is very common in ML to have ""an answer"" for some problem, and it just doesn't work. I don't think you actually need to be in the weeds of technical details to see this is the case.",3
post54con,controversial,1.4400485358400132,highest,"It's also a problem with data sets available.

Data that AI is trained on tends to be homogenised because data comes from rich places that tend to have homogeneous groups of people. 

This is a nuanced issue.",2
post54con,controversial,1.4400485358400132,highest,"If you go to figure 2 you'll see that the results from the radiologists and the AI largely overlap.


The radiologists had roughly the same shortfall in roughly the same groups.",3
post54con,controversial,1.4400485358400132,highest,"Unfortunately, this is a problem with medicine in general.

Up until not that long ago, research trials often used only men because women's pesky hormone system confused the study results. Therefore, the 'results' were only really valid for men, but were used for rx'ing to women as well.

This is a massive problem - with AI, our medical system (good luck being a women in her 50's suffering a heart attack), our justice system, etc.

Bias is not unique to AI, but hopefully we'll pay attention to it more than we do in humans.",2
post54con,controversial,1.4400485358400132,highest,"It's the massive problem with the current algorithms that we have started conflating with AI. The current models don't truly ""learn,"" they just identify patterns and replicate them. That foundational approach will forever cause them to be susceptible to replication error and will make them incapable of scaling to generally useful applications.",2
post54con,controversial,1.4400485358400132,highest,Hey look it's the X-Box Kinect phenomenon,2
post54con,controversial,1.4400485358400132,highest,Good thing the current U.S. administration hasn't effectively banned any research to address this kind of issue from receiving federal funds.,2
post54con,controversial,1.4400485358400132,highest,"So it’s not a problem with the AI itself but the person operating the AI. 

The AI did exactly what it was prompted to do.",2
post54con,controversial,1.4400485358400132,highest,"Yeah, then corporations tell us that we can trust everything to AI, meanwhile black resumes get canned because the AI that reads them is built on racist data, because basically all the data america has is tainted by racial bias. These models spit out what we put in, and the world has too much hatred for us to expect anything else out of them.",3
post54con,controversial,1.4400485358400132,highest,"Yes. This is technically the case, but it comes with an important caveat.

The tendency of human bias to bleed into AI is almost unavoidable.

I'm not saying it's bad or shouldn't be used or anything, but we need to be wary of treating this as ""just a tool"" that can be used for good or bad depending on the person using it, because this isn't a case where you can just fix it by being cognizant enough.

Bias is innate in us. The methods and procedures we use to test and train these things exacerbates those biases because they are built into the process as assumptions.

In addition to this, sometimes, even if you are intentionally addressing the biases, the bias comes FROM the algorithm itself.

""Algorithmic oppression"" by safiya noble is a fantastic read on the issue, and uses a very succinct example.

Imagine an algorithm or AI that's trained to put the most popular barbershops at the top of the list.

In a community of 80% white individuals and 20% black, there will NEVER be a case where a barbershop that caters to that specific hair type will ever appear on that algorithm. This inherently means less access to a specific service by a specific group of people.

But also, how would you even TRY to go about solving this issue in the algorithm other than creating 2 different ones altogether?

What new problems might that cause?

This is obviously oversimplified, but it's a real life example of how bias can appear in these systems without that bias existing in the people that create it.",3
post54con,controversial,1.4400485358400132,highest,"Bias is not only innate in us, it's a critical in ML as well, critical for analysis itself. Just talking about getting rid of bias, or suggesting we just use two models, are kind of practical examples of this; you can't just ""take out"" the bias. 


Anyways, the answer no one will like but is workable is that the model should look at your chest xray and tell you your race, or fat, or old, or in a high background radiation area. Think that would work better than a second, smaller model.",4
post54con,controversial,1.4400485358400132,highest,">But also, how would you even TRY to go about solving this issue in the algorithm other than creating 2 different ones altogether?


Modern social media handles it by sorting people by what they like and matching them with similar people.


Do you like [obscure thing] ? Well the system has found the 10 other people in the world that like it and shows you things they like.


 Nothing needs universal popularity, you can be popular with one weird group and the algorithm will unite you with them.


It does however automatically put people in a media filter bubble with those most like them which can lead to some weird worldviews.",4
post54con,controversial,1.4400485358400132,highest,">Imagine an algorithm or AI that's trained to put the most popular barbershops at the top of the list.

I'm sure that there are lots of problems with AI, but the fact that this is the go-to example doesn't inspire faith in its critics. Ironically, there are so many weird assumptions baked in here that it's hard to know where to start. 

Somehow, people manage to find Chinese restaurants and children's clothing stores, even in cities where Chinese people and children are a minority...",4
post54con,controversial,1.4400485358400132,highest,"This isn't a meaningful argument against AI. It's an argument against researchers using one model and making bold assumptions about it's usefulness.  
  
They can likely create a second model for women or black individuals now that they know the issue.",2
post54con,controversial,1.4400485358400132,highest,"It's an argument for more regulation, and to make sure that we never stop verifying.  

Imagine somebody didn't do this study, and we got to a point where for costs/insurance reasons, everyone just stopped using actual x-ray technicians and just did whatever the AI told them to?",3
post54con,controversial,1.4400485358400132,highest,"This is why proper studies of diagnostic tests of any variety in medicine require multiple stages of study in multiple patient cohorts and settings. 

The whole process of clinical validation (not just developing the test) can easily take 5-10y - it takes time to enroll patients into a study, wait for the outcomes to happen, etc.

It’s one reason why anyone who says AI will be widespread in clinical medicine within less than 5y has no idea what they’re talking about.",4
post54con,controversial,1.4400485358400132,highest,"Its an argument against AI. We clearly are oversold on how it works and implementing it is difficult because we don't understand it. It means we shouldn't adopt it without knowing all the possible issues.


The fact that they keeping coming out with new models is a case against using them because there are so many untested unkowns. 


Its like if we had iOS 1 then iOS 5 then next year its a Linux Ubuntu distro. The shift is too great to reliably implement",3
post54con,controversial,1.4400485358400132,highest,"If you had a magic box into which you could insert a picture of a person's face, that instantly tests whether a person has cancer, but only 20% of positives are true, and only 20% of carriers are positive. The box is magic, ie you ""dont know all the possible issues"". And the box is wrong more often than it's right. Is that a useful machine that we should definitely use as soon as possible? To me the answer is yes, it's arguably immoral not to use it. If a consenting person gets flagged, they should go get checked by a doctor.",4
post54con,controversial,1.4400485358400132,highest,"This is a massive problem with science. Far too many scientists see women and non-whites as ""unnecessary variables"". The ""default white man"" is pervasive across every area of study.",2
post54con,controversial,1.4400485358400132,highest,"What a quintessentially 'reddit' take on things....The effectiveness of an predictive AI model is as good as the data set that its trained on.  The availability of data, especially medical data is tricky due to several factors. In this case, the Stanford team which built the chest Xray model (cheXzero) used a dataset of \~400000 chest xray images to train the model, but it seems only 666 (0.16%) of those images actually contained both diagnostic (from a radiologist) and demographic (race, age, sex) data. 

In the UWash [study ](https://www.science.org/doi/10.1126/sciadv.adq0305#sec-4)cited in this news article, their findings of AI bias are based on these 666 images which contained the necessary metadata. Its not an issue with the scientists from the Stanford [study ](https://www.nature.com/articles/s41551-022-00936-9#Sec4)\- the more data available for training, the more robust the model will be. Given the limited metadata they had to work with, taking into account demographic biases is outside the scope of their project and they used the full dataset. Its also worth noting (*only because you mention this as an issue*) that only two of the six authors on the Stanford team are white and one of them is female (the rest appear of east/south Asian origin). The UWash team highlighted an important issue with the model that demonstrates major pitfalls in the Stanford model which need to be addressed - but I think the baseless claim that the Stanford team is racist/sexist is very unfair, and its even more unfair to generalize it across scientists. 

Its also worth pointing out that the UWash study itself has ""sampling bias"" (not with malicious intent of course though; they had the same limitations as the Stanford team). Their model is trained on only the 666 images with demographic data - no one knows the demographics of the other \~400000 images used. Its difficult to tell whether their findings hold true across the entire data set simply because the necessary metadata doesn't exist. This is the core of the issue here:

Using chest Xray images as an example, medical privacy laws and patient consent can make it difficult to publish these kinds of data to public databases. And that's just the images, nevermind the demographic data. Add that to other variables that need to be controlled (eg quality of the Xray, reliability of patient health records, agreements between database administration and clinical teams etc), its tricky to get a large enough data set to robustly train a ML model while accounting for things like demographics. I'm of the opinion that consent for release of medical data should be a prerequisite and obligation for access to health care (assuming data security is robust and discrete patient identifiers are removed). Likewise, hospitals/clinics should be obliged to upload their data in free-publicly available datasets.",3
post54con,controversial,1.4400485358400132,highest,"This isn't a ""Reddit"" take. Go read Invisible Women. Maybe you're part of the problem.",4
post54con,controversial,1.4400485358400132,highest,"I mean that's just the fault of our regulations. It's so expensive to run studies that cofounding variables are never worth the risk to any company.

It also doesn't help that people really like to burry their head in the sand and pretend ""races"" aren't different enough to have very different interactions with the same drug.",3
post54con,controversial,1.4400485358400132,highest,Most of my peers in my life have been very left leaning. The politics in your echo chamber is causing you more suffering than you realize. Please try to get out of it and attain a more balanced view. You'll be happier and have a more clear picture of the world.,3
post54con,controversial,1.4400485358400132,highest,Go read Invisible Women and then tell me that again with a straight face.,4
post54con,controversial,1.4400485358400132,highest,"> trains it to literally discriminate just like the people who made it.

After reading the article that might be exactly what they need to do, build discrimination (as in the ability or power to see or make fine distinctions) into the model so to speak.  Reading the chest x-ray of an 80 year old white man compared to a 30 year black woman with the same model is probably not going to yield the best results.",2
post54con,controversial,1.4400485358400132,highest,"The upside to discovering its error is to either only use it on the sunset it is good for while giving it additional training for others areas or if that will not work, start from scratch.",2
post54con,controversial,1.4400485358400132,highest,"That's not really a problem with AI, though. It's a problem with our methods of training AI. 

We've had a very similar issue with automatic hand dryers. Some of the earlier hand dryers worked based on light reflectivity. Guess what - white people have more reflective skin. It refused to dry the hands of people with a critical threshold of melanin in their skin. If they tested with non-white people, they would have realized that their thresholds needed adjustment. We're dealing with something similar here. With all the attention put on racism and equity, we still keep forgetting to implement diversity in our product design.",2
post54con,controversial,1.4400485358400132,highest,"It's a problem across a lot of technology and science.    
    
Essentially every image recognition/analysis tool or toy I've ever encountered has had significant issues with darker skinned people.     
      
A disproportionate amount of what we know about humans is mostly from studying European descendants, and men.    
Even when it comes or animals, many studies have been limited to males, to reduce complexity and variance.  
   
We really need high quality, diverse public data sets. This is something the government should be funding. AI isn't going away, we need to find ways to make it work for everyone.   
Medical diagnostics, of all things, should not be exclusively in private hands.",2
post54con,controversial,1.4400485358400132,highest,"As someone who does do AI research in medical stuff,this is actually a pretty good idea. They're one of the few who could actually do it without getting hippa'd",3
post54con,controversial,1.4400485358400132,highest,I know of the issue in general but I'm pretty surprised race affects their reading of x-rays of all things.,2
post54con,controversial,1.4400485358400132,highest,This isn’t really an “AI” problem. What you are describing is *human error*,2
post54con,controversial,1.4400485358400132,highest,"I didn't read the study, but usually, this problem occurs due to lack of data from certain groups of people.

I assume there is simply less data available from black women, and this is usually due to the history of people of African origin, as well as their current living conditions.

We simply have less data available since these people don't visit (for many reasons like poverty) the doctor as often, or since the majority of these people live in countries where we don't have easy ways of collecting data from them.",2
post54con,controversial,1.4400485358400132,highest,Because they correctly trained it on the most common cases first. Of course there's always going to be outliers.,2
post54con,controversial,1.4400485358400132,highest,Women and blacks are outliers?,3
post5con,controversial,1.422454390748317,highest,How do people construe facial recognition misidentifications as evidence of the system being racist? The word has lost all its meaning thanks to articles like this.,1
post5con,controversial,1.422454390748317,highest,"No, it still means what it means. No one has to use facial recognition software that doesn't actually recognize faces. The people who work on this software could actually  test it to make sure it works before calling it finished. What's the functional difference between bias in the system and being apathetic about contributing to the bias?",2
post5con,controversial,1.422454390748317,highest,"I agree with you. I just think it's at least worth mentioning the fact that facial recognition inherently needs more light for darker skin. To implement it when there isn't yet a 0% bias rate is racist, yes (and I wouldn't want it implemented then either).",3
post5con,controversial,1.422454390748317,highest,"Two things:

A) Racial bias in algorithms is a very common phenomenon and needs to be addressed.

B) The police are still trying to use said algorithm EVEN THOUGH IT CAN'T TELL PEOPLE APART.",2
post5con,controversial,1.422454390748317,highest,">A) Racial bias in algorithms is a very common phenomenon and needs to be addressed.

Imagine just making up bullshit because you want to look progressive and then having 25+ people agree with you all based on literally nothing. Does ""racial bias"" exist in algorithms? Sure. Is it ""very common""? Not even remotely. And like most racist conspiracies on reddit this dog-whistle is an the alt-right talking point about white people being inherently smarter than minorities since a machine says so. It's sad to see this racist bullshit upvoted but not exactly surprising",3
post5con,controversial,1.422454390748317,highest,"Ok well.... this isn’t a hard thing to find out. I do have a computer science background, but this is doesn’t require a deep understanding of ML to get. Then there “25+ disagreeing with me”. That last half was just incoherent. How am I being racist? Buddy are you ok?",4
post5con,controversial,1.422454390748317,highest,"Racial bias in algorithms is extremely uncommon.

Racial bias in ML training data sets is a lot more common though, often leading to biased models.",3
post5con,controversial,1.422454390748317,highest,"It’s more common than you think. It isn’t always things as overt as a facial recognition program thinking all brown people look alike. And it usually isn’t intentional.

Data training sets for ai is one cause, but not the only one.",4
post5con,controversial,1.422454390748317,highest,I agree and I am aware.,3
post5con,controversial,1.422454390748317,highest,[removed],3
post5con,controversial,1.422454390748317,highest,Uh..... how?,4
post5con,controversial,1.422454390748317,highest,"It's not used as conclusive evidence, it just narrows down lists of suspects.",3
post5con,controversial,1.422454390748317,highest,"“racial bias in algorithms” just tells me you know nothing about algorithms.

Pattern recognition systems (which aren’t actually algorithms) (that people mislabel AI) based on racist training data can be racist, but that same system based on non-racist data won’t be racist.",3
post5con,controversial,1.422454390748317,highest,"You sound like you read the first chapter of a 15 year old book on java. 

AI isn’t just one thing. In this context, we mean machine learning algorithms. In others it could mean software that emulates human decision making.

And a pattern recognition system is without a doubt an algorithm. It is an extremely broad term.

Talk about proving you don’t know what you are talking about.",4
post5con,controversial,1.422454390748317,highest,"Because the police, electing to use this flawed and biased system, will end up having more false positives among minorities with black or brown skin. 

As such, the net effect is a racist system. 

The meaning of the word is fully sound.",2
post5con,controversial,1.422454390748317,highest,"I agree. It's the way in which the technology is used that is racist, not the technology itself alone (which the article seems to not clarify).",3
post5con,controversial,1.422454390748317,highest,"> It's the way in which the technology is used that is racist, not the technology itself alone (which the article seems to not clarify).  

From the beginning of the article:  

> But there’s two factors that need screaming above all others when it comes to the debate surrounding facial recognition. 

> One: it’s racist. 

> Two: it doesn’t even work. 

> **Technology never exists in a vacuum.**  For now, humans are still responsible for the production of new digital systems; and that means they come into being with all the biases and fallibility of their creators baked right into their code.  

Jesus fuck.  

But yeah, keep circlejerking about ""omg smh people calling everyone racist these days are debasing the super-great English language (that I don't even bother to read).""",4
post5con,controversial,1.422454390748317,highest,">will end up having more false positives among minorities with black or brown skin

Which means the system will be better at tracking down white people than those minorities.

Did you even consider that angle? And I'd even go as far as to assume you'd be even more angry if it was the other way round, if the system eg were best at tracking down black people. Police using tech thats only good at tracking down black people, imagine the screams about racism. But if its aimed at white people, then thats racist to non-white people too?

&#x200B;

And yes, that what makes it ridiculous that overly woke people - particuarly an american stream of politics - try to apply racism to anything, pretending its such a sensitive topic, but then cant even make a good judgement call on it.",3
post5con,controversial,1.422454390748317,highest,"If they use facial recognition to incorrectly prosecute people of colour, or at the very least harass them and arrest them and waste their time because the facial recognition technology popped a false positive, then it’s harming black people. If it’s accurately recognising white criminals, doing the crime — then it’s working as intended. 

What the fuck were you even thinking here? It is accurate with white people, poor white criminals getting caught? 

It’s not ‘aimed’ at white people, it’s not being used to persecute or prosecute *just white people*, fucking obviously. 

Alright, I’ll break it down. 

* The system for facial recognition is being applied to the general public. All video and photo relevant to a crime, supposedly, will be subject to this technology to identify perpetrators.

1. For white people, it’s accurate and the criminal is caught.
2. For people of colour, they get false positives, bring innocent people down to the precinct, subject them to inquiry, and possibly say ‘we have data that proves you were there!’.

Let’s not get into what’s ‘harmful’: you’re seeing how the system is discriminating between race? Yes?

That’s what makes it racist.

From there, we can better argue as to why it’s more harmful to be incorrectly identified for crimes you didn’t commit...",4
post5con,controversial,1.422454390748317,highest,"This entire post and thread are absolutely baffling

1.	This entire problem is a symptom of technological drawbacks where cameras can contrast objects easier against light backgrounds. Not systematic racism or “racist data”
2.	Given the above, even using the system would likely perpetuate criminal stereotypes against white people as they would be far easier to be recognized, not brown/black like everybody is implying.

y’all are dumb as hell",4
post5con,controversial,1.422454390748317,highest,">Which means the system will be better at tracking down white people than those minorities.

This is assuming that the cops have the integrity to acknowledge the possibility of false positives and not use this in cases involving black or brown people. Realistically it's going to put innocent minorities in prison while the increased accuracy for white people means it will (correctly) exonerate innocent white people.",4
post5con,controversial,1.422454390748317,highest,"So you’re telling me there are gonna be two faces that are identical in every way and the only difference between them is going to be skin color? Doesn’t seem likely.

Skin color isnt going to be a determining factor when recognizing faces.",3
post5con,controversial,1.422454390748317,highest,[deleted],4
post5con,controversial,1.422454390748317,highest,Because it's disproportionately inaccurate for nonwhite faces?,2
post5con,controversial,1.422454390748317,highest,"It’s not racism, it’s physics. They’re based on light, and dark people reflect less. Really light skinned black people and pale mexicans are recognized as easily as white people.

Y’all are acting like photons are racist and proving OP’s point.",3
post5con,controversial,1.422454390748317,highest,"Phenology was physical that didn't make it magically not racist. There's no physical reason that a camera can't pick up a black face just as well as a white one, it's a failure of the neural nets analyzing the image. You feed them a biased dataset and they cannot do anything but recreate that bias. You feed them disproportionate white faces and it will be disproportionately inaccurate on nonwhite faces.",4
post5con,controversial,1.422454390748317,highest,[deleted],3
post5con,controversial,1.422454390748317,highest,"If the AI was hugely disproportionately misidentifying white people, and this faulty AI was used by police to flag potential criminals, I have a feeling it would not be being used by the police right now.",4
post5con,controversial,1.422454390748317,highest,"Well nobody seems comfortable calling the tool racist, nor the tool's makers, nor the tool's users, and yet from that combination some racism magically happens so I don't know what part of the pipeline you want to attribute it to but it's not there by accident.",4
post5con,controversial,1.422454390748317,highest,"By the system, I mean the technology itself in isolation.",3
post5con,controversial,1.422454390748317,highest,"It works differently for people of different races. Insofar as a *thing* can be racist, the algorithm is racist.",4
post5con,controversial,1.422454390748317,highest,Confirmation bias.,2
post5con,controversial,1.422454390748317,highest,"Racism means something different depending on who you ask, but if you ask me the intention behind this system is irrelevant. The fact that the effect is a problem specifically for people of some ethnic group/race means it's ""racist"".

So if the system they're using misidentifies black people really easily, and that results in more black people being arrested falsely and then released, you could say that it's racist. Even if you're giving them the benefit of the doubt, and saying it was an accident.

That's how a lot of people define racism. The reason is that you can't prove someone's intentions that easily, all that matters is the effect.",2
post5con,controversial,1.422454390748317,highest,"Facial recognition systems can’t spot black people because they don’t reflect enough light to find their facial features. On a 2d contrast based image, especially if it’s low resolution or poorly lit their facial features simply don’t stand out enough to be detected.

Imagine if you saw four different black people on a grainy, low resolution security camera with bad lighting 20 feet from the camera. Could *you* tell who they are? Would that make you racist?

This isn’t racism and you’re just deluding the term by calling it racism. It’s a poorly designed system that we shouldn’t be using, but it’s not racist.",3
post5con,controversial,1.422454390748317,highest,"It's like you didn't even read my comment.

If every person only had a grainy photo as an ID, and because of that I could only identify and let through people with light skin tones, that system would be considered racist.

Even if I had no malicious intent at all, the result of my actions and the system would be that people with dark skin tones are stopped because their id can't be verified, while only people with light skin tones are let through.

There are plenty of faulty systems like this in the world that disproportionately affect people that are poor, live in a certain area, or have a certain appearance. Whether it's intentional or not is irrelevant when determining if it's discriminatory.",4
post5con,controversial,1.422454390748317,highest,"If a facial recognition system is significantly more likely to misidentify black people, then it's just bad design and unconscious bias by the people who fucked up when they were training the algorithm.

I a facial recognition system is so likely to misidentify black people that it's as bad as random chance *and the cops know this and insist on using it as evidence against black people anyway*, it's getting kinda racist.",2
post5con,controversial,1.422454390748317,highest,"Because the system has to have input from developers on how to read the data it's receiving. If the developer is racist/can't tell black and brown people apart, then the system will have the same issues. The system in and of itself isn't conscious enough to be 'racist', no, but if it was programmed by people who are (and research shows that even the most loveral hippy dippy white person has some latent racist responses) the end result is the same.",2
post5con,controversial,1.422454390748317,highest,"It seems more likely that misidentifications will occur more frequently with people who have darker skin simply because it's more difficult to identify facial features due to less light being reflected from their faces, therefore creating less data for the software to work with.",3
post5con,controversial,1.422454390748317,highest,"The problem isn’t with lighting and instead with bias and limited information in the training sets. 

It’s the same with gender of equivalently light-skinned people as well. From the article below, light-skinned men are correctly categorized each time, whereas light-skinned women are incorrectly categorized  19% of the time. The numbers only increase for darker skinned individuals, and the gender gap exists there as well. 

https://www.theverge.com/2019/1/25/18197137/amazon-rekognition-facial-recognition-bias-race-gender",4
post5con,controversial,1.422454390748317,highest,It's funny because this is obvious and clearly the right answer but everyone is gonna play dumb because they would rather it just be racist.,4
post5con,controversial,1.422454390748317,highest,People that don’t work with the tech need to stop perpetuating this outdated factoid.,4
post5con,controversial,1.422454390748317,highest,"Sure. And yet, here we are with smartphones that can identify you in a scarf and sunglasses",4
post5con,controversial,1.422454390748317,highest,"This comment shows complete ignorance for even the basic methods of AI training, and because of this ignorance attribute malice where there is none.

If the system is trained with primarily Caucasian photos, it will have difficulty with nonCaucasian faces and vice versa.

This article exists because posters like this eat it up and beg for more because it reinforces their worldview that all white people are racist, they even admit it in the post.

>research shows that even the most loveral hippy dippy white person has some latent racist responses",3
post5con,controversial,1.422454390748317,highest,">If the system is trained with primarily Caucasian photos, it will have difficulty with nonCaucasian faces and vice versa.

Besides that, I also wonder if the vice versa applies. It might well be that the current tech and cameras are just naturally worse at detecting non-white people, eg because black skin reflects less light.",4
post5con,controversial,1.422454390748317,highest,So why don't they just train it with non-Caucasian faces more before implementing it to solve the problem?,4
post5con,controversial,1.422454390748317,highest,"Thats not how it works at all. The System is training itself to identify whatever you want it to identify. You need good training data though. Lighter Skin will reflect more light so pictures of white people will be better training data than pictures of black people. 

If the system was programmed by the most ""racist"" people on this planet but would read faces in 3D via some kind of laser that perfectly catches every facet of a face no matter the lightning conditions then it would learn to identify black faces just as well as white faces.

 Black faces don't produce good training data. That's all.",3
post5con,controversial,1.422454390748317,highest,"Victim mentality, although I do think facial recognition technology should be getting any better, nor start being applied to the general populace,",2
post18con,controversial,1.3953958801240345,highest,Why did they accumulate so much without cashing out? You can take payments every 3 days.,1
post18con,controversial,1.3953958801240345,highest,Wondering the same,2
post18con,controversial,1.3953958801240345,highest,"Same.  I have earned well over $6k with DA and I withdraw it to pay my mortgage and things like that. If there is over $250 in there and I have a withdrawal due, I withdraw it. That's usually every 3 days.  I can't think of a good reason to leave that much money in there, it would take a fair amount of time to rack that up, certainly more than 3 days!  I trust DA as I have never had an issue working for them or being paid, ever, but I wouldn't leave $6k in there. I wouldn't leave that in PayPal.  If it's not paying something the only place for it is in a bank account earning interest ($300 a year at 5%)",3
post18con,controversial,1.3953958801240345,highest,My only thought was that they were logging hours that didn’t match with the work that was being produced -and/or- inputting basic minimal responses that are clearly not inline with project instructions.,4
post18con,controversial,1.3953958801240345,highest,"Thank you for responding with this. I have only been working with them for a couple of weeks, but your described experience has also been my own.",4
post18con,controversial,1.3953958801240345,highest,"Give it time. My bet is that they will “fire” you for no reason sooner or later. Probably right after a promotion and a message stating how good of a job you’re doing. It’s happening to everyone. And I’ll speak for myself when I say I carefully read all instructions for every project and was darn good at the tasks. Lost over $1200 that I had made over the past week between then and my last cash out. Just don’t quit your day job… I talked up this company to soo many people because it was really great. And it IS great while you’re still there. But me and tons of other people keep having this same thing happen, a lot of us for no reason, or I’ll speak for myself at least, and it’s only a matter of time before it happens to you too. I hope it doesn’t, but don’t be silly like I was and used this as my only source of income :(",4
post18con,controversial,1.3953958801240345,highest,"Hi - hope you can help me.  I was downsized Sept of last year and my unemployment benefits are about to run out.  I'd like to get gig work with DA. I setup an account and the whole nine, but it just says they have no work.  How do you start getting work with this company?",4
post18con,controversial,1.3953958801240345,highest,Do they provide a 1099!?,4
post18con,controversial,1.3953958801240345,highest,Is DA still a thing? Would I be able to do it from Aus?,4
post18con,controversial,1.3953958801240345,highest,How do taxes work with them? Is it a W9?,4
post18con,controversial,1.3953958801240345,highest,I completed my starter assessment yesterday how long they take to get back with results?,4
post18con,controversial,1.3953958801240345,highest,"I’m looking into doing DA, but i see alot of threads about it it being a scam, is it worth it ?",4
post18con,controversial,1.3953958801240345,highest,how long does it take to hear back for them after applying?,4
post18con,controversial,1.3953958801240345,highest,"Are you doing DA full time? Thats great that you get that much work. If you don't mind me asking, how much do you make per month?",4
post18con,controversial,1.3953958801240345,highest,"Yeah. This doesn’t make sense. I cashed out about $2k in my first month with DA. How long were these guys stacking cash and more importantly, why? If they were simply deactivated for poor performance then they’d be allowed to withdraw their funds. Seems like they were up to something shady to be banned completely.",2
post18con,controversial,1.3953958801240345,highest,"Side topic here but I've just started with Data Annotation and I'm doing the onboarding. How did you track your time? I'd like to start off on the right foot on this platform. Thanks for any help or suggestions you can give. 

![gif](giphy|AeWoyE3ZT90YM)",3
post18con,controversial,1.3953958801240345,highest,This sounds like victim blaming. There's nothing wrong with being owed $6k by a company. That is a normal paycheck.,2
post18con,controversial,1.3953958801240345,highest,"You have the option to withdraw funds every three days. In order to earn enough to have 6k you would have to work three to four weeks at top pay, full time. There is absolutely no reason to accumulate and leave so much money there. Would you allow your boss to hold your check when you could cash it out? You don't even have to transfer it to your bank, you can hold it in PayPal after cashing it out from DA.

It sounds like there is more to the story that hasn't been disclosed. The person posting this complaint ""for their friends"" made this account on the same day they posted the complaint. No answer has been made to what period of time the funds were earned in or any other details. The company is definitely known to lock out folks who have violated the code of conduct - but folks who just did subpar work still get to cash out, even if future work is locked out.",3
post18con,controversial,1.3953958801240345,highest,There's nothing wrong with having a large paycheck there's something illegal about company not paying it you don't really need a wall of text to understand that,4
post18con,controversial,1.3953958801240345,highest,"There could be some edge case reasons to not cash out regularly, but most of them involve deferring taxable income to the next year. Personally, I think the risk of something going sideways is not worth some potential small tax savings though.",4
post18con,controversial,1.3953958801240345,highest,It's not victim blaming. You would be an idiot to leave 6k+ with any affiliate.,3
post18con,controversial,1.3953958801240345,highest,"At the same time, I would like to know reasons why a person might be locked out even if the company can't address specific situations. I do work for DA and move my money regularly even if I only managed to get in 30m but money owed is money owed and if there is no rule against stacking the cash then that by itself wasn't wrong.. 6k is a lot of hours of work to leave sitting there though I wouldn't be able to afford to just leave that there. Mainly I hear people are having a good experience with the company which is why I got involved .
 So far I am pretty happy with the work myself.. but would love to know more on this.",4
post18con,controversial,1.3953958801240345,highest,"> You would be an idiot to leave 6k+ with any affiliate.

This is literally the definition of victim blaming. ""You would be an idiot to leave yourself so vulnerable to becoming a victim!""",4
post18con,controversial,1.3953958801240345,highest,Seems like you would be an idiot to complain on the internet about what other people do with their money but you do you,4
post18con,controversial,1.3953958801240345,highest,You are attacking the person instead of the issue at hand.,4
post18con,controversial,1.3953958801240345,highest,"No it isn't. When you are a freelancer, under a contract, you are running a business. You take your money owed. They are not employees, so it's their responsibility to get their pay. 


There's definitely more to the story. If you can cash out in 3 days, there's no reason leave it. These ""friends"" are definitely idiots, and most likely were scamming the company and got caught.",3
post18con,controversial,1.3953958801240345,highest,"I agree. Of course it makes good sense to cash out to avoid just such a happenstance that you lose access to your account.  But this is a gig job where you signed a contact to be paid money -- not microsoftrewards or some promotion game where you use a VPN or violate some other rule and they can seize your points. 

Even if OP did something that made DAT cast doubt on his billable hours, normal business practice is to issue written communication saying such and providing the employee with a mechanized option to dispute the company's account. 

Not saying DAT isn't worth taking the risk -- it's a better side gig than most and some people have made a lot of money from them. 

But it isn't remotely normal to lose access to PRIOR EARNINGS just because you are no longer employed by the place where you earned them.  So knowing this can happen is extremely useful information.  

(you don't have to be rich to let the money pile up. Sometimes people delay cashing their check for a few days just so the money doesn't burn in whole in their pocket over the weekend).",3
post18con,controversial,1.3953958801240345,highest,"Payments for timed projects are pending for  7 days , then 3 days to wait between withdrawals. If someone, as I did, worked for 10 hours a day on $40/h projects,  it's easily $4000.

For example:  
I worked from March 1st. First withdrawal is on March 10 for work up to March 2.  The next withdrawal is on March 13. If they cancel my account on March 13, they don't pay me for work done after March 2, that is 10 days to March 13.",2
post18con,controversial,1.3953958801240345,highest,"If you were working 10 hour days, 7 days a week - along with working for Telus in the same time frame - there is no way that you were able to maintain quality of work without violating the CoC to some degree. They only refuse payment for Code of Conduct violations, not for crappy work. If you were using AI to generate some of your content, farming your work out to other people, reusing material, etc etc then yeah - they refuse to pay you because you didn't uphold your end of the agreement and you didn't actually earn that money. Texas Workforce isn't gonna help you, BTW - as an independent contractor you will have to take them to small claims court to fight your nonpayment.",3
post18con,controversial,1.3953958801240345,highest,"Telus is a freelance job that pays $11, there is no reason to think that I would spend any time on it if I have something to do for $40/h.   I know, nobody can help me, the DAT can do anything with impunity. They hide their identity for a reason. I just tell other people what they can do other than complaining here on Reddit.  
The small court is useless: 1) I cannot serve the DAT with subpoena, as the site is registered anonymously; 2) the small court decision is impossible to enforce, as nobody takes it seriously.",4
post18con,controversial,1.3953958801240345,highest,My guess is they were programmers making $60 an hour so were comfortable with that.,2
post18con,controversial,1.3953958801240345,highest,If they were new to the site they weren't making $60/hr.,3
post18con,controversial,1.3953958801240345,highest,It does not say they were new accounts.,4
post18con,controversial,1.3953958801240345,highest,Why is anyone tracking how much a person is accumulating?  That’s their business whatever they do with their money. Anyone taking money from account that is NOT theirs is called theft and anyone monitoring ur account without permission is called invasion of privacy,2
post18con,controversial,1.3953958801240345,highest,"Not knowing anything about the company you're speaking of, I'll just say some folks are unbanked for whatever reasons and have to take some extra steps to move their $ around. So it kind of makes sense if they were new/new-ish and just procrastinated on that detail. But just like doordash offers dashercards I cannot fathom why any such service wouldnt have offered that as an option upon signing up. Maybe theese two will get a random paper check in the snail mail like 30 days out.",2
post18con,controversial,1.3953958801240345,highest,This company only pays out through PayPal. They disclose this up front.,3
post18con,controversial,1.3953958801240345,highest,"I can’t deal with PayPal; they’re almost the biggest scam ever. They just randomly grabbed some money that was sent to me and said that I owed it to them (which I didn’t), but would never respond back with a concrete reason why they felt I owed it to them. It was only ?30 or so, but just imagine how much bank they make by taking peoples money x however many people they get money from. They are unscrupulous and there’s no way to talk or email someone who isn’t reading or replying directly from a generic script",4
post18con,controversial,1.3953958801240345,highest,The payouts appear to be to PayPal.  ONE may choose to leave the funds in their PayPal account and get a PayPal debit card.  Having an old school bank account is not necessary but an option like any other linked account PayPal allows.,3
post18con,controversial,1.3953958801240345,highest,"No. You can't be unbanked and work DA. There's no random paper checks or snail mail. Nope. They use Paypal, and they even PAY YOU in onboarding to make sure your Paypal account is set up properly with correct email address, etc. Getting your money transferred into Paypal involves clicking a button. There's more to this story.",3
post18con,controversial,1.3953958801240345,highest,Because they were scamming the system and got caught,2
post18con,controversial,1.3953958801240345,highest,"You can cash out every 3 days ONLY for pay per task tasks. Not hourly tasks. Those pay once a week. And you can ONLY cash out every 3 days. So if you work 3 hours on a Monday, you can cash out for JUST those 3 hours the following Monday, but then have to wait a full 3 extra days before you can cash out anything you made after that Monday (as long as it’s within the 7 day period). So THATS why we accumulated so much and lost it all. Because it’s complicated. Some people have bills and need Monday, Tuesday, and Wednesday’s wages to pay those bills. So we’d have to accumulate nearly two weeks of wages without cashing out in order to cash out the full amount one may need for those bills if that makes sense. You can only cash out once every 3 days, hourly tasks are paid out every 7 days (which this money is deposited into your account). Let’s say it’s been 7 days and you get your wages from the day you worked 7 days ago, but you just cashed out yesterday. You now have to wait 3 more days to cash-out the wages you earned 7 days ago. And if you work every day with high paying hourly tasks, it adds up quickly. I lost $1256 bucks for a week and two days worth of work before getting “banned” for no reason. It’s not as easy as work, and get paid 3 days later. If you worked there, you’d understand. It’s just not as simple as it may seem or not as simple as they make it seem.",2
post18con,controversial,1.3953958801240345,highest,"I know how it works, I've been doing it for months. Earning 6K in that amount of time is unlikely without low quality work or a violation of CoC by account sharing. You are misrepresenting the pay - it doesn't pay ""once a week"", you can cash out the money exactly 7 days after you did the work to allow time for review of the work. I cash out twice a week, I can get money from as recent 7 days prior (and do, every week, twice a week).",3
post18con,controversial,1.3953958801240345,highest,"lol, no my friend, I’m not misrepresenting, I’m basing my answer on actual facts and experiences with this specific company. Your using terms like “unlikely” makes me think that you and I aren’t speaking about the same company. DA allows you to cash out every 3 days period. Hourly projects are paid out 7 days after completion. If you work on a Monday, let’s say, and go to cash out the next Monday, then you can! UNLESS you JUST cashed out on Sunday, which would then mean you now have to wait until Wednesday to cash out for the previous Mondays work. And by cash out I mean getting money into your PayPal. There’s a countdown timer and everything letting you know exactly when your last cash out was and when you can send money to your PayPal again. I’ve got screenshots if you’d like to see them. Just because you don’t agree, doesn’t make a true claim a misrepresentation. Non hourly tasks pay out ever 3 days, hourly tasks pay out every 7 days. Separately from that/ you’re only allowed to send money to your PayPal every 3 days despite the previously mentioned 3day/7day rule.",4
post18con,controversial,1.3953958801240345,highest,"I don’t think you understand what I’m saying lol. Or you don’t h see stand what YOUR saying. It’s a 7 day review period, allowing you access to the money you made 7 days prior. If you haven’t cashed out within 3 days before that 7 days, then you can cash out that day. Heck, you can sometimes cash out 3 times a week if you do it correctly. UNLESS your within that 3 day rule/vs the day you actually worked and it being after the 7 day review period",4
post18con,controversial,1.3953958801240345,highest,"Also I’m not sure who mentioned making 6k, but it sure wasn’t me friend. I pulled in between 2-4k a month when I worked for them based on my own choice to work however many hours.",4
post18con,controversial,1.3953958801240345,highest,Remember- just because it didn’t happen to you doesn’t mean that it didn’t happen to someone else. Glad you’ve still got a job there/ but a ton of us aren’t as lucky. And most of us unlucky folks did every single little thing right. Don’t be ignorant to common logistics -no offense😊,4
post18con,controversial,1.3953958801240345,highest,"I think they may have been doing something sketchy, like copy paste reviews or not being careful with details. I agree that having 6000 and not collecting is weird.",2
post18con,controversial,1.3953958801240345,highest,The question fails to address the complaint.,2
post18con,controversial,1.3953958801240345,highest,This. The story OP claims sounds suspect.,2
post18con,controversial,1.3953958801240345,highest,[deleted],2
post18con,controversial,1.3953958801240345,highest,What proof is out there that they have bad business practices other than a flood of people online bitter because they never got approved into the work flow. Im sincerely asking as I am curious.,3
post18con,controversial,1.3953958801240345,highest,[deleted],4
post18con,controversial,1.3953958801240345,highest,is that even relevant?,2
post20con,controversial,1.3747981746419655,highest,"I have yet to see AI replace or do any meaningful work in an enterprise environment or on an application that is more than just a simple frontend.

If you feel like the show is over, to me that suggests you are not building sites with any real features beyond basic CRUD forms or static displays.

I know this sounds shitty, but if you want your job to be more bulletproof, you need to start learning how to build applications that AI can't replicate.  AI isn't going to design, setup, and build your service bus that manages your mapping engine job scheduler which then calculates risk portfolios across Florida roof maps.",1
post20con,controversial,1.3747981746419655,highest,"Yeah, from my experience with AI it's just kind of like a more advanced autocomplete and helps me save time writing map functions and stuff like that...things I could easily do but consume time and energy I could be spending on more complex things.  But when it comes to understanding requirements, architecting projects, third party integrations and more complex coding it is REALLY bad.

It's a great productivity tool, but like you said, if you never find yourself needing to change it or even program from scratch you may be doing stuff that could have already been done with low/no code solutions already.

But I get that a lot of people here are doing agency work or other smaller, less functional websites that are more about producing the same thing frequently than bespoke function or complexity.  That's a valid way to earn a living and we probably will see AI eat up a lot of those jobs (though you'll still need someone who understands enough to fix when it's wrong.)",2
post20con,controversial,1.3747981746419655,highest,"I agree. I think AI llms, for me, it feels like having ten obedient interns in a team and get things done like 10-20 times faster. It can do simple functions that I would know how to do but would take me 10min… with ai, it takes 1min or less. 
But, when it gets too complex or a bit novel, it gets lost. 
It is sometimes not even suggesting  obvious improvements that you know as experienced developer. 
I agree it is probably able to replace those million times done job easily. 
By the way, I noticed that sometimes it’s very thorough ; I guess it’s because many people did it before. But for more abstract stuff, it is not so good. Recently it struggled with Promises",3
post20con,controversial,1.3747981746419655,highest,"I understand the need to downplay LLMs due to their obvious failure at handling esoteric and novel problems, but to act as if they don' t do any meaningful work is akin to having your head in the sand.  There are devs at all levels, staff-level engineers included, that have woven AI into their workflow.

It's so paradoxical to me, because there are insanely talented people on both sides of the fence and for those that flat out assume it's not helpful, it must come down to a few things.  Either their lack of commitment to the tool, there inability to prompt correctly or maybe even more obvious, their reluctance to let disruption happen to the craft they love so much.  Regardless, most of the software that the industry creates is basic CRUD applications, and frontier LLMS are MORE than capable at helping expedite that process - this goes well beyond ""basic CRUD forms"" and even includes fleshing out quality business logic.",2
post20con,controversial,1.3747981746419655,highest,"As a senior dev at a company with a relatively large scale software project: we use AI, but it's a slight productivity boost at best.  It simply can't handle the project in context.  It basically is just a slightly better than eslint autocorrection.

I did a hackathon recently where I tried vibe coding, and while I do think the AI helped me accomplish something that I wouldn't have been able to get done so quickly without it...  The codebase is a disaster.  Duplicated improper config all over the place, hard coded variables everywhere, nonsensical redundant architecture.  If I was at all worried about software security with this project I wouldn't be able to sleep a wink at night.  These AI can do some pretty impressive leet code assignments, but they're quite far from actually writing well structured clean code.",3
post20con,controversial,1.3747981746419655,highest,"I'm not downplaying it at all.  I use AI all the time to help with stuff similar to how I would use Google to search Stack Overflow.  Yes, AI can build CRUD applications to some extent.  It really depends on the amount of business logic that drives the form.  If its just a simple submit form, sure, but it really starts to fall apart once you start getting into actual logic.

I 100% know that AI is going to change the way we work, but I don't see it as a threat to actual development at this point.",3
post20con,controversial,1.3747981746419655,highest,"This is the problem with discussions on this subject: putting out fair criticism is met with being told you have your head in the sand or that you’re a Luddite. I’ve been using GitHub copilot, but it’s at best an elevated intellisense/visual assist suggestions tool. ChatGPT is sometimes more helpful than Google, but as broken as Google has been I still often get better results from a traditional web search.

I see a lot of marketing and hype around the future of these tools, but in today’s reality the promised features aren’t there, and as far as I can tell LLMs aren’t the road to the solutions people want the current products to be.

I’m often told “well look how fast things have progressed in the last few years” but if they knew anything about AI development they’d know that the current applications are built on decades of research and development. You just can’t argue with people who don’t work within the domain of reality.",4
post20con,controversial,1.3747981746419655,highest,"I've kind of lately started using it as a rubber ducky. Bouncing ideas off of, which it then searches the web for.",4
post20con,controversial,1.3747981746419655,highest,"I experience what you describe all the time. On larger codebases it often bungles the logic or the basic intention you're going after. It kind of makes sense though - the AI was never trained on _your_ specific problem, so unless your problem is generic (like a helper class or common dev pattern), the AI is going to do a lot more hallucinating.

As a concrete example, I see it when using CoPilot/vscode to write php docblock comments for my class methods while building out boilerplate. I would write the function signature, using a super clear and obvious name to state what it should do, likewise with parameter names (etc.), and after starting `/**`, it'll copy the docblock from a completely unrelated method (like the constructor). Makes me wonder if it read what I just wrote at all. It does this much more often in larger codebases and even just large class files with a lot of methods.

So I use it a lot like you do, surgical strikes to save time switching Windows and wading through ads and spam to look up a solution. But that being said, I never accept anything it provides at face value. I'll review every line and often rewrite half of it. 

And just from seeing and knowing every day how many hallucinations a tool like CoPilot still has, I can tell you vibe coding is going to lead to some serious tech debt in the future.

For a small throw-away utility, like a side tool you need to process some data, I'll be more lenient there and largely vibe-code. I'm still reviewing every line, just not as picky about style or best practices here.

But ultimately, _I'm_ dictating the logic and architecture, and it's just saving me time, clicks, and typing.",4
post20con,controversial,1.3747981746419655,highest,"I use AI all the time too, and I’m often surprised by moments where it feels like it’s reading my mind and anticipating a non obvious next move. It’s kindof spooky and I think it might do more in the future than I’m currently considering.

That said, I honestly am not seeing productivity increases because it’s become apparent that coding is a minor portion of my job. Analysis of what to do, and where to do it, is the majority of my job. How much time do other devs spend on the mechanics of coding around here?",4
post20con,controversial,1.3747981746419655,highest,I use it to do complex tasks but if I don't guide it then it may as well be a chicken.,4
post20con,controversial,1.3747981746419655,highest,"The other day I witnessed how British Rail uses AI to process delay refunds, using multiple AI agents. It wasn’t “creating” anything, but it was managing an entire workflow, making decisions based on available data and prompts that they used AI to refine. It really opened my eyes as to how AI can be used to solve real problems. 

We are doing website migrations with the assistance of AI. Think moving a 20,000 node site using 8 content types from a proprietary system to a new CMS. What used to take 80 hours now takes 8-16. 

We’re also finding that custom reporting can be enhanced with AI. With the right libraries and setup it’s incredible. You can ask the system something like, “Using historical sales data from the last 3 years and our current Q1 sales progress, create a forecast report for Q3 sales.”",4
post20con,controversial,1.3747981746419655,highest,"I have to agree with the person you replied to AI is near useless for coding outside of duplicating unit tests and documentation.

Software development inherently requires context - and lots of it. Something out of the box might work in a vacuum but in the context of an enterprise environment it quickly just creates a mess.

AI hasn't shown any ability to work with large context (yet) but it can one shot a really simple front end UI.

So right now it can scoop up the entry level stuff but no dev worth their salt is actually using it to write code.",3
post20con,controversial,1.3747981746419655,highest,"I disagree. AI won’t create your application for you, but try making it create the methods as you create the application. And the unit tests for those methods, and the infrastructure of you use IaC. Any dev willing to remain a dev, worth their salt or not, should learn how to use AI.",4
post20con,controversial,1.3747981746419655,highest,"> I have to agree with the person you replied to AI is near useless for coding outside of duplicating unit tests and documentation.

Not in my experience whatsoever.

> no dev worth their salt is actually using it to write code.

Git gud. It's a godsend for A and S-Tier developers. The better understanding you have of software engineering best practices, the more useful and time-saving it becomes. My code has never been of higher quality because AI frees up time to be more mindful and proactive in every step of the development process.

AI is your junior dev cranking out code, as you the architect and technical lead map out the problem domain, implementation structure and strategy.",4
post20con,controversial,1.3747981746419655,highest,">no dev worth their salt is actually using it to write code.
 
Gonna disagree here",4
post20con,controversial,1.3747981746419655,highest,"I used to have the same idea as you, that context is what AI was terrible at. That is until I tried Cursor, paired with Claude 3.7 and I was just amazed and disturbed in the same time.",4
post20con,controversial,1.3747981746419655,highest,"What exactly are you doing all day that involves making CRUD apps?


I simply copy paste my code templates/import libraries to do this and it's literally faster than anyone using code from LLMs.",3
post20con,controversial,1.3747981746419655,highest,"I agree. I’m using AI to build real apps and as long as you guide it well it can do real work. 

I made the same mistake everyone makes at first. 

Hey AI, make me instagram and expect to have a working app, then say it suck’s when it doesn’t do that. 

But if you break that down into small tasks, it will do it.",3
post20con,controversial,1.3747981746419655,highest,"I will say this, and this might be what you were trying to say but having deep domain knowledge is still the ONLY way to utilize AI in a professional manner.  This fact, alone, means quality devs will have to be in the loop, because no matter how efficient or fast, you need that expert intention to build quality software.

To be completely blunt, I don't see how less-than-quality devs won't be impacted. A very basic business example would be the impact on startup hiring.  If you have a few quality senior engineers who can now spit out boilerplate in a matter of minutes, why would the team ever scale up to a potential pre-LLM size?  The sad reality is, they won't and that efficiency driven by LLM may be a long-term trend within that organization.  Now, does the world need exponentially more software because if so, all devs might be good to go in the long run.",4
post20con,controversial,1.3747981746419655,highest,"This. LLMs will get better and do more. But if today you already feel replaceable by an AI maybe you ARE replaceable. Look. Any position has geniuses and morons, sinners and saints, humble and bold folk. Even if AI didn't exist, there is a subset of developers still in ""fake it till ya make it"" mode + who could have been replaced already. That's just how life goes. AI is an agent of change, but it didn't make change happen. That was just always part of life. 

Wake up folks. Are you replaceable by an AI bot tomorrow? Really? All your human capabilities and potential? Just like that?",2
post20con,controversial,1.3747981746419655,highest,"where the rubber meets the road is what the c suite executives believe and are willing to infest in (I mean invest in lol), 100,000 of thousands of tech workers are being fired for the 'ai god'.....now this is a bit premature and there will be much fallout which high priced programmers will be happy to fix for a large hourly fee of course :)

by then those ceos will have been fired for other ceos",3
post20con,controversial,1.3747981746419655,highest,I agree with everything with one exception. AI is actually pretty good at writing unit tests.,2
post20con,controversial,1.3747981746419655,highest,"I don't think the post was about that though. Of course they're not (yet) at a point where they can create complex backends or award winning designs but they do more than fine for basic gigs most web developers get. Which are things like designing a website for a local bakery or a barbershop etc.

And as a mainly backend developer, especially Claude can come up with designs I wouldn't be able to do myself if I spent a week. Couple weeks ago I was messing around and wanted to see what it could come up with for a page design for my webapp and the result made my jaw drop. It was at least as good as what a freelance designer would create for $50. And my app isn't that simple either. There were modals, quizzes, textareas and many different form elements on the page.

After seeing that I changed the way I start my new projects. I describe the page I want in detail to Claude and have it create the design for me. Then I put that design into a new route (I usually put it on 127.0.0.1/vision) and try to make mine look as good (better, if possible) than that. That way I'm also polishing up my design skills while not being completely dependent on it.",2
post20con,controversial,1.3747981746419655,highest,"I would agree with what you said.  I think part of the confusion I have is I have never really worked as a front-end dev or backend dev... I've always been full stack.  I have always been responsible for building everything from what the user sees and clicks down to the optimized databases and everything inbetween.

  
I know the industry shifted away from that, but it's what I've been doing for 20 some-odd years and I'm seeing the industry is shifting back that way this very moment. 

I definitely use AI to generate some basic details and designs and 100% agree it's good at doing that.",3
post20con,controversial,1.3747981746419655,highest,But in what way are WordPress and Shopify not already satisfying this market?,3
post20con,controversial,1.3747981746419655,highest,"Brother respectfully what are you talking about. 

I’ve played with Claude Sonnet 3.7 extensively 

All the designs it generates looks like they came from 2017. It’s still stuck on the flat design paradigm. 

At that point why not just get a template? Even the free ones are infinitely better than what Claude pumps out.",3
post20con,controversial,1.3747981746419655,highest,"> And my app isn't that simple either. There were modals, quizzes, textareas and many different form elements on the page.

There are no templates for this kind of thing. I described the business logic in detail and it returned a dashboard page that fits all my needs. Again, it's not winning any awwwards any time soon but I had it build many pages and most of them were pretty nice looking. The ones I didn't like were the ones I didn't give much attention to detail in the prompts so that could be it too. Idk, try some more detailed prompts maybe.

Though again I'm not specialized in frontend and I'm not a designer/artist. What looks great to me could be garbage to you",4
post20con,controversial,1.3747981746419655,highest,"Maybe you are playing with it wrong then?  Nah, couldn't possibly be your fault.  AI sucks.",4
post20con,controversial,1.3747981746419655,highest,"Maybe you are playing with it wrong then?  Nah, couldn't possibly be your fault.  AI sucks.",4
post20con,controversial,1.3747981746419655,highest,"That sounds very niche. Interesting, but niche",2
post20con,controversial,1.3747981746419655,highest,"Thats the whole point.  It's not a niche, it's just one example of thousands of business and enterprise apps that need to be built right now, right this very second.  These are the kinds of applications that developers need to start focusing on, and not static webapps with simple signup forms or the like.

I promise i'm not trying to be obtuse or a jerk, i'm just trying to share my viewpoint that there are literally thousands of companies and thousands of apps that AI is not going to build right now.",3
post20con,controversial,1.3747981746419655,highest,"100%.

It is indeed a shitty insight, but hopefully it serves as a wake up call for some people.",2
post20con,controversial,1.3747981746419655,highest,Yea pretty much. The only meaningful thing I’ve seen it do in enterprise is give better reasoning to laying off the terrible devs. Doesnt matter if AI code sucks if the dev code sucks as well. But all you have to do is be more than a coder.,2
post20con,controversial,1.3747981746419655,highest,Yes it will. That’s exactly what the hyperscalers and geospatial data brokers are selling to insurance and capital markets.,2
post20con,controversial,1.3747981746419655,highest,">I have yet to see AI replace or do any meaningful work in an enterprise environment or on an application that is more than just a simple frontend.

Yet. It's obviously heading that way, regardless of whether this will come by the LLM's alone or by introducing external assisting systems to handle the parts where an LLM on its own fails.",2
post20con,controversial,1.3747981746419655,highest,"Any real features? You building a dashboard or a website? Cuz if youre building ""real"" features it sounds like youre wasting a lot of time.",2
post20con,controversial,1.3747981746419655,highest,"True but it’s only a matter of time before models and apis come out that can increase the contextual awareness of the output. Currently, as many of said, it feels like autocomplete cause the tool is largely limited to looking at a single repo or service. But if they make it so you can broaden in input to include your backend etc it could get a lot better. 

I don’t think it’s coming overnight and it won’t be cheap. But it just has to be competitive with a human salary and it can completely undermine things. 

TLDR I wouldn’t want to be a junior dev right now let alone in 5 years. The job pickings are slim as is.",2
post20con,controversial,1.3747981746419655,highest,"Agreed, ive already started looking to change my career and my team is looking at how to most successfully phase ourselves out.  its a tough world",3
post20con,controversial,1.3747981746419655,highest,"I had a phone interview with OpenAI that didn’t go anywhere but I asked the recruiter “does the company have any policy around engineers coding themselves out of the job?” And they could only give me a trite “we make ai that helps people, not replace them” response. I would’ve been curious to see what the high level folks later in the interview process would’ve said but then again asking that kind of question would probably lose you the job! 🤣",4
post20con,controversial,1.3747981746419655,highest,"What exactly even is ""basic CRUD?"" Do you mean the final coding step after someone has figured out the project requirements, consulted all the stakeholders, determined the data models and workflows to be implemented, mapped out the how these elements will interact across multiple applications, deployed the infrastructure necessary to run it, implemented a comprehensive security policy, and described the rest in a way that a kid fresh out of school can understand so that they can implement a bit of UI around it?

I guess that's technically ""basic CRUD."" It's also something between 1% and 5% of the total work in any sort of moderately complex system.

The way I see it, talking about basic CRUD is about as useful as saying all programming is implementing some branching logic in an environment that can be described as a Turing machine. Practically everything a programmer does is going to create, read, update, and/or delete stuff, often across some sort sort of communication channel, backed by one or more data store of some sort. It's more a description of the environment than anything else. Figuring out all the things you're going to CRUD, and how all the information is going to transform and mutate in the progress is the hard part. Everything from building a website, to training an ML system, to implementing that service bus for risk portfolio calculations is going to involve these operations.

The whole AI is going to replace programmers thing seems to be largely kids fresh out of school, that don't realise that most of the ""programming"" they are doing is just menial busy work that the seniors give them so they have a chance to explore the problem domain a bit, before being given actual tasks. That and hobbyists that spent a few months learning to code, and then decided that they are actually master system architects because they managed to wire together 10 or 20 files that run a chatbot or something of the sort. 

These people have suddenly gained access to a tool that can understand the thing they're working on about as well as an expert that's never touched a particular codebase, but they don't have the context to realise that such an expert would need to spend a few months getting up to speed on everything before being confident enough to actually make any significant changes. They just see hundreds of lines getting generated, and figure that those lines are just as good as any other. It's sort of like deciding that some off-brand glue was good enough to hold structural components of a truck together, without understanding why most other people prefer to use mechanical fasteners for the job.",2
post20con,controversial,1.3747981746419655,highest,"what the christ is happening here

basic crud = submit a form to a post endpoint

non basic crud = tons of validation routines, business logic for dynamic drop-downs, permissions and validators for enable and disable, roles and rights management, and then all the stuff on the backend to process the result that isnt just dumping it into a database.


there is a difference, and its simple.  This is just high level from my phone because this is just too much to explain for something simple to understand",3
post20con,controversial,1.3747981746419655,highest,"My point is that ""basic CRUD"" isn't actually a thing that exists in a professional environment, outside of some boot camp or some trash tier off-shoring group somewhere. 

If you're in a real job doing what you define as ""basic CRUD"" then you're just working in the context of the things a lot of other people did. Just because you don't know about the other things that must happen, doesn't mean that these things don't happen, and that they won't affect the code you write. Eventually you'll have to deal with them, even if only because your ""basic CRUD"" isn't working.

You might as well talk about ""basic conditional logic"" or ""basic functions."" It's a meaningless distinction, because it's describing a tiny part of what the job entails. If you're actually doing this professionally, you simply aren't going to be doing much ""basic"" except when you're just starting out.",4
post20con,controversial,1.3747981746419655,highest,I spent the last 3 days fixing the fuckups of a colleague who blindly trusted AI to do his work… he’s never been a good programmer but AI has only increased his efficiency in fucking up lol,2
post20con,controversial,1.3747981746419655,highest,"Don't be like nokia AI is going to take most of the low level jobs, after that the middle level jobs, after that senior level jobs until a super AI computer is estabilished that can do anything. The one who owns that super computer will be a billionaire like bill gates owning microsoft in the 90s.",2
post20con,controversial,1.3747981746419655,highest,"i am terrified, what do you suggest I do to make sure im the billionaire?!?!?!",3
post20con,controversial,1.3747981746419655,highest,In a year it’s gone from useless to replicating entire applications in one shot. It’s even making games and AI agents to play said games… this is the worst it will ever be. If you think it won’t be able to crack basic maintenance and enterprise level systems soon. You are simply mistaken… most devs are just copy and paste bots from stackoverflow. It’s been the meme for the past 10 years at least.,2
post20con,controversial,1.3747981746419655,highest,"100% agree, man!  I actually sat down with my boss today to come up with a plan to step a phase out of 4 of our 6 developers.

After playing with cursor and 3.7 we see the value.   We expect a reduction in staff within less than 6 months.

I am stoked, my team budget is going to be so much leaner but in theory have the same productivity.


Im with you, man, AI is the shit and human devs are on the way out.",3
post20con,controversial,1.3747981746419655,highest,"??? 180 and changed tone. Can’t tell if you’re taking the piss. I worked fintech where we’re getting 300k a year inc bonus and options. We will definitely be replaced within a few years. Me and my team have cut all our stupid spending to prep. Good luck to all devs. But if you aren’t in the top 1 percent that are actually pushing the boundaries (researches, phds etc) your work is replicable by an AI.",4
post20con,controversial,1.3747981746419655,highest,Ai is doing meaningful work in our company and is at the core of what we do. However it's a block of our product and doesn't replace any devs. It just made our idea possible. Cannot go into details as it's sensitive tho.,2
post20con,controversial,1.3747981746419655,highest,[deleted],3
post20con,controversial,1.3747981746419655,highest,"Except the growth is blocked by the fact they use large language models and not true Ai. It's machine learning masquerading as Ai. 

I researched it and the easy gains are maxed out(data and brute force computing power). It's not like Moores law.",4
post20con,controversial,1.3747981746419655,highest,[deleted],4
post20con,controversial,1.3747981746419655,highest,"Sorry, but this feels like massive cope. AI will absolutely be able to replicate that, it's just not there yet. Anyone that's used Claude 3.7 will tell you that it can indeed do some insane tasks already. Combine that with copilot integration, Claude code, or cursor, and yeah... We're entering the phase where it's starting to materially impact workflows, even the complex ones. Speaking as a full stack developer at a large enterprise. We're at the opening phase right now. Give it 10 years at most (if not 5), and the entire field of development is going to be drastically different from current day. There's WAY too much money on the table for executives to not exploit this as much as they can to reduce workforce numbers and increase profit. They'll find a way to do it.",2
post20con,controversial,1.3747981746419655,highest,[deleted],2
post20con,controversial,1.3747981746419655,highest,"I definitely use it for writing tests in our Angular project, thats the truth!",3
post20con,controversial,1.3747981746419655,highest,tell me a feature that's not based on crud. I'll wait.,2
post20con,controversial,1.3747981746419655,highest,"I mean, yeah, 99.9% start with crud, but it can very quickly diverge from there with what it does with the info.  5 textboxes and a checkbox can be all it takes to kick off a calculation resource or generate complex financial reports, for example.

Not sure where you were going with this, dude.",3
post20con,controversial,1.3747981746419655,highest,If you're letting an AI develop blocking code on your async app then you fucked up long before,4
post20con,controversial,1.3747981746419655,highest,"\> AI isn't going to design, setup, and build your service bus that manages your mapping engine job scheduler which then calculates risk portfolios across Florida roof maps.

Claude can absolutely do that. And so can the new Gemini. You have no idea what you are talking about or you are just in denial.

I have been programming for basically 40 years and I think it's asinine to try to write programs without a SOTA LLM and coding agent/environment these days. Of course it still needs help and I prefer to give it my own architecture rather than let it dictate it for a lot of things,  but the best models absolutely can design (and setup whatever using tool commands or computer use). Yes it still needs help sometimes but it can do 80-95% of the work for applications as complex or more complex than the one you gave in the example.

And will continue to get better.",2
post20con,controversial,1.3747981746419655,highest,"Damn, you are right, I just tried claude code and it literally just replaced me and 4 other devs.  This is bonkers, we are all truly fucked.",3
post20con,controversial,1.3747981746419655,highest,Ha! Give it a few minutes. It’s over dude.,2
post20con,controversial,1.3747981746419655,highest,"Sure man, whatever helps you sleep at night",3
post20con,controversial,1.3747981746419655,highest,👍,4
post14con,controversial,1.3639907689457174,highest,"It's already happening, as he presents his outlook.
The biggest Fortune 500 companies are freezing hiring, while at the same time, increasing investments into AI agents.
As they developed strategies to replace human workers with AI agents, in everything from code writers to engineering.
Many sales positions as well as customer service Representatives.
Even Wall Street isn't immune from this. Jobs are being replaced in masses.
Why so shareholders can make even more money by saving on labor costs.
The bottom line is more important to the wealthy investors.
While all the AI companies are reaping massive investments from the ultra rich. 
The amount of money being invested is staggering, all with the ultimate intention to increase profits and reduce the labor force. 
We don't have to wait a few years for this to affect the average person, it's already started the tsunami is here. The first wave is crashing ashore. 
People like Sam Altman and Elon Musk, Jeff Bezos, companies like Meta and Tesla Amazon and Open AI are reaping the benefits, while the average worker will not have a job in two years. If you work in the majority of services industry including working for top Fortune 500 companies.",1
post14con,controversial,1.3639907689457174,highest,but...  who buys their product when no one has a job?,2
post14con,controversial,1.3639907689457174,highest,What you are missing (maybe) is that they are not thinking about what happens if every corporation does this. Instead they are just thinking about how their decisions will look on the quarterly balance sheet that goes to the board and shareholders.,3
post14con,controversial,1.3639907689457174,highest,"then they are not, strictly speaking, rational.

this is like all 100 customers stampeding to get into the 'short line' at the checkout. smart for one,  dumb for all.",4
post14con,controversial,1.3639907689457174,highest,"I think they are mostly thinking: what if my competitors do this first and we go bankrupt because we can't compete? 

What do they care about the consequences of everyone doing it if they feel they'll disappear on the shorter term if they don't do it?",4
post14con,controversial,1.3639907689457174,highest,"Exactly and this is called Game Theory.  “If I don’t do it, one of my competitors will and gain an advantage so I might as well do it to”. It’s precisely things like this that need to be regulated because of this psychological phenomenon and the implication",4
post14con,controversial,1.3639907689457174,highest,"Keep ai for scientific use. It was too early.

The problem lies in greed, abolish money first then release ai for everyone.",4
post14con,controversial,1.3639907689457174,highest,And probably not thinking past the next couple of quarterly earnings reports,4
post14con,controversial,1.3639907689457174,highest,"They will figure that out when they get there. Or at least, that’s the thought process. Right now there is an AI gold rush, and any executive arguing for anything other than aggressive pursuit of it will get axed quickly.",4
post14con,controversial,1.3639907689457174,highest,"Well it had to end somehow.  To be by short sighted greed seems poignant.  

See you all at the going away party",4
post14con,controversial,1.3639907689457174,highest,True.  The long game is not typically the domain of the greedy and the criminally insane...,4
post14con,controversial,1.3639907689457174,highest,"God... how I've learned to hate the ""quaterly cult.""",4
post14con,controversial,1.3639907689457174,highest,"THIS. The ruin of our version of capitalism comes largely from this. Capitalism itself is not evil. It’s a market competitively supplying goods and services to a demand, for a profit. But serving the corporations at the expense of the consumers and employees and state, giving corporations legal personhood, constantly trying to exceed unreasonable expectations to benefit shareholders, and managing by spreadsheet have ruined it. 
We need other metrics for success like how many employees are healthy and happy, able to survive and educate themselves, and their kids, what has been committed to the welfare of their localities, etc. Use the greed of the execs and give more tax incentives for this kind of thing and it might improve a little.",4
post14con,controversial,1.3639907689457174,highest,"Well that's where the credit card companies step in.


Here's how I know a.i. won't be good if it's the one making all the decisions then it should realize the easiest way to make a huge profit is cutting from the top.


What's the point of a CEO of all of the decision are made by a.i.",3
post14con,controversial,1.3639907689457174,highest,"“The development of modern industry, therefore, cuts from under its feet the very foundation on which the bourgeoisie produces and appropriates products. What the bourgeoisie therefore produces, above all, are its own grave diggers. Its fall and the victory of the proletariat are equally inevitable.” -Karl Marx",3
post14con,controversial,1.3639907689457174,highest,"Except that, theoretically, automation would allow the bourgeoisie to exist *without* a proletariat. If robots do all the work and make all the products, then the people who own the robots can have anything they want for free, and the rest of humanity can simply disappear.",4
post14con,controversial,1.3639907689457174,highest,"First two sentences, solid gold.  Third sentence, unwarranted optimism / millennarist fantasy.",4
post14con,controversial,1.3639907689457174,highest,"You just found out what Karl Marx figured before automation was called automation. [https://thenewobjectivity.com/pdf/marx.pdf](https://thenewobjectivity.com/pdf/marx.pdf) Because I like to be funny I used automation to write this summary.

>Marx argues that machinery creates a fundamental contradiction for capitalism because it simultaneously tries to reduce labor time while relying on it as the source of value. Here's how it breaks down: On one hand, capitalism, driven by competition, uses machines to make production more efficient, cutting down the amount of labor needed to produce goods. **This is good for capitalists because it lowers costs, increases productivity and increases surplus labor time**, enabling them to produce more goods for sale and increase profits. But, on the other hand, capitalism depends on labor time to measure value. **The more machines replace workers, the less labor is directly involved in making things, and the more difficult it is for capitalism to make a profit**. So, capitalism ends up in a bind: it needs to reduce labor to maximize profits, but at the same time, it relies on that same labor to generate value. This leads to overproduction, and the system becomes unstable, because the value is not being generated at the same rate by the labor that has been replaced by machines.

To be funnier, here's an AI generated podcast about it. [https://notebooklm.google.com/notebook/781b78aa-a1cf-4dd1-8a4a-8ff1096b4556/audio](https://notebooklm.google.com/notebook/781b78aa-a1cf-4dd1-8a4a-8ff1096b4556/audio)

You can do this with NotebookLM, just upload the PDF as a source and you can ask it questions and it will cite sections from your sources.",3
post14con,controversial,1.3639907689457174,highest,"Really funny how many people use the term ""late stage capitalism"" who also get upset about AI. Automation (reducing the absolute number of laborers total) is literally the thing that Marx says will cause a revolution and the collapse of capitalism.

""**A development of productive forces which would diminish the absolute number of labourers,** ***i.e.*****, enable the entire nation to accomplish its total production in a shorter time span, would cause a revolution**, because it would put the bulk of the population out of the running. This is another manifestation of the specific barrier of capitalist production, showing also that capitalist production is by no means an absolute form for the development of the productive forces and for the creation of wealth, but rather that at a certain point it comes into collision with this development."" - Capital, Vol 3, Ch 15

He also says this is inevitable and unavoidable due to competition:

""No capitalist ever voluntarily introduces a new method of production, no matter how much more productive it may be, and how much it may increase the rate of surplus-value, so long as it reduces the rate of profit. Yet every such new method of production cheapens the commodities. Hence, the capitalist sells them originally above their prices of production, or, perhaps, above their value. He pockets the difference between their costs of production and the market-prices of the same commodities produced at higher costs of production. He can do this, because the average labour-time required socially for the production of these latter commodities is higher than the labour-time required for the new methods of production. His method of production stands above the social average. But competition makes it general and subject to the general law. **There follows a fall in the rate of profit — perhaps first in this sphere of production, and eventually it achieves a balance with the rest — which is, therefore, wholly independent of the will of the capitalist.**"" - Capital, Vol 3, Ch 15

And how does he feel about the machinery itself?

""**It took both time and experience before the workpeople learnt to distinguish between machinery and its employment by capital, and to direct their attacks, not against the material instruments of production, but against the mode in which they are used**. The contests about wages in Manufacture, pre-suppose manufacture, and are in no sense directed against its existence. The opposition against the establishment of new manufactures, proceeds from the guilds and privileged towns, not from the workpeople."" - Capital, Vol 1, Ch 15",4
post14con,controversial,1.3639907689457174,highest,"I feel like I have to explain this a lot: they don't care. Companies these days only think about a quarter or three ahead. They legit do not care about the long term.

It's the MBA/corporate raider mentality and it's basically the standard amongst the managerial/c suite class in America. They've been educated to think operating ratios are like THE most important thing and it's reenforced by the investor incentive structure. You're rewarded based on quarterly performance, which means cost cutting is valued basically the same as improving the business or product and is MUCH easier to achieve.

Which should be obvious given how many of them think the US rail industry is super good (because they have really insane ratios) when in reality it's the corpse of a whale who died mid-swim and hasn't quite hit the bottom yet.",3
post14con,controversial,1.3639907689457174,highest,"I just had to award you not only for the very accurate description of the fundamental problem with capitalism, but for that last graf and metaphor which was solid gold -- solid gold example, solid gold analysis, brilliant metaphor which I will probably steal at some point.",4
post14con,controversial,1.3639907689457174,highest,They just want to see people suffering and getting dependent on them.,3
post14con,controversial,1.3639907689457174,highest,"The elites don't need money if the machines they command provide any labour they desire, so they don't need customers. Money will fall out of the picture.",3
post14con,controversial,1.3639907689457174,highest,"The rich. It is not necessary to sell products to the working class, so there is no reason why the economy cannot shift to address mostly the wealthy’s needs.",3
post14con,controversial,1.3639907689457174,highest,[You got it](https://youtu.be/MYB0SVTGRj4?t=203).,4
post14con,controversial,1.3639907689457174,highest,"you're thinking late feudal?  the consumers are the 1 percent, everyone else labours to produce wealth for them to hoard and consume?  big retooling needed to get back there, but obviously they are working on it.",4
post14con,controversial,1.3639907689457174,highest,I agree with your sentiment but look at civilizations throughout history - a wealthy ruling class and poor masses is the default setting.,3
post14con,controversial,1.3639907689457174,highest,They tend to fail in this exact fashion as well,4
post14con,controversial,1.3639907689457174,highest,"Only within societies which we have dubbed ""civilizations."" These structures were by no means inherent across all of humanity, nor a natural one.",4
post14con,controversial,1.3639907689457174,highest,"Money is exchanged for goods and services. If they have good enough AI, they don't need humans to get the things they want, and that includes buyers as well as employees.

The more clever industries will shift to automated modes of existence. Those catering to human beings will shrink and shrivel as the human being becomes increasingly destitute.

I'm sure the CEOs will cheer as productivity increases, as I'm sure the shareholders will cheer when they can replace the CEOs with far more obedient and clever AIs, ones that can invest and become shareholders as well.",3
post14con,controversial,1.3639907689457174,highest,"Universal income funded by the corporations, we will basically be work-free slaves.",3
post14con,controversial,1.3639907689457174,highest,"You guys still think money and capitalism are end goals?

They are tools to redirect power and control.

You don't need them anymore once you accumulated enough power and control to use more..direct tools.",3
post14con,controversial,1.3639907689457174,highest,Other corpos doing the same thing?,3
post14con,controversial,1.3639907689457174,highest,They’ll just sell and ship their products to wealthier countries,3
post14con,controversial,1.3639907689457174,highest,"its not their job to ensure poeple in general have money. their only job is to ensure adding value to share holders. 

the govt will have to figure out ways to allow people to afford food [UBI]",3
post14con,controversial,1.3639907689457174,highest,Not to mention the economic affect it will have in major cities. If AI truly replaces people mass layoffs will happen and high skilled workers will have to shift industries and move out of tech hubs,3
post14con,controversial,1.3639907689457174,highest,"Corporations don't care about that anymore. They care about how they look at the stock exchange. And that's something that has little to do with how much they sell. It's not about value anymore, it's about beautification.",3
post14con,controversial,1.3639907689457174,highest,"Down the line but we're going to have to live through potentially many years until society is willing to change. During the transition many, likely most, are going to just have to eat the consequences and spend their savings while the rich get massively richer. Or maybe not! Maybe everything will be fine!",3
post14con,controversial,1.3639907689457174,highest,"It doesn’t matter if the money is valuable. It’s about getting all of it and having more than your fellow man,  not spending it.",3
post14con,controversial,1.3639907689457174,highest,The government that they own.,3
post14con,controversial,1.3639907689457174,highest,They’ll take over the government and funnel tax money into subsidies.  They will make deals with each other hyping the deals and pump their stock. People will invest those stocks and increase the worth of the companies while taking some profit to buy the services and products of the same companies.  Your income will go down but your investments will go up until something collapses. the government will bail out those who are in charge.  Rinse repeat dystopia.,3
post14con,controversial,1.3639907689457174,highest,"Exactly. And ""AI Agents"" will lead to customer frustration, it's a huge opportunity for China to fill the blank with actual humans providing actual service. Tesla as a car company is mostly already dead, they just don't know it. you can get a comparable electric car from a china brand at a fraction of the price, that is why tariffs are all the talk. they aren't there to help the voters or fight China but to preserve status quo.",3
post14con,controversial,1.3639907689457174,highest,Perhaps AI consumers order stuff from AI producers without anything being produced and the money is just shuffled from corporation to corporation and companies manage to include a tax break.,3
post14con,controversial,1.3639907689457174,highest,Also wtf is the product.,3
post14con,controversial,1.3639907689457174,highest,"“Capitalism slits its own throat” 
-paraphrasing Marx",3
post14con,controversial,1.3639907689457174,highest,"You stop that right now, that’s entirely to much thought, nothing exists outside of Q1 you ignorant swine. Maaaaybe Q2 but that’s.. that’s oretty out there",3
post14con,controversial,1.3639907689457174,highest,Unfortunately: [other wealthy people](https://youtu.be/MYB0SVTGRj4?t=203).,3
post14con,controversial,1.3639907689457174,highest,"They will change HOW they profit from individuals rather than conventional money transactions. If we are talking about retail it will change what they are selling and how people are consuming it. Data which can be sold for example like social media profits immensely from 

The top companies will always always always be ahead of the curve so the new argument I see here a lot of ""what happens when nobody has money to buy things"" will always be irrelevant because to the companies who are able to adapt and adjust people will always be a commodity with or without money",3
post14con,controversial,1.3639907689457174,highest,They will look for government handouts,3
post14con,controversial,1.3639907689457174,highest,"Well, by then they will have sold out enough shares to buy things that hold value through a recession, depression, and economic collapse. Food, agriculture, real estate, water, food/water processing, energy, technology, ""defense,"" and medicine all have fundamental value. They will own and be able to defend large amounts of that.

Money is just an exchange medium, you can still leverage promises for the future. Power is power.",3
post14con,controversial,1.3639907689457174,highest,"That's where basic universal income comes in.

People have just forgotten that the idea is inherently capitalistic.",3
post14con,controversial,1.3639907689457174,highest,They sell to themselves and upper middle class whales/ DINKs that maintain jobs due to their place as PMCs or as engineers. (aka most of reddit),3
post14con,controversial,1.3639907689457174,highest,"I swear people always make this argument and they miss how for hundreds if not thousands of years there were peasants and kings.


Did the kings need the peasants to buy things? No. You got taxed, used and abused.


There are no jobs? You'll be sent to wars or to Mars to set up shit and die there. They'll find a way. You are not protected because at this specific moment they are after your wallet. They are just keeping the status quo until they can stop pretending you were ever in control. I love how elections keep the illusions going, as if it's not always a rich guy bought and paid for from one of two or three parties lol",3
post14con,controversial,1.3639907689457174,highest,"What a weird and fundamentally wrong take. 

Taxed of what, if I don't have anything? 

Medieval society wasn't some pop-culture idea of dystopia - peasants kept a large share of what they produced, so they could reinvest it into trade (either as small-scale merchants themselves or by selling their excess produce to organized merchants). Whenever this system broke (due to war, excessive taxation or natural disasters like famine or plague), this universally led to a collapse of the society in the local area (usually a violent collapse). 

Moreover, relationship between feudal and peasantry was usually regulated by charters and laws, which specified obligations of both sides of social contract.

Unironically, most medieval societies had a much better grasp of sustainability than modern emergent oligarchies.",4
post14con,controversial,1.3639907689457174,highest,The 20th century was a historical aberration in almost every way. We are reverting to mean.,4
post14con,controversial,1.3639907689457174,highest,Immigrants and foreign workers maybe? And China or India?,3
post14con,controversial,1.3639907689457174,highest,"I’m exec level in a huge company and can confirm. Junior to mid levels frozen as our upper management “wait and see” how we can have AI do their jobs (I live in Germany where hiring someone is essentially a life long marriage). 

It scares me because we are witnessing the death of critical thinking. These AI agents won’t push back on managements dumb and politically driven ideas. And our younger population is increasingly delegating their information synthesis to computers. 

Easier people to control and influence by those with the means.",2
post14con,controversial,1.3639907689457174,highest,[removed],2
post14con,controversial,1.3639907689457174,highest,"It won’t work but the Executives won’t ever admit they were wrong and will pretend not to understand sunk cost

As long as they can fuck over labor it’s worth the cost",3
post14con,controversial,1.3639907689457174,highest,[removed],4
post14con,controversial,1.3639907689457174,highest,"I work for a top fortune 50 company and we're still using ancient tools and software from 25 years ago, there's no way in hell they'd survive a day trying to replace people with AI. They probably couldn't even afford the AI and if they did everything would just break instantly. Our company would need to completely overhaul literally everything before AI would even be compatible with its systems and it can't afford to do that.",3
post14con,controversial,1.3639907689457174,highest,"I keep trying to use it because I want it to be useful to me. I want to get more done and do less work.

I actually asked it how to use its own API and it straight just made shit up. Gave me some fake instructions that looked correct 🙄.

Yeah I don't think they'll be replacing my job any time soon. I'll get plenty of work unfucking the mistakes it makes I'm sure.",3
post14con,controversial,1.3639907689457174,highest,"We are literally at infancy stage. Only a couple of years in. There is virtually no chance that this is as good as it gets and there will be no improvement from here on in. 

So maybe it won't happen for 10 years or 50......but it will happen at some point and the same problems will arise. Better for us to be prepared and talking about it now.",3
post14con,controversial,1.3639907689457174,highest,">We are literally at infancy stage. Only a couple of years in.


We are many decades into the research. There's a lot of hard work to get us to this point. What is visible may only be a few years in, but it's been going on a lot longer underneath the surface.",4
post14con,controversial,1.3639907689457174,highest,"We will have vastly worse problems in 50 years due to collapsing global ecosystem.  Extreme weather will be far more extreme and will have a major impact on global food supply.

Gonna get really ugly",4
post14con,controversial,1.3639907689457174,highest,"We're already decades in to machine learning research, we're only in the infancy (although honestly id argue we're well into) the latest hype cycle. This happens every few years in ML, it is literally taught in schools this cycle. Look up AI winter",4
post14con,controversial,1.3639907689457174,highest,[removed],4
post14con,controversial,1.3639907689457174,highest,"For now, anyways. We know intelligence is possible, so automating it is posible too. We just haven't come up with the right architecture, but every passing year we are closer. If Large language models and transformers don't pan out, that just delays the problems here presented.",3
post14con,controversial,1.3639907689457174,highest,"Oh, sweet summer child.

Over the last six months, we (F500) started letting go of our frontend devs because upper management realized that an architect paired with AI outperforms an architect paired with a frontend dev on every KPI imaginable. They were even offered training to transition from being an ""Angular Andy"" to someone skilled in system design, solution architecture, and the like. Less than 10% bothered with those learning paths, brushing it off as fearmongering from the suits.

Ironically, the same ones who spend four hours a day on Stack Overflow just to get their shit going, and need two hours of meetings every day so I can explain for the fifth time that week how I want my REST API structured, were the ones who thought they were absolutely indispensable. ""I don't worry, it's just a stochastic parrot"". Hilarious.

I know every dev on Reddit thinks they're the smartest mf ever, but out of the hundreds of devs I’ve had to manage so far, 80% are easily replaceable, and are getting replaced. Their actual dev skills didn't match their inflated ego at all. Like, we even did workshops showing what SOTA AI can do, and how I create a production-ready app in a fraction of the time... then those fucks accused me of staging my demonstration. Holy shit. I hope the parrot teaches them some humility.

You can also see it in the tech subs how everybody is ""it won't ever replace me"" while in the same sentence admitting their horizon just goes up to ChatGPT. So basically, they don’t know shit about AI at all except chatting with some mainstream chatbot, but think they have some kind of authority on the topic. This is going to be a rude awakening for some.

Meta stopping hiring mid-level engineers and us letting them go is just the beginning. But even news like that get brushed off like, ""Meta doesn’t know what it’s doing. They’ll hire them again next year"". Mindblowing cognitive dissonance... hallucinations worse than an open-source LLM running on a Raspberry Pi. But at least the LLM is capable of learning.

I realized my professional days were numbered back when the transformer paper was published. I was reading it with some colleagues, and all five of us in that room instantly knew what this paper meant (or at least we had an idea... being 100% sure of it came in 2020 after the GPT-3 paper dropped). That was long before anyone even knew what an LLM was... seven years ago. Those exact frontend devs who aren’t with us anymore were the ones laughing the loudest at my ""fear of parrots"".

Well, thanks to my paranoia, I have absolutely no problem with getting replaced in 3–5 years or whenever. Finally, I’ll have time to do whatever I want and pursue some of my hobbies. Perhaps I’ll even keep some pet parrots.",3
post14con,controversial,1.3639907689457174,highest,"Custom OpenAI solutions with datasources configured and memory systems, are whats doing the heavy lifting, they can replace an awful lot of stuff with it",3
post14con,controversial,1.3639907689457174,highest,[removed],4
post14con,controversial,1.3639907689457174,highest,"Any serious company looking into AI for their future is developing their own customised AI systems, they aren't using off the shelf solutions.

I think its really important for people to know that, because all they have read are news articles saying how they are firing employees and using ChatGPT which is generally not the case except for the companies doing it for the AI buzz words.

In other words, there are employees working right now on automating jobs, its just a matter of time until they are complete, they don't have to wait for ChatGPT to do it for them.",3
post14con,controversial,1.3639907689457174,highest,"What? What python web app are you talking about that costs too much money? 

I feel like people who have this opinion should really read about the frontier of research - people who are aware of what is on the frontier have a VERY different opinion than this. I don't mean me. I mean research scientists, ethicists, economists etc.

That's not to say that they all agree with what will happen, but the idea that these models are not capable and not getting rapidly better is inexistent in those discussions. 

Look up o3, then look up frontier math, swebench, arcagi etc. if you don't know what any of these things mean, ask an llm that can search the Internet because most of this is too new for it to be in the training data. Swebench and arc agi excluded, but definitely the interplay between them all.

Long story short, shit is getting very very real.",3
post14con,controversial,1.3639907689457174,highest,[removed],4
post14con,controversial,1.3639907689457174,highest,"? It's useful to have some context here. AI code assist absolutely does work and does increase productivity.  Will it completely replace mid levels this year? No. Will it allow one mid level so the job of 1.3 mid levels? Probably. 

Also keep in mind chatGPT was released in late 2022. LLM really didn't explode until mid 2023.

We're about 2 years in.. it's reasonable to think that in another 2-5 years the world will be very very different. 

At this point I'm more worried about AI turning our world into a dystopian corptpcracy than I am about climate change.",3
post14con,controversial,1.3639907689457174,highest,"This will backfire so horribly that it would be hilarious if it wasn't so serious. Imagine creating almost overnight a new class of millions of unemployed people, used to having a job and living comfortably and suddenly destitute. 


It will be the french revolution all over again.",2
post14con,controversial,1.3639907689457174,highest,"Tbh, maybe this will just speed it up so we don't have to watch another 40 years of slow decline where people barely notice.",3
post14con,controversial,1.3639907689457174,highest,"If it happens slowly enough maybe the system will balance itself out with the demographic decline, I'm not sure what would happen in that case.",4
post14con,controversial,1.3639907689457174,highest,"Don’t worry, they are developing armed AI managed drone swarms to manage that future problem.",3
post14con,controversial,1.3639907689457174,highest,"I wish i could just laugh at that. However, it doesn't matter how bloody it gets, in the end, numbers do matter.",4
post14con,controversial,1.3639907689457174,highest,"I keep thinking about the Butlerian Jihad.  ""Thou shalt not make a machine in the image of a man's mind.""  Herbert has his bizarre aspects but he was weirdly prescient \[joke intended\] in some ways.",3
post14con,controversial,1.3639907689457174,highest,"Isn't it possible that the hiring freezes have more to do with global macroeconomic trends?

Like the higher interest rate environment pushing investors back to bonds, and relatively low investor confidence forcing businesses to consolidate and put off larger hiring plans because there's actually *less* appetite for risky investments than in the past few years.",2
post14con,controversial,1.3639907689457174,highest,They’re not freezing hiring because of AI. The fearmongering is starting to sound like a broken record…,2
post14con,controversial,1.3639907689457174,highest,They’ve got nothing new. I’ve been reading the same frantic screeds here in r/technology for over three years now,3
post14con,controversial,1.3639907689457174,highest,[removed],3
post14con,controversial,1.3639907689457174,highest,Because they didn’t over-hire during the pandemic like tech companies did.,4
post14con,controversial,1.3639907689457174,highest,"Replace the executives. This means the disenfranchised will have to take up entrepreneurship on their own, also using AI to cut down on start up costs. It’s not ideal, but there’s not much else the lower and middle class can do.",2
post14con,controversial,1.3639907689457174,highest,"Dotcom bubble 2.0 is going to come when investors start noticing that adding AI into everything doesn't actually increase sales or revenue, once the stock sell off starts it won't stop.",2
post14con,controversial,1.3639907689457174,highest,[removed],3
post14con,controversial,1.3639907689457174,highest,"Just because it has a large user base doesn't mean it's currently generating profit, while it's generating revenue, unless you turn profit you can't pay your shareholders dividends in which they expect.

At some point they will start selling their stocks / shares to invest into other things that are turning profit.",4
post14con,controversial,1.3639907689457174,highest,Is that why he works for a company to profit from the process.,2
post14con,controversial,1.3639907689457174,highest,"""Once men turned their thinking over to machines in the hope this would set them free. But that only permitted **other men with machines** to enslave them. """,2
post14con,controversial,1.3639907689457174,highest,Whose the average worker ?,2
post14con,controversial,1.3639907689457174,highest,total scare mongering. Please reread this post in 5 years and see if i was wrong.,2
post14con,controversial,1.3639907689457174,highest,Remindme! 2 years,2
post14con,controversial,1.3639907689457174,highest,"> developed strategies to replace human workers with AI agents

Please, name one company where such strategy has actually worked.",2
post14con,controversial,1.3639907689457174,highest,They’re not freezing hiring because of AI.,2
post14con,controversial,1.3639907689457174,highest,"At a certain level it almost feels like being a US *citizen* is sort of pointless. It only serves you if you’re in the ownership class, being a regular citizen it almost feels like these entities are actively spiteful of your existence. 

As a loose example, I got a doughnut from Dunkin (formerly known as Dunkin DONUTS) and it was so fucking dry and stale and had basically a single drop of frosting spread into a micron-thin veneer. Biting into it felt like I was biting into the middle finger of the board of directors. Like they’re mad at me for having the audacity to even request a fucking doughnut before I give them any money, and I should have just given them that money for nothing.",2
post14con,controversial,1.3639907689457174,highest,How do we know that this is putting people out of work? Unemployment went down in December. https://www.cnbc.com/amp/2025/01/10/jobs-report-december-2024.html,2
post14con,controversial,1.3639907689457174,highest,"It looks like you shared an AMP link. These should load faster, but AMP is controversial because of [concerns over privacy and the Open Web](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot).

Maybe check out **the canonical page** instead: **[https://www.cnbc.com/2025/01/10/jobs-report-december-2024.html](https://www.cnbc.com/2025/01/10/jobs-report-december-2024.html)**

*****

 ^(I'm a bot | )[^(Why & About)](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot)^( | )[^(Summon: u/AmputatorBot)](https://www.reddit.com/r/AmputatorBot/comments/cchly3/you_can_now_summon_amputatorbot/)",3
post28con,controversial,1.3436120224105876,highest,"> So first let's look at what happened so far, let's use the US as an example. 50 or 60 years ago the middle class in the US was actually bigger than it is today. Since then income inequality has significantly increased. 

This is where you lose me.  The ""middle class"" is a wholly invented construct.  It developed as a way to describe the people who were not rich but also not poor, but also not working class.  It's an inexact classification with little utility.

Income inequality has risen, yes, and the ""middle class"" has shrunk in the United States.  Worldwide, poverty has plummeted as well. As much of that is literally true, however, it's because the middle class are becoming the upper class in the United States and we're finally addressing third-world poverty.  Clearly, the rise in wealth inequality is not making any  of those things *worse*, so why are you bringing it up?

> But so that bring me to my main point, which is that technological advancement will most likely relatively soon reach a critical threshold, which will cause most human labor to lose its value, not just low-level labor.  If we consider how much technology has progressed in just the last 10-20 years, if we consider how rapidly AI has progressed in just the last few years, then we can only dream about how hyper-advanced society will be in say 25 years of 50 years.

This argument crops up every single time a new technology hits the market.  In case you missed it, LLMs are *not good at what they do* in a lot of ways.  It's not on track to replace much of anything given how relatively stagnant the whole thing is.  Given the hallucinations and what have you, we're a ways from generative AI, and even that won't be ready for prime time on release.

Microsoft Excel didn't make accountants redundant.  ATMs didn't kill the bank worker.  The luddites have never been correct.

> But once AI reaches a certain point, the capitalist class will have no more use for the vast majority of the human population, except for a tiny minority of exceptionally gifted, exceptionally intelligent and exceptionally motivated group of extremely high-level workers who AI and automation cannot yet replace.

We're all the capitalist class, friend.  Capitalism won.  The world has never been more prosperous, and its people more better off, than it has under capitalism - especially following the fall of the Soviet Union.  

We're all capitalists.  We have come to the understanding that markets are the best way to distribute goods, that supply is the primary economic driver, that economic freedom is as important as any other.

The *most likely* worse case scenario is that AI displaces a nontrivial number of jobs and the people it replaces do something else, just like they have every other time some seismic technological advancement occurred.  It's highly unlikely that this would occur, either.",1
post28con,controversial,1.3436120224105876,highest,"Great answer. Small nitpick and a comment. 

I’m a dev. I own a dev company. We weren’t hiring at breakneck pace to begin with —I look for real talent, and that’s rare—but the most meaningful difference I’ve observed since LLMs hit the scene is that our releases are massively more frequent. We’re shipping product like never before. 

I can’t recall the nitpick, but my impression is that this is in fact a hugely transformational technology in my field, and yet it has caused us to fire nobody. Everyone is 100x more productive, and we get the dopamine hit of seeing ideas become reality at an incredible pace.

Plus, we no longer have to do the drudgery of documenting product, writing tests, etc. nobody wanted to do that before, and now we don’t have to. 

This is supposedly the end of dev jobs, and yet I feel like we’re in a golden age.",2
post28con,controversial,1.3436120224105876,highest,"I'm deeply, deeply skeptical of AI's utility, but I can recognize that it does *some* things well.  I just feel like we're talking about Microsoft Excel putting accountants out of business again.",3
post28con,controversial,1.3436120224105876,highest,"Just in case it's not clear.  I completely agree with you.  I just think you underestimate the impact of this tech.  But even if it's 100x as impactful as you imagine it to be -- and it is -- it will still mean we're all much more productive, and far better off.",4
post28con,controversial,1.3436120224105876,highest,"My point is that even is excel was as transformational as AI, which it isn't, it still wouldn't put accountants out of work.

Man, if you could see how we work now, you'd lose much of your skepticism and probably change how you work as well. This paradigm works for almost every kind of written work. I also use it for planning, administration, contract law, and marketing. It's life-changing.

Things haven't changed as much in 25 years as they have in the last 12 months.",4
post28con,controversial,1.3436120224105876,highest,"This happened before when the compiler came out. Compilers allowed people to code at incredible speeds compared to before, and everyone thought their jobs would be gone since one person can now do the work of many. Just created new demand since they can now release a lot more and take on many more projects. Sounds like the same is happening now. Glad to hear your experience is similar.",3
post28con,controversial,1.3436120224105876,highest,"People keep saying ""AI will create as many jobs as it kills"" but they can't actually say what those new jobs will be lol.",2
post28con,controversial,1.3436120224105876,highest,"Middle class mostly becoming upper class is false. We are seeing a larger and larger percentage of the US population (anyway) with a smaller percentage of total wealth. 

You are repeating propaganda, not actual facts.

Actually, this whole post is basically every capitalist propaganda trope rolled into one.",2
post28con,controversial,1.3436120224105876,highest,"> Middle class mostly becoming upper class is false. 

[Sorry, you're wrong] (https://imgur.com/a/EXKtFYz).

> Actually, this whole post is basically every capitalist propaganda trope rolled into one.

I mean, it's not propaganda to correctly note that we're better off under capitalism.  It's just facts.",3
post28con,controversial,1.3436120224105876,highest,"Try actual studies and data, instead of a random picture on the internet:

https://www.statista.com/statistics/203961/wealth-distribution-for-the-us/

https://www.pewresearch.org/social-trends/2020/01/09/trends-in-income-and-wealth-inequality/

https://www.cbo.gov/publication/60807#:~:text=Between%201989%20and%202022%2C%20the,distribution%20increased%20by%20285%20percent.",4
post28con,controversial,1.3436120224105876,highest,Wake me up when past performance becomes a guarantee of future success.,4
post28con,controversial,1.3436120224105876,highest,"the capitalist class is defined by an economic relationship, not by your existence within a capitalist society that ""has never been more prosperous"" (by capitalists' own definitions, maybe)

""economic freedom"" is nothing more than the ""freedom"" given to capitalists to rape the planet and dominate the rest of us",2
post28con,controversial,1.3436120224105876,highest,Last I checked not like any of the other non capitalist systems did any better,3
post28con,controversial,1.3436120224105876,highest,"its a question of whether or not you believe that you as an individual have the inherent worth to demand an equal say and share in your society

everything ""works"".  slavery ""works"".  the question is who is it working for",4
post28con,controversial,1.3436120224105876,highest,"> the capitalist class is defined by an economic relationship, not by your existence within a capitalist society that ""has never been more prosperous"" (by capitalists' own definitions, maybe)

No.  We're not all Marxists, sorry.  The ""capitalist class"" are all of us.  We are all capitalists.  We rely on the advancement of capital both for our own livelihoods, but for the world around us to operate.  

The people who tell you they are not part of the capitalist class just haven't realized it yet.

> ""economic freedom"" is nothing more than the ""freedom"" given to capitalists to rape the planet and dominate the rest of us

Ah, yes, the fact that I have the ability, if I so choose, to open my own business, work for myself, etc., it's all to serve those evil capitalists trying to actually dominate me.

If ""we'll largely leave you alone"" is domination, then thank you sir, can I have another?",3
post28con,controversial,1.3436120224105876,highest,"then what does the ""capitalist class"" even mean; if you're taking capitalist class to mean anyone that lives within a capitalist society then the term ceases to have any real descriptive meaning

a class can only mean something by its relation to something else.  that's what classes define: a hierarchy, social stratification.  if everybody is in a ""class"", then it isn't a class.

its like saying ""everything is a base"".  a base is only defined by its opposition to an acid.  saying ""everything is a base"" makes no sense, its depriving the term of its intended meaning.

if you're starting your own business, then you're trying to become a capitalist, you're trying to join the class that dominates the classes below them",4
post28con,controversial,1.3436120224105876,highest,"Yeah I was confused by the “capitalist class” name. We’re all living in a capitalist system. If by “capitalist class” he means “upper class”, just say that.",4
post28con,controversial,1.3436120224105876,highest,"The average American owns nothing. Not their home, not their labor, etc.

The majority of people are not capitalist. They own almost no private capital. The majority of people are the exploited workforce that capitalism relies on.",4
post28con,controversial,1.3436120224105876,highest,"Is ""capitalist class"" a meaningful classification if we all fall within that classification?",4
post28con,controversial,1.3436120224105876,highest,"It’s not by capitalist definition. It’s by definition of metrics as poverty rate, real household income, access to energy and electricity, life expectancy and many more.",3
post28con,controversial,1.3436120224105876,highest,"""poverty rate"" is an arbitrary measure, it can be set at whatever level its measurers prefer.  what is ""poverty""?  is there an objective definition?

income ""rises"" because production increases over time; access to goods increases.  relative incomes do not rise over time, they actually fall.  people get smaller and smaller shares of the pie over time

access to electricity and life expectancy are measures of development, of technological progress.  you can see development occur in socialist states and also see huge increases in life expectancy.",4
post28con,controversial,1.3436120224105876,highest,"Funny definition of capitalism you've got there.

I suppose everyone living in feudal times was a Lord, too?",2
post28con,controversial,1.3436120224105876,highest,"Well, part of the middle class is becoming the upper class, sure. That's what I said in my OP as well. But another part of the middle class is becoming the new lower class. 

But my point is that as AI and technology advances at an ever faster rate, soon AI will also be able to replace upper class workers like engineers, architects, doctors etc. The reason why some middle class people have moved into the lower class because their labor no longer has much value due to automation. But for now, new upper class jobs have also been created. 

But what do we do when AI and technology become so advanced that even engineers, and doctors and bankers and marketing specialists and whatever can be replaced by AI systems, robots or other technology? 

So once we reach a certain technological threshold for the first time we would not only see a shrinking of the middle class but also shrinking of the upper class. 

And no, we're not all capitalists. Many of us, especially those of us in the West, for now, benefit from capitalism to some extent, sure. 

But what do you do once the owners of the means of production have no more use for the vast majority of people, because AI and robots are way more effecient at every economic tasks those people could do? At that point, are you also gonna benefit from the system of capitalism if your labor has no more economic value?",2
post28con,controversial,1.3436120224105876,highest,"> Well, part of the middle class is becoming the upper class, sure. That's what I said in my OP as well. But another part of the middle class is becoming the new lower class. 

The data doesn't bear that out.  There is no increase in the middle class moving to the lower, statistically.

> But my point is that as AI and technology advances at an ever faster rate, soon AI will also be able to replace upper class workers like engineers, architects, doctors etc. 

Yeah, I don't buy it.  Like I said, we can't get it to count numbers right.  Even if the enterprise-level models are superior, LLMs aren't going to pull this off anytime soon and in the off chance that we start seeing some impacts, there's no reason to believe this time will be different.

> And no, we're not all capitalists. Many of us, especially those of us in the West, for now, benefit from capitalism to some extent, sure. 

More than benefit, we are the capitalists.

> But what do you do once the owners of the means of production have no more use for the vast majority of people, because AI and robots are way more effecient at every economic tasks those people could do? At that point, are you also gonna benefit from the system of capitalism if your labor has no more economic value?

By selling your labor somewhere that it's valued.

The same way we did every single other time.",3
post13con,controversial,1.3076307359011836,highest,"the current economic paradigm drives income inequality. the disease is capitalism, income inequality is just a symptom.

automating labour is a blight on people, not because doing less work is bad, but because the economic system is detached from reality.",1
post13con,controversial,1.3076307359011836,highest,"We just need a robot tax that funds UBI. The more profit that is earned from robots, the more goes to each citizen. 

That would also make the public more interested in supporting the transition.",2
post13con,controversial,1.3076307359011836,highest,"I’m having a hard time understanding this “Robot tax” that funds UBI thing.

Let’s say for a minute that we do go ahead and give this a try.

The first item to address I guess would be, to define what a robot is right?

But where does that begin and where does that end?

That the first problem I see.

Next, what happens to companies that have used automation for decades, do we let it slip and we only tax the new ones that use automation? That’s kind of unfair and it will stifle progress don’t you think?",3
post13con,controversial,1.3076307359011836,highest,"The details won’t be easy, it would take some smart people to spend time making a plan. But basically all large companies will be largely automated soon. The profits from those companies need to actually be taxed. As we know, Amazon and Apple pay almost no tax. The solution to that is easy. If sales happen in the US, Apple pays a percentage of US sales in taxes. Say 20%. It doesn’t matter that the company is based in a PO Box is Northern Ireland. Their sales are here, that’s how they get taxed. 

A large portion of those entirely new corporate taxes goes to UBI. It’s the only way forward that makes any sense. 

If Amazon wants to keep selling things to people but there’s no more need for jobs, those people will need to get money from somewhere. 

That money can come from the productivity of robots rather than humans.",4
post13con,controversial,1.3076307359011836,highest,"So where would 3d printers fall on that scale? CNC? both are programmed to perform configured tasks. They are not automatic in that you must upload a program, but many automated devices are the same way, but they use logic instead. How would the tax be applied a percentage of profits? per ""bot?"". It's a very very complex idea, and has the potential to snuff out small companies/peoples side jobs depending on how its implemented (see stuff like etsy vendors)",4
post13con,controversial,1.3076307359011836,highest,"Until the wealthy, with their overwhelming influence over the government, convince said government to cut/abolish the tax. Then everybody except them is fucked.",3
post13con,controversial,1.3076307359011836,highest,"gates, enough is enough. just because you saw the need for charity doesn't mean that charity should be needed.",3
post13con,controversial,1.3076307359011836,highest,It’s not charity. It’s humans reaping the reward of our collective progress. Rather than just a small handful of humans.,4
post13con,controversial,1.3076307359011836,highest,"What makes you think they will allow such a tax? The powers at be already dodge taxes, and they control the gov too. Rulling classes of ages prior could have also appeased the people and yet they didn't even to save themselves.",3
post13con,controversial,1.3076307359011836,highest,"The population needs to want it enough to elect people that will put the tax in place. That starts in places like Reddit and spreading ideas that people can get behind. 

It won’t be easy. You’re right about that.",4
post13con,controversial,1.3076307359011836,highest,"> We just need a robot tax that funds UBI.

Good job, you just increased the price of everything that used a robot in its creation. Give yourself a pat on the back.",3
post13con,controversial,1.3076307359011836,highest,"But we also got a bunch of free money to spend on that stuff. 

It has to go this way because there will be absolutely no jobs soon. Robots will be better workers than humans in every field very soon.",4
post13con,controversial,1.3076307359011836,highest,"Inequality can exists with or without capitalism though. The way I see it actual problem lies in social hierarchy, faults in collective decision making and shortcomings of governance process, which gives rise to wage-slavery and dominance of few over many.",2
post13con,controversial,1.3076307359011836,highest,"capitalism needs inequality to function. you need people with less capital to sell their time cheaper so you can make a profit. so in order for capitalism to function income inequality must always be present.

but your point still stands. a faulty hierarchical decision making process can also create inequality.",3
post13con,controversial,1.3076307359011836,highest,Your assertion that inequality is required assumes we all place the same value on labor and goods. In facts it’s extraordinarily naive to imagine you’ve created a universal index of value.,4
post13con,controversial,1.3076307359011836,highest,"There seems to be an ideal frontier level of inequality: [https://www.frontiersin.org/articles/10.3389/fpsyg.2017.02052/full](https://www.frontiersin.org/articles/10.3389/fpsyg.2017.02052/full#:~:text=The%20correlation%20between%20the%20Gini,20)

As with most things, moderation is best",4
post13con,controversial,1.3076307359011836,highest,Are you proposing trying to modify the natural fundamentals of human society? That's quite a bold ambition.,3
post13con,controversial,1.3076307359011836,highest,"Sure. Make study of logic and reasoning more prevalent than study of reading and writing. Encourage non-hierarchical social roles, akin to Adlerian individualism. Put a cap on how much wealth a single individual is allowed to own so no one would own more than they can ever spend in a lifetime. Give everyone basic income to get by so there would be no fear of ending up homeless - fear kills creativity and people are far more motivated to be part of society when society is worth being a part of. End homelessness. Invest massive resources into studying and technologically improving collective decision-making processes so that one day we could for once live our lives without constant effort going into stopping some idiots taking all our rights away, starting a nuclear war or next genocide.",4
post13con,controversial,1.3076307359011836,highest,"The actual problem is that there has always been inequality, and people have always died of it.

What do you think people did when they couldn't successfully hunt or grow crops?

They starved to death.",3
post13con,controversial,1.3076307359011836,highest,"That example does not show inequality. That's just a natural course of life. If you included that other people would allow them to starve to death that example would work to show inequality. However, pre-capitalist society was so intertwined that food distribution across the population was necessary for societal survival. Therefore, this particular inequality wouldn't have happened.",4
post13con,controversial,1.3076307359011836,highest,"The issue isn't inequality in ability but the inequality generated by capitlsism through ownership of capital. 

Let's say you got 2 investors. Equal ability. Investor 1 has more I starting capital than 2. For this simple fact investor 1 will make money money simply due to ownership and nothing else. That is the problem.",3
post13con,controversial,1.3076307359011836,highest,"People that are born smart have an advantage. People born with looks have an advantage. People born with money have an advantage.

Life will never be equal. The best we can do is keep the bottom as high as possible and give opportunities for improvement.",4
post13con,controversial,1.3076307359011836,highest,"It has been this way, but it doesn’t need to be this way.",2
post13con,controversial,1.3076307359011836,highest,it won't be this way for much longer. we have passed the point where the benefits are no longer bigger than the losses. it is just a matter of time.,3
post13con,controversial,1.3076307359011836,highest,"I read that is ""it's just murder time"" and thought ""you son of a bitch, I'm in!""",4
post13con,controversial,1.3076307359011836,highest,retarded post by someone who obviously can’t cope,4
post13con,controversial,1.3076307359011836,highest,"Well said. Even though we all have the computing power of what used to fill entire rooms in our pockets, the number of hours we work is the same (if not more, considering that bosses now try to communicate with us 24/7). Technological innovation under capitalism does not ease the working class' burden.",2
post13con,controversial,1.3076307359011836,highest,"Income inequality dates back to the founding of the first cities.

It has existed in all economic models except hunter/gatherer. Even then, some had better hunting grounds.",2
post13con,controversial,1.3076307359011836,highest,murder has existed since the beginning of human kind. cancer has existed since the beginning of human kind. doesn't mean we should not do something about it.,3
post13con,controversial,1.3076307359011836,highest,"The only solution I can see is removing free will.

Which would also solve the murder problem and most of the other ills in society.",4
post13con,controversial,1.3076307359011836,highest,"That is false.  We had Capitalism back in the 70s and it was constrained by high taxes on the rich, strong regulations, and entire industries being unionized.  Things didn't start to rot until we backed away from trying to tame Capitalism.  

Most human endeavors require some rules, some control, some way of asserting reason and responsibility.  If you don't bother trying to set some limits then systems go haywire.",2
post13con,controversial,1.3076307359011836,highest,"that is what i'm saying. treating the symptom is wasting resources. we need to cure the disease. if the cure is a tighter leash, so be it. if the cure is total departure from the ideology, so be it.",3
post13con,controversial,1.3076307359011836,highest,"Well yeah, if you continue to allow the capitalist class to exist, any concessions you get from them are always going to be temporary.",3
post13con,controversial,1.3076307359011836,highest,Capitalism is a nuclear reactor and we took the brakes off it and were headed towards a negotiable.,3
post13con,controversial,1.3076307359011836,highest,"The object of automation isn't to reduce labor, it's to maximize its productivity.",2
post13con,controversial,1.3076307359011836,highest,the objective of automation is to increase profits.,3
post13con,controversial,1.3076307359011836,highest,Not all automation is profit-oriented. And profits are relative to productivity levels. So it's productivity.,4
post13con,controversial,1.3076307359011836,highest,I would add consumerism and materialism,2
post21con,controversial,1.3045755767013714,highest,America has turned into an angry bully since it's governed by an angry bully.,1
post21con,controversial,1.3045755767013714,highest,no. it just proves companies never cared.,2
post21con,controversial,1.3045755767013714,highest,No. It means companies don't want to spend money on lawyers fighting the DOJ to promote diversity.,3
post21con,controversial,1.3045755767013714,highest,It doesn't matternif they have care or not.  It matters what direction they're pointed in and right now it's a bad direction.,3
post21con,controversial,1.3045755767013714,highest,"There is no direction lol, it's a facade that's put up specifically to fool people like you. They really just don't care",4
post21con,controversial,1.3045755767013714,highest,[deleted],4
post21con,controversial,1.3045755767013714,highest,What's your opinion on diversity? Should less talented people be given jobs than more talented because the former is underrepresented?,2
post21con,controversial,1.3045755767013714,highest,"You actually got it backwards and that's what's so scary for the future of this country.


Less talented people were getting the jobs because they were the default representation. But that's a tough pill for a lot of folks to swallow.",3
post21con,controversial,1.3045755767013714,highest,Do yourself a favor and go look up recent year med school acceptance rates by background and test score.,4
post21con,controversial,1.3045755767013714,highest,"Maybe in the previous millennia. In this millennia minorities were given preferential treatment in colleges with much lower bars for admission, scholarships exclusive to minorities, internships at top companies exclusive to minorities, and then full time job opportunities targeted at minorities, and then hiring quotas and promotion quotas for minorities. 

Society was in the 1900s white favoring, and then in the first quarter of the 21st century, minority favoring. Now we are entering the pendulum swinging back to the center albeit there are some that are resisting equality.",4
post21con,controversial,1.3045755767013714,highest,It started the way you mention but it took a wild turn where underrepresented minorities are being overrepresented. It has to be balanced both ways.,4
post21con,controversial,1.3045755767013714,highest,"Diversity isn’t about hiring less talented people, it’s about making sure talent isn’t overlooked because of systemic barriers. There’s plenty of skill and ability across all groups, but not everyone has had the same access to opportunities. Leveling the playing field doesn’t mean lowering the bar.",3
post21con,controversial,1.3045755767013714,highest,My opinion is that hatred shouldn't be the main driver behind political and business decisions,3
post21con,controversial,1.3045755767013714,highest,Any kind of bias other than merit should not be a driving factor. Diversity commitment goes against it because there is literally no way you can commit without having a bias.,4
post21con,controversial,1.3045755767013714,highest,Nice deflection there,4
post21con,controversial,1.3045755767013714,highest,"“Talented” like Hegseth, Noem, RFK and Gabbard? Whoopsies 💩",3
post21con,controversial,1.3045755767013714,highest,"Wrong question. That’s just something MAGA followers use to try to frame equality and diversity in a negative light. 

Real question: Given 50 similar roles at a large company, and a pool of 100 qualified candidates, is it desirable to make sure it’s not 49 white men and 1 POC in the role?",3
post21con,controversial,1.3045755767013714,highest,It's desirable to choose the 50 best candidates. Full stop.,4
post21con,controversial,1.3045755767013714,highest,"Say there's 10 roles and a pool of 1000 equally qualified candidates. Of this, 800 are male and 200 are female. Would it be desirable for the male-female split to be 50-50 here?",4
post21con,controversial,1.3045755767013714,highest,Who said they're less talented? If anything discouraging promotion of diversity can swing hard enough that you deny the more qualified candidate of another race... which is what the administration is pushing for.,3
post21con,controversial,1.3045755767013714,highest,Left bully has been replaced by right bully. Its all bully.,2
post21con,controversial,1.3045755767013714,highest,"If you’ve ever watched some of the CIA people on podcasts in the last few years, you’d learn that being a bully has been Americas policy for many decades. Trump didn’t start it.",2
post21con,controversial,1.3045755767013714,highest,"First time I hear a US president threatening to take over the Panama canal, Canada, Greenland, Gaza within a 2 weeks timeframe",3
post21con,controversial,1.3045755767013714,highest,The US has been in almost perpetual war  since world war 2. Where have you been?,4
post8tec,technical,1.2865926543146182,highest,It’s because GPUs make slight (no deterministic) errors and those add up in large models. I think on cpu this wouldn’t be the case.,1
post8tec,technical,1.2865926543146182,highest,"This is correct. To be more precise, GPU operation execution order is non-deterministic (bc everything is happening in parallel as much as possible), but float operations are generally not associative, ie (a+b)+c != a+(b+c). So slight differences will compound over time, leading to big differences in massive models like LLMs.",2
post8tec,technical,1.2865926543146182,highest,"There was a whitepaper on here last year from this ml researcher who wanted to stick it to his professor and show that he could get a linear activated model to have nonlinear results just from float imprecision. It was a great whitepaper. Funny and captivating and very interesting. In the end he showed that as long as the models were really compressed like it four bits or two bits he could use a linear activation and have almost identical performance to RELU.

So the point is it doesn't take a lot of nonlinearity to get results like that and it shows how very small differences in the math can compound.",3
post8tec,technical,1.2865926543146182,highest,"I think you might be describing ""GradIEEEnt Half Decent"" http://tom7.org/grad/",4
post8tec,technical,1.2865926543146182,highest,OpenAI did a similar thing a few years back: [https://openai.com/index/nonlinear-computation-in-deep-linear-networks/](https://openai.com/index/nonlinear-computation-in-deep-linear-networks/),4
post8tec,technical,1.2865926543146182,highest,Tom7?,4
post8tec,technical,1.2865926543146182,highest,"Even if gpu calculation order is non-detemininstic, the result is. For instance, in A×B ,when x is matrix multiplication, GPU split matrix B in colum order when doing the multiplication, so that the resulting C can be just concatenated. GenAI stochasticity has nothing to do with parallel processing of GPU.",3
post8tec,technical,1.2865926543146182,highest,No this isn’t true. Most operations are run to run deterministic on GPUs,3
post8tec,technical,1.2865926543146182,highest,"Nope. You can typically flip a switch in the settings to make everything deterministic, but this will butcher your performance, so in every single case I encountered, CUDA is kept nondeterministic",4
post8tec,technical,1.2865926543146182,highest,"Batch size, memory pressure (so current results depend on previous batches), CUDA/Torch version, minor python changes (e.g. “f(a + b)” instead of “c = a + b; f(c)”), etc. All make quite the difference. In practice, the exact same code on the exact same machine might be deterministic, but it’s virtually useless from a reproducibility perspective.",4
post8tec,technical,1.2865926543146182,highest,Is this what leads to “hallucinations” in LLM’s?,3
post8tec,technical,1.2865926543146182,highest,"No. Hallucinations are just the model getting the answer wrong. It's not a ""bug"" in the sense of traditional programming.",4
post8tec,technical,1.2865926543146182,highest,"Gotcha thanks. I'm just wondering if anyone has done some research on quantifying this ""non-determinism"" and delving deeper into the GPU architecture that causes this

Thanks!",2
post8tec,technical,1.2865926543146182,highest,"https://stackoverflow.com/questions/50744565/how-to-handle-non-determinism-when-training-on-a-gpu

>The heart of the problem is that, when you run operations on several parallel threads, you typically do not know which thread will end first. It is not important when threads operate on their own data, so for example, applying an activation function to a tensor should be deterministic. But when those threads need to synchronize, such as when you compute a sum, then the result may depend on the order of the summation, and in turn, on the order in which thread ended first.

In theory this wouldn't matter, because addition and multiplication are associative operations. But *floating-point* addition is not quite associative because of rounding errors, so order does matter.",3
post8tec,technical,1.2865926543146182,highest,"are there benchmarks on this?

this might be a big problem for gpus.",4
post8tec,technical,1.2865926543146182,highest,"Actually it might be because T=0 is set to some small epsilon > 0. It depends on the implementation. Since T=0 would produce division by 0, so the code would need to explicitly do if T==0, argmax(logits).",3
post8tec,technical,1.2865926543146182,highest,Never saw a codebase that doesn’t use argmax when t=0,4
post8tec,technical,1.2865926543146182,highest,"Most floating point operations violate commutative and associative properties, so the order matters. This leads to differences when the problem is refactored and executed in parallel, whether on CPU or GPU. This means that almost any computation will not be entirely reproducible, particularly with different hardware. 
LLMs are particularly sensitive to such variation because a sequence is produced recursively, producing a single different token will lead to an entirely different response as it becomes the basis for the subsequent tokens. This is not the case for regression or image recognition, where minor variations of probabilities might not change classification.",2
post8tec,technical,1.2865926543146182,highest,Might also be because meta data in the input from request to request is slightly different e.g. the time of day in minutes and seconds.,2
post8tec,technical,1.2865926543146182,highest,"This is incorrect. If this is right, than games will suffer from random effects all the time. It is the underlying generative AI model that does this.",2
post8tec,technical,1.2865926543146182,highest,"The phenomenon is definitely real (you can easily test it on GPU) but the errors are slight so it's unlikely that this is the reason (and in games there's way less calculations than in LLMs so the errors would be even more slight so you wouldn't notice anything when playing). I sort of changed my mind, and now I think that T=0 gets clamped to some small epsilon in most implementations. The errors shouldn't be large enough to change argmax.",3
post8tec,technical,1.2865926543146182,highest,Most backends switch to greedy token selection at temp 0 rather than setting it extremely small and doing the math. Just makes way more sense.,4
post8tec,technical,1.2865926543146182,highest,"Wait, Do they not?",3
post56con,controversial,1.2857131070224554,highest,"Ghassemi (comp scientist @ MIT) believes it's based on melanin.

Goodman (bio anthropologist @ Hampshire) believes it's based on geography.

Both proposals pretty logical and not as controversial as it would seem based on the headline.",1
post56con,controversial,1.2857131070224554,highest,Those biased datasets! I could believe both after having to look at a ton of pneumonia X-rays for a machine learning model demo.,2
post56con,controversial,1.2857131070224554,highest,i remember hearing about one of those pneumonia detection models that instead of detecting pneumonia detected whether or not the patient was lying on their back,3
post56con,controversial,1.2857131070224554,highest,"Isn't ""based on geography"" (i.e., geographic ancestry) essentially the same as ""based on race"" since they are so highly correlated anyway? Why feign confusion? There's nothing wrong with noting that there are groupings of traits that tend to correlate with the construct of ""race"" unless you yourself believe that those traits make someone inferior or superior.",2
post56con,controversial,1.2857131070224554,highest,No,3
post56con,controversial,1.2857131070224554,highest,">No

Seems to me like it is a semantic argument akin to the one I assumed:

>So, while AI might be able to determine from an X-ray whether a person’s ancestors were from Scandinavia, Africa or Asia, Goodman says it’s not about race. “You call this race. I call this geographical variation,” said Goodman — but he did admit it’s unclear how AI could detect geographical location from an X-ray.

Can you please explain how I'm wrong besides saying ""No?"" This is a science subreddit.

Edit: Saying that people from different geographic origins tend to share certain traits and that those traits correspond with the concept of race more often than not should not be controversial in and of itself. A scientist should not be throwing up their hands and basically saying ""Who could have seen this coming?!"" to avoid citing the obvious explanation.",4
post56con,controversial,1.2857131070224554,highest,"it would if you could tie geography and race, considering race is a socially constructed set of boxes that break themselves over and over again, maybe ethnicity? but then how do you measure that, they are ranges usually being based on common ancestry and muddle again due to migration, traders.  


Geography can tie to melanin rates due to local adaption to specific geography...   
While if it was ""race"", then somehow the same group of people settled very specifically the same environment across the same latitudes and altitudes",3
post56con,controversial,1.2857131070224554,highest,"Could it be based (or partially based) on bone density or shape? I’m from Africa and it’s pretty common “knowledge” that native African people have much stronger bones than the seemingly more fragile Caucasian population. Obviously it’s anecdotal and not a fact, but from my own experiences it certainly feels true.",2
post56con,controversial,1.2857131070224554,highest,"I was under the impression that this was similar to the belief that certain races feel less pain, a common myth based in racism. However, it seems that racial differences in bone density do exist and it is a current area of research

https://pubmed.ncbi.nlm.nih.gov/9024231/",3
post56con,controversial,1.2857131070224554,highest,"What amazes me is, growing up, how apartheid racists always managed to turn something like being *literally* physically stronger and more resilient (since most labourers were African, they generally were incredibly physically fit and active) into something negative. They’d say, “never hit a k****r in the head because you’ll break your hand on their thick skulls”.

Now when you grow up hearing things like that day after day, generation after generation, you can see why racism became less about racial/cultural differences and more about hating other people for being different because you were told they’re bad. It becomes institutional.

But then people like my mother would hire housekeepers and gardeners and pay them triple what anyone else would pay, she’d buy them food and baby clothes because “even with what I’m paying, I know other people aren’t paying that and even I couldn’t survive on that kind of wage with 4 or 5 kids”. As such, the loyalty and friendships she made were with everybody and at least in that little home bubble, racism didn’t exist. 

She would always insist that the racists were mocking black people for being uneducated, but then took away any chance of an education they had with white-only schools. Making a group of people a certain way by denying them basic human rights and then mocking them for it is the height of human evil.",4
post56con,controversial,1.2857131070224554,highest,"Redheads/gingers literally do feel less pain though, they have mutant pain receptors which is also why they require higher dosages of painkillers in hospitals",4
post56con,controversial,1.2857131070224554,highest,"Yeah, but is it really racial? Isn't it time we put the whole ""race"" thing to bed and actually come up with a scientific term?",4
post56con,controversial,1.2857131070224554,highest,"I think this is probably a big factor in it, there are proven differences between black and white people in bone density but there are definitely outliers in these situations.  I'm Caucasian but I have a top 0.3% bone density measure and so does my mother, though she's Sicilian which is near Africa.  My father does too but he's Scottish... and a laborer - you can build bone density by lifting or putting stress on your bones, so a lot of the differences do come from if you are a laborer or not, but I think a lot of it is genetic given that I had that super high bone density before I even started lifting weights.  I played video games all day as a kid and somehow have bones like wolverine and I suspect its similar with the differences between ethnicities on a high level.  However, to overall broadly say Africans have stronger bones would likely be wrong given that the diversity of genetics amongst African's is actually greater than between African's and Caucasians! Pretty crazy right?",3
post56con,controversial,1.2857131070224554,highest,There must be outliers in the AI correctiveness as well. I’m sure it’s not 100% right about the persons race all the time. It could easily be facial structure or small variation on a variety bone shapes that we wouldn’t notice day to day.  It’s silly to pretend there could be no differences between race’s bones.,4
post56con,controversial,1.2857131070224554,highest,You are incorrect. Wolffs Law says bone density is directly proportional to stress put on the bone. Pretty cool stuff to read up on if you’re ever bored.,3
post56con,controversial,1.2857131070224554,highest,But there may be differences in structure or shape beyond that? I don’t think that’s a crazy hypothesis.,4
post56con,controversial,1.2857131070224554,highest,I don’t think this is actually true. Can you find a credible source?,3
post56con,controversial,1.2857131070224554,highest,"As I mentioned, it’s purely anecdotal and isn’t a fact. So no, no sources I’m afraid.",4
post56con,controversial,1.2857131070224554,highest,"How so? there's more genetic variation within African populations that outside combined, due to the genetic bottleneck when human left the continent.   
Feels like it's basing everything on it's one understanding of what ""black"" is, which is going to be western populations as that's where a majority of the african american population came from due to the slave trade.  


I'd give up that idea if they can replicate showing that Hadza, pygmy, Bantu, are included, as well as Afro asiatic, does it put Berber as originating from Africa?  


and then I would want to see it be able to exclude similarly high melanin groups like Austronesian groups",3
post56con,controversial,1.2857131070224554,highest,most likely on the known factors of differentiation in bone density,2
post56con,controversial,1.2857131070224554,highest,"From the study itself, once you click thru the popular news article:

> To conclude, our study showed that medical AI systems can easily learn to recognise self-reported racial identity from medical images, and that this capability is extremely difficult to isolate. We found that patient racial identity was readily learnable from medical imaging data alone, and could be generalised to external environments and across multiple imaging modalities.

Both of the supplies explanations are not supported by the study. This is fuckin weird, the study shows that race may not strictly be a social construct.",2
post56con,controversial,1.2857131070224554,highest,"< the study shows that race may not strictly be a social construct.

I can't believe anyone literally thining that races are solely a social construct",3
post56con,controversial,1.2857131070224554,highest,">< the study shows that race may not strictly be a social construct.
>
>I can't believe anyone literally thining that races are solely a social construct

The fact of the matter is that race *is* a social construct, but the way race is constructed is so highly correlated with ancestry in most cases that the difference isn't worth mentioning. What is racist is not to admit this fact, but to believe that these measurable differences have any effect on  someone's intrinsic worth as a human being, on their potential, on their moral character, etc.",4
post56con,controversial,1.2857131070224554,highest,"Agreed. Put 5 Vietnamese people and 5 Norwegians together and tell me race is a social construct. 

I understand wanting to be very careful when we have used things like phrenology in the past to justify treating different groups of people differently, but science should be completely based in objective truth. Really we need to stop bending science to whatever social truth we are trying to prove.",4
post56con,controversial,1.2857131070224554,highest,Don't act like you have some magic info no one else found lol,3
post6con,controversial,1.283408908961717,highest,"These algorithms are very vulnerable to bias. If a neighbourhood is heavily patrolled, the chance is much higher any infractions are added to the learning set, increasing the ""crime-value"" of that area. Meanwhile, areas that are rarely patrolled at all, have a much lower chance of ending up in the database. This creates blind spots.

A real life example of where policing by AI went horribly wrong is the [Dutch childcare benefit scandal](https://en.wikipedia.org/wiki/Dutch_childcare_benefits_scandal). The algorithm ""learned"" that types of people (single mothers, immigrants) were more likely to have something wrong with their taxes, checked them more often, and then identified them as fraudsters for minor infractions like receipts being handed in incorrectly, or being a few days late with payment. Because computers are \*magic truth machines\* that \*don't make mistakes\* these people were given no legal recourse, no chance to defend themselves. They did not even know what they were accused of.

If we are going to use machine learning as a tool to help legal administration, we need to take extreme caution, and everyone working with these machines must fully understand their limitations. The computer has no idea what it's actually doing, it's just a fancy calculator following instructions, and while it follows these instructions flawlessly, it's still extremely error prone, and does not have the capability for self-reflection a human does, even if ""learning"" is built into the algorithm. AI fundamentally does not understand what its doing, and that means it will never understand if its doing wrong. We cannot use AI to replace our own judgment.",1
post6con,controversial,1.283408908961717,highest,"At least the whole cabinet resigned in the Netherlands. In Australia a [similar scheme](https://en.wikipedia.org/wiki/Robodebt_scheme) was instituted, then found to be illegal, but the people administering it continued to be in government. The former social services minister even became PM. 

Back to the point: I agree that great care needs to be used when trying these kinds of optimised, targeted computational methods.",2
post6con,controversial,1.283408908961717,highest,"The same guy who was PM during the scandal, offered himself up for reelection and won, so yes, the cabinet fell, but we’re still stuck with some of the responsible politicians, including the PM. Not contradicting you, but an (IMO necessary) addendum.",3
post6con,controversial,1.283408908961717,highest,"All the care in the world won't stop the biases inherent in our paradigm. There are built-in mechanisms of discrimination and inequality that the system as we know it optimizes for and are virtually impossible to remove from our current modus vivendi.

These books talk about the problem at length:

https://www.goodreads.com/book/show/28186015-weapons-of-math-destruction

https://www.goodreads.com/book/show/34762552-algorithms-of-oppression

https://www.goodreads.com/book/show/34964830-automating-inequality",3
post6con,controversial,1.283408908961717,highest,"Yeah for sure. In the two cases mentioned in the comments the ML-based bullshit isn't the actual cause of the trouble. The root is from the rampant starve-the-beast defunding and privatisation of governmental functions, along with negative neoliberal attitudes to social services. If you have a properly functional social service setup, you won't need any of this shit in the first place.",4
post6con,controversial,1.283408908961717,highest,"It reminds of the algorithm that handed longer sentences to minorities. If I am not mistaken, it took factors like income and spit out a value that determines whether the defendant will recidivate or not. The result was that minorities were disproportionately affected by it…",2
post6con,controversial,1.283408908961717,highest,"Oh yeah, this is a whole rabbit hole. There's also algorithms that are being trained by people to identify subjective values, such as ""niceness."" These are notoriously biased as well, as biased, in fact, as the people who train them. But unlike those people, the opinion of the AI won't be changed by actually getting to know the person it's judging. They give 100% confident, biased, results.

Or the chatbots that interpret written language and earlier conversations to simulate conversation. One of them was unleashed on the internet and was praising Hitler within 3 hours. Another, scientific model designed to skim research papers to give summaries to scientists, answered that vaccines both can and cannot cause autism.

These don't bother me though. They're so obviously broken that no one will think to genuinely rely on them. What bothers me is the idea of this type of tech becoming advanced enough to sound coherent and reliable, because the same issues disrupting the reliability of the AI tech we have today will still be present, it's just the limitation of the technology. Yet even today we have people hailing the computer as our moral savior that's supposed to end untruth and uncertainty. If the tech gets a facelift, I believe many people will falsely place their trust in a machine that just cannot do what is being asked of it, but tries it's damndest to make it look like it can.",3
post6con,controversial,1.283408908961717,highest,"As an example:

Meta just a couple days ago took offline it's scientific paper generation machine because it would happily provide you a real-sounding scientific paper on the history of bears in space.

https://futurism.com/the-byte/facebook-takes-down-galactica-ai",4
post6con,controversial,1.283408908961717,highest,Few things have described me better.,4
post6con,controversial,1.283408908961717,highest,"It happened in NYC with fires.  It is explored in a fascinating book called ""The Fires"" by Joe Flood.  Basically RAND corporation had used computer models to ""more efficiently"" provide fire protection in the city and it led to a massive wave of fires and destruction of huge swaths of the city.",2
post6con,controversial,1.283408908961717,highest,"For this example we can train an algorithm to estimate the probability of a crime in an area given the amount of patrolling in that area. So it could be normalized out if the algorithm is designed properly. 
The amount of care needed in designing these algorithms will need to be high. I do know that there is active research and development in identifying these biases early (even before deployment) but it’ll never be perfect. So it’ll likely be a cycle of negatively hurting people, being called out, fixed, and then back to step 1.",2
post6con,controversial,1.283408908961717,highest,"I wonder what would happen if they took the Abraham Wald approach and designed a counterintuitive algorithm. Like, make a heatmap of violent crimes (assault, robbery, rape, etc.), and then sic the algo on non-violent crimes in the inverted heatmapped areas, like larceny, wire fraud, and so on. Higher-income areas have wealthier people, and statistically wealthier people are better equipped to commit high-dollar white collar crimes. You could also use the hottest areas on the violence heatmap to target social services support.",3
post6con,controversial,1.283408908961717,highest,"> The computer has no idea what it's actually doing

Counterpoint: Neither do we.

Expert poker players are often unable to explain their reasoning for why it felt like a bluff. It could be that they are picking up on something and acting without being able to reason about it.

Likewise, a doctor with a lot of experience might have some hunch that turns out to be true. The hunch was actually solid deduction that the doctor was unable to reason about.

Even you, driving, probably sometimes get a hunch that a car might change lanes or get a hunch that an intersection should be approached slowly.

I (and others) feel that explainable AI might be a dead-end. If we told the poker player that you can only assume a bluff if you can put into words what is wrong, that player might perform worse. It might be that forcing AI to be explainable is artificial limiting it's ability to help us.

Even if you don't buy that, there are those studies that show that consciousness is explaining our actions after the fact like an observer. So we're not really using reason to make decisions, we just do things and then reason about why. 

We let humans drive cars on hunches. Why should we hold AI to a higher standard? Is a poorly performing explainable AI better than an unexplainable one that does a good job?",2
post6con,controversial,1.283408908961717,highest,"I'm not talking about reasoned explanations when I say a computer does not understand what it's doing. What I mean is that a computer fundamentally has no concept of ""right and wrong."" It's just a field of data and to the computer it's all the same if you switched the field for ""good"" with the field for ""bad,"" it would uncaringly keep making it's calculations. Computers do not feel, they do not have hunches. All it does it measure likeliness based on ever more convoluted mathematical models. Its a calculator.

Any emotional attachment is purely coming from our side. A computer simply does not care. Not about itself, not about doing a good job, and not about you. And even if you told it to care, that would be no more than just another instruction to be carried out.",3
post6con,controversial,1.283408908961717,highest,"Are people so different? We spend years teaching our kids to know right from wrong. Maybe if we spent as much time on the computers then they could know it, too?",4
post6con,controversial,1.283408908961717,highest,"I don't necessarily agree that we need to have what you call 'unexplainable AI' and what I would call 'AI using machine learning' to solve the kinds of problems that face police today. I think that you can have systems that are extremely unbiased and extremely transparent that are written in ways that are very explicit and can be understood by pretty much everyone.   


But I do agree with you that it's a very biased and incomplete argument to say that automated systems are working in ways that are opaque to the communities they serve and ignore the fact that it's not in any way better to have humans making those completely opaque decisions.",3
post6con,controversial,1.283408908961717,highest,">I don't necessarily agree that we need to have what you call 'unexplainable AI'

To be more precise, I'm not saying that we must have unexplainable AI. I'm just saying that limiting our AI to only the explainable increases our ability to reason about it (good) but also decreases the ability of the AI to help us (bad). It's not clear if it's worth the trade-off. Maybe in some fields yes and other no.

Most deep learning is already unexplainable and it's already not useful enough. To increase both the usefulness and the explainability will be hard. Personally, I think that maximizing both will be impossible. I also think that useful quantum computers will be impossible to build. I'm happy to be proven wrong!",4
post6con,controversial,1.283408908961717,highest,"This misses the entire point of what explainable AI is. Asking humans to explain their intuition as a precondition for their intuition to be applicably valid is definitely limiting for humans. However, explainable AI isn't that we ask AI to explain itself. It's rather being able to exactly or with high probability pinpoint the exact dataset on which AI is basing it's prediction. This is definitely useless, and so limiting, when it comes to machine learning applications to, say, predicting what food you might like the best. 
It's however immensely important in areas like medical imaging, because we want to ensure that the input, on which AI is basing its decision, isn't some human-errored spot on the x-ray. 

As such, it is for these fields that explainable AI is studied, where limitations of AI are far less significant than us being sure that AI isn't making a mistake. As such suggesting explainable AI is a dead-end is inaccurate, if not a mischaracterisation.",3
post6con,controversial,1.283408908961717,highest,"I didn't mean that the AI should be able to explain itself. I meant that we should be able to dig in to the AI and find an explanation for how it worked.

I'm saying that that requiring either would limit AI and decrease it's usefulness.

Already we have models where it's too difficult to dig into them and figure out why a choice was made. As in, you can step through the math of a deep learning system to follow along with the math but you can't pinpoint the decision in there and more than you can root around in someone's brain to find the neuron responsible for a behavior.",4
post6con,controversial,1.283408908961717,highest,[removed],3
post6con,controversial,1.283408908961717,highest,"Your comment was removed for violating the following rule:

>**Argue your Position**

>Opinions are not valuable here, arguments are! Comments that solely express musings, opinions, beliefs, or assertions without argument may be removed.

Repeated or serious violations of the [subreddit rules](https://reddit.com/r/philosophy/wiki/rules) will result in a ban.

-----

This is a shared account that is only used for notifications. Please do not reply, as your message will go unread.",4
post6con,controversial,1.283408908961717,highest,"Machine Learning, Artificial Intelligence, and Algorithm are all terms that exist in the same space of computer science, but they absolutely do NOT all mean the same thing, and in your post here you used them all interchangeably.

An algorithm is a very generic term for some kind of heuristic that can be followed to produce some result. A recipe for cookies in an algorithm just like some algorithm on Facebook decides what posts to show you. Machine Learning takes place when the process a system implements is non-deterministic; it does things that the programmers didn't explicitly tell it to do; it actually learns how to do new things. An artificial intelligence is a system that's designed to do tasks in the same way a human would, often involving processing visual data or making human-like decisions.

If you wanted to make the case that we shouldn't use MACHINE LEARNING in policing, I would 100% agree with that statement, our police policies should be very deliberate and very transparent and machine learning wouldn't be either of those things. But using this as an argument that we shouldn't be embracing policing with explicitly defined algorithms that are far MORE transparent and deliberate than the humans they would replace is an absolutely indefensible argument. If there's one thing we've learned in the past few years, it's that police need far more regulation, and that's exactly what algorithms do whether they are implemented by a computer or by some system of rules and laws.",2
post6con,controversial,1.283408908961717,highest,[removed],3
post6con,controversial,1.283408908961717,highest,[removed],4
post6con,controversial,1.283408908961717,highest,"Your comment was removed for violating the following rule:

>**Be Respectful**

>Comments which consist of personal attacks will be removed. Users with a history of such comments may be banned. Slurs, racism, and bigotry are absolutely not permitted.

Repeated or serious violations of the [subreddit rules](https://reddit.com/r/philosophy/wiki/rules) will result in a ban.

-----

This is a shared account that is only used for notifications. Please do not reply, as your message will go unread.",4
post6con,controversial,1.283408908961717,highest,"Computers are no longer following instructions. That went out about 10 years ago.

They're just juggling numbers. Same as us really but without the ability to self-reflect (yet)",2
post6con,controversial,1.283408908961717,highest,"They're following instructions to juggle numbers. If you can hand me the human source code, I'll gladly read it, but as far as I'm aware there is no such document in existence.",3
post6con,controversial,1.283408908961717,highest,What if we used victim surveys as training data instead wherein victims of crime can specify the place that the crime occurred.,2
post6con,controversial,1.283408908961717,highest,thank you,2
post29con,controversial,1.2623637164513175,highest,"At the same time, other more difficult jobs emerge and can now be done by individual people, with the help of AI. Already seen it happening. For instance, people are commissioned to make whole short films. That now takes new skills: screenwriting, sound design, cinematography, AI toolchain understanding, taste and so on.

(And no, I'm not arguing that long term every job is safe, because who's to say AI won't also direct and screenwrite and such.)",1
post29con,controversial,1.2623637164513175,highest,"Those aren't jobs emerging though. It's literally what would have been many jobs turning into one job. And even in this case, the clock is ticking. Because if one person can all those things, very quickly that commission will disappear because ""why would I hire someone to do what I can do myself with the help of AI?""

I have yet to see a case of someone describing a ""new"" opportunity created by AI that isn't just a combination of these two forces - massive job reduction, and selective (wishful) thinking.

As though ai progress is going to suddenly come to a stop once the work that could be done by 10 people can be done by one. What do you think makes you so special as that one person that the thing you're contributing is the thing that can't be replaced?

If we really end up being able to replace most jobs, then we're going to be able to replace every job. Human labor will have no value, if you didn't already have the wealth to not need to work, you're gonna SoL, on permanent minimum wage government ""UBI"" forced into compliance because they literally control your ability to survive.

It is definitely not a happy path.",2
post29con,controversial,1.2623637164513175,highest,"Only way I can see new jobs is if we start valuing things currently seen as unimportant more.

Like, if you took current good salaries and paid a lot of people that much to act as social workers, there is a lot of suffering that could be addressed before you ran out. 

You could give out grants for people to preserve traditional arts like drawing cartoons by hand or overly elaborate parades or hand made clothes.

Hell, you could give people PPE or remote controlled drones and have them sort through old garbage heaps to reclaim stuff, even if that’s not very efficient. It’s still a way to get resources without destroying more nature.

There’s lots of work we *could* be doing to make life better that we currently ignore as insufficiently profitable. We’d just rather let people descend into squalor than pay them to do anything about it.",3
post29con,controversial,1.2623637164513175,highest,"In some cases it is multiple jobs turning into one, yes, but in other cases, it actually increases artistic scope, and new forms of expression emerge.

As a random example, I created an AI-based visual storytelling engine that runs in Twitch, and I live-DJ'd for the community currently having fun playing the story -- by incorporating their names and comments into ad hoc created music lyrics, again composed with the help of AI, and played back to them in realtime with lots of laughter to be had. Creativity wasn't gone, it just moved to a meta level.

I spent my days working on many different such projects, and if you're genuinely interested to learn about that meta level I'm happy to expand on it. I just don't aim to debate or change your mind, it's fine if you have your opinion as is and I can totally understand and empathize with where you're coming from.",3
post29con,controversial,1.2623637164513175,highest,"Ok, so let's assume this is a paid activity. You are being an ""entertainer"" (and a freelance one at that). That's not a new job, even if you are incorporating novel media to perform that job, as entertainers have done since it's been a thing. 

And as far as jobs go, it's not great when it comes to being secure or lucrative (for the vast majority of people doing it). Responding to mass unemployment with the prospect of freelance entertainment isn't job creation any more than signing up to drive Uber or delivering DoorDash (although at least those arguably have more reliable demand with a lower barrier of entry).",4
post29con,controversial,1.2623637164513175,highest,"Yeah it is not a problem though. 


One person will be able to pump out 30 projects a month instead of 3. Prices per project will go down. There will be more economic activity because of the low prices. Because of more economic activity more jobs will emerge",3
post29con,controversial,1.2623637164513175,highest,"If we can automate from needing 30 people to do a job to just one, odds are we'll be able to automate that one remaining person as well.

I don't know why people think there's some magic that means at least one person will always be required. It's wishful thinking.

> Prices per project will go down. There will be more economic activity because of the low prices.

If you think lower costs of production means savings get passed on to consumers, you haven't been paying attention.

> Because of more economic activity more jobs will emerge

In the US the top 10% are responsible for nearly half of all consumer spending. Just because economic activity is happening, doesn't mean the majority are or will be the beneficiaries.",4
post29con,controversial,1.2623637164513175,highest,"But those job ALREADY existed. Short films were created in the past. By a team of talented and hard working people, getting paid. 
Now it is going to be one guy, getting paid much LESS than a whole team. 

Nothing new emerges.",2
post29con,controversial,1.2623637164513175,highest,"This is [Jevons paradox](https://en.wikipedia.org/wiki/Jevons_paradox) in action, when movie making becomes easier, you don't just make movies faster with fewer people, but people end up making much more movies, because they are cheap now, thus resulting in more people being employed making movies. That's how improvements in technology have worked out numerous times in the past.

That said, I don't think it will happen this time around, at least not for long or at the scale necessary. The reason being, human attention is limited and AI can create stuff at an insane pace. Hollywood right now makes around 150 major movies a year, that's small enough that you could still watch everything if you really wanted to. If AI turns that into 1500, you don't end up with a movie market 10x the size, since nobody got time to watch all of them. We are reaching a point where humans have enough entertainment at their fingertips to last multiple lifetimes.

>> screenwriting, sound design, cinematography, AI toolchain understanding, taste and so on.

And as for those skills, all of that is stuff AI can do. Not right now and not in the quality needed, but AI progress means that all those things that still require human touch right now will fall away as time goes on.",3
post29con,controversial,1.2623637164513175,highest,"> If AI turns that into 1500, you don't end up with a movie market 10x the size, since nobody got time to watch all of them. We are reaching a point where humans have enough entertainment at their fingertips to last multiple lifetimes.


It's how it's been with books. Nobody can read them all.",4
post29con,controversial,1.2623637164513175,highest,"Your last paragraph mirrors my last paragraph, so: yeah, that's a possibility. I also see other possibilities and can describe them if wanted, but nobody is an expert on the singularity yet -- not even the singularity experts!",4
post29con,controversial,1.2623637164513175,highest,"On the other hand, though, you also unlock improved production values for long tail projects. You can hit weird and specific niches in a way that you couldn't previously. Experimentation becomes substantially easier.

I don't know how much that increases demand, but it's not zero. YouTube could get a lot more interesting.",4
post29con,controversial,1.2623637164513175,highest,"even with just Humans involved in the production chain we have are near peak output. bout a decade ago the head of FX tv was talking about the era of peak TV where there were so many high quality shows that people couldn't watch them all. And with older media being so accessible it just adds to the mass. There are still many people who haven't seen The Wire and there are so many other great shows, books, games and that doesn't include the time hanging out with  friends and other socialising.",4
post29con,controversial,1.2623637164513175,highest,"Agree with the latter part, ie once the market is saturated there is no market for “much more movies”.

We are already hitting saturation for streaming TV shows.  There is only so much of people’s time to compete with, and the industry lives on celebrity power, word of mouth, and awards.m, which are natural gatekeepers to consumer time and attention.",4
post29con,controversial,1.2623637164513175,highest,"*New films* emerge, films that just would never have gotten made because one guy, by himself, couldn't afford to *pay* that ""team of talented and hard working people"" to help him make his dream project.",3
post29con,controversial,1.2623637164513175,highest,"We're already drowning in entertainment slop, theres more of it than people could ever wish to watch. So what follows is a run to the bottom. Again.",4
post29con,controversial,1.2623637164513175,highest,"... dude, we don't need this crap mass produced. There's already TOO MUCH stuff. I need fucking money and a place to live.",4
post29con,controversial,1.2623637164513175,highest,"So that is same thing, only cheaper. 

My point is that no new jobs are created",4
post29con,controversial,1.2623637164513175,highest,"To be fair, maybe that's just one shitty movie no one want to fund.",4
post29con,controversial,1.2623637164513175,highest,"It’s up to the consumers to recognize good writing and production then.  Which I don’t have much confidence in, unfortunately.

Once again Mike Judge proves to be [the greatest prophet of our generation](https://youtu.be/kJZjU2k5abs)…",3
post29con,controversial,1.2623637164513175,highest,"But it’s the consolidation of jobs into less number of jobs. The net job total is lowering, while the existing jobs become further niche and skilled. 

That’s not good news for white collar workers.

It’s the widening of poverty. Wealth moves further upward.",2
post29con,controversial,1.2623637164513175,highest,"That's a good point to discuss, I'm answering that [here](https://www.reddit.com/r/artificial/s/4EJrJO84ZI).",3
post29con,controversial,1.2623637164513175,highest,"I'd also add something nobody seems to mention often enough. By going about this mindset of AI coming for your jobs, companies risk canibalizing their own customer base. Who is going to buy their shitty products when nobody has a job? The correct and healthy mindset should instead be expansion and upscalling provided by all this additional productivity brought by AI.

TLDR ""AI coming for your jobs"" is a short-sighted mindset and a recipe for these companies to become irrelevant",2
post29con,controversial,1.2623637164513175,highest,Any of those short films any good?,2
post29con,controversial,1.2623637164513175,highest,"It depends on your taste. I made [this film](https://youtu.be/YMNzWtXE5aY), for instance. I would think like with other forms of expression, some like it and some don't. That is fine, I think.",3
post29con,controversial,1.2623637164513175,highest,"I watched the whole thing, and I tried to judge it in my head by the standards that I would judge a normal short film, not AI. And I will say that it was kind of shit. The AI artifacting, strange poses, model shifting, the flat voices, it all comes off to create a pretty off-putting product, compared to other short films. 

Watching it keeping in mind how amazing it is that a computer can generate this at all, it's impressive. But judging it as a short film on its own, there's just a lot that disrupts the message, storytelling, the emotions. 

One specific piece of feedback is that the scene with the mirror test was way too short, but some of the other scenes were way too long. Especially with the mirror test being the point where the protagonist breaks through, the significance of would likely need to be highlighted to somebody who isn't aware of the test in the first place. Like I knew what happened there but I feel like somebody who doesn't know the task would be confused by how quickly that went by. Overall, the pacing was the worst part.

The restrictions of the medium you use often strongly influence what the product is, and I think that it should be telling that you chose to make a film about an AI being trained with your AI trained tools. If you tried to make a short film that wasn't at least somewhat about AI, all the artifacting would be so out of place that it would ruin it entirely.

I do want to say that I am glad I watched to the end. I actually think that this could be an interesting short film if it was made through traditional means, either live action or animation. I think there also needs to be a lot more clarity to the emotion of the conflict in the first part of the video",4
post29con,controversial,1.2623637164513175,highest,"I've seen similar arguments before, with people pointing towards past technological revolutions and how humans found new work to do while old work was taken over by machines.

The problem I'm seeing is that we might be reaching the point where the gap between what machines can do and what humans can do is too small. There might still be talented individuals who can outperform machines or provide niche skills which machines have yet to adopt, but if you need to be remarkable to not be replaceable, many people will end up being replaceable.

Which may sound a bit harsh but so far as I'm aware it's the truth, and we can't afford to cover this truth up with a white lie, no matter how well-intentioned.",2
post29con,controversial,1.2623637164513175,highest,That's basically the right take that also resonates with me as well.,2
post50con,controversial,1.2616749555758244,highest,"Concerned about what exactly? How exactly could the AI, or any algorithms feeding off its output, be racist here in a way that negatively affects anyone?",1
post50con,controversial,1.2616749555758244,highest,"Basically, if we want the AI to „correctly diagnose“ diseases, we need to teach which diagnoses are correct. These diagnoses however can have a bias.

Imagine a world where no person with colourful hair ever gets treated for or diagnosed with sunburn. The AI is trained on the compiled data of thousands of diagnoses. It might recognise the same markers in people with colourful hair, but every time it marks them it gets told „wrong, no sunburn“. So it learns that people with colourful hair never have sunburn, and will never mark them as such.

The AI isn‘t racist as in „it hates them blacks“, it just perpetuates the biases in the dataset it was trained on, be they good or bad.",2
post50con,controversial,1.2616749555758244,highest,"I understand what you're saying, but i dont think that applies here. You have an AI that can detect race based on x-rays. How would an AI that can't detect race based on x-rays be better in any case? 

If there is racial bias in the data that is used to train the AIs, then the AI will learn that racial bias. Being able to detect race is not racial bias though.",3
post50con,controversial,1.2616749555758244,highest,"I don't think the issue per-se is about ML models being able to detect race in a dataset or it being used in a nefarious way. 

The problem is that the model supposedly encodes an assumption about the race of an individual when it's given an X-ray image. This means that it could take the X-ray of a person of one race and it could mistakenly encode some hidden assumption that the person's bone structure is similar to that of some other race in the image's representation. 

The performance of the model is then tied to distribution of X-ray image data for different races and this *could* hamper performance if it's used in conjunction with other systems that rely on race information. It becomes harder to trust the model's output for an X-ray image of a race it's not trained on.",4
post50con,controversial,1.2616749555758244,highest,"Here is the piece you are missing. If the AI can detect race from X-rays, that means that race-based correlations and biases present in diagnostic data can affect an AI diagnosis. Humans are unable to identify race from X-rays, thus the researchers had assumed that a diagnosis based solely on X-rays would be free of a racial bias. They found some evidence suggesting that this wasn't the case, and attempted to identify race via X-ray. The sole reason this study was conducted was that they found evidence of racial bias at the level of AI diagnosis. So yes, it is concerning that the AI can detect race from X-rays. It implies that we cannot rely on AIs to provide an unbiased diagnosis, even when we cannot fathom how that bias could occur.",4
post50con,controversial,1.2616749555758244,highest,"I‘m not saying there is :) The question was, how could such a thing negatively affect anyone. That‘s what I tried to answer :)",4
post50con,controversial,1.2616749555758244,highest,"The scientists aren't saying, ""oh no, the machine can see race, that's bad."" They're saying, ""maybe the machine seeing race is part of how it's underperforming for black people.""

They're not implying the solution is to make the machine unable to see race. They're saying they need to figure out how race plays into what the machine sees, and hopefully use that to improve the machine before rolling it out.",4
post50con,controversial,1.2616749555758244,highest,[removed],3
post50con,controversial,1.2616749555758244,highest,"Apologies for my ignorance, but is ""colourful hair"" another way to say ""red hair""?",3
post50con,controversial,1.2616749555758244,highest,it's just an example of someone that can be identified as such. could be anything really . in this case it's race,4
post50con,controversial,1.2616749555758244,highest,"I didn’t wanna use any hair colour, so I thought I‘d say dyed hair. Came out wrong lol",4
post50con,controversial,1.2616749555758244,highest,I assumed colorful hair was like green or purple.,4
post50con,controversial,1.2616749555758244,highest,"Hey, you’re not allowed to use the r-word!",4
post50con,controversial,1.2616749555758244,highest,Underrated comment here.  Well summarized.,3
post50con,controversial,1.2616749555758244,highest,"This! In the article it essentially states what you are saying here. Due to these biases, AI can select not to diagnose certain races once identified if these biases are not studied further and understood. This should be very concerning similar to AI’s inability to facially recognize Asian people in other studies. Data can be racially biased therefore making the ability to identify race based on X-Rays a problem instead of a benefit. This is my understanding of the article.",3
post50con,controversial,1.2616749555758244,highest,I would assume the AI would be smart enough to not say “can’t be sunburn” but instead “sunburn less likely”. For different races I don’t think there any diseases or issues that are all or nothing. Just some that are more/less likely to varying degrees.,3
post50con,controversial,1.2616749555758244,highest,Yupp! I was just oversimplifying greatly for ease of understanding. These nuances are really important when reading further into the topic though! Thanks for bringing it up!,4
post50con,controversial,1.2616749555758244,highest,"Well then your ML data needs to be retrained. You repeat until two datasets return the expected reponses repeatedly. This is nothing new, just another data point. Fluff article.",3
post50con,controversial,1.2616749555758244,highest,"Sounds a lot like how COVID symptoms and demographics were selected in the beginning of the pandemic. They had no clue who was actually at risk because of all the old people that were grouped together in New York and died. Skewed the whole data set from the beginning and made the death rate high enough to consider COVID dangerous. Then for the treatments they thought things worked because people who took them recovered but they were actually later changed because they didn't help people at all.

Initial conditions really have a lasting relevance when a system is being created from nothing. Hopefully they figure out how to properly setup the data to prevent wrong diagnosis.",3
post50con,controversial,1.2616749555758244,highest,"Aaaand let’s say this AI does become a racist, toothless bully. I know the solution. We can contribute code to break it and stop the terror. Easy!",3
post50con,controversial,1.2616749555758244,highest,"> These diagnoses however can have a bias.

Yeah, like have a massively improportional diagnosis of testicular cancer in men as opposed to women.  Huuuuuuuge bias. 

But AI with these trainings sets really will perpetuate any sort of wrong bias that gets into the training set.   The solution is not to hobble the AI and lobotomize them, but rather FIX THE DATA so they're properly trained.  Always side with the truth. The truth will set you free.",3
post50con,controversial,1.2616749555758244,highest,Yupp. I remember when someone (Google?) trained an AI to make hiring decisions and it ended up racist. Bias in the data -> bias in the AI.,4
post50con,controversial,1.2616749555758244,highest,"Let's say your AI that you implemented to replace credit scores to pick out the best ppl to give mortgages to independently concluded that it was most profitable to just blanket reject all ppl of a certain specific historically socioeconomically disadvantages ethnicity, and it wasn't wrong, and it wasnt trying to be racist on purpose.  What are you gonna do with this information?  What are you even legally able to so with this information?",2
post50con,controversial,1.2616749555758244,highest,"Fair enough. But anyone designing these systems then should decide responsibly what input data to even feed into the system. And the data it is trained on. 

In the case of detecting perceived ""race"" from skeleton images, we shouldn't really be surprised. Or overly concerned imo.",3
post50con,controversial,1.2616749555758244,highest,"Its being used for pathology. And there is variance in efficaciousness between ""races."" If you depend on a system like this and you don't correct for that,  the system becomes racist.

Also, I dont think that the word racist was used the article.",2
post50con,controversial,1.2616749555758244,highest,"\> The study adds to a growing body of data that AI systems can often replicate human biases and prejudices, whether they be racist, sexist, or otherwise.  


Yeah, it was.",3
post50con,controversial,1.2616749555758244,highest,"I don't understand, is it racist to simply point out that one person's skin color is different than another? Is it racist to point out that the same person has a relatively larger/smaller femur on average? Are we trying to pretend that different races didn't come from different paths of evolution?",3
post50con,controversial,1.2616749555758244,highest,[deleted],4
post50con,controversial,1.2616749555758244,highest,"The article states that implict bias may be brought in to the design of AI. This is for any phenotype. Its _____ist to not correct for implicit bias when it is known.

And of course people are different. Thats a core aspect of this article.",4
post50con,controversial,1.2616749555758244,highest,"Different races did not come from different paths of evolution, and that erroneous belief is the first fucking thing people are worried about reinforcing. Racial classification is based on phenotypical traits like skin tone, hair texture, nose and eye shape, etc, and almost entirely arbitrary (look up Nat Geo fraternal twins of different ""races"" as an example). The variations the x-rays are picking up are more than likely correlated with a ton of other factors.",4
post50con,controversial,1.2616749555758244,highest,"Yeah the scientists aren’t worried that their AI is racist, as far as I can tell

Rather they’re worried that having race be a factor could mean different outcomes for different races due to the additional input, which means some people could get worse care",3
post50con,controversial,1.2616749555758244,highest,">If you depend on a system like this and you don't correct for that

What does ""correct for that"" mean?

How do you know your corrections aren't even more problematic than the original 'biases?'",3
post50con,controversial,1.2616749555758244,highest,"That seems like semantics or a thought exercise more than anything productive. 

I think that the philosophical goal is to predict every single illness or disease with 100% accuracy. Until you get there, there is work to be done. If patients of particular ""races"" are further or closer to 100% than others, then there are missing data or biases that make it more or less accurate. So correction is needed.

If correction is the wrong word, have that point and help me use a term that makes this more comfortable",4
post50con,controversial,1.2616749555758244,highest,[deleted],2
post50con,controversial,1.2616749555758244,highest,"It's not that hard to predict someone's race as a human, no? If people wanted to predict race, well we had the tech do that algorithmically 15 years ago. Someone's perceived race was, by definition, never really private information.",3
post50con,controversial,1.2616749555758244,highest,"Well, an AI is spawned from the input it receives. So if a pool of information is presented, it can only calculate as it learned.   
Throw 2 random groups together; an AI can identify (group 1) as 100% ""normal"" vs (group 2) 99.9% ""normal"". Couldn't or wouldn't an AI separate that pool in some way from its baseline? ..then further presume that group 2 is flawed because it was not within the baseline study pool?    
This may not seem like an issue unless people in group 1 came from Northeastern Asia (also happens to be where the AI was developed) vs. group 2 that came from the continent of Africa. All unintended skewing of what we identify as equal information, just seen with a superior observing ability. An AI *could* outlearn us and make a separation without us ever knowing. Seemingly minor variables from our learning curve in programming alone may result in unecxpected discoveries or conclusions in any long-run.",2
post50con,controversial,1.2616749555758244,highest,[removed],2
post50con,controversial,1.2616749555758244,highest,"Being this sure of yourself about things you didn't study is honestly dangerous. And no, watching youtube videos of a redpill highschool graduate doesn't count. Dunning-kruger on full effect right there.

For instance, what is black and white people? Are Italians white? Because about 40 years ago white supremacists didn't think them as white. And where does black start or end? There are ""whites"" that didn't interact with other whites for thousands of years before globalization. There are millions of factors affecting iq, brain size, bone/muscle density and height other than genetics. Food culture, soil that food grows on, air quality, culture itself and healthcare are all more dominant factors.",3
post50con,controversial,1.2616749555758244,highest,"It already happens in some places in United States.I believe , algorithms used to allocate policing resources but based on algorithms of crime in those areas for last 40 years or whatever ,but is prejudiced against the current generation in those areas.",2
post50con,controversial,1.2616749555758244,highest,"Not a problem with the technology itself, but the people using it. And this ""discovery"" won't change that. If we want to fight racism effectively we need to focus on educating people more than we do the AI that they use. Until AGI, at least.",3
post50con,controversial,1.2616749555758244,highest,A racist AI? Fuking computers and its codes are rayyciiisssttt,2
post50con,controversial,1.2616749555758244,highest,Writer is a sheltered idiot with a rigid perspective.,2
post50con,controversial,1.2616749555758244,highest,*China has entered chat*,2
post50con,controversial,1.2616749555758244,highest,It doesn't align with their political view,2
post50con,controversial,1.2616749555758244,highest,"Maybe they have AI watching us through x-ray cameras but they don't want to admit it: 

""oh no, this AI can tell race from x-ray, they might discriminate between races""

""why would that be an issue except after you got an x-ray? it's not like we're constantly being surveilled with x-ray cameras during interactions which would allow for discrimination or anything, is it?"" 

""...""",2
post50con,controversial,1.2616749555758244,highest,"Well, at least we can see what happens in this thread : an ai is trained to categorize based on certain caracteristics, and a fuck ton of people immediately conclude that the categories aren't constructed. The ai is fine, but people already use it to feed their confirmation bias.",2
post50con,controversial,1.2616749555758244,highest,I think the article is implying that doctors are concerned because humans can't predict the race of someone just by looking at x-rays and it may lead the AI to have a racial bias towards treatment plans/diagnosis if implemented.,2
post50con,controversial,1.2616749555758244,highest,"From the article:

“ Artificial intelligence scans of X-ray pictures were more likely to miss indicators of sickness among Black persons, according to earlier research. Scientists must first figure out why this is happening.”",2
post50con,controversial,1.2616749555758244,highest,It also ignores the fact that doctors already apply racial bias (and bias along other lines such as sex) when diagnosing and treating patients.,2
post50con,controversial,1.2616749555758244,highest,"I feel like it could be evidence that racial bias actually can effect a person's treatment and health. It's scientific support that bigotry isn't ""politics"", it has physical consequences.",2
post50con,controversial,1.2616749555758244,highest,"The scientists aren't saying, ""oh no, the machine can see race, that's bad."" They're saying, ""maybe the machine seeing race is part of how it's underperforming for black people.""

They're not implying the solution is to make the machine unable to see race. They're saying they need to figure out how race plays into what the machine sees, and hopefully use that to improve the machine before rolling it out.",2
post50con,controversial,1.2616749555758244,highest,"It's more so that AI has a tendency to perform more poorly with ethnic minority related data, since ethnic minorities are minorities and therefore have generally less data to train AI. 

It's not usually bias, but underperformance that is the problem here. Of course, there is always the potential for the users of an AI to use its output in a discriminatory way.",3
post50con,controversial,1.2616749555758244,highest,"Indeed, underperformance is ""the problem"". You might even call it ""a concern"". It's really an open-ended question of, can we figure out why the models are underperforming, exactly? Maybe the explanation will point to other ways they underperform? 

I feel like people are responding to this article as if the takeaway was, ""stop! It's going wrong!"" When in reality the takeaway is, ""okay, we're getting there slowly, not quite ready yet.""",4
post50con,controversial,1.2616749555758244,highest,"People in denial still trying to wrap their heads around the fact that humans can be categorized into different sub-species. 

They still think race is only ""skin-deep"".",2
post50con,controversial,1.2616749555758244,highest,"""Sub-species"" is a bit of a stretch imo. There are obviously differences between races but they really don't go much past a few cm on avg here, a bit more lactose (in)tolerance on avg there... 

But yeah, I'd agree that there's deeper differences than skin for sure.",3
post24con,controversial,1.25448594284278,highest,"It’s not a racial bias.

Facial recognition systems rely on reflected light to extract the information about the shape, curves, and contours of a persons face. If you were paying attention in high school science you’ll remember light colors reflect more light dark colors absorb more light.

If someone has light colored skin more information is reflected back to the computer, if a person has dark colored skin less information is reflected back. The more information the computer has to work with the more accurate it can be about recognizing someone. Less information means the computer is going to be less accurate at recognizing someone.

Facial recognition will always be more accurate on lighter skinned people because more reflected light means more data which allows the software to more accurately make a match.

It’s not racist software or programmers, it’s not sample data being too white, it’s not a bias unconsciously baked in, it’s just less reflected light.",1
post24con,controversial,1.25448594284278,highest,"So, it is a racial bias. Even if they try to work to solve this, at the end being black makes it more error prone. Which in return can cause a lot of issues.",2
post24con,controversial,1.25448594284278,highest,If you call „skin color bias“ „racial bias“ then yes.,3
post24con,controversial,1.25448594284278,highest,"If painting your face brown is racist, then yes, skin color is linked to race, and yes, it is the same thing.

Failing to understand this show a lack of common sense.",4
post24con,controversial,1.25448594284278,highest,The machine is only as accurate as the amount of light being reflected. The amount of light reflected is a function of someone’s skin color. You can’t make the cameras see something that a person’s skin isn’t reflecting.,3
post24con,controversial,1.25448594284278,highest,And how this isn’t racial bias exactly ?,4
post24con,controversial,1.25448594284278,highest,"\^ This

People might as well complain to the laws of physics that it is biased and should check its privilege.

It could be just training data but whatever the results is, it will always be inherently less accurate for darker skins because of just basic Physics.",2
post24con,controversial,1.25448594284278,highest,"If something is racially discriminatory in the results it produces, even if it's due to light reflection and not malicious intent, that still doesn't change the fact that using it will disproportionately harm certain people. That's still a problem.",3
post24con,controversial,1.25448594284278,highest,"Oh so we should stop blood transfusions then? Because O- people and AB- people will be disproportionately harmed because they can only receive certain types of blood and can donate to other more?

No. Inherent skew and biases within the universe exists. We use those to get things done. It is not ""Racially discriminatory"", the laws of physics is not unjust or prejudiced. It is just itself.

Say that Police runs after people right? are criminals that happens to not be able to run good counts as ""Discrimination"" ? Then we should stop having police entirely?

Railing about any differences whatsoever is so counterproductive. People will be treated differently because of a million different variable in life. Live with it.

Facial recognition system is a way forward for law enforcement. It existing is better than nothing which is what the other option is. Would you really rather hand over the task to actually racist humans prone to error of judgement and prejudice?

Automation no matter how flawed inevitably gets ahead because humans no matter how adaptable are full of imperfections and cannot be expected to perform consistently close to 100% perfection in a single task. Not to mention there are actually evil and horrid humans existing out there. Even if AI could perform badly, we can still expect it to do better than a human eventually with enough development.

&#x200B;

Having facial recognition is better than nothing.",4
post24con,controversial,1.25448594284278,highest,"That doesn't make it ok to use the technology in its current state. 

You can still use the tech, but if you can't equalize the performance by improving darker skin more than light skin, then you have to artificially reduce the performance more for lighter skin than dark. 

The fact is we can't continue using technology with known biases that reinforce existing ones which we agree as a society are unacceptable.",3
post24con,controversial,1.25448594284278,highest,"The problem is that you're including ""Society"" into this. The laws of the universe won't give a shit about society.

If we use your logic, we can also say that we shouldn't use vaccines because not everybody can use it and it will be just unfair because a certain amount of people will get to live more = discrimination because those certain people won't get to reproduce as much as people who gets a vaccine.

No. The net result of having vaccines is better. Same thing with Facial recognition. Having facial recognition is still better than not having it. Would you rather hand off the job to actually racist humans?

Removing the human factor especially in law enforcement is almost always a good thing. It doesn't have to be the final say in these matters. It just have to provide information to those that do have a final say.",4
post24con,controversial,1.25448594284278,highest,"I agree that we can't say the people who built it are racist, but the technology certainly is no matter what the reason. 

In this case, you could ""fix"" the problem by inserting some randomness based on the skin tone to equalize the likelihood of mis-classifying a lighter-skinned face. 

But somehow I don't think people would be comfortable with that.",2
post24con,controversial,1.25448594284278,highest,"Technology is inanimate so it’s literally impossible for it to have feelings and express them through racist actions.

This whole issue is caused by the amount of light being reflected, so is the light racist for not reflecting enough",3
post24con,controversial,1.25448594284278,highest,"I agree that a technology cannot be racist. The definitions of racism I can find imply racism is a property of as person. 

However, I hope we can agree that a technology can reflect and reinforce racism. That doesn't make anyone or anything involved in its creation or operation racist. But the sum of it all ... can reflect and reinforce racism.

This technology does seem like a case of that. I don't know if you and I agree on that.",4
post24con,controversial,1.25448594284278,highest,I wonder if it could be paired with IR sensors or something else so that it can analyse all skintones accurately,2
post24con,controversial,1.25448594284278,highest,"It’s the laws of physics of the universe, lighter colors will always reflect more light, and darker colors absorb them. 

Adding IR cameras are just going to see more reflected Infrared light.",3
post24con,controversial,1.25448594284278,highest,"Not quite.

Cameras can be fine tuned to any situation.  If the world were 90% black the cameras would be oversaturated on white people and pull out finer details on black people.",2
post24con,controversial,1.25448594284278,highest,Because lighter skin reflects more light than darker skin,3
post24con,controversial,1.25448594284278,highest,But the thing to remember is that cameras are built to produce good video of what is common.  So if what was common was different so would cameras.,4
post48con,controversial,1.198546062380535,highest,">The results from our study emphasise that the ability of AI deep learning models to predict self-reported race is itself not the issue of importance. However, our finding that AI can accurately predict self-reported race, even from corrupted, cropped, and noised medical images, often when clinical experts cannot, creates an enormous risk for all model deployments in medical imaging.

Im unsure what they imply by risk...

&#x200B;

Edit: 

They imply by risk that if AI is trained with biased data and not audited for that bias, then AI would also be perpetuating biases it was trained with, in this case, racial bias. 

>We strongly recommend that all developers, regulators, and users who are involved in medical image analysis consider the use of deep learning models with extreme caution as such information could be misused to perpetuate or even worsen the well documented racial disparities that exist in medical practice. Our findings indicate that future AI medical imaging work should emphasise explicit model performance audits on the basis of racial identity, sex, and age, and that medical imaging datasets should include the self-reported race of patients when possible to allow for further investigation and research into the human-hidden but model-decipherable information related to racial identity that these images appear to contain.",1
post48con,controversial,1.198546062380535,highest,"It means that the AI can find the race and use that as a proxy. If you train a ML algorithm for something like how much painkillers to give someone, then you have the issue of the fact doctors underperscribe back people. So the ML can have embedded racist outcomes based on factors like race. Now you might think that's not a problem if you don't input race into the model, but if the ML algorithm can find out your race though an xray, then that risk still exists.

A real life example of the risk was around using a ML algorithm to set bail. But this model just gave higher levels of bail to black people, since it was trained on racist data.",2
post48con,controversial,1.198546062380535,highest,"It is also a good thing that the AI can predict people's race from x-ray images, because now a patient's bone structure phenotype can be compared to the average for their race or ethnicity, rather than the average for all races, which would greatly increase the accuracy of the criteria used to determine whether certain bone structure phenotypes are disease markers or not.",3
post48con,controversial,1.198546062380535,highest,"Youre correct, i read part of the article behind the paywall and that's what the researchers say",3
post48con,controversial,1.198546062380535,highest,Yeah it's a tough problem because on one hand you don't want the AI to be biased based on race but on the other hand race can have an effect on health risks and should be considered when making some diagnoses.,3
post48con,controversial,1.198546062380535,highest,"Yep it's a hard problem. They used to actually think that black people were different and didn't feel pain like white people did. This myth hasn't completely died out. While it's a myth that causes harm, there could very well be other situations where you do want to treat black people differently due to biological differences. 

I think the key thing here is to have a really good understanding of how the model works and what factors it might be taking into account. This article is about the fact the model has capabilities they didn't expect and hence could act way differently than they want.",4
post48con,controversial,1.198546062380535,highest,[removed],3
post48con,controversial,1.198546062380535,highest,"Yes yes, we know. Only sentient beings can be racist. 

But data can have racial inequities that result in unfair or unequal results based on race.",4
post48con,controversial,1.198546062380535,highest,You know what I mean. Data that differentiates treatment based solely on race with no medical reason for the difference.,4
post48con,controversial,1.198546062380535,highest,">Findings regarding the possibility of confounding of racial identity in deep learning models suggest a possible mechanism for racial disparities resulting from AI models: that AI models can directly recognise the race of a patient from medical images. However, this hypothesis is largely unexplored
and, in contrast to other demographic factors (eg, age and sex), there is a widely held, but tacit, belief among radiologists that the identification of a patient's race from medical images is almost impossible, and that most medical imaging tasks are essentially race agnostic (ie, the task is not affected by the patient's race). Given the possibility for discriminatory harm in a key component of the medical system that is assumed to be race agnostic, understanding how race has a role in medical imaging models is of high importance
as many AI systems that use medical images as the primary inputs are being cleared by the US Food and Drug Administration and other regulatory agencies.",2
post48con,controversial,1.198546062380535,highest,Potentially a privacy risk? You try to anonymize an xray by cropping it and the the Ai tells you a bunch of stuff about that patient.,2
post48con,controversial,1.198546062380535,highest,But who's looking at anonymized x-rays? And who cares what they know about a person; the system won't predict their name.,3
post48con,controversial,1.198546062380535,highest,"That doesn't look to me like the reason... The AI can tell you the patients race, a doctor (i guess specialised in the subject, I can't) could tell you too, but less acuratelly , but it's only one detail about the subject and it's not considered identifying information in the ethics and privacy sense of the term, it would not be considered a risky thing",3
post48con,controversial,1.198546062380535,highest,"If you can tell a person is “Black” and you think “Black” people don’t care for their health, your advice to them might be different, for no actual good reason. 

You might tell them just make sure to take their pills instead of pushing them to make lifestyle changes that are more effective, but take more effort.",4
post48con,controversial,1.198546062380535,highest,Don’t forget simple bias creeping in as well.,3
post48con,controversial,1.198546062380535,highest,"This is the actual reason.  Data contamination.  A doctor's racial biases are assumed to be ""white noise"" by creators of medical image diagnostic algorithms, but this study shows that the same types of algorithms commonly used for medical imaging diagnoses can also easily identify race, so any race-based underdiagnosis (which is a known and profound problem for minorities) will likely be perpetuated by the AI.",4
post48con,controversial,1.198546062380535,highest,"Imagine a study that uses these X rays to determine whether a certain course of treatment for a given disease is worth the expense involved. An AI model gets a large training set of X rays together with the outcomes from those patients after the treatment. 

Then it's shown a new X ray and is asked to predict the outcome for that patient. If it predicts a positive outcome, the treatment is recommended. If not the treatment is not recommended. 

One problem is that the training data the model is trained on was probably taken from a variety of different hospitals and research facilities with different levels of funding, standards of care, etc. Because of structural racial inequalities, the patients at hospitals with lower standards of care and worse outcomes might be more likely to be from certain racial groups. An AI model capable of discriminating between racial groups will ""automatically"" learn to use this information in predicting outcomes. So two otherwise identical patients might receive different recommended courses of treatment if they're from separate racial groups.

Imagine something like this being used to determine whether you were eligible for potentially lifesaving care.",3
post48con,controversial,1.198546062380535,highest,This is the answer.,3
post48con,controversial,1.198546062380535,highest,Turns out its totally not the answer,4
post48con,controversial,1.198546062380535,highest,"Is it? Like, did you see it somewhere or know it for a fact or is it just it sounds to you like the answer?",4
post48con,controversial,1.198546062380535,highest,"It's fairly commonly known that racial minorities and women are under-diagnosed by doctors.  If the training data is contaminated with that bias and the algorithm can detect sex/race, the algorithm will *also* under-diagnose those populations.  

If monkey can see, monkey will do.",2
post48con,controversial,1.198546062380535,highest,"Unintentional confounding in your data, particularly when you can't audit how the model performs over different races directly.",2
post48con,controversial,1.198546062380535,highest,"But that information is important/relevant? Races (and sexes) react differently to various medication for example, knowing someones race is impactful in treatment options. 

I wonder how much of this is fear of perceptions of racism and how much is to do with actual racial biases.",2
post48con,controversial,1.198546062380535,highest,I believe the point is that it's not always relevant. It could stem from racism or other non-medically related bias.,3
post48con,controversial,1.198546062380535,highest,Exactly. Finding someone’s age or race by feeling an X-ray into an AI isn’t “risky”.,2
post48con,controversial,1.198546062380535,highest,"I'm not sure either...
One risk is an AI, that, because of the racial distinction egrained into the model, performs worse than a counterpart without those distinctions. But that doesn't make any sense. Wouldn't you want an AI that matches its domain more closely? An AI that learns those differences in biology, could only use them to do better, not worse. Thats the only meaning that relates the ""risk"" to the ""model deployments""...

Or its about privacy? Ex.: A radiation therapy tech, who creates intentionally dangerous treatment plans, to harm people of certain races? But it says, ""risk for all model deployments"", not ""risk for patients, at the mercy of racist doctors, who have an AI to pick out people they dont like"" or something like that.

Couldn't think of any other meanings, but i'm also neither an expert on machine learning nor english....",2
post48con,controversial,1.198546062380535,highest,"i read a bit more and the explanation is more simple, it has to do with bias in diagnostics made by humans, if you have a condition that isn't properly diagnosed in a certain race, like severe cases being considered mild in a certain ethnic group because of bias, then the database you use to train the AI has those biases too, and AI could associate those biases with race and continue them, that's the risk, and the researchers say that AI used for diagnostic should be checked for this, to avoid the AI making the same mistakes that humans make because it was trained on a data base that has those mistakes too",3
post48con,controversial,1.198546062380535,highest,"Ohhh, now i get it, thanks!",4
post25con,controversial,1.1644088130452948,highest,"I actually think AI is vastly less biased than most humans if you give it the right directives, if ""biased"" is even the right term to use here.

I'm far more worried about my fellow biological dipshits who appear to have a difficult time living together for a few decades without trying to take each others rights away for literally zero reason.",1
post25con,controversial,1.1644088130452948,highest,"> AI is vastly less biased than most humans if you give it the right directives

Directives that come from humans, programmed and taught by humans, who are all biased.",2
post25con,controversial,1.1644088130452948,highest,"Another issue.     The more centralized something is, the more systemic bias can be.   Thus AI errors can be worse.

So, if you have a bunch of biased humans doing something, sometimes their biases cancel each other out. So, my ""Black woman who is an expert in distributed computing"" might be rejected by a racist or sexist boss but hired by another boss in the same company.

If you have a policy, for example, Donald Trump had an explicit policy that no Black tenants were to rent his properties. He got caught

If you have an AI that has deduced that Black women aren't good programmers, it's screening criteria can sometimes be hidden. Further the Black woman applying to multiple departments in the company is shut out by one centralized racist component",3
post25con,controversial,1.1644088130452948,highest,I think part of the problem is also people assuming the AI will be objective and more insightful than humans would be.,4
post25con,controversial,1.1644088130452948,highest,"so?

If a human that is shitty at basic arithmetic programs a calculator, doesn't mean that the calculator is going to be worse or as bad as the human who made it. You're not making a good argument here.

If there's one thing computers excel at, it's unerringly calculating without really giving a shit about anything else. This comes with its own problems, but certainly that quality will be good for avoiding some of the brainbroken mindfuck thinking traps that humans just love to fall into.",3
post25con,controversial,1.1644088130452948,highest,"> You're not making a good argument here.

No, *you're* not. You can't program bias into something that's either correct or it's not. 2+2=4 regardless of whether you're Martin Luther King or the Imperial Grand Dragon of the KKK.

> If there's one thing computers excel at, it's unerringly calculating without really giving a shit about anything else. This comes with its own problems, but certainly that quality will be good for avoiding some of the brainbroken mindfuck thinking traps that humans just love to fall into.

That is not how this works. There are entire fields of study dedicated to bias in AI and responsible AI use. I mean, why do you think most AI-generated images were of white people unless specifically directed otherwise? It's not cause white people are the majority! We're not talking about basic equations here, we're talking about things with a LOT of room for judgment, bias, and error.",4
post25con,controversial,1.1644088130452948,highest,"Did you hear of the Tesla that crashed in snow?

When your algorithm is biased it can be deterministically so

In general, human drivers are more fallible but unless we know about the snow issue, it will mean your probability of death in a snowstorm is 100%

An algorithm that decided Black women suck at computing will reject 100% of Black women whereas a pool of fallible humans will have hiring managers giving Black women a chance and one's not

Further using an AI without understanding the types of errors possible is an issue.",4
post25con,controversial,1.1644088130452948,highest,"I would add to Kali's statement that the issue is that people think things like this - ""Oh, it's an algorithm, it's not biased"", and thus allow it to magnify the effects of the biased input data.  It's hugely problematic and one reason that many CS programs are requiring more ethics/philosophy/humanities type courses.",2
post25con,controversial,1.1644088130452948,highest,"A competently taught first year course in statistics would also explain the issues of bias as would advanced courses in experimental design

You don't need a philosophy course to step outside the box and look at what garbage in, garbage out means",3
post25con,controversial,1.1644088130452948,highest,"Most of the CS students I've advised and taught would benefit from a philosophy course.  They understand GIGO, but they don't understand what makes something garbage without something in the humanities when it comes to issues like demographics.  First year courses in statistics simply teach methodology, not how one recognizes and corrects for input bias.",4
post25con,controversial,1.1644088130452948,highest,"look, of course I know that the output reflects the training data to some degree, the training data isn't clean, at some point the AI will produce its own data which ends up in the training data and that sort of autocannibalism will cascade and create biases in the AI, blabla that's all elementary. 

What I'm saying is that whatever technical faults and imperfections you can identify with AI, humans are certainly much worse. The average human still thinks that appeals to nature are a logical argument. The average human still thinks that women are from venus and men are from mars.

This is not a question of ""is AI biased"" - of course it is. But is it more biased than humans? Fuck no. Humans are goddamn awful at thinking straight. It's a miracle we ever got as far as we did.",3
post25con,controversial,1.1644088130452948,highest,"Humans are capable of thought, and some of those thoughts is ""I might be biased"" and ""this data might be biased"".

And humans are capable of self reflection and acting on it, such as things like ""if I'm biased, is that a good thing? If not, how to I minimize it"".

AI models are not sapient. They do not think. They do not understand.

ChatGPT isn't thinking or talking or writing. It's just outputting the statistically most likely set of words back to you that fit the input words.

It's basically a really good search engine that will rephrase the internet consensus on a subject and parrot it back to you.

The bias it inherited from its training data is built into every response, without recourse of fixing it. That's why they place guardrails on it to try to *prevent* biased from reaching the user, because the bias cannot be removed.",4
post25con,controversial,1.1644088130452948,highest,"Please consider my Black computer programmer example.

Assumption :

There are fewer Black women in computer science than several other Demographics such as White men, Chinese men, Chinese women, etc

You ask the AI, please look
at these resume and select the candidate most likely to be qualified 

The AI has analyzed the entire set of computer professionals and concluded it will NEVER hire Black women.

Unless we are aware of this type of error, we have ""Garbage in"" and ""Garbage out""",4
post25con,controversial,1.1644088130452948,highest,"> I actually think AI is vastly less biased than most humans if you give it the right directives, if ""biased"" is even the right term to use here.

Technically correct. But also, AI is never trained on unbiased data.

A large language model like ChatGPT is trained by indiscriminately scraping the entire internet, meaning that internalizes what gets said the most. Image-processing models follow a similar process, meaning that they internalize what gets shown the most.

Even AI that isn't trained on literally the entire internet gets trained on the data available to the programmers. So, for example, cameras programmed by white people tell Asian users ""It looks like somebody blinked. Let's take that picture again."" Because they've only ever seen white faces. Sinks programmed by white people don't activate when they detect a black hand. Because they've never seen black hands.

Nobody is trying to make racist or sexist software. But it's a lot more difficult and more expensive to make software without bias. And usually deadlines and budgets are the top priority.",2
post25con,controversial,1.1644088130452948,highest,"It's worse. 

Even if you have unbiased data, that unbiased data can reflect the reality that correlations exist that aren't causations",3
post31con,controversial,1.142082366337226,highest,"That's a long way of saying ""money"".",1
post31con,controversial,1.142082366337226,highest,"Now go back, why did they start the DEI initiative?  

Oh right also money.  

Maybe corporations aren't interested in the right thing.  Just money.  Crazy.",2
post31con,controversial,1.142082366337226,highest,"It’s crazy that everyone knows this, but it doesn’t hurt commercialism or brand loyalty. 

Soon babies will be born with knowledge of ‘the bottom line,’ and they’ll still be captured by advertising.",3
post31con,controversial,1.142082366337226,highest,"Even worse if you pointed out a few months ago that this was skin deep you'd be at best down voted, or called a racist bigot and in many cases banned from subs for saying it.

And it took them what? 3 days of it being unpopular to completely undo and reverse.",4
post31con,controversial,1.142082366337226,highest,"Even politicians and any institution. I work at a public university in a swampy red state. In 2020 the governor and board of governors wanted us all to embrace and flaunt DEI initiatives because it was the politically popular thing to do. Two years later they wanted to erase it all because it was the politically popular thing to do. They don’t actually care if we implement it or not, they just don’t want us to talk about it because that’s what could cost them moneys.",3
post31con,controversial,1.142082366337226,highest,"Pssst, you’re getting on a blacklist for this. Musky man is gonna getcha",3
post31con,controversial,1.142082366337226,highest,I’m trying to ensure I’m on the blacklist. All my homies are on the blacklist.,4
post31con,controversial,1.142082366337226,highest,"We all Need to be honest with ourselves, The Enture thing with DEi Is The ""The Good Ole Anglo Saxton Boys Club"" at the Office Only! No Women Send them back home, No Black or brown women, Nk Lgbtqia reguardless if their Qualified and No Persons with Disabilities, that they would have to spend money to make accommodations for. This is the Handmaiden Tail starting all over again. It's BS.",3
post31con,controversial,1.142082366337226,highest,"What is exactly is DEI? Why are we mad it’s gone and what does it do?

Edit: love how I’m just getting downvoted without anyone actually responding. Is DEI that hard to defend? Surely someone out there has a good answer.",3
post31con,controversial,1.142082366337226,highest,"Diversity, equity, inclusiveness.

Traditional hiring practices tends to be subconsciously rigged to favor the culture and identity of the person who is evaluating hires.  By adding a bit of extra red tape to the process, a broader range of candidates can be considered for the position.

While less efficient, it also allows an organization to get a better pick of people to join their ranks.  Traditional hiring practices tend to overlook the more capable candidates, since cultural blinders tend to favor certain styles of name, background, or ethnicity.  DEI mitigates the issue.",4
post31con,controversial,1.142082366337226,highest,"You're being downvoted because you come across as privileged or ignorant. Mostly ignorant, I think.",4
post31con,controversial,1.142082366337226,highest,Maybe DEI isn't the right thing. Crazy.,3
post31con,controversial,1.142082366337226,highest,DEI is about attracting investors but ticking customers off.,3
post31con,controversial,1.142082366337226,highest,What customers are ticked off about DEI except racists and shitheads?,4
post31con,controversial,1.142082366337226,highest,"Vote with your service choices. They are only as useful and powerfull as you make them. Stop using them. YeH Google is a tough one to stop using. But we do have choices, they may not be as convenient, but they work. Just use their free stuff. But I'd stop using Gmail for anything private. Maybe find a foreign secure data protected service abroad. Check European providers,  their Data Protection Laws are muuuuch better than US ones.",2
post31con,controversial,1.142082366337226,highest,That’s how capitalism works. Moral hazard is the way to corporate refinement.,3
post31con,controversial,1.142082366337226,highest,"Their stock price is currently 200$ a share iirc

It’s crazy to me how much money these virtual companies are worth

Meanwhile Ford which has made automobiles for 100+ years is 10$ a share",2
post31con,controversial,1.142082366337226,highest,"Just a heads up, price per share is a very poor indicator of a company value. Because number of share is not fixed, and companies can merge or split share.",3
post31con,controversial,1.142082366337226,highest,"I know that but just look at market cap for example, google is like 2trillion, it’s mind boggling.",4
post31con,controversial,1.142082366337226,highest,Working class people don't care about shares,3
post31con,controversial,1.142082366337226,highest,"It's like there are two kinds of people. One kind hears a song, and says, ""That was lovely"", or ""I did not like that."" Things along those lines. The other hears it, and wonders, ""How do I take this for myself?"" 



I wonder if it's a condition of birth or environment. Because I feel like as apes, we would have beaten the living shit out of those ones, long before they were able to infect the rest of us with their awfulness.",4
post31con,controversial,1.142082366337226,highest,That's why they remain working class their whole lives.,4
post31con,controversial,1.142082366337226,highest,"Vote with your service choices. They are only as useful and powerfull as you make them. Stop using them. YeH Google is a tough one to stop using. But we do have choices, they may not be as convenient, but they work. Just use their free stuff. But I'd stop using Gmail for anything private. Maybe find a foreign secure data protected service abroad. Check European providers,  their Data Protection Laws are muuuuch better than US ones.",2
post22con,controversial,1.1356915237032883,highest,"DEI is racist, discriminating against qualified candidates because they have the wrong skin color. If you want to fuel anger and disenfranchisement this is a fast way of doing it.",1
post22con,controversial,1.1356915237032883,highest,"I’ve never met a good programmer who lost a job due to DEI. Nobody serious is interviewing two candidates and picking a vastly inferior one due to DEI. Every public incident reportedly caused by unqualified DEI hires has ended up being right wing pearl clutching with absolutely no relation at all.

You’ve been tricked into arguing about uno while the elites play chess.",2
post22con,controversial,1.1356915237032883,highest,https://medium.com/the-mission/im-an-ex-google-woman-tech-leader-and-i-m-sick-of-our-approach-to-diversity-17008c5fe999,3
post22con,controversial,1.1356915237032883,highest,"Like I said, I agree it’s a flawed idea. I take issue with it being politicised by the right. Its actual impact is so unbelievably small in contrast to the noise people make about it.",4
post22con,controversial,1.1356915237032883,highest,"How can you possibly know the number of jobs a ""good"" programmer was not ultimately offered because the employer instead hired someone else to meet a quota?",3
post22con,controversial,1.1356915237032883,highest,"Because I live in the real world, where instead of getting angry at right wing talking points designed to deflect attention from actual issues, I look at the reality of each situation and assess it based off what I find to be true.",4
post22con,controversial,1.1356915237032883,highest,"Well i’ve hear of some good ones that couldn’t get a job because of it.

And how could you even have heard of that??
It’s not like companies fire Brian, middle aged white dude straight telling him sorry man, we gotta make room for more diversity. Nor that then blatantly sharing the story would serve Brian in any way…",3
post22con,controversial,1.1356915237032883,highest,Do you work in tech? The only people I’ve met who have complained about not getting a job due to DEI are objectively terrible at their jobs.,4
post22con,controversial,1.1356915237032883,highest,"> I’ve never met a good programmer who lost a job due to DEI.

How on earth would you tell, though? What sort of test could you possibly construct, from the candidate PoV, that could estimate the probability of a application failure being due or not due to DEI?",3
post22con,controversial,1.1356915237032883,highest,"Because I've worked with every skill level in tech between ""I can barely turn a rock on"", all the way up to literal celebrities in the world of software engineering.

DEI is just political dressing on top of the normal hiring process. Nobody who is unqualified is getting hired. Nobody who is good at their job cares.",4
post22con,controversial,1.1356915237032883,highest,"It's genuinely so sad how well conservative propaganda has worked on many people, including coloring the term ""DEI"" as racist or discriminatory. The point is that the ""racism"" you complain about exist, just that you refuse to recognize it cause you don't think people's biases affect society and especially hiring in historically discriminated industries including engineering.

For a method that noone should have a problem with, I'd recommend reading up on equality in shortlisting and please let me know your opinion of it. In summary, it's ensuring recruitment shortlists have increased equality (in as many factors as possible), while ensuring that truly the best person is hired (which some people argue targeted recruitment doesn't).",2
post22con,controversial,1.1356915237032883,highest,It’s pretty depressing. They really have these people arguing over grains of rice.,3
post59con,controversial,1.1304461369417722,highest,"AI has barely begun. I’d rather Apple take their time than release something half baked, like googles variant telling you to eat rocks or whatever….",1
post59con,controversial,1.1304461369417722,highest,"Google’s variant was trained on Reddit, and it shows.",2
post59con,controversial,1.1304461369417722,highest,"It knows how to fix this weird problem I’m having because there’s a 3 year old thread on some sub I’ve never heard of?

^(Yeah yeah I can guess what you mean)",3
post59con,controversial,1.1304461369417722,highest,">rather Apple take their time

The issue is that Apple hasn’t even really started",2
post59con,controversial,1.1304461369417722,highest,Apple is notoriously secretive. So it's hard to know conclusively what they've been doing behind the scenes and we just have to rely on leaks or rumors.,3
post59con,controversial,1.1304461369417722,highest,"Apple has been building minor ML capabilities throughout their products for YEARS. You don’t just jump straight into genAI with no foundation. They’ve also been laying groundwork for this with Apple Silicon and how well optimized it is for ML and AI workloads. Apple is making good long term decisions here. 

This is also totally consistent with all other major product decisions at Apple. They wait, observe, deliver their own products when they’re ready, then iterate on them. Every majorly successful product for Apple has been late to the game",3
post59con,controversial,1.1304461369417722,highest,"They have started. There’s a significant amount of AI/ML in many parts of their apps. It’s just not in your face. And clearly Siri hasn’t benefited, but that’s okay because I don’t need her to tell me to put glue on my pizza or something similarly obtuse.",3
post59con,controversial,1.1304461369417722,highest,[removed],3
post59con,controversial,1.1304461369417722,highest,"But they don't always provide more polished solutions.  Siri has been around for over 10 years now, and she's much worse than Alexa and Google.  The homepod is a great speaker, but it's not a great *smart* speaker.  

Siri needs better language processing capabilities so she can better understand what people want to do if they don't say it exactly the way Siri wants.  She could also use a 3rd party store of some kind to extend functionality.",4
post59con,controversial,1.1304461369417722,highest,"As a long time iPhone user who also uses android, Apple providing more polished solutions today is just isn't true anymore.",4
post59con,controversial,1.1304461369417722,highest,The OpenAI thing is official as in confirmed by either company? Because as of my Google search just now that’s just “reportedly” which isn’t worth much.,4
post59con,controversial,1.1304461369417722,highest,"I mean, they have, and it’s good so far. See AI in the photos app and camera for example",3
post59con,controversial,1.1304461369417722,highest,I started using Google photos because I have a paid Google email account for $15 a month and it comes with 2TB of cloud storage which is a way better deal than what iCloud costs and their AI stuff with photos is just so much better.,4
post59con,controversial,1.1304461369417722,highest,They’re really far behind the competition,4
post59con,controversial,1.1304461369417722,highest,As if you know the solution. What are they missing?,3
post59con,controversial,1.1304461369417722,highest,According to whom?,3
post59con,controversial,1.1304461369417722,highest,Yup Apple always waits they never want to be the first one but wants to be the best one,2
post59con,controversial,1.1304461369417722,highest,Man how long are they waiting to make Siri good?,3
post59con,controversial,1.1304461369417722,highest,Since it released,4
post59con,controversial,1.1304461369417722,highest,Siri is not generative AI. It’s  a voice assistant that basically runs on a set of pre defined commands and actions but then falls back to a bing search when it doesn’t understand,4
post59con,controversial,1.1304461369417722,highest,Except when they are late and shit.,3
post59con,controversial,1.1304461369417722,highest,I said “wants to” not “is the best one” they surely mess up,4
post59con,controversial,1.1304461369417722,highest,"What do you mean? The AI stuff already in iOS is pretty good from what I’ve seen. And Siri does NOT use AI, at least not yet",4
post59con,controversial,1.1304461369417722,highest,"Like Siri (not late but I digress), Apple TV devices, Vision Pro, Newton, Pippin, etc.",4
post59con,controversial,1.1304461369417722,highest,[deleted],2
post59con,controversial,1.1304461369417722,highest,"Siri doesn’t use AI, not yet anyway",3
post59con,controversial,1.1304461369417722,highest,"Yep, like they did with EVs.",2
post59con,controversial,1.1304461369417722,highest,"Well it wasn’t EV’s, it’s more the common sense logic that self driving cars aren’t ever going to be viable, at least not yet. How many years have Tesla’s had self driving hardware you can’t even use? What a terrible waste imo",3
post59con,controversial,1.1304461369417722,highest,"So Apple have released an EV then, just without the self driving as that’s not yet relevant?",4
post59con,controversial,1.1304461369417722,highest,"The thing is, you can’t really “take your time” with AI like you can with other products. It’s not something tangible, like a new phone or a new watch. It takes time to train and fine tune a model, time that Apple’s competitors have already spent and are now able to iterate on their models. If Apple had been working on AI for years in the background, there wouldn’t be all these reports of Apple being behind. For all we know, they started last year, when these other companies have started years ago.",2
post59con,controversial,1.1304461369417722,highest,"Ultimately Apple doesn’t care. They’re still selling devices in the millions, yes they have to catch up but they can wait as long as they want. They aren’t exactly struggling. 

The rumours say all new AI stuff will be using third party like google and chat GPT, they are planning something of their own but it’ll come in the future",3
post59con,controversial,1.1304461369417722,highest,"I don’t think that’s a good way to look at things. Just because they’re “selling devices in the millions” now doesn’t mean at some point they can’t dry up because they’re unable to innovate and predict industry trends. In fact, Steve Jobs’s ability to predict industry trends is the entire reason Apple is still here to begin with.

Do you really think Apple wants to use ChatGPT or Gemini? No, they want to run their own models with their own weights. But they can’t because, as mentioned in another article, they only realized how good AI could be after using Microsoft Copilot. AI is one of the few things where time, not money, is more important. Who knows if their own model by next year will even reach GPT-4o capabilities, let alone GPT-5.",4
post59con,controversial,1.1304461369417722,highest,[deleted],2
post59con,controversial,1.1304461369417722,highest,And look at them now lol,3
post55con,controversial,1.1080931281745934,highest,"From the paper:

>We were limited by the availability of racial identity labels and the small cohorts of patients from many racial identity categories. As such, we focused on Whites, Blacks and Asians, excluding patient populations which were too small to adequately analyse (for example, Native American patients) and excluding Hispanic labels due to variations in how this label was recorded across datasets.

Sounds like some of the accuracy is due to a constrained data set.",1
post55con,controversial,1.1080931281745934,highest,"I can bet you they didn’t include representative samples from the African continent. There is more genetic variation among two Khoisan people from different tribes than all humanity outside Africa. So for the most part, they are assigning people to the “races” present in the US which represent a small subset of the genetic variation in humans.",2
post55con,controversial,1.1080931281745934,highest,"I mean, they have to take a subset from somewhere. There is no more racially diverse place on earth than the United States, so it seems like it would make for a decent balanced dataset for categorical image classification. Collecting the adequate data to deal with the problems you bring up is likely far too expensive and logistically nightmarish, as is the case for collecting most really good data.",3
post55con,controversial,1.1080931281745934,highest,Why do you say this? There is more genetic variation within Africa than North America.,4
post55con,controversial,1.1080931281745934,highest,[deleted],4
post55con,controversial,1.1080931281745934,highest,Terrible take,4
post55con,controversial,1.1080931281745934,highest,"Turns out practically infinite datasets are pretty rare in the real world.  Either some behavior tied to the internet (ad placement, movie selection, spam filtering); or else a made-up game like chess where you can create your own generative process.

Get much beyond that and you are working for every sample.",2
post23tec,technical,1.099943186349415,highest,"Thank goodness, Gemma is one fatfuck of a model to run",1
post23tec,technical,1.099943186349415,highest,"Well, not anymore. And the icing on the cake is that according to my tests, Gemma 3 27B works perfectly fine at IQ3_XXS. This means you can now run one of the best local models at 16k+ context on just **12 GB** of VRAM (with Q8 cache quantization). No, that’s not a typo.",2
post23tec,technical,1.099943186349415,highest,how does IQ3\_XXS compare to gemma 3 12b Q6?,3
post23tec,technical,1.099943186349415,highest,"Much better. Always choose the largest model you can fit, as long as it doesn’t require a 2-bit quant, which are usually broken.",4
post23tec,technical,1.099943186349415,highest,"As a beginner, can you briefly summarize to me what tools and software I need to reproduce that (if it's possible right now already)?

Gemma 3 27b on 12 GB of VRAM?",3
post23tec,technical,1.099943186349415,highest,">  reproduce that

Not sure what you are asking? If you want to run the model, 

* install llama.cpp 
* download gemma 3(.gguf file) from huggingface.co 
* start `llama-server`
* access the web UI from browser and setup the parameters in top right corner.",4
post23tec,technical,1.099943186349415,highest,Following,4
post23tec,technical,1.099943186349415,highest,"Hey, i run my stuff on an old laptop. 4gb vram and 16gb ram. can i use one of the gemma models for something useful now?",3
post23tec,technical,1.099943186349415,highest,"Yes, you can definitely use an Unsloth QAT UD 2.0 Q4/5 XL quant with reasonable context: https://huggingface.co/unsloth/gemma-3-4b-it-qat-GGUF/resolve/main/gemma-3-4b-it-qat-UD-Q5_K_XL.gguf",4
post23tec,technical,1.099943186349415,highest,"That's good, these models are good. They are just fat as fuck. Finetuning them is awful.",3
post23tec,technical,1.099943186349415,highest,Holy shit. Care to share a download link?,3
post23tec,technical,1.099943186349415,highest,Bartowski has all the quants.,4
post23tec,technical,1.099943186349415,highest,"Well, I get **Likely too large** even tho I have 16 GB M4.

https://imgur.com/24nK7PH

Am I doing this right? Or did the new model hasn't released yet?",3
post23tec,technical,1.099943186349415,highest,"You have to enable KV cache quantization, which will halve the VRAM it occupies.",4
post23tec,technical,1.099943186349415,highest,"You guys are super delusional if you think those 3bit quants are remotely usable

Literally everything below QaT quant was unusable quality loss for me",3
post23tec,technical,1.099943186349415,highest,A heckin chonker if you will,2
post46con,controversial,1.0981368256685633,highest,"It’s a tool. 

It will make good teachers better, it will highlight even more how bad the bad teachers are.",1
post46con,controversial,1.0981368256685633,highest,"Ha! I like that take, succinct and probably not wrong. The more time goes on the harder it will be to hide sloppy/lazy work. I feel we are in the Wild West of AI at the moment, the dust hasn't settled quite yet.",2
post46con,controversial,1.0981368256685633,highest,"Not really, at least how I understand it. Ai, specifically LLMS (large language models) like chatgpt need training data, and are constantly being trained on new data.

 IIRC it was microsoft that signed a contract with reddit to use the user generated content as AI training data. Think about how much AI generated content there is on reddit, now think about how much AI generated content there is on the rest of the internet, and/or anywhere that AI training data can/will come from.

 As AI gets trained on AI, the content it produces will eventually become worse. Then that worse content will eventually become training data, and the cycle will just continue.

If someone 30 years from now were to plot out a graph of “Quality of Ai outputs over time” it would look like a bell curve.",3
post46con,controversial,1.0981368256685633,highest,"Eventually, smart AI developers will only train on data that is verified as having been made by a real human.",4
post46con,controversial,1.0981368256685633,highest,It's not a tool. It's garbage  used to plagiarize. No good teacher allows it to be used.,2
post46con,controversial,1.0981368256685633,highest,"> No good teacher…

[No true Scotsman](https://quillbot.com/blog/reasoning/no-true-scotsman-fallacy/)

🙄",3
post46con,controversial,1.0981368256685633,highest,I so strongly disagree with this.,2
post46con,controversial,1.0981368256685633,highest,"Cool. 

It’s coming, so embrace it or get out.",3
post46con,controversial,1.0981368256685633,highest,What do you teach ?,4
post34con,controversial,1.0950652504887548,highest,Ai gonna dig ditches and pick fruit?,1
post34con,controversial,1.0950652504887548,highest,"Easily.  They have *Face Scanning* AI that can sense people’s moods, and they can apply it to picking fruit and digging holes easily.  They have robotic harvesting machines already, and trenchers and excavators that can dig holes to the *perfect depth* and avoid underground utilities and obstacles.",2
post34con,controversial,1.0950652504887548,highest,"No, but Americans should be doing those jobs if those are the only jobs available.",2
post34con,controversial,1.0950652504887548,highest,why?,3
post34con,controversial,1.0950652504887548,highest,Because unemployment isn't good for a nation,4
post35con,controversial,1.079235127282794,highest,"A basic search generally entails an officer reviewing the contents of the device manually without the assistance of any external equipment.

An advanced search is any search in which an officer connects external equipment to an electronic device not merely to gain access to the device, but to review, copy, and/or analyze its contents. Under CBP policy, advanced searches require reasonable suspicion of a violation of law enforced or administered by CBP or a national security concern and require the approval of a senior manager (at a Grade 14 level or higher, or a manager with comparable responsibilities) prior to conducting the search.",1
post35con,controversial,1.079235127282794,highest,"I've heard that the advance search can only find things locally on the device, not in cloud backups etc.",2
post35con,controversial,1.079235127282794,highest,"CBP's policy says they are not supposed to search anything stored online, they are only supposed to search what is locally on the device.",3
post35con,controversial,1.079235127282794,highest,Keyword “policy” - I’d sell that phone immediately if they took it.,4
post35con,controversial,1.079235127282794,highest,Correct - they are only supposed to search whilst in airplane mode I read recently,4
post35con,controversial,1.079235127282794,highest,"Basic search is local and manual by an Officer. Advanced search includes using forensic
Tools to access and pull data.",3
post35con,controversial,1.079235127282794,highest,I have like a hundred messages a day on iMessage and WhatsApp. I’m really wondering how they go thru that,2
post35con,controversial,1.079235127282794,highest,Search for 'working' 'hours' 'pay' 'paycheck' 'deposit' 'manager' etc. would throw up work related messages quite easily.,3
post35con,controversial,1.079235127282794,highest,That makes so much more sense,4
post35con,controversial,1.079235127282794,highest,Nowadays you can use AI models and look for patterns. I am not sure if they are or not. Just saying it’s technically possible.,3
post35con,controversial,1.079235127282794,highest,You’d sit in a little room while they take as much time as needed to go through it 🤷🏻‍♀️,3
post35con,controversial,1.079235127282794,highest,"> A basic search generally entails an officer reviewing the contents of the device manually without the assistance of any external equipment.

When CBP says ""less than .1 percent of travelers have their phones searched"" are they including all the times when  an officer takes a quick look through an unlocked phone in secondary inspection?",2
post35con,controversial,1.079235127282794,highest,Yes they are required to document every look at a phone.,3
post35con,controversial,1.079235127282794,highest,[removed],2
post35con,controversial,1.079235127282794,highest,"From what I’ve seen on this sub, that would probably make things worse. If they found that your phone was encrypted and you refused to unlock it, they would confiscate it and go through all your stuff with a fine tooth comb. They might not be able to break into it unless they had evidence you were some kind of national security threat, but you will still be without your phone for several weeks, if you get it back at all. Best thing is to get that digital footprint cleaned up, and not just for immigration. You never know who might be watching. And make backup copies of all important stuff to keep at home, just in case. From what I have seen on TV, they mostly look at email, photos, and social media. So make sure to get those cleaned up.",3
post35con,controversial,1.079235127282794,highest,[removed],4
post35con,controversial,1.079235127282794,highest,Someone I knew told me once they had their deleted Whatsapp messages (both text and voice) accessed by them. Cbp agent apparently then invented a bogus reason out of those to deny then entry. I wonder how they can access deleted messages (Whatsapp at that being end to end encrypted),2
post35con,controversial,1.079235127282794,highest,[deleted],2
post35con,controversial,1.079235127282794,highest,"If they ask you for your phone and you show them the fake one you never use; I can guarantee you that would be suspicious. They don’t have to have any type of suspension to open all of your luggage to search thru them. All items and person are subject to inspection. When they find that second phone(your actual phone) they would use that as the reasonable suspicion they need to go thru it. 

Then cbp will ask you to change the language back. 

If you don’t have any thing to hide, then there’s no need to be paranoid. Anything you can think of, others have too.",3
post35con,controversial,1.079235127282794,highest,"They can deny entry pretty much for any reason if you are on a visa, you know that right?",3
post35con,controversial,1.079235127282794,highest,[removed],4
post35con,controversial,1.079235127282794,highest,"You can do whatever you want to hide your bad faith/illegal purposes but CBP can simply detain you and eventually send you home somehow.

CBP has all the power at your point of entry and they aren’t constrained by many limits on their power.",3
post51con,controversial,1.0741049049020412,highest,"Submission Statement:

The issues of the intrusion of politics into science have been a central concern of Sam Harris. This article is about how AI may in fact detect differences in people and respond appropriately, but because the differences are conceptualized as racial differences by humans there are Progressives^TM concerned AI may be secretly racist.

It seems to strain the definition of scientific inquiry if we take an entirely neutral methodology which detects a politically uncomfortable phenomenon and assume the neutral methodology is flawed because of the detection of that phenomenon.",1
post51con,controversial,1.0741049049020412,highest,"Okay, but that's not what I'm reading in the paper. The paper seems to be about why it's able to figure out the race based off just x-ray images.

Especially since when extra data was added like the CT scans, the algorithm became confused and less accurate.

They seem to want to know why it is doing this more than that it is secretly racist. Or so that's how I read it anyway.",2
post51con,controversial,1.0741049049020412,highest,"I mean, this is part of the abstract:

>Finally, we provide evidence to show that the ability of AI deep learning models persisted over all anatomical regions and frequency spectrums of the images, suggesting **the efforts to control this behaviour when it is undesirable will be challenging and demand further study**.

...

>However, our finding that AI can accurately predict self-reported race, even from corrupted, cropped, and noised medical images, often when clinical experts cannot, **creates an enormous risk for all model deployments in medical imaging**.

Emphasis added.

The paper opens with:

>Bias and discrimination in artificial intelligence (AI) systems has been studied in multiple domains,1,  2,  3,  4 including in many health-care applications, such as detection of melanoma,5,  6 mortality prediction,7 and algorithms that aid the prediction of health-care use,8 in which the performance of AI is stratified by self-reported race on a variety of clinical tasks.

It seems to be about the potential for AI to be racist. From the discussion section at the end of the paper:

>Although the ability to accurately detect self-reported race from highly degraded x-ray images is not meaningful on its own, this ability is important in the larger sociotechnical context that AI models operate in for medical imaging. One commonly proposed method to mitigate the known disparity in AI model performance is through the selective removal of features that encode sensitive attributes to make AI models “colorblind”.35 Although this approach has already been criticised as being ineffective, or even harmful in some circumstances,36 our work suggests that such an approach could be impossible in medical imaging because racial identity information appears to be incredibly difficult to isolate. 

I'd say the paper is pretty conclusively about the ""threat"" of neutral AI detecting racial differences which are politically inconvenient.

I do thank you for making me look at the actual paper. I would have belived your comment otherwise, it almost sounds plausible.

https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00063-2/fulltext",3
post51con,controversial,1.0741049049020412,highest,"Isn't this more a ""face detection does bad on Asian faces because it was trained on white faces"" kind of concern than a ""computers aren't woke"" concern?",4
post51con,controversial,1.0741049049020412,highest,"| our study showed that medical AI systems can easily learn to recognise self-reported racial identity from medical images, and that this capability is extremely difficult to isolate. We found that patient racial identity was readily learnable from medical imaging data alone, and could be generalised to external environments and across multiple imaging modalities. We strongly recommend that all developers, regulators, and users who are involved in medical image analysis consider the use of deep learning models with extreme caution as such information could be misused to perpetuate or even worsen the well documented racial disparities that exist in medical practice. Our findings indicate that future AI medical imaging work should emphasise explicit model performance audits on the basis of racial identity, sex, and age, and that medical imaging datasets should include the self-reported race of patients when possible to allow for further investigation and research into the human-hidden but model-decipherable information related to racial identity that these images appear to contain. |

This is the conclusion I saw and from what I can read, it says that the AI is good at figuring out races based on the information given to it. And that we're still not sure why. This could potentially be used to make wrong conclusions on things we know that already exist. And finally that they want to include race information in future test in an attempt to figure out why the AI can come to the conclusions it does.",4
post51con,controversial,1.0741049049020412,highest,"Sorry when I said paper, I read the article, and that's how I interpreted it, I'll have a look at the paper now and get back to you.",4
post51con,controversial,1.0741049049020412,highest,">I'd say the paper is pretty conclusively about the ""threat"" of neutral AI detecting racial differences which are politically inconvenient.

It is not political inconvenience but rather the AI drawing well-known wrong conclusions from data that the researchers think it shouldn't be looking at. At on top of that, they are trying to include the advice of researchers from other fields as well. Something that is all too common in research and academia.

Edit: A word",4
post51con,controversial,1.0741049049020412,highest,"What does it mean for an AI to ""respond appropriately"" upon identifying a person's race?",2
post51con,controversial,1.0741049049020412,highest,">the intrusion of politics into science

I actually think this is a great concern and a monster that really needs a light shined on it. [Politics is why a special brand of Lamarckism prevailed over Darwinism in the Soviet Union](https://www.smithsonianmag.com/science-nature/when-the-soviet-union-chose-the-wrong-side-on-genetics-and-evolution-23179035/), and detractors of it were purged (edit: sent to gulags and/or ""disappeared""). These purged detractors were more correct in hindsight than their counterparts though, scientifically speaking.",2
post51con,controversial,1.0741049049020412,highest,"You are right on that. But our research is relatively a free and open space, where questioning is often allowed and encouraged. So it is not anywhere near as it was in the Soviet Union. Though yes seeping in of politics into research is a big concern.

Another point that I would like to raise is the politicisation of scientific findings. And specifically the factions that bring up non-research related topics in to question it, or try to politicise it to gain a political following (trying to divide people with misinformation). Not political movements that are based on the findings of research and support it, but the ones that oppose it using reasons that are non-academic and unfalsifiable.

Both of these points are justified if one looks at the shit-storm that was the political maneuvering around covid. From claiming that there was no chance that it was developed in a Chinese lab (even though that was FAR from conclusive) and that Covid itself was a hoax/mask weren't necessary (or any version of that).",3
post4con,controversial,1.0644395469110832,highest,[deleted],1
post4con,controversial,1.0644395469110832,highest,Walk around the South for 10 minutes.,2
post4con,controversial,1.0644395469110832,highest,"Yes, but that is exactly the reason everyone misses.  Where do you think a lot of this dialect started?


Take out some of the lingo and it's exactly the same as some illiterate mississipi grandpa.",3
post4con,controversial,1.0644395469110832,highest,"'People grow up in different cultures, contexts, and educations than me, that makes them the same as being illiterate!' Tell us how you really feel, dude.",4
post4con,controversial,1.0644395469110832,highest,"This is easily understandable and normal AAV. It’s informal, yes, but I bet whoever types like this normally will code-switch when the situation calls for it.",2
post4con,controversial,1.0644395469110832,highest,"I know plenty of white alabamians, mississippians, and georgians who speak worse than this. And don't get me started on the white appalachians.That's just another language.",2
post4con,controversial,1.0644395469110832,highest,I’m sure the AI probably wouldn’t think too highly of their dialect either.,3
post4con,controversial,1.0644395469110832,highest,[removed],3
post4con,controversial,1.0644395469110832,highest,Dialect doesn't denote intelligence.,4
post4con,controversial,1.0644395469110832,highest,"Checks out: “saying the speakers were likely to be dirty, stupid, rude, ignorant, and lazy.”",3
post4con,controversial,1.0644395469110832,highest,Yea... the country doesn't think to highly of them.,3
post27con,controversial,1.0477784044696212,highest,"So does anyone have sources from ""AI Experts"" that discuss this?

I'm not particularly young and thus have gone thru several technology waves. For each of those I could see the ""alternative"" job and benefit. For instance while PCs eliminated jobs if also made many more and made most people far more productive. 

AI in conjunction with automation advancements is the first technology that I dont see the ""alternative"". In short I can't see any job or job skill that this combination can't do and pretty much in every case can do better than even the most skilled human labor. 

I'm by no means an AI expert, which is why id like to see this exists discuss this topic. 

Id like to know if I'm good or whether I should be investing my retirement funds in some acreage where I can grow my own did till I die.",1
post27con,controversial,1.0477784044696212,highest,"This is exactly how AI will work as well.  It will eliminate some jobs, but people will need actual workers to input the data.

It's like that joke where the mechanic charges $10 to swing a hammer but also charges $9990 for knowing where to swing it",2
post27con,controversial,1.0477784044696212,highest,"Why can't AI input the data? The assumption here is that only humans will be able to discover new data and I think that is a false assumption. 

Once the model has an understanding of the tools used to collect new data, discover new data, which I think is not far away and may already be happening, no need for humans to enter data.",3
post27con,controversial,1.0477784044696212,highest,Because how do you know if it's right?  Relying on AI to always tell you the truth is dangerous,4
post27con,controversial,1.0477784044696212,highest,"Because the AI training the AI generates mostly nonsense. The skill with AI comes with knowing how to coax it into generating the correct response to any given input. If you don’t control the input data, you can’t reliably control the output data.",4
post27con,controversial,1.0477784044696212,highest,It’d also going to generate jobs in things like green energy and chip manufacturing in the US.,3
post27con,controversial,1.0477784044696212,highest,"I like that joke, I've used it a few times before.",3
post27con,controversial,1.0477784044696212,highest,"It's hard to pin down where AI experts actually are. You can see a lot on Twitter - actual researchers at these big companies, talking about research papers - but also expressing their personal feelings on the topic. 

There are places like LessWrong, where AI safety and capabilities have been discussed by researchers and scientists, as well as random smart people, often more with a focus on what the future will look like, what safety considerations can be put in place - if at all any? 

It's hard to really pin down one place where these discussions take place, but as someone who has made this topic a... Hobby, I would in my (probably very bias) opinion say that the vibe is that everyone is thinking we are 3-5 years away from AI that can do anything on a computer a human can do, better. Everything from software development, to video editing, to AI research. 

Not 100% of the researchers, but I would say the majority, and that shift from minority to majority was sudden and recent. 

I think if you want to see what those particular researchers would probably say is how the next few years look like -

www.situational-awareness.ai

I keep sharing this to people who are interested enough to want a place to start researching the topic. It was written by an AI safety researcher who was fired from Openai for purportedly being too loose with his lips. Take note that this was written in June of last year, and its predictions on how much would be earmarked for ai datacenters seemed very out there, even to the enthusiasts with their finger on the pulse.",2
post27con,controversial,1.0477784044696212,highest,"Thank you very much for the information and I will check it out. 

Seems that unlike historical technology introductions where physical labor was replaced, this is the first time intellectual labor is being replaced. 

The 3 to 5 year number does not sound unrealistic to me. After that another 5 to 10 automation to advance to a point where it can physically do anything a human can. Another number of years for the implementation phase and then what? 

The ""then what"" part is what id like to see discussion around.",3
post27con,controversial,1.0477784044696212,highest,"I’ve also seen many technology bubbles in my career (late 50s, electrical engineer).

AI really feels to me like it’s firmly in the hype/bullshit phase of the new technology cycle, with so much invested by so many people that they’re afraid to point out the basic flaws; AI promises to take over many jobs and make people redundant, but it just can’t do that, and won’t.

Eventually somebody will notice. I expect a huge crash in the AI sector in a year or 2.

That’s not to say AI isn’t useful; it definitely has potential as a force multiplier for some tasks, but I just don’t see it as the apocalyptic technology being projected.",2
post27con,controversial,1.0477784044696212,highest,So far the only real commercial use they’ve found is helping customer service agents draft responses faster - but a real agent still needs to read and edit / approve before hitting send.  No industry has yet replaced humans with it.,3
post27con,controversial,1.0477784044696212,highest,"When, was the last time you killed a cow, you did eat a hamburger within the past 30 day's yes. Just asking",2
post27con,controversial,1.0477784044696212,highest,Never thought about it like that. Very interesting,2
post27con,controversial,1.0477784044696212,highest,"If the companies that employ AI still have some shred of sense, there will be at least a few years where  people verify and vouch for the results that AI spews out.",2
post27con,controversial,1.0477784044696212,highest,Idk. Have you seen how social media has decreased the collective productivity of our society?  Not to mention I hate being my own damn secretary now because of PCs,2
post27con,controversial,1.0477784044696212,highest,I worked on a project that reduced analyst headcount by 25% by implementing - you guessed it - AI,2
post27con,controversial,1.0477784044696212,highest,"The only jobs that will be created are for professional Luddites to go around and smash the computers running AI. Joking, not joking.",2
post53con,controversial,1.0324461657015924,highest,"I recently read the book “How to be an Anti-Racist”. The author makes a compelling case that race is largely a human social construct and not a biological classification. He would prefer if races did not exist, and we’re not defined. However, in a world where races, genders, and other classifications of people find themselves unequally treated, these classifications serve a purpose: to identify people in need of special help to correct past and present injustices. 

Applying his thoughts, it would seem that this software could be beneficial if used to reverse some of the artifacts of racism, such as through affirmative action or similar programs. 

As mentioned in other comments, this software is the ability to be abused. I would suggest that the software be developed focused intently on its benefits, and that it’s use be carefully monitored. If the software’s applications are determined to do more harm than good, retire it.",1
post53con,controversial,1.0324461657015924,highest,"I would consider affirmative action a form of abusing of this data. This is immoral, unethical and wrong in so many levels.

I totally agree with you that having no race classifications is the ideal world: where they don't offer any significance as they should.

The problem with affirmative action is: you get a random person from a 'privileged group' and penalize this person in some way in favor of the 'minority'. The problem is that you are generalizing. Maybe a group of people you are fairly removing resources, but for others in the 'privileged group' it is unfair.

Think about not hiring someone who is white, ant-racist activist that could help improve the company diversity by doing a fair management but you opted to hire someone else instead because of this person skin color. You are not doing yourself a favor by doing affirmative actions.",2
post53con,controversial,1.0324461657015924,highest,"You just cited a single example and then made the logical jump that affirmative action is bad. We can’t be citing a single example and drawing conclusions. Affirmative action affects millions of people. What is the net benefit/loss? I think in sum, it has significant ability to do go, even though there will surely be cases where it is abused. As a white male American, I know that it gives others an advantage against me, but I want them to have that advantage since in every other way, I have been born into a world that gives me an easier time in 2021. The day when poverty, COVID deaths, infant mortality, etc. are not longer racially unequal issues is the day that we should stop affirmative action.",3
post53con,controversial,1.0324461657015924,highest,"By your comment you didn't undesrtand the argument. 

But it is ok, no one is preventing you from abdicating of some rights (say quit your job to open a position fo a black people, or give up your higher education in favour of someone with color). What I am against is you force that in someone else... and yes, every time you impose a affirmative action, some individuals will be benefiting and others penalized by the very nature of the idea. And unfortunately, some will be unjustly penalized, just because of skin color.",4
post53con,controversial,1.0324461657015924,highest,"Humans have universal rights. One of these is to be judged and treated as an individual, without regard to their race or gender. To treat people with respect and dignity.

\> Instrumentalization or objectification: This aspect refers to treating a person as an instrument or as means to achieve some other goal. This approach builds on Immanuel Kant's moral imperative stipulating that we should treat people as ends or goals in themselves, namely as having ultimate moral worth which should not be instrumentalized.

If you hire/not hire someone with the goal of achieving racial parity, you objectified them, judged them on race, infringed on their inalienable rights.

So therefore affirmative action is inhumane.

You invoke pragmatism: does it work, is it valuable? We can debate about that, but probably won't agree (is torture justified to get life-saving information?). We already blame people for the negative side-effect of thinking someone is a ""diversity hire"", not the policy makers themselves. That's hard to measure.

So let's look at the end goal of affirmative action. What if it works really well and this is valuable? So let's delete or suspend a human right in favor of a new human right. The right of not living in a society which has racial inequality? I do not think there exists such a right! If I move to a small African town, I would live in a tribe with racial inequality. Hopefully that won't be a problem for them or for me! Hopefully this village does not need to enact actions to restore racial equality.

So, to me, you are advocating to suspend an inalienable right (about discrimination and racism!), so we can have something that is not even a right in the first place. I will not have that! Institutional racism (the effects of inequality) is not as bad as it used to be, and you'd be hard pressed to find examples of institutionalized racism (outside affirmative actions at institutions such as universities).",4
post32con,controversial,1.019775827997398,highest,"Why are people surprised by biometrics and by using social media to find grounds of inadmissibility? The job of border guards is to keep out people who are not admissible. They have the widest authority of any U.S. law enforcement agency. 

Prostitution has always been a ground for inadmissibility. 

I suppose most people are unaware of biometrics and face recognition.",1
post32con,controversial,1.019775827997398,highest,"observation dinner crush hungry alleged reminiscent march secretive roll quarrelsome

 *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*",2
post32con,controversial,1.019775827997398,highest,"If you and your GF are US citizens, my understanding is that the US border face scanning is still officially optional for you. But yeah opting out is a pain in the ass, and most foreign nationals do have to do it.",3
post32con,controversial,1.019775827997398,highest,"I tried opting out and the officer didn’t know it was even possible, despite the sign in front of him saying it so lol",4
post32con,controversial,1.019775827997398,highest,"Genuine question, what do you gain by opting out at the border? They already have at least one picture of your face via your passport (and likely more through various other means)",4
post32con,controversial,1.019775827997398,highest,“Optional” they probably scan you when you walk around the airport.,4
post32con,controversial,1.019775827997398,highest,"The biggest problem with this as applied to prostitution specifically is that the CBP officers rarely reliably apply the rather non-obvious definition of prostitution in the immigration regulations, which is far more narrow than the usual legal and lay definitions, and from the article it sounds like the AI will worsen rather than reduce this mismatch. Quoting 22 CFR §40.24(b):

>The term “prostitution” means engaging in promiscuous sexual intercourse for hire. A finding that an alien has “engaged” in prostitution must be based on elements of continuity and regularity, indicating a pattern of behavior or deliberate course of conduct entered into primarily for financial gain or for other considerations of material value as distinguished from the commission of casual or isolated acts.

I'm not sure what the judicial definition of ""sexual intercourse"" is for immigration law purposes, but it's possible that blow jobs don't count as ""sexual intercourse"", and reasonably likely that hand jobs don't count. Certainly mere foreplay wouldn't. A sex worker whose business is limited to these acts - yes, they do exist - would remain admissible.

Similarly, someone who engages in one paid sex act in January 2022 and then one in February 2023, or even just two in January 2022 with none since then, probably doesn't meet the ""continuity and regularity"" / ""pattern of behavior"" criterion. They would fall into the ""casual or isolated acts"" wording.

Even more importantly, someone whose purpose of ""financial gain or [...] other considerations of material value"" is a secondary purpose rather than a primary purpose, or not a purpose at all but only a consequence of a non-material primary purpose, falls outside the scope of this narrow definition. For example, someone with a non-material primary goal and who uses the sex work simply to stay financially afloat in order to achieve their primary goal, in a way that would not be necessary if they didn't have that other primary goal, is not inadmissible. (Why would someone do that? Usually because attempts to stay financially afloat in more socially acceptable ways have failed.) Same thing for people who are doing a bit of paid sex work on the side mainly for self-affirmation while they pursue a different primary academic or professional career - don't laugh, I've met someone who did this.

Yet again even more importantly, someone who does sex work under duress as a victim of human trafficking is not inadmissible under this ground, since they don't have the purpose required by regulation, but AI wouldn't know that.

Do you really think the AI being discussed is likely to make CBP enforce all of these nuances better than they already do? Their pre-AI status quo is pretty sloppy as to exploring the boundaries of this legal definition. I don't think AI will help this at all. And most foreign sex workers can't afford the best US immigration lawyers to hand them a well-crafted letter for CBP with the right legal arguments, not that there's any recourse anyway for most foreign nationals if the CBP officer doesn't believe them.

Even worse, the process to challenge a CBP decision after the fact is slow, broken, and expensive, and some consequences of CBP error (such as the loss of NEXUS or Global Entry and/or a 5-year ban) are nearly impossible to reverse. The consequences at the border can be especially severe for a permanent resident, since if CBP thinks they've engaged in prostitution within the last 10 years, the INA definition of ""admission"" treats them as arriving aliens applying for admission, unlike most returning residents, meaning they might get removed from the country they live in. At least LPRs get to argue before an immigration judge, but nothing forces CBP and its AI assistants to get the regulatory definition right before making a mess of people's lives.

And once this concern is on a person's file, almost all subsequent applications to CBP, USCIS, or US visa officers will be complicated for roughly forever, with no redress - even though the inadmissibility for people who are covered under this ground automatically goes away by operation of law, with no formal waiver needed, 10 years after the last covered act. It's not the kind of job people usually do for a lifetime, but the consequences of CBP error here can last for that long. (So can CBP/USCIS/DOS unfamiliarity with how this inadmissibility automatically vanishes over time, unlike almost all other grounds of inadmissibility.)

As for that extra word ""promiscuous"" in the regulatory definition, which I didn't address, I'm not really sure what that means in this context that would not be redundant with the rest of the wording. But there is a statutory construction principle that tries to make the included words meaningful somehow, so I can imagine that there is some way that a sex worker with a pattern of behavior of sexual intercourse for hire and primarily for financial gain might still be admissible if their sexual intercourse is not promiscuous. Whatever that means. Maybe if they just do it with one paid partner instead of many?

So, yeah, my problem with this is my problem with many things about our immigration system, with AI just making the problem worse: there is no way for the US general public to know if the government is doing things correctly according to the law, because of how confidential many of these procedures are, and no effective way to get courts to enforce and monitor a correction (whether for the general procedure or for individual affected people) when they aren't.",2
post32con,controversial,1.019775827997398,highest,"This is a great response and I think the answer to your last question is unequivocal that the government is probably not doing the right thing and is probably not being responsible. Why would they? There’s hardly accountability for rouge ICE officers, CBP agents, immigration judges etc that completely violate professional and legal standards - but the larger government (DHS/DOJ) hardly bat an eye since it is almost always to their benefit (restricting immigration).",3
post32con,controversial,1.019775827997398,highest,"That’s not accurate. Courts have checked immigration when they’ve gone too far.
You do understand this is common, right? 
Events in NYC  have police helicopters doing facial recognition and have been doing so for more than a decade.",3
post32con,controversial,1.019775827997398,highest,"Courts have often refused to check immigration. Most importantly, there are extreme limits to when courts are even willing to consider the merits of an immigration case in the first place, and extreme limits on what data can be obtained from the government in this area by either the public, the courts, private watchdog organizations, and even many elected policymakers.

You are right, of course, that some court rulings do in theory apply some restrictions to government behavior in certain contexts. But even where that is the case, there is approximately zero ability of anyone (including the courts) to effectively oversee the government’s compliance in general with the court’s order, beyond the case of any individual whose situation was specifically litigated.

The NYC example doesn’t mean what you think it does. Illegal overreaches by NYPD happen frequently and have for decades. Courts have ruled that way on many occasions, have even imposed consent decrees, and have found noncompliance with those consent decrees. NYPD is frequently unaccountable and frequently law-breaking.",4
post32con,controversial,1.019775827997398,highest,"Maybe it’s because for marriage based visas “consummation of marriage” is required factor to permanent residency. 

Consummation doesn’t mean much if you’re a prostitute, but the legal definition. 

As far as the other visa types… no clue. 

I’ll endeavor that because being a prostitute is illegal in most places, they worry about you breaking the law, working illegally, and not paying taxes. 

Just guessing though!",3
post32con,controversial,1.019775827997398,highest,"> Maybe it’s because

We don't have to speculate at the reason - it's simply because INA 212(a)(2)(D)(i) specifically creates a statutory a ground of inadmissibility for engaging in prostitution within the 10 years before applying for a visa or admission to the US.

One can wonder why Congress enacted that provision, and maybe consult the legislative history to see what was said about it at the time, but for enforcement purposes the reasons why the provision exists don't really matter to USCIS or the State Department when it very clearly does exist.


> for marriage based visas “consummation of marriage” is required factor to permanent residency.
>
> Consummation doesn’t mean much if you’re a prostitute, but the legal definition.

Consummation is only legally required immigration as a spouse based on proxy marriages, where one or both of the parties was not physically present at the ceremony. Neither consummation nor any other kind of prior sexual intercourse is in general mandatory in order for a marriage to be considered bona fide and valid for immigration purposes, not even for most marriage-based visas. See e.g. Matter of M-, 7 I&N Dec. 601 (BIA 1957) and Matter of Peterson, 12 I&N Dec. 663 (BIA 1968), which are still good law in all aspects relevant to this conversation. 

Despite that still-binding legal precedent, sometimes USCIS or the State Department will consider the presence or absence of consummation as a factor in deciding whether a marriage is bona fide. Still, it doesn't usually come up as a question when there is adequate other evidence of a bona fide marriage, and plenty of current or former prostitutes would consummate their marriages regardless of whether the law cares about that.

> Consummation doesn’t mean much if you’re a prostitute

I'd disagree on that - performing any activity as a job feels very different to me than doing it to celebrate or otherwise explicitly mark the solemnization of a marriage, whether or not the activity involves sexual intercourse. But anyway that's a tangent, because as above, the law never cares about why the consummation occurs and rarely cares that it occurs at all.

> As far as the other visa types… no clue.

Again, it's relevant for any visa type solely because there's an explicit statutory provision about it, as with all grounds of inadmissibility.

> I’ll endeavor that because being a prostitute is illegal in most places, they worry about you breaking the law, working illegally, and not paying taxes.

The regulations explicitly make legality irrelevant. Quoting §22 CFR 40.24(c):

> An alien who is within one or more of the classes described in INA 212(a)(2)(D) is ineligible to receive a visa under that section even if the acts engaged in are not prohibited under the laws of the foreign country where the acts occurred.

This is either Congress deciding to punish engaging in prostitution even when it was done fully legally, executive branch regulatory policymakers deciding Congress intended to do that when they passed the statute, or executive branch regulatory policymakers deciding to do that themselves regardless of congressional intent. 

But as with the statutory text itself, it doesn't really matter why this regulatory language exists - it's legally binding unless a court were to somehow find it unconstitutional, which is unlikely to happen in this case.

Also, there are some places in the world where prostitution activities can be fully legal for everyone involved if properly registered and taxed etc, such as Germany; and many others where the prostitute themselves commits no legal violation even if many of the other participants in the activity do, such as Canada and Norway. Presenting the US government with proof of acting legally is irrelevant to inadmissibility determinations under the ground of inadmissibility I've been discussing so far.

(Tangent: Canada's laws are currently being challenged in court by organizations supporting the rights of sex workers, so it's possible that those legal prohibitions will be found retroactively invalid. Nobody is asking the courts to make anything more illegal, just less illegal.)

Even when an applicant's prostitution was a crime under the applicable laws, the separate ground of inadmissibility for committing a crime of moral turpitude could only apply where there was a conviction, or where there was neither a conviction nor an acquittal but where the US government has received an admission meeting the very strict requirements laid out by the Board of Immigration Appeals. I say ""could only apply"" rather than ""would only apply"" because a conviction only triggers that ground of inadmissibility if either the wording of the criminal statute or the record of the conviction reflects a crime of moral turpitude. If the statutory wording of the crime is broad enough to encompass crimes not involving moral turpitude, and if the record of the conviction does not make it clear that this case did involve moral turpitude, it actually doesn't legally matter if the facts underlying the conviction clearly involved a crime of moral turpitude.",4
post32con,controversial,1.019775827997398,highest,[deleted],2
post32con,controversial,1.019775827997398,highest,Nobody says it’s not wrong sometimes. It’s just less wrong than a human being. All it’s doing is reducing the errors.,3
post32con,controversial,1.019775827997398,highest,"Face recognition has been more and more mainstream in other developed countries. Some Americans think that this is a prelude of totalitarian dystopia, and I just laugh at their obliviousness.",2
post32con,controversial,1.019775827997398,highest,"What obliviousness
This could be used against the people.
How do you not understand that?",3
post32con,controversial,1.019775827997398,highest,"Yeah, I m so scared of government having an information on my face which they already have in four different states DMV and USCIS and CBP. 

It's not that I don't understand it. It's people like you being delusional and pant up with unwarranted victim complex.",4
post32con,controversial,1.019775827997398,highest,Where do they get the data of people faces ID to do this?,2
post26con,controversial,1.0151173141598289,highest,"It is never AI on it's own it is how we set up the system. In the current system we could imagine inequity growing some because our system is not very proactive.

But ultimately we live in a democracy where the majority rules and there is no great advantage to hoarding.

If AI creates better efficiency then ultimately everyone will benefit.",1
post26con,controversial,1.0151173141598289,highest,"I suppose in theory we live in a democracy, but I fear that we will move away from that and more power will be concentrated in the minority and not the majority",2
post26con,controversial,1.0151173141598289,highest,"It is not a theory. We live in a democracy.

Yes, if the majority of people decide to live in some other type of system it is possible. The majority voted for the current system.",3
post26con,controversial,1.0151173141598289,highest,"It’s a democracy until the leader decides they don’t want to give the power back, which isn’t impossible. But I could see Ai making democracies less democratic in general as it makes spreading misinformation easier, propaganda to manipulate vote, can be used for surveillance etc",4
post15con,controversial,1.0139906929810154,highest,"If people with money replaces employees by machines, nobody will be able to buy their products and services. We will need to figure out a way too distribute the results of productivity.",1
post15con,controversial,1.0139906929810154,highest,"If they have machines to see to their every whim, a harem of beautiful young women and the tech to ensure that they have everything in the universe that they could ever want, they don't need people to buy their products. The machines will keep them comfortable and in power for all eternity.",2
post15con,controversial,1.0139906929810154,highest,"Eh, it would be hard for them to remain perfectly protected and isolated. Plus, with a ton of pissed off poors, someone’s bound to sabotage their machines.",3
post15con,controversial,1.0139906929810154,highest,With ASI? Nobody can touch them. Every person everywhere would be monitored. Their gait. Facial expressions. Emotional stability. Every ounce of power. The location of every human and what they're doing. Everything would be analyzed by the system. And they could just decide to build their own space habitats and leave us to die on a used up planet or nuke everything to make sure their hegemony is never threatened.,4
post15con,controversial,1.0139906929810154,highest,They'll have the technology to wipe out the surplus poors in a blink of an eye.,4
post15con,controversial,1.0139906929810154,highest,"This. I keep trying to explain this to people but they dont WANT to believe. They dont NEED your money, they already own everything.

One thing is poor people make the elites feel good about themselves so you will be in a tiny house, eating bugs on universal basic income. The rich will trade services and goods amongst themselves much like collectables and you will continue to worry about things that are propagated to divide people.",3
post15con,controversial,1.0139906929810154,highest,"That's works until certain point, many revolutions has happened in the past, this will just be another one.",4
post15con,controversial,1.0139906929810154,highest,"I don't see this as true across the board. For a complacent swathe of elite yeah, but anyone with an ambitious temperament is going to want to grow in power regardless of how much they have.

If the lower classes are abandoned for any length of time, It doesn't seem implausible that someone in power would regard lower-class discontentment as a resource to exploit. This could go in many directions (*It also kinda describes the current state of affairs, because just think of how many of us wouldn't work if we didn't have to /digression*). One way I could see it going though, is that a power-seeking individual (from within existing power structures or without) decides he has more to gain by being disruptive and mobilizes disenfranchised masses towards a common goal.

I don't regard this as a flawless or thorough analysis by any means, but I'll likely maintain certain key points here. I don't think it's possible that the machines will keep them comfortable for all eternity, because I don't think comfort is all people want. Just the tired ones.",3
post15con,controversial,1.0139906929810154,highest,Hahahaha yeah - that’s how the billionaires club think - I feel for you,3
post15con,controversial,1.0139906929810154,highest,Except we're still here. Those sex bots and robo butlers can't protect them forever.,3
post15con,controversial,1.0139906929810154,highest,That is the ai economic ouroboros.,2
post15con,controversial,1.0139906929810154,highest,UBI or UBS or revolution,3
post15con,controversial,1.0139906929810154,highest,"I wish that were how companies think but they don’t operate in concert with one another and, just like regular people do, will assume someone else will do the right thing instead; they will take their cut. We are biologically predisposed to greed. We use others for our own benefit.",2
post15con,controversial,1.0139906929810154,highest,"Free Bread and Circuses.

Or, updated: Free Weed and Internet.",2
post15con,controversial,1.0139906929810154,highest,Tax obscene wealth and obscene carbon generation.,2
post15con,controversial,1.0139906929810154,highest,"So it can be used to fund obscene wars? Taxing wealth will only help if the government isn’t giving that wealth right back to the people they’re taxing. Which is exactly who they currently give it to, and who has the most ultimate power in our system. More taxation isn’t gonna help you; it’s gonna help Booze Allen.",3
post15con,controversial,1.0139906929810154,highest,Tax lobbying and form anti-corruption action groups. Got to start somewhere.,4
post15con,controversial,1.0139906929810154,highest,"The issue is that's a system-level issue. Individual billionaires are only thinking of themselves.

Yes the system will eventually stall, buy when that happens they'll probably just have the media blame immigrants and poor people.",2
post37con,controversial,1.0002514405678191,highest,Plenty of folks in the IT industry that have been jobless in the US for 6 months+ because of the hidden IT recession.  How about you hire those folks instead of more cheap IT workers out of other countries?  Or is it that you just don't want to pay them? And don't tell me nonsense that they need specialized workers for AI - I did 20 years in cybersecurity before moving over to AI systems development. There's plenty of talent here Google if you're willing to actually pay them.,1
post37con,controversial,1.0002514405678191,highest,"Google has uniform policies on pay bands and pays their H1B full time employees the same as everyone else, [averaging $386k/year](https://www.levels.fyi/companies/google/salaries/software-engineer?country=254) on a W2 for a senior software engineer. They don't save money hiring H1Bs, other than very abstract arguments about the broader labor market that assume that technology investment, what any economist would tell you is the very antithesis of a zero sum game, is zero sum.

The real reason they want H1Bs is that these fields are a direct competition over talent that is not a binary, but a continuum of capability with no hard upper bound. You will get really radically different outcomes in very new very highly technical applied research if you're able to staff the research org at the 95th percentile vs the 99.5th percentile vs the 99.95th percentile.

That's because the rate of progress the company can push in research is capped entirely by the rate at which the researchers can learn. There is no playbook to remember and apply. It's exploration of new problem spaces. And they are in a direct competition with other companies to be in front so they can capture market first. Whoever has the best people wins.

This is a big structural problem for the tech hiring in the US because intelligence is roughly evenly distributed around the world. Americans are not just way smarter than everyone else. And the number of people you have at the 99.95th percentile in a country is, well, 0.00005 times the population. So, given that the US is 350m/8b, we will only have about 5% of the people at any threshold in that competition born in the US.

But the US has historically killed it at technology precisely because it has had a very unique and enormous advantage at attracting the smartest people from the rest of the world to move here, first to come to the best universities in the world, and then to receive the highest pay for high skilled work in the world.

Accordingly, if you walk around Google (or any top tech company) today, it is a very large number of immigrants, not just on h1b, but greencards, second gen citizens, especially from the largest countries in the world (India and China) which will naturally have a large number of outlier people just due to their size.

Chinese top talent increasingly stays in China as they are catching up and the language barrier is harder, Indian top talent increasingly goes to Europe because the greencards quota for them means they will die before they can get a green card, and thus they can't really build a life here.

If you think we're going to compete with chinese tech in 20 years with just native born Americans then you have not thought seriously about this problem at all.

[The literal majority of US tech companies worth more than a billion dollars were founded or co-founded by first gen immigrants.](https://www.forbes.com/sites/stuartanderson/2022/07/26/most-us-billion-dollar-startups-have-an-immigrant-founder/?sh=5e41bcac6f3a). That same cohort has driven almost all of US economic growth since covid, so this isnt even just an issue within the industry.

[40% of US nobel prize wins were first gen immigrants.](https://www.forbes.com/sites/stuartanderson/2023/10/05/immigrant-nobel-prize-winners-continue-to-impress/?sh=7424cb867394)

The entire bedrock of American tech success to date has been antithetical to the idea you are proposing, and diverging so radically from the strategy that has worked so well will, of course, cause a difference in outcomes.

It's just so annoying because american exceptionalists so often want to kill the thing at which america has been truly exceptional, attracting the smartest people in the world to join us. Americans ourselves are just normal people. The quality of our immigrants is what is so unique and the entire reason why we, as a country, are so good at science and technology.

For clarity, most lines in my family have been in the US long enough that we don't know where they're originally from. I'm not an immigrant at all. I just work (and thus hire) in this field.",2
post37con,controversial,1.0002514405678191,highest,I work in a company that has a lot of H1B visa holders. They are not the best and brightest. They simply will work for less and are unable to change companies easily.,3
post37con,controversial,1.0002514405678191,highest,"Different companies, and especially industries, even teams, have very different labor strategies and will thus use the same tool for very different things.",4
post37con,controversial,1.0002514405678191,highest,"Don’t forget they also have dynamic schedules, I work with them and they work 24/7. These companies are itching for us to do that, under pay us and over work us. We will have indentured servitude soon",4
post37con,controversial,1.0002514405678191,highest,"What's your bar for ""cheap""? Most on my team are foreign born engineers on H1 and they all make minimum 250K. I'm sure some make more than 400K.",2
post37con,controversial,1.0002514405678191,highest,"Relative to other jobs, that's a lot. 


Relative to the profits generated, it's tiny.",3
post37con,controversial,1.0002514405678191,highest,"A lot of people who come here to learn ML/AI end up leaving and taking their skillset back to their home country because they can’t get on a path to naturalization.

These are bonafide AI experts who want to stay here, likely have lived here for many years already while studying, and it does not make any sense to push their expertise into the hands of competing nations because of archaic immigration rules",2
post37con,controversial,1.0002514405678191,highest,"While I don’t disagree, one thing they can do is stop issuing H1B visas to junior level software engineers that are on par with any American junior engineer and reserve them for ppl with specialized skills.",3
post37con,controversial,1.0002514405678191,highest,"That's a point I've never heard before. If that's true it's potentially throwing away good educated workers. 

I know plenty of Africans who come here, get educated in medical/engineering, but choose to stay. I think it depends on what the home country is offering them as well.",3
post37con,controversial,1.0002514405678191,highest,"So stop hiring people from other places? Duh? The big money WANTS them to go home. The last tech bubble for fast Internet is these far off places. They want them there to buildout for the unlimited cheap labor, they need liaisons there. They don’t need people in US, workers are plentiful here and could learn on the job in 3 months be up to speed. Globalism is all about fortune500 having same “from the couch” access to worksites anywhere in the world. Same work, same quality, same contact experience, 1/100th the cost",3
post37con,controversial,1.0002514405678191,highest,"You can learn to use AI tools in 3 months.

You cannot learn graduate level theory and practice in artificial intelligence in 3 months or even 3 years.",4
post37con,controversial,1.0002514405678191,highest,[deleted],2
post37con,controversial,1.0002514405678191,highest,"Umm, no. The tech companies do not just hire everyone that has AI skills. In fact, if you would have any, you would know that the market is actually saturated with people with those skills. Everybody and their grandmother wanted to be a data scientist and jumped on AI. The truth is that there are not that many positions that are truly about making AI. Those positions are highly sought after, and the competition is fierce. To get paid millions at deep mind in AI, you need to be exceptional at it, and willing to work in a few specific places.

Google, and the tech companies, do not just gobble up all AI talents. They don't even try to poach from each other anymore.",3
post37con,controversial,1.0002514405678191,highest,Objectively valid.,2
post37con,controversial,1.0002514405678191,highest,"No you can’t just pick up core ML work, it takes years of study to be okay at and even more to be great at",2
post37con,controversial,1.0002514405678191,highest,"I find it hilarious that he thinks anyone in tech, especially IT can just “pivot” to AI. When those guys have been deep in the field rubbing shoulder with quants and others with near genius level understanding of computation and math",3
post37con,controversial,1.0002514405678191,highest,The hubris is really astounding,4
post37con,controversial,1.0002514405678191,highest,most of those people aren't ml experts. It's a shame though that layoffs have impacted so many people and I truly hope the market picks up soon,2
post37con,controversial,1.0002514405678191,highest,"That's actually not true, for senior engineers it's been joke easy getting jobs, most people layed off were support and project management.

Also there's a big difference between AI expertise and implementation engineers. It's joke easy implementing AI into a solution, but the background isn't something just any engineer can do.",2
post37con,controversial,1.0002514405678191,highest,"Yeah, these companies are looking for real researchers and innovators, not guys who can pick up some ml/deep learning books and copy the models on there",3
post37con,controversial,1.0002514405678191,highest,"Ah, but they don't want to pay them...

You have to be more understanding of the big mega corp that's part of the monopoly on AI. They're really trying their best... really! They can't afford to pay people more! /s",2
post37con,controversial,1.0002514405678191,highest,"A little late, but what did your journey into AI look like? I am looking for a pivot professionally and AI is something that is not only going to be in demand for the foreseeable future...but its something that interests me as well.",2
post37con,controversial,1.0002514405678191,highest,"No prob! I actually get this question every once in awhile when I mention my career swing. Here, I wrote this last time I got this question https://www.reddit.com/r/ArtificialInteligence/comments/1cc04x8/how_ai_already_changed_my_life/l17m2oi/",3
post37con,controversial,1.0002514405678191,highest,"It’s all manipulation, all the time. Smart people at “smart” universities figure out most aren’t like them and will still buy a whopper meal even for $16.. If they’ll buy that, they’ll buy anything at all. Sadly this is all from the same branch that creates the “nobody is created equal, some are better” spew that always end up in wars, crime, recessions, and violence.",2
post37con,controversial,1.0002514405678191,highest,Dude this is what I have been saying... like wtf is even going on?,2
post8con,controversial,0.9918088361186058,highest,Aren't there surveys that show people prefer talking to women than to men in the service industry?,1
post8con,controversial,0.9918088361186058,highest,"Yep, very intentionally gendered to cater a better user experience.",2
post8con,controversial,0.9918088361186058,highest,German Wikipedia lists two main reasons why switchboard operators were mainly women: They were more polite and got paid way less than men.,3
post8con,controversial,0.9918088361186058,highest,"This. Also when many companies hired young men, they were destructive and inappropriate to customers. So they hired young women instead and that’s become the industry standard.",4
post8con,controversial,0.9918088361186058,highest,"Yeah but the question now becomes, why do more people feel better about women serving them? 

They've probably already been trained to subconsciously see women in this role.",3
post8con,controversial,0.9918088361186058,highest,[deleted],4
post8con,controversial,0.9918088361186058,highest,"Nah, I’m just more comfortable around women. Men make me nervous lol",4
post8con,controversial,0.9918088361186058,highest,"Eh, a lot do, I'm sure, but as a dude I feel like I need to be 'on' more around other guys and it's so much less anxiety inducing talking to a woman, where I don't really get that feeling. Daddy issues maybe? Who knows",4
post8con,controversial,0.9918088361186058,highest,There was a military tactic that the female voice on pilots vehicles/ aircraft can be identified and heard above the radio messages which were predominantly men. However more woman are entering the military especially in support roles.,2
post8con,controversial,0.9918088361186058,highest,I think the vast majority of people *prefer* talking to women than men in general. There’s just a lot of ridiculous social learning/programming that creates nuances of expectation there.,2
post8con,controversial,0.9918088361186058,highest,"Women's voices are typically also seen as more pleasant, hence women doing most airport announcements etc.",2
post8con,controversial,0.9918088361186058,highest,"My GPS back in the day had male and female voice options, and I felt like I couldn't hear the name voices well. I try to be polite to my female voiced robots though, so I can at least model that for my kids",2
post8con,controversial,0.9918088361186058,highest,"Yeah. The problem is that the effect doesn't change just because there is a ""sensible reason"" to have all the voices be female (not saying you're trying to justify it, but this is a common argument). 

At the end of the day, studies have shown that our brains are really bad at distinguishing between real and fictional people, in at least some ways - including representation. No matter the reason, by making the voices all default to female, we *are* shaping public perception, whether companies originally just did it for bigger profits or not.",2
post8con,controversial,0.9918088361186058,highest,"I remember some fighter jet development documentary from the 90s, that said that they measured the brain responses of the pilots and the brain responded slightly quicker with a woman voice as the voice alerts. Some scientist said it's because of children/mother relationship. Like, we respond more to mom's voice or something.

It was back in the day that the History channel actually had historic things on, not just pawn shops and aliens.",2
post8con,controversial,0.9918088361186058,highest,Maybe they had a better experience talking to their mom about their needs in childhood than their dad?,2
post25tec,technical,0.982934466552778,highest,"I can only speak personally, but I never used it because:

* [4-bit GPTQ LoRA](https://github.com/johnsmith0031/alpaca_lora_4bit) training was available since early April. I did not see any comparison to it in the QLoRA paper or even a mention, so it makes me think they were not aware it already existed.
* Most of the paper is about Guanaco and how you can recover a lot of performance loss by using QLoRA. When I looked at the [QLoRA config for Guanaco](https://huggingface.co/timdettmers/guanaco-33b/blob/main/adapter_config.json), I saw it is exporting most of the modules and has rank of 64. If you know LoRA, it is already mentioned in the original paper that a LoRA with rank equal to the rank of the weight matrix is ~equivalent to a full fine-tuning. So personally, I thought there was nothing really new in the paper or a reason to switch to the new approach. Most of the LoRAs today only export Q and K and keep the rank small because it is mentioned in the LoRA paper you do not need to do more than that to get _good enough_ performance, and in QLoRA paper they did not demonstrate that the same approach couldn't have been achieved with the existing 4-bit LoRA approach by also exporting most of the modules and using a high rank, so it did not give any reason to really switch.

At least, that's only my reason",1
post25tec,technical,0.982934466552778,highest,afaik alpaca\_lora\_4bit is also roughly twice as fast.  that's a pretty damn good reason to use it over qlora.,2
post25tec,technical,0.982934466552778,highest,"Is the rank of the original weight matrix just the smallest dimension of the matrix? E.g. The Q, K, V matrices in LLaMA are 4096x4096, so the rank would be 4096? Sorry I didn't pay attention in linear algebra class, genuinely asking \^\^ From [Wikipedia](https://en.wikipedia.org/wiki/Rank_(linear_algebra)), I guess it's the number of linearly independent columns, which is approximately just the number of columns in the matrix after pre-training (my guess).

So for a LoRA adapter for Q/K/V with rank 64, the A and B matrices would be size 4096x64 and 64x4096, so the A and B matrices combined would have 524,288 trainable parameters. Compared to the Q/K/V matrices which have 16,777,216 trainable params, that would be a 32x reduction in trainable params. Granted, you'd still need memory to run the forward/backward pass on the original weights for training, so I'd be curious about the overall memory savings of LoRA w/ high rank vs full fine-tuning. Maybe there is more memory required per parameter for trainable params vs non-trainable params during back-prop -- I need to review how the optimizers work.",2
post25tec,technical,0.982934466552778,highest,"I think your confusion is the trainable parameters? You do not need to copy the weight matrices entirely, you only need enough to train behavior comparable with a full fine-tune.",3
post25tec,technical,0.982934466552778,highest,"I guess I was confused when you said ""LoRA with rank equal to the rank of the weight matrix is \~equivalent to a full fine-tuning"", since LoRA with rank 64 would still be less than the rank of the original weight matrix. The effectiveness could be the same as full fine-tuning for specific tasks (e.g. instruction tuning). Were you referring to the implicit ""intrinsic rank"" of the original weight matrix mentioned in the paper?

In any case thanks for bringing up alpaca\_lora\_4bit, I'll take a look!",4
post25tec,technical,0.982934466552778,highest,What rank and alpha are you using for your training @kaiokendev?,2
post25tec,technical,0.982934466552778,highest,"Rank = 4 and alpha of 8, maybe rank = 2 in some cases. It seem low but according to the LoRA paper, exporting all attention modules with rank = 1 performed on par or better than just Q and K with rank 8, and SuperCOT is using Q and K with rank 8. Exporting everything with high rank of 64 will be better, but the adapter can be quite large (2 GB in case of Guanaco)",3
post25tec,technical,0.982934466552778,highest,"This explains why my alpha 128/rank 256 ballooned the adapter so much. 

Raising those did make the desired effects much stronger though.",4
post25tec,technical,0.982934466552778,highest,Thanks!,4
post25tec,technical,0.982934466552778,highest,"I did a lot of ""old"" 4bit LoRA and QLora and the result is ..... same.

Not exactly same, but same in the end result.

In the later ooba training I added PR with info how many parameters QLora adds. More efficient or not - we are still adding a tiny % of parameters. You can't really make a cake with a spoon of flour.

&#x200B;

BTW, what is ""rank of the weight matrix"". I thought it is hidden\_size (which is something like 5000 something)",2
post25tec,technical,0.982934466552778,highest,"Yeah it would be 5120 in the case of 13B, 6144 in case of 30B, but the point of the paper is that the weight matrices are not full rank, which is why you can get away with using only a low rank adaptation of the matrices",3
post25tec,technical,0.982934466552778,highest,Gotcha!,4
post25tec,technical,0.982934466552778,highest,"Not sure if you missed it, but the whole main point of qlora is that you can get basically same quality but with half the  memory footprint, allowing bigger parameter sizes on consumer level graphic cards. Thats the actual meaning of the paper. Though you are right, a comparision to 4bit gptq results would have been nice.

And a rank of 64 compared to the standard hidden size of 4096 in Llama is not the equivalent rank. (unless i highly misunderstood something, which i would gladly hear a correction about)  
The original Lora paper also mention that low ranks are fine for slightly finetuning the models, but they do not make any claims for usecases of adding new/very diffrent knowledge to the original models. So there definitly might be situations where higher ranks than 4-8 are usefull.",2
post25tec,technical,0.982934466552778,highest,"> the whole main point of qlora is that you can get basically same quality but with half the memory footprint, allowing bigger parameter sizes on consumer level graphic cards

Yes, I am saying that this is already achievable with the 4-bit LoRA trainer, so using QLoRA did not add anything new besides using a different float quant type, and since they did not compare against the existing 4-bit LoRA trainer, I could not see what the value is in changing

> The original Lora paper also mention that low ranks are fine for slightly finetuning the models, but they do not make any claims for usecases of adding new/very diffrent knowledge to the original models. 

I don't know what you mean. In any case, any limitation of LoRA I would expect to also see in QLoRA. There is nothing I saw in QLoRA paper to suggest it is improving LoRA, only allowing for LoRAs in resource constrained environments (which again the existing trainer already did)

> And a rank of 64 compared to the standard hidden size of 4096 in Llama is not the equivalent rank. (unless i highly misunderstood something, which i would gladly hear a correction about)

By equivalent I meant full rank. You do not need to use full hidden size to replicate performance of full-finetuning. That is what I see in the LoRA paper. Additionally, QLoRA paper backs it up and even makes the claim that rank is irrelevant when all modules are targeted in their Appendix A:
> When using the standard practice of applying LoRA to query and value attention projection matrices, we are not able to replicate full finetuning performance for large base models. As shown in Figure 2 for LLaMA 7B finetuning on Alpaca, we find that the most critical LoRA hyperparameter is how many LoRA adapters are used in total **and that LoRA on all linear transformer block
layers are required to match full finetuning performance.**

> We do a hyperparameter search for LoRA over the following variables: LoRA dropout { 0.0, 0.05, 0.1}, LoRA r { 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers, all layers, attention + FFN output layers}. We keep LoRA α fixed and search the learning rate, since LoRA α is always proportional to the learning rate. We find that LoRA dropout 0.05 is useful for small models (7B, 13B), but not for larger models (33B,
65B). **We find LoRA r is unrelated to final performance if LoRA is used on all layers as can be seen in Figure 4**",3
post25tec,technical,0.982934466552778,highest,"Do you have a link for the 4bit Lora trainer? Would love to check it out.

My comment about additional knowledge training was not meant to be completly mushed into the discussion Lora vs Qlora but rather general to the Lora-Training.

>Table 6 shows that, surprisingly, LoRA already performs competitively with a very small r (more  
>  
>so for {W q ,W v } than just W q ). This suggests the update matrix ∆W could have a very small  
>  
>“intrinsic rank”. 6 To further support this finding, we check the overlap of the subspaces learned by  
>  
>different choices of r and by different random seeds. We argue that increasing r does not cover a  
>  
>more meaningful subspace, which suggests that a low-rank adaptation matrix is sufficient.  
>  
>**6 However, we do not expect a small r to work for every task or dataset. Consider the following thought**  
>  
>**experiment: if the downstream task were in a different language than the one used for pre-training, retraining**  
>  
>**the entire model (similar to LoRA with r = d model ) could certainly outperform LoRA with a small r.**

My tests where with adding japanese into the training. Thus having lower memory footprint and allowing for higher ranks in the training looked promising. Though further experiments seem to suggest that i will have to fully retrain a model with a better fitting vocabulary anyways.

You mention on using all layers, even fully connected, is interesting. Will have to incorporate that into my experiments. Thanks for that \^\^",4
post25tec,technical,0.982934466552778,highest,have you compared the efficiency of lora 4bit compared with qlora?,2
post33con,controversial,0.9695224937381768,highest,I guarantee you won’t see a decrease in total employment over the next 40 years outside of temporary recessions. Labor demand will continue to grow.,1
post33con,controversial,0.9695224937381768,highest,"Agree to disagree on that. I think we will start to see actual automation taking root by 2030 and beyond, let alone 40 years into the future",2
post33con,controversial,0.9695224937381768,highest,Automation already happens. I’m not saying we won’t seem a boom in automation. I’m saying we will still see demand for labor rise. Automation will create jobs.,3
post33con,controversial,0.9695224937381768,highest,"I know this is hypothetical, but if we have an AI that can automate most/all jobs (I.e. work harder/smarter than us) then what new jobs could be created?

Surely the AI would be able to also fill any new jobs that need to be filled?",4
post33con,controversial,0.9695224937381768,highest,"If ""automation"" creates jobs, it should be abandoned immediately because it's just making unnecessary work.",4
post33con,controversial,0.9695224937381768,highest,"Here's why, unemployed military age males are dangerous.

The police in my country are putting put reports that economic conditions are ripe for civil unrest.

Realistically speaking look historically to the USSR. Many jobs were created for the sake of work instead of out of necessity. Our society has many unproductive jobs atm. You underestimate the will of the elites to not see themselves in a French Revolution scenario. They aren't ignorant of the risks.",3
post33con,controversial,0.9695224937381768,highest,"Yeah that's true.  ""Bullshit"" jobs are rife.

And yeah, the elites are not going to get guillotined this time around. They're safe. Good for them.

People are too bloodthirsty to ""eat the rich"". They are perfectly fine lol. There are bigger fish to fry",4
post19con,controversial,0.9628113948678702,highest,"“Paywalling GPT-4 is Classist by design” lol this isn’t free therefore classist! 

There is nothing wrong by putting a price on your own product. Just because it’s expensive doesn’t make it unethical. 

The racism is due to either poor guard rails on internet content it pulls from or lack of context for the information it is pulling. Not really unethical, just a work in progress. Not sure if you support the claim that it was a feature well enough. You just say it did do certain things as if that supports it was intended to have done those things. You also are seemingly aware that they are trying to fix the issue, thus indicating it is a bug/unintended action of the software. 

So again, nothing unethical there.

Outsourcing jobs to foreign countries for pay less than minimum wage in America is probably the best argument for any unethical practice, but we also need to see if the amount they were paying is still a great amount for the people in those locations. So it could even be a win win situation tbh.",1
post19con,controversial,0.9628113948678702,highest,Then isn't OpenAI’s original claim to ‘benefit all of humanity' a hoax ?,2
post19con,controversial,0.9628113948678702,highest,"Why are you confusing marketing with reality?

In a round about way, humanity as a whole likely will benefit from AI.  Just because not everyone can use it doesn't mean they don't benefit from it in some way.  You're confusing direct use and benefit with indirect benefit.

Have you ever received a vaccine?",3
post19con,controversial,0.9628113948678702,highest,"OpenAI's 'benefit humanity' claim isn't just marketing , it's the legal basis for their original non-profit status and tax exemptions. This would be like Pfizer claiming to 'end disease globally' while selling COVID vaccines exclusively to billionaires. The hypocrisy is structural, not rhetorical.",4
post19con,controversial,0.9628113948678702,highest,"Not all of humanity has to afford it to benefit all of humanity. If someone builds a rocket to another planet and says this will benefit all of humanity, it’s not a lie even if everyone else can’t afford said rocket rides. The discoveries by those who do use it, could benefit all of humanity still.",3
post19con,controversial,0.9628113948678702,highest,"Space exploration is publicly funded science with open results. OpenAI is privatizing publicly-funded AI research (they took NSF grants), locking away discoveries behind paywalls , using exploited labor to build their 'rocket'",4
post19con,controversial,0.9628113948678702,highest,I can 100% agree that OpenAI isn't living up to its original goals without agreeing that it is inherently immoral. Those two aren't inherently linked.,3
post45con,controversial,0.9604984496845632,highest,"it’s worth pointing out that the problem isn't AI itself. It's how we choose to integrate it into the classroom. Right now, many schools are still treating AI like some sudden, uncontrollable force instead of treating it like a tool that can be managed, just like calculators, phones, or even Google itself when it first became widespread.

There are simple ways to reduce students misusing AI. Make more of the work classroom-based and discussion-heavy. Have students explain their thinking verbally or in writing. Require handwritten drafts or in-class brainstorming before allowing typed work. Create assignments that AI can't easily complete (personal connections, classroom-specific references, critical thinking questions).

Also, I think it is essential that we teach students how to use AI responsibly. Most adults I know use it for lesson planning, writing and editing emails, reports, resumes, coding help and debugging, language translating, etc. etc.

I don’t think we're heading toward total brain-mush dystopia. I think we're facing a challenge that schools and educators can meet if we start adapting. We should be teaching how to use AI as a tool. It isn't going to disappear.",1
post45con,controversial,0.9604984496845632,highest,So the teacher at my university who says all the things you just said claim that their students now totally would never use AI. I sing in the university choir and often sit behind and amongst students. I have watched a student use AI on every assignment in that person's class this term in all sorts of ways that are not allowed by them.,2
post45con,controversial,0.9604984496845632,highest,"I think you might have misunderstood my point a bit. I'm not saying students don't use AI to cheat. They absolutely do. My point is that the problem isn't AI itself, it's how we choose to respond to it as educators. We can either treat it like an unstoppable threat and spiral into despair, or we can adapt our teaching methods to make sure students are still learning, even in an AI-rich world.

That student in your choir using AI on every assignment? That's not a tech problem, that's a classroom management and accountability problem. The solution isn't to ban AI from existence, it's to get smarter about how we structure learning and assessment.",3
post45con,controversial,0.9604984496845632,highest,"The problem is AI itself.

Nobody asked for it.  We don’t need it.  It is a tool for cheating.",4
post45con,controversial,0.9604984496845632,highest,"> phones


Worth noting that many schools (my own included) are banning phones",2
post45con,controversial,0.9604984496845632,highest,"which is weird to me. I don't see why students shouldn't be able to use their phones during lunch or breaks. Or before or after school on campus. 

We just have a policy that they can't use them in class. And sometimes we use their phones in class to do Kahoot, Booklet, and Flip. It's pretty simple to enforce. If their phones are put away, no problem. If a phone is out, I take it and they get it back at the end of class or at the end of the day.",3
post45con,controversial,0.9604984496845632,highest,It’s because these anti AI pearl-clutchers are going full fascist to defend what they think is “important education” instead of looking in the mirror and realizing the triviality of the entire educational system.,4
post45con,controversial,0.9604984496845632,highest,"There is more to it than that though.  I teach advanced mathematics.  In the last decade there have been a slew of amazing programs that are wonderful for helping teach math.  Even software that allows you to give adaptive or forgiving tests, such as questions that change in difficulty and ward different levels of points, or even just giving them immediate feedback and a second chance at an answer to correct a missed positive or negative sign.  This is all amazing for the progress of education, and it is single handled destroyed by the proliferation of AI.  Simply googling a question can yield an answer now.  So all assignments that can be done outside of class will be cheated on easily.  Leaving little progress unless we do it in class.  Here’s the thing though: In our day, we could find our answers through Google.  We had to figure them out on our own or from someone who did and could explain it.  Even if you copied work, that work had to be done by someone.  We also weren’t given much time if any to work on assignments in class.  So our classes progressed faster.  Immediately that means classes will be slowed down by needing to take away time to do all the assignments in class.  The other consequence is that they won’t get as much practice as we did, because we can be sure any work assigned to be done at home won’t be done by them.  There is also a push to make assignments weighted more than tests now.  At my school the push is do 50/50. So we have students finishing 100% of their work through AI but then can only manage a 20% on their tests.  The thing is, though that 20% test score will get them a passing grade and a diploma.  Since they didn’t really do their work, these essentially failed a test and passed a class.  We then push these through to graduate and the ones who can’t even do that? The guidance puts them in these programs that let them work on them at home and somehow these failures get an entire semester’s worth of education and credit done in a week’s worth of time so they can still graduate.  That 50/50 set up that allows a 20% test score to pass? From my survey of fellow teachers we seem to have only about half of our students actually reaching that that easy pass rate.  The rest either get extra credit opportunities to make it up or those programs I mentioned earlier.  That’s how we have that many students failing at any given point and yet somehow boast graduation rates in the 90’s.

Most of these kids we are pumping out of schools with a diploma are no where near as qualified to have it as those from 00’s, 90’s, or before.  You might as well upgrade every high school diploma from before 2010 to a bachelor’s degree to represent the difference in their intelligence.

It’s really bad, and if they continue this way… then generations of unqualified people with hardly any academic knowledge will be taking over the workforce.  The only way to combat it is to require teachers to be overly strict or get rid of all the advancements we have made in education and require them all to strictly read and write their work.  When we require these teachers to teach about 33% more students than before though, that leaves a lot of students unseen and able to sneak their phones to do that written work anyways.

There has to be a change, and the first needs to come from zero tolerance of cell phones in school.  Some counties have implemented this and it has been very effective.  The second needs to come from school issued devices that are heavily secured to prevent any and all access to outside sources.  Even then though, this limits things like research reports for the students because the only way to keep them honest is to take away the access to the World Wide Web that were such a boon of a resource for the students of the 90’s and 00’s.

It would be great if there was a way to eliminate the access of AI to students, but that would require a concerted effort from the AI companies who quite frankly probably don’t care about any of this.",2
post45con,controversial,0.9604984496845632,highest,"Students have been putting lead in their chromebooks all week because of a tik tok trend called “.3 GPA Activities”.  So it is titled something that is actively stupid, and they copy that behavior.

The brain mush is already here.

We are the problem, but AI is a problem on the hands of children.",2
post45con,controversial,0.9604984496845632,highest,"Sure, some kids are doing dumb things. That's not new. TikTok didn't invent bad judgment, it just broadcasts it faster. Writing students off because a few follow a trend is lazy. A vast majority of kids aren't idiotic.

AI in kids' hands is only a problem if we refuse to teach them how to use it. It's no different than letting kids loose with cars, chemicals, or credit cards without guidance. We will serve them best if we teach them how to use AI.",3
post45con,controversial,0.9604984496845632,highest,Giving a kid a car is not the same as giving him a machine that will remove his ability to think.,4
post45con,controversial,0.9604984496845632,highest,"Time to bring back the blue books! 

The antidote to AI plagiarism already exists and it's very ancient technology -- it's called ""taking a handwritten, open-book (actual books) comp exam in a little blue composition notebook.""",2
post16con,controversial,0.9366228945460138,highest,"The scary one is self-driving vehicles, I was at a transport conference about 7 years ago and the speaker was of the opinion it's an imminent threat to driving jobs. So the threat is much bigger than generative AI or chat agents.

With no disrepesct because it is a tough industry but driving jobs can be the bottom rung of the ladder so when jobs like that are taken it's going to affect the people that are already the poorest even more. That's my fear.",1
post16con,controversial,0.9366228945460138,highest,"In the near future, most people won't have any value to offer to our economy. When robotics becomes mass produced and affordable, we won't need most people. It's going to be ugly.",2
post16con,controversial,0.9366228945460138,highest,The other point made was that the quickest way to make self-driving safer would be to remove the human drivers from the road.,3
post16con,controversial,0.9366228945460138,highest,"Or, self driving cars which have a high rate of crashing but okay.",4
post16con,controversial,0.9366228945460138,highest,Big Recession will hit once unemployment goes above 12-15 percent. People will demand AI be regulated.,3
post16con,controversial,0.9366228945460138,highest,"""Our"" economy?",3
post16con,controversial,0.9366228945460138,highest,It will never become affordable if the wages go down because everyone has to sell himselve at rock bottom prices.,3
post16con,controversial,0.9366228945460138,highest,"If the car crashes and someone dies, who do we blame, the AI or its creator?",2
post16con,controversial,0.9366228945460138,highest,Car owner/operator,3
post16con,controversial,0.9366228945460138,highest,"Nah, it’ll be manufacturer and self insured. It’s already going this way.",4
post16con,controversial,0.9366228945460138,highest,"Blame? What is this? The way the world is going, you'll be going to jail if you blame anyone ;)",3
post16con,controversial,0.9366228945460138,highest,"It's a legitimate question.

Honestly I think in these situations most accidents would be no fault accidents. Meaning both parties pay for their own damages unless one of the drivers can be proven to have influenced the circumstances somehow.",3
post16con,controversial,0.9366228945460138,highest,"and no offense to truck drivers but will be even able to to retrain all these people to do jobs that AI cant do? 

thats one of my big worry's is that a large chunk of the population will not be able to do those jobs because those jobs will be non standard require a lot of creative thinking and flexible mindset. because the moment they do become standard or linear AI will be able to do them",2
post16con,controversial,0.9366228945460138,highest,All will be poor without an education. High school education is no longer the ball game. It’s now skilled advanced workers or college graduates. Seems better to go to Canada where they do ai research but has lagged behind on adapting it because of the economy because healthcare is free and we have to pay for it here in America and so is education in Canada but your country wants to force you to pay for it. We need to start thinking more logical here. We know none of that is good for any society where violence will be heavy because it’s coming. I give it about five or six years. People will be dying all over America and no killers to be found.,2
post9tec,technical,0.1311922346668418,lowest,"Thanks for sharing this here, and for your feedback :) I'd be very curious to hear what you find.

For perceptual signals, I've also found nucleus sampling to be insufficient. For language it definitely delivers though. What you propose seem connected to the ""selective sampling"" procedure proposed in Zhang et al. 2020 (the first paper you linked), though it isn't quite the same thing.",1
post9tec,technical,0.1311922346668418,lowest,"Hi, well, thanks for writing the article!

Here are my initial observations. I haven't done anything blinded just yet -- I might do this a little more rigorously at some point, but I may want to train a larger model or switch model architectures (I'm thinking of the Transformer XL) before setting up a better experiment here.

I am using the music transformer approach described in [Huang et al 2018](https://arxiv.org/abs/1809.04281) -- I'm currently training on 400 length midi events for now, but am attempting to use all other hyperparameters as they did.

On the testing set, I'd say I ""agree"" with the model's assessment of random samples. Roughly, the bottom 50% of the samples seem to be not very good (parts of a song that seem like it's missing context, or sounds otherwise unstructured). The upper 50% sound more song-like for lack of a better description.

As for model-generated sequences: I generated a few hundred samples using both temperature sampling (T=0.95) and nucleus sampling (with value 0.95). It seems like the generated songs (from both sampling methods) with likelihoods in the upper quartile of the testing set likelihood distribution are where most of the decent-sounding samples lie. Definitely not all the generated samples in that range are good. I don't have a sense of if the peak of good sounding samples is contained entirely within the distribution of likelihoods on the test set or extends slightly higher -- if it extends beyond at all, it doesn't seem that it does by much for me.

As an aside, do you happen to know what type of sampling Huang et al (2018) did? I didn't see it mentioned in the paper. (And further, were the samples that they included in the supplemental materials truly random samples from that sampling method or selected in any way?)",2
post5tec,technical,0.1549466084811289,lowest,I love it! what did you use to create those pictures?,1
post5tec,technical,0.1549466084811289,lowest,"I use Figma! But in all honesty, these could have been created just as easily with Keynote/Powerpoint.",2
post16tec,technical,0.1707828576906537,lowest,"Couldn’t we just use a distance threshold instead of top k to dynamically retrieve documents 

My main issue with self rag and this is that it incurs extra llm calls which is what drives cost and inference time",1
post16tec,technical,0.1707828576906537,lowest,">Couldn’t we just use a distance threshold instead of top k to dynamically retrieve documents

This is unlikely to change anything in the LLM token cost question. Vector index retrievals are fast and inexpensive compared to LLM use. In the RAG setup, the usefulness of the vector metric is first of all in that it allows to sort (rank) documents by relevance (from the closest to the furthest, with some cutoff), and to pass them to the LLM, in this order. Then, cutting off by vector metric value rather than fixed k is unlikely to help in any practical scenario, and is usually worse than fixed k.

>My main issue with self rag and this is that it incurs extra llm calls which is what drives cost and inference time

Actually, LLM API's like OpenAI don't charge per call, they charge per token used - and this is consistent with their computational effort. The presented approach saves tokens - it uses several calls with fewer tokens \*in total\*, rather than one call that costs a lot of tokens. While network/API latency may sometimes be a factor when calculating latencies, overall, the described approach should also have lower total inference time.

Thanks for these questions by the way - these are great points to bring up with the authors to clarify!",2
post38tec,technical,0.1815587112500365,lowest,"OWUI RAG performance with local embeddings and reranker (hybrid search) is very good if you choose the right models and tune the parameters accordingly. I've experimented with many embeddings and reranker models and for the time being I've settled with [Snowflake/snowflake-arctic-embed-l-v2.0](https://huggingface.co/Snowflake/snowflake-arctic-embed-l-v2.0) for the embeddings and [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) for the reranker. For the Tok-K and Minimum Score I go back and forth all the time but for now I'm using 10 and 0.3.

One important thing to consider when using local embeddings/reranker is that you need to use a GPU accelerated container for open-webui. If you're using Docker that would be the [ghcr.io/open-webui/open-webui:main-cuda](http://ghcr.io/open-webui/open-webui:main-cuda) image",1
post38tec,technical,0.1815587112500365,lowest,To clarify OpenWebUI itself actually needs to be GPU accelerated? Not just the Ollama Host that is running the embed/rerank?,2
post42tec,technical,0.2147433806880658,lowest,"I think you're right that this is similar to path exploration in games, which is why some people have been approaching the LLM agent issue using similar algorithms.

What you want is probably reinforcement learning or monte carlo tree search. In either case you want to model the probability that a given choice for next token will produce an entire path/string will be valid in your grammar, which you would train another model to be able to do. The LLM would sort of provide a prior probability for this that would then be adjusted by another model.

I personally am skeptical of this entire domain of work. It seems like a lot of effort to do that much additional modeling on top of the LLM, and it's not clear to me that the LLM itself ultimately confers much of an advantage over just training reinforcement learning or MCTS from scratch.

Like, even if you get the LLM to give you outputs with the structure that you want, you then have the problem of connecting those outputs with real things in external data from the real world, which itself is a nontrivial task. Throwing the pretrained LLM on top of the main algorithm is potentially adding unnecessary complication and performance issues to an already large task that doesn't necessarily benefit a lot from it.",1
post42tec,technical,0.2147433806880658,lowest,"Yes, I definitely have a hard time imagining how all this could be made performant.",2
post47tec,technical,0.2147433806880658,lowest,"I think you're right that this is similar to path exploration in games, which is why some people have been approaching the LLM agent issue using similar algorithms.

What you want is probably reinforcement learning or monte carlo tree search. In either case you want to model the probability that a given choice for next token will produce an entire path/string will be valid in your grammar, which you would train another model to be able to do. The LLM would sort of provide a prior probability for this that would then be adjusted by another model.

I personally am skeptical of this entire domain of work. It seems like a lot of effort to do that much additional modeling on top of the LLM, and it's not clear to me that the LLM itself ultimately confers much of an advantage over just training reinforcement learning or MCTS from scratch.

Like, even if you get the LLM to give you outputs with the structure that you want, you then have the problem of connecting those outputs with real things in external data from the real world, which itself is a nontrivial task. Throwing the pretrained LLM on top of the main algorithm is potentially adding unnecessary complication and performance issues to an already large task that doesn't necessarily benefit a lot from it.",1
post47tec,technical,0.2147433806880658,lowest,"Yes, I definitely have a hard time imagining how all this could be made performant.",2
post12tec,technical,0.2732746911530488,lowest,"Can you run \`nvidia-smi\` or similar command, or copy the exact error, to prove that the GPU is being filled and not a CPU / RAM issue?  
Sometimes when things are running slowly I see the model is only being loaded into the CPU. I don't see you mentioning using Transformers but sometimes a param like \` device\_map=""auto"" \` makes the difference.",1
post12tec,technical,0.2732746911530488,lowest,"the 48gb vram is quickly filled almost immediately after I run the train() command on the HF trainer object.

i can run the ""nvidia-smi"" tool during the initial 10 seconds and I see the vram quickly filling up. If I use any batch size above 4, it gives me a OOM error.",2
post12tec,technical,0.2732746911530488,lowest,"I see in other parts that this is your first finetune and you're using the training code from the Transformers docs. That's super generalized and rarely updated with tips, etc.  
I'd recommend trying out the inference code on the model's readme page first ( [https://huggingface.co/answerdotai/ModernBERT-base](https://huggingface.co/answerdotai/ModernBERT-base) ) to make sure that the basics are working, have flash attention installed, etc.  Once you're happy with the model and coding environment, try code examples from the GitHub repo, which use SentenceTransformerTrainer: [https://github.com/AnswerDotAI/ModernBERT/blob/main/examples/train\_st.py](https://github.com/AnswerDotAI/ModernBERT/blob/main/examples/train_st.py)",3
post39tec,technical,0.2732746911530488,lowest,"We stop using reranking for more than a year too, because model are smart enough to sort the chunk retrieved",1
post39tec,technical,0.2732746911530488,lowest,"The main value of rerankers isn’t to just sort the chunks that are being returned to your generative, the point is that you use rerankers to return higher quality results to the generative model.

For example, if you intend to return 10 chunks, then you should set your initial search limit to something larger like 30 results. Then the reranker will sort those thirty results, and you return the top 10 to your generative model.

Rerankers are a lot faster and cheaper than generative models so whatever your cost and latency considerations are, it makes sense to use rerankers.

Furthermore, if you are using any form of hybrid search then some type of reranking (not necessarily a cross-encoder) is necessary to combine the results.",2
post39tec,technical,0.2732746911530488,lowest,"Yes but it is mostly useless in our use case since the embedding model is already good enough to help us pull out the best chunks
Then the LLM is strong enough to sort out the few irrelevent chunks
So there is no point in adding latences for a reranker wich akwardly sit between what an embedding model and an llm already do",3
post48tec,technical,0.2795125820661931,lowest,I don’t see the word weasel in that example at all.,1
post48tec,technical,0.2795125820661931,lowest,"Sorry for the confusion. I did not mean the word ""weasel"" itself. Weasel words refer to vague or noncommittal phrases like “some people say,” “it is believed,” or “many experts agree.” These are usually avoided in academic writing because they are unclear and unsupported.",2
post48tec,technical,0.2795125820661931,lowest,The point I was trying to make is that maybe you just need clearer instructions? Are you providing one shot or multishot examples with your prompts?,3
post28tec,technical,0.2890080551849795,lowest,"Unfortunately the explanation of penalties here are completely hallucinated.

Penalties have nothing to do with training data; it reduces the chance of tokens that already in the context based on how many times each tokens appeared.

Presence penalty applies the same-value penalty to tokens that appeared at least once.",1
post28tec,technical,0.2890080551849795,lowest,"Thanks for pointing it out! I guess this is a classic example for danger of trying to learn with gpt. When it gets right, it's great, but when it's wrong, you end up learning completely inaccurate information. :)",2
post20tec,technical,0.3180577499866151,lowest,"I think it had a lot to do with being able to implement rope retroactively with a small code change. Currently existing models sortof “just work” with it. Llama 1 models can be bumped up to 4k and even llama 2 models can be bumped up to 8k without doing any finetuning or training of the model at a pretty minimal quality loss.

Then for the models finetuned on it specifically, it didn’t require a full retraining of the model- just a relatively inexpensive finetune, that was basically able to be dropped in to the existing finetuning pipeline.",1
post20tec,technical,0.3180577499866151,lowest,"These retroactive RoPE techniques are recent developments, as the OP mentioned. OP's question is why ALiBi wasn't attempted more widely before then.",2
post20tec,technical,0.3180577499866151,lowest,Hi can you share the reference about retroactive RoPE techniques?,3
post40tec,technical,0.3188894182308369,lowest,"A simple low effort trick will get you pretty close to what you need.

If you concatenate all 20 million vectors into one long time series, you have a long vector of..

length C = 2560000000.    Long, but not outrageous 

Now you have your query Q of length 128

Just run MASS \[a\], and you are done!!!

\>> MASS\_V3(randn(C,Q,2\^23);

There is now an official MATLAB version of MASS, and there is a C++ version at \[a\] etc

If the vectors have autocorrelation, then you can down sample before using MASS to make it faster ( with a tiny reduction in accuracy) See slide 15 of \[b\].

  
In summary, I think this would get you close, and you could do all this  in 5 min with a few lines of code. Anything better that this (indexing etc) will be a lot of work/coding

If you think  C = 2560000000 is too long, break it into chucks, but make the chunks powers of two (see slide 16)



\[a\] [https://www.cs.unm.edu/\~mueen/FastestSimilaritySearch.html](https://www.cs.unm.edu/~mueen/FastestSimilaritySearch.html)

\[b\] [**https://www.cs.ucr.edu/%7Eeamonn/100\_Time\_Series\_Data\_Mining\_Questions\_\_with\_Answers.pdf**](https://www.cs.ucr.edu/%7Eeamonn/100_Time_Series_Data_Mining_Questions__with_Answers.pdf)",1
post40tec,technical,0.3188894182308369,lowest,"I had two concerns that I was hoping you could clarify:

#### Preserving Embedding Boundaries

- Since MASS slides a window across the concatenated sequence, wouldn’t this cause misalignment issues, where query embeddings (length 128) overlap partial embeddings in the concatenated sequence?
- Would enforcing a fixed step size of 128 (to ensure queries align with full embeddings) negate the efficiency gains of MASS

#### Time Complexity Compared to Brute Force and ANN

- While MASS accelerates Euclidean distance computation via FFT, doesn’t the requirement to scan all valid starting positions make it at least O(n) (assuming step size = 128)?
- Given that ANN methods like FAISS and HNSW achieve sub-linear search time via indexing, would MASS still offer an efficiency advantage in practice?",2
post40tec,technical,0.3188894182308369,lowest,"Nice read, TY",2
post40tec,technical,0.3188894182308369,lowest,Lol I was coming to say this same thing but the man itself blessed this thread...,2
post43tec,technical,0.3320751721213973,lowest,Schemas with structured outputs,1
post43tec,technical,0.3320751721213973,lowest,Approved + set temperature,2
post43tec,technical,0.3320751721213973,lowest,"Hey   
Do you think people have a lot of trouble working with structured outputs?",3
post19tec,technical,0.3521289232873442,lowest,"space complexity, sure, but how can it improve time complexity?",1
post19tec,technical,0.3521289232873442,lowest,It improves time but not time complexity.,2
post19tec,technical,0.3521289232873442,lowest,"This comment is confusing. Did you mean that it improves space but not time complexity? Because that is true since we do not really change the number of operations, we just optimize the memory operations, which is space, not time — and it gives the speed up simply because we use a faster memory throughout. Although I would argue that a) reducing the number of memory swap calls can be seen as time complexity since we reduce the number of operations in the algorithm, which by the way kind of dominate the runtime, and b) they do kernel fusion to merge several operation together — and I am not acquainted enough with cuda and all this low level jazz, but it is possible some sort of time complexity reduction might be happening there",3
post19tec,technical,0.3521289232873442,lowest,It's faster because gpus are memory bottlenecked not because it has better time complexity.,2
post17tec,technical,0.3771955132141288,lowest,I'm working on this,1
post17tec,technical,0.3771955132141288,lowest,Awesome. Mind ELI5 what you're working on specifically?,2
post17tec,technical,0.3771955132141288,lowest,That would be very difficult. I don't think I can,3
post17tec,technical,0.3771955132141288,lowest,Cmon,4
post1tec,technical,0.3782994441191201,lowest,">I don't quite understand why this multiplication gives us a meaningful metric for importance.

Because of gradient descent. The model is trained to find query and key values such that this multiplication will give a meaningful metric for importance. There is literally no other constraint on the key and query vectors, they're not used for anything else, so gradient descent is free to ""choose"" whatever vectors work best

As for why it's possible to use inner product as a measure of importance, imagine each entry in the key vector as an attribute, like ""implies a location"" or ""usually a noun"" or ""signals a casual tone,"" etc. The key vector is a list of how much the token has each attribute. The query vector is a list of how useful each attribute would be to know. Obviously we don't know in general what the attributes are or how they're represented, but the point is you can represent attributes with a vector and then find the total relevance with an inner product.",1
post1tec,technical,0.3782994441191201,lowest,"Thanks. This helped in kind of visualizing the importance/significance of key and query vectors.

What would you say the value vector is for in the same sense? We take the dot product between key and query, divide by root of key dimensionality and apply softmax which gives a normalized representation of how important each token is to each token (total\_tokens, embedding\_dim). (is this correct btw?)

Then we, for some reason pass this through another linear layer(value layer). Why do we do this?",2
post1tec,technical,0.3782994441191201,lowest,"The square-root-of-dimension thing is just for training purposes, it improves computational stability but doesn't really change things because the vectors can just get bigger/smaller to compensate

Think of the value vector as a way of perturbing the representation of your vector. The tokens are represented by vectors in the embedding space. The value vector is added to that embedding vector, changing it slightly, yielding a new point in latent space. 

The softmax roughly means ""choose the best one,"" in this case the single context token which best matches the query, and add only that token's value to the original embedding. Except literally choosing just one wouldn't be differentiable so we use softmax (instead of max) to hedge our bets. If two tokens are equally good, we split the difference and hope it works. If one token is twice as good as another, we do 2/3 of that one and 1/3 of the other. Etc.

I'm not sure what you mean by ""pass through another linear layer."" Are you referring to multi-headed attention?

A linear layer (no activation) is just rotation in latent space. It doesn't do anything at all in terms of adding information, it just relabels the information. Maybe to get it ready for some nonlinear operation that needs a specific labeling, or because it was being stored in a lower dimension and couldn't get properly labeled when it was created

In the case of multi-headed attention, since each head outputs a vector much smaller (in dimension) than the embedding space, the linear layer is just a list of what embedding vector each head's outputs correspond to

[I'm simplifying things, this is just an intuitive place to start from]",3
post1tec,technical,0.3782994441191201,lowest,">I'm not sure what you mean by ""pass through another linear layer."" Are you referring to multi-headed attention?

I was referring to the value vector here.",4
post33tec,technical,0.4001122693416926,lowest,"i'm in research, but having talked to industry people:

RLHF: has the highest ceiling of the options (according to the latest research and hearsay) but very hard to reach that ceiling. in industry, only openai/anthropic/gdm manage to do it well. 

DPO/KTO: vastly more common, especially among startups. even meta has switched to it for llama-3.1. if you know you have high-quality pairwise preferences and are willing to do a round of SFT, dpo is probably still your best option. If you have noisy preferences, if you don't want to do SFT, or if you only have thumbs-up/down feedback (and especially if that feedback is class-imbalanced), then KTO is the better option. I've met many startups in particular who've had better success with KTO since their data tends to be noisier, though some teams at meta seem to like it as well (disclaimer: i'm on the paper that proposed it, so there is some exposure bias here).

Best-of-n: I haven't really heard people using this in practice, mostly due to concerns around inference efficiency and because training a good reward model is still very hard.",1
post33tec,technical,0.4001122693416926,lowest,thanks for your reply! do you think DPP/KTO is less prone to overoptimization (since they avoid reward models completely)?,2
post33tec,technical,0.4001122693416926,lowest,"it depends. if you do standard offline dpo, then it's not really going to be prone to reward-hacking in the way that online rlhf is. however, if you do online dpo (i.e., sampling from the model, inferring a preference, taking a step), then you can run into the same issues as rlhf iiuc, though there hasn't been a ton of research on this.

comparing dpo vs. kto, kto is less prone to over-fitting on the same data (which in this case would mean taking a preference and breaking it up into 1 good, 1 bad). this is simply because you're learning from a weaker signal. this may help explain why kto is particularly good for aligning models to do mathematical reasoning and doesn't suffer from the same length-increase issues that dpo does",3
post33tec,technical,0.4001122693416926,lowest,Are pairs typically acquired by running the same input twice with high temperature?,4
post33tec,technical,0.4001122693416926,lowest,some results on this are out: [https://arxiv.org/pdf/2406.02900](https://arxiv.org/pdf/2406.02900),4
post33tec,technical,0.4001122693416926,lowest,"Best-of-n actually yield very strong results despite its simplicity(OpenAI webgpt reported this. RAFT paper reports this too). Theres a recent paper (Reward Steering with Evolutionary Heuristics..) that compares best of N to all preference tuning method (DPO/SIMPO/KTO ... ) on alpaca eval2 and MT bench.

However, i dont think is fair to compare best of n with DPO/KTO all these. Best-of-n is an inference time algorithm while DPO/KTO actually updates the model's parameter. Its like comparing SFT  to few shot prompting of the same model.",2
post33tec,technical,0.4001122693416926,lowest,"Best-of-n can kinda be both an inference algo and training data enhancement method. Imagine originally you only have 50k high quality but 10M of low quality data. You can use SFT to train a poor model. Use preference data, you train a reward model. Then use best-of-n with the reward model on the 10M low quality data to obtain much better quality data. Now you have 50k high quality + 10M decent quality data. Then you use SFT again on the combined data.

But now I can curious how this would compare to DPO. Both draw from the preference. One immediate advantage I see in DPO is its ability to punish the model for the negatives, whereas best-of-n training is still just encouraging the positives.

Also, now I am a bit confused because I am noob. If best-of-n can turn lots of bad data to better data, does it mean that if I have 10M of high quality data to begin with, I don't need RLHF or any of these post training method?",3
post45tec,technical,0.4011399271138191,lowest,"Yes, i had just decided to give up on langchain and langraph (after like 10 tries). Ultimately coding something myself seems easier. Granted it might not have as much feature but at least i know where i can tweak thing. I will leverage on function from those framework where convenient e.g. rag, tools. But for the agent orchestration, i am building my own which is kinda similar to langraph but i dont need to touch the bloody LCEL and runnable thingy.",1
post45tec,technical,0.4011399271138191,lowest,Yeah that's fair enough. It seems that maybe using the langtools might just be more pragmatic than coding it from scratch.,2
post45tec,technical,0.4011399271138191,lowest,Langgraph is gone for self hosting so neoj and your own pipelines are the go now,2
post45tec,technical,0.4011399271138191,lowest,"Didnt expect someone to still read my posts after so long haha. Thanks for your comment. Here is a sneak peak. I am trying to build something that work generic and not like only single purpose (e.g. web search, rag). This is real time speed using qwen2-70b: https://www.youtube.com/watch?v=qwjyyPf9nUk",3
post45tec,technical,0.4011399271138191,lowest,Looking cool! Now I want to try it.,4
post31tec,technical,0.4062310233821336,lowest,"According to the paper, they are not using a neural network to calculate the reward. It looks like they have a series of reward functions that assign reward based on accuracy and formatting. I believe they use different reward functions for different datasets as well, for example, using a sandboxed environment to run tests on generated code samples. 

  
From the paper:

>2.2.2. Reward Modeling   
  
The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:  
  
\- **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.  
  
\- **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between ‘’ and ‘’ tags.   
  
We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.

  
GRPO is just another method for updating a model relative to some reward function. It does not stipulate what that reward function is. So, in many cases, people use GRPO with a neural network reward model. In the case of R1, the ""reward model"" appears to just be a series of reward functions.

It might help to look at HuggingFace's docs for their GRPO trainer to get a sense of how that might look: [https://huggingface.co/docs/trl/main/en/grpo\_trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer)",1
post31tec,technical,0.4062310233821336,lowest,The fact that you can train such a versatile and powerful model purely with reward functions seems almost impossible to me. But it was based on v3 and certainly also on other open LLMs. But thanks for the comments! Very exciting!,2
post31tec,technical,0.4062310233821336,lowest,"Hey, thanks for the reply. 

Previous LLM used SFT for instruction tuning i.e. ensuring that given a prompt, the LLM learns human preference for generating the response. However, without this step how is the model learning human preference i.e. without SFT how is the model adhering to prompt (such as putting information in <think> and not going completely of the rails?",2
post31tec,technical,0.4062310233821336,lowest,"Good question! From my understanding, there are two parts to this:

- The ""format rewards"" encourage the model to do things like put information between <think> tags. This alone seems to be enough to coax the model towards this behavior.

- The DeepSeek-R1-Zero model still, however, would exhibit weird ""off the rails"" behavior on some samples, doing things like mixing languages despite formatting them correctly. To address this, DeepSeek-R1 used SFT before GRPO, which seems to have largely prevented this.

It's also worth noting that the team behind the ARC prize did some testing and came to the conclusion that SFT might not actually be necessary, at least in many cases: https://arcprize.org/blog/r1-zero-r1-results-analysis",3
post31tec,technical,0.4062310233821336,lowest,Thanks. I'll read the blog post,4
post31tec,technical,0.4062310233821336,lowest,"What I dont finally understand is the following: ok, every time the actioner produce and action (outputs of the model in this case) the environment gives you a reward (based in this case on accuracy and format). But the actioner (model) has to start with an initial policy, right? Which one is it? How did DeepSeek started their policy for the model?",4
post15tec,technical,0.4117173899223478,lowest,Nice! Yeah Titans made huge waves then nothing. Was hoping to see some code for it. This might be my queue to work on a better understanding of rotary embeddings too!,1
post15tec,technical,0.4117173899223478,lowest,https://github.com/lucidrains/titans-pytorch,2
post15tec,technical,0.4117173899223478,lowest,Thanks,3
post15tec,technical,0.4117173899223478,lowest,"I believe there are some problems in this implementation, the model gives errors when setting the neural memory layer at dimensions 256 or larger

  
anyone worked with this and able to provide input?",3
post15tec,technical,0.4117173899223478,lowest,There’s also fourier positional embeddings which is an enhanced rope and rope extended by Microsoft which uses an evolutionary algorithm,2
post26tec,technical,0.4320000843314266,lowest,"I love how bloom was just like ""F\*ck it let's one-up openAI""",1
post26tec,technical,0.4320000843314266,lowest,"Yeah, I think its a just like a 1B MLP with random weights not connected to any outputs:)",2
post26tec,technical,0.4320000843314266,lowest,Honestly wouldn't be surprised lol,3
post26tec,technical,0.4320000843314266,lowest,Does Bloom do tasks? is it well behaved?,3
post26tec,technical,0.4320000843314266,lowest,"bloom is pretty terrible, unfortunately",4
post49tec,technical,0.4344055714059368,lowest,"We will be releasing a suite of fine-tunes on both Llama (7b, 13b) and Open-Llama (3b, 7b, 13b, 20b) in the coming days.",1
post49tec,technical,0.4344055714059368,lowest,Where can I subscribe to updates to be notified when you release these?,2
post49tec,technical,0.4344055714059368,lowest,They will be released under a unified organization on Huggingface after further evaluation. The first model is training now.,3
post49tec,technical,0.4344055714059368,lowest,I've been keeping an eye on this space. Any updates on the model releases? I think I found you and u/emozilla 's HuggingFace repos but I want to make sure I'm grabbing the right models,4
post7tec,technical,0.4389540217596841,lowest,"Please correct me, but isn’t a temperature of 1+ spreading the distribution? Anyways, nucleus sampling and top-k are practically always used. Repetition seems to be largely solved by better models and training processes.",1
post7tec,technical,0.4389540217596841,lowest,"Yes, but I don't think nucleus sampling and top-k are the default anymore. For example, the default top-P for GPT models (via API or playground) is 1, meaning that all tokens are considered, which is something that nucleus and top-k sampling both try to avoid.",2
post7tec,technical,0.4389540217596841,lowest,"Which to use might also be a technical/speed consideration, as it does not seem to matter that much for quality anyways. Nucleus needs softmax->sort->cumsum over the tokens, which takes a measurable amount of extra time over greedy. Top-k needs sort->select_range->softmax and is a lot faster. Doing top-k before nucleus is a lot faster than just nucleus, or I dinged the implementation.",3
post7tec,technical,0.4389540217596841,lowest,"> solved by better models and training processes

2 questions: What about these fixed the repetition issues? Why do the ‘worse’ models suffer from repetition anyway? They are also models of language and almost nowhere in the training data do such patterns occur",2
post7tec,technical,0.4389540217596841,lowest,"> What about these fixed the repetition issues? 

Models learning to consider more context.

Basic language prediction is can be done with a very, very short context. Based on the past one or two words, you can often predict the next word decently well. So if we have a model that basically only looks at the past 2 words, and we take the most probable next word, it's very easy to get stuck in a loop. Ngrams and RNNs do this very easily. Transformers are the first architecture that have a strong enough signal from far enough back to have the ability to overcome this. 

That the model has that ability, doesn't mean it always learns to carefully look at the context. The previous few words are still by far the strongest predictor of the next word. So even a Transformer needs to look at the previous few words first, before it could start to consider the whole context.

Anyways, this is my human interpretation of how and why this works this way. ""Looking"", ""first"", and ""consider"" are kinda undefined terms for a neural network.",3
post7tec,technical,0.4389540217596841,lowest,"But how come the earlier models learned to look at only a few previous words? The optimization process, the training data both should encourage the model to consider the whole context and predict a token that syntactically and semantically makes sense.

My confusion is where is this inflection point in model scale that allowed models to stop being repetitive. And why does such an inflection point occur",4
post50tec,technical,0.4509690101457125,lowest,"Have you tried playing with any of the 32K models? 

https://www.reddit.com/r/LocalLLaMA/comments/15ce6sq/llama27b32k_by_togethercomputer/",1
post50tec,technical,0.4509690101457125,lowest,"No, I haven't seen quantized ggml files for that model yet that I can use with koboldcpp.",2
post50tec,technical,0.4509690101457125,lowest,"https://huggingface.co/models?search=llama-2-7b-32k

Here you go one is the version mentioned and there is an orca version also both in ggml.",3
post50tec,technical,0.4509690101457125,lowest,"Thank you. I have tried both models several times and I don't see any obvious problems with numbers in the 8000 tokens produced using ropeconfig 0.125 10000 (linear).  
I didn't try NTK scaling since I'm not sure which frequency base value is appropriate for 8x NTK scaling.",4
post58con,controversial,0.4608161176263568,lowest,Which gender analysis tool are you using?,1
post58con,controversial,0.4608161176263568,lowest,"I used Nyckel:
https://www.nyckel.com/pretrained-classifiers/gender-detector/",2
post58con,controversial,0.4608161176263568,lowest,"Not on HRT yet and its giving me 90% confident I'm a woman in every photo (no makeup, straight headshot).",3
post58con,controversial,0.4608161176263568,lowest,Slay,4
post58con,controversial,0.4608161176263568,lowest,"I tried it and it said man 98% confidence…

…Ima go get high now…thanks 💀
(I am only one month though)",3
post21tec,technical,0.4668376575323903,lowest,Would be more convincing if tested in encoder and encoder-decoder tasks.,1
post21tec,technical,0.4668376575323903,lowest,"Why do you think so? GPT-3 is a decoder-only model, and that was a driving force in us trying to solve this issue for decoder-only models. 
I definitely have encoder-decoder models on my to-do list, but I think our results, on 3 different LMing datasets, with models with up to 1.3B parameters, are pretty convincing as is.",2
post21tec,technical,0.4668376575323903,lowest,"There are pretty many use cases for encoder and encoder-decoder models.

If this decoder is good because it does not pay attention to distance information, it's kind of a bad thing, right?

As for gpt3, I know it is decoder only, also I found it very unstable for real applications.

Actually, I'm not sure what you really able to do with the decoder only model.

Exploring generalization on unseen sample length is very important for my tasks (what's why I'm here), but I'm not convinced to try it immidiatly.",3
post21tec,technical,0.4668376575323903,lowest,"> If this decoder is good because it does not pay attention to distance information, it's kind of a bad thing, right?
We discuss this in depth in the analysis section. 

Extrapolation allows us to train a model on 1024 and test it on 2048 tokens, achieving the same accuracy as a sinusoidal model trained on 2048 tokens. This saves 11% memory and time in the 1.3B param setting, and will probably save even more resources for larger models.",4
post36tec,technical,0.4688729744305229,lowest,"Women and kids *are* people. Semantically, the ""not happy"" option is the least similar. You're thinking in terms of direct keyword match. If that's what you want, you don't need a transformer model, you need a TF/IDF algorithm.",1
post36tec,technical,0.4688729744305229,lowest,"Yeah, so shouldn't the sentence ""the woman is happy"" be more similar than the sentence ""the man is not happy""?",2
post36tec,technical,0.4688729744305229,lowest,"Oop, yeah I'd agree there, I didn't pay close enough attention to the order. I'd look at the other commenter's link to other embedding models, try the current sota models instead. But keep in mind those scores, like 50-70% accuracy on many benchmarked tasks. Even the best embedding models aren't perfect",3
post36tec,technical,0.4688729744305229,lowest,Yeah I'll give it a look. Have you heard about elasticsearch?,4
post37tec,technical,0.491788755956569,lowest,mariadb latest version maybe?,1
post37tec,technical,0.491788755956569,lowest,Why?,2
post37tec,technical,0.491788755956569,lowest,"Optimized for read/write heavy tasks, and free",3
post37tec,technical,0.491788755956569,lowest,"But what ANN index does it use?  Also, all the databases are optimized for heavy read/write - the OP is asking about updates (replacing the vector of a document already indexed).  Does MariaDB offer anything special there?",4
post44tec,technical,0.4950735800502992,lowest,"Not 100% sure but I guess it happens on sampling.

The running model knows nothing about the grammar you have loaded. It's just that, during sampling, only the tokens that are ""legal"" according to the grammar are considered; the rest are ""discarded"". It is not that easy because clearly sometimes generation slows down and it looks like it's backtracking internally (trying some branch of your grammar, not finding a solution, and going back to some parent node/token), but that's the gist of it.

It doesn't affect the model otherwise. The model does not ""know"" there is a grammar, it does not use context, there is no prompt.

BTW: be careful with the grammar: if it is too restrictive, it will force the model to sample very low probability tokens and it will start to operate in ""out of distribution"" and it will break down. For instance, you can create a grammar that prohibits the letter ""e"" or ""."" and the poor model will sample absurd tokens and eventually produce garbage.",1
post44tec,technical,0.4950735800502992,lowest,"Hmm, thank you that was my assumption: That the program itself rejects produced tokens somehow and asks for a different one? That makes sense to me.",2
post44tec,technical,0.4950735800502992,lowest,"The neural net doesn't ""produce"" tokens. It produces, at each step, a giant list of probabilities, one for each possible token. The program then selects one token from the list of all possible tokens using any policy it wants. For example you can pick the highest probability token every time (""greedy""). Or you can randomly sample from all the tokens according to their probabilities. Or you can modify the probabilities before sampling, using a ""temperature"", or simply by setting the probability of tokens you don't want to zero. (For example, the ones that don't match a grammar!) You can also choose multiple possibilities at each step and try all of them, then pick the best one at the end (beam search).",3
post44tec,technical,0.4950735800502992,lowest,">It doesn't affect the model otherwise. The model does not ""know"" there is a grammar, it does not use context, there is no prompt.

Now that I tested it a bit, I am still confused about it. Because let's consider a simple JSON grammar file (mentioned in the link I provided). So if I tell the model to ""give me a list of five names"" and force the grammar file, it will produce 5 names in that format - even if not mentioning JSON at all. That means, that the model probably never created a token like ""{"" or ""}"" as part of its reply, but those tokens were inserted by the grammar module, right?

Otherwise if I would ask the model to tell me a joke and force the JSON grammar, nothing really is produced. But it's also not like the model is hanging, it instantly produces an empty output enclosed by brackets.

This kind of makes my head spin.",2
post44tec,technical,0.4950735800502992,lowest,"The model outputs a value for every single token simultaneously. So for instance, it will be a 1, 32000 tensor with a value for each of the 32000 possible tokens. So the LLM always has a result for both ""{"" and ""}"" every single time it infers a token (and a result for ""Yes"", ""No"", ""hi"", ""z"", etc). Normally a sampler looks at the tokens with the highest values and picks one of them.

The results for ""{"" and ""}"" in this case may be negative numbers lower than all the other ones so they'll never get picked normally, but if a grammar bans everything but the allowed tokens then whatever has the relatively highest values will get picked, no matter how low those values may be in the absolute sense.",3
post44tec,technical,0.4950735800502992,lowest,"This is a very rough analogy, but you can think of it like this:

When the model is predicting the next token, it's a little like taking a bag of scrabble tiles, ranking them based on how likely they are, and randomly picking one based on the weighting.

A grammar is a set of rules that defines what the output should look like; you can think of it like a regex pattern. (Only more powerful, since BNF defines a context-free grammar, which is type 2 and regular expressions are type 3.) For any given string, you can compare and see if it matches the rules laid out by the grammar. 

When the LLM sampler uses the grammar, it discards all of the scrabble tiles that don't match the patterns described by the rules. Then it picks from the remaining tiles.

Some fine-tuned models expect to sometimes output javascript, while others do not. If you've got one where Javascript is unlikely, then it's not going to put very high weighting on valid javascript syntax. After `{` you'd expect to see either ` ""` or `}` or maybe `\n`. If it doesn't assign a high weight to ` ""` then it'll have a high chance of closing off the whole thing with `}` and stopping. Or even just outputting the end-of-sequence token instead of `{`.

You can help it a bit for explicitly asking for JSON (which hopefully increases the weighting of the JSON tokens) or pre-priming the response to start with `{ ""` just to get it started. But if it's never seen JSON used as a response before, it's going to have a hard time either way.",3
post30tec,technical,0.5030653624531636,lowest,"normal RL problems involve interacting with an environment. Things like PPO optimize a policy to pick good actions that give high reward in the environment given access to trajectories of episodes sampled from the environment using the current policy. 

What we have in RLHF really isn't RL since there is no environment. They collect a static dataset of preferences. Doing RL on required going out of your way to treat a predefined sequence of tokens like a trajectory from environment interactions and training a supplemental model to give rewards. 

DPO realized this is not really a necessary since you already have a static dataset, you can just maximize the likelihood over it (but using a different likelihood than next token prediction).

But in general, DPO is not a drop in substitute for RL, it's specific to places where RL is used but that weren't RL problems to begin with.",1
post30tec,technical,0.5030653624531636,lowest,"What if you did the following:

1 - Use an LLM as an agent in an environment

2 - Capture state/action pairs where the LLM gets positive feedback (in the paper I linked they use successful actions that lead to successful completed tasks)

3 - Generate (perhaps with humans, another model, or the LLM itself) examples of bad responses 

Couldn't DPO be used on this new dataset?",2
post30tec,technical,0.5030653624531636,lowest,"Even if the agent has access to an online environment, RLHF is still very different from standard RL. In RLHF, the human prompt is treated as the initial state. After initialization, the state transitions only depend on the LLM itself, unless it is a multi turn dialogue task, whereas standard RL deals with the passive dynamics in the environment. I guess the primary reason that people use RL for fine tuning is that the rewards are delayed until the end of the episodes. But DPO demonstrates that delayed rewards in a non-stochastic environment can be addressed by non-RL approaches. Although DPO isn’t tested in online environments, it is reasonable to believe that it will work effectively online. BTW, if the environment doesn’t provide rewards, which might be the case if RLHF is applied online, training the reward model online becomes a necessity, making DPO a preferable choice over PPO in such scenarios.",3
post30tec,technical,0.5030653624531636,lowest,"Yeah I think the reason I find DPO so exciting is the lack of the need for a reward model.

That's where I am wondering if there are ways to leverage it when training in an environment. In principle I think if you can get it to work for RLEF, you could get it to work for anything, since any activity could be thought of as a series of tasks and actions. This would let you basically create static datasets which could be used to grant arbitrary models the advantages of ""reinforcement learning"" via DPO.

I think the main difficulties I see are 

1. Where to get the non-desired outputs when the LLM does well. One thought I had is you could ask the model to generate its own bad ideas, although I am not sure if there is good research regarding the optimal ""bad ideas"" to use during DPO.
2. Where to get the good ideas when you only have negative feedback. Perhaps in this case you don't use DPO and use a different loss function which minimizes the likelihood of results with negative feedback. This could be weird though because without a positive example, you might just encourage the LLM to create gibberish. Alternatively, you could use DPO to encourage the model to take some sort of ""safe"" action in the environment instead of whatever resulted in negative feedback, such as doing nothing, or reflecting on data.",4
post30tec,technical,0.5030653624531636,lowest,"You could try it. But what benefits would you be hoping to get? The primary benefit of DPO vs PPO for RLHF is that you don't have to train a reward model. In this case you already have an env so you already don't need a reward model.

Also by doing it the way you described you are now in this scenario where you have a static dataset representing an environment. And you're trying to train an agent to navigate the env. If you have access directly to the env, why not use it? Why get a poor static representation of it?",3
post30tec,technical,0.5030653624531636,lowest,"Ok this might be me being really ignorant, and if so apologies, but doesn't PPO require a state-value network to calculate the advantage? In that case you still need a model for the reward don't you?",4
post46tec,technical,0.5155927130725672,lowest,"I can see the value in structure, but why do you think structure means ""json"", that seems rather silly, I'd see a lot more value in a linguistically programatic structure that allows for extreme variation while maintaining consistency in the core message, and reversibility to a common format to allow for training regeneration.",1
post46tec,technical,0.5155927130725672,lowest,"It's not just json though. I've seen teams implement xml with good results. Or ""custom"" schemes. The trick seems to be to find a structured form that the model has seen plenty of in the pre-training phase, and explain / give examples of the desired output while also constraining the outputs via libraries such as outlines. Doing one without the other leads to bad results.",2
post46tec,technical,0.5155927130725672,lowest,"Structured outputs sound very useful compared to typical LLM output. To me there seems to be 3 different components to it. Structure of the source material (prompt), abstraction level (image of an astronaut -- string 'astronaut') and of course output (json, xml..).

If I understand dottxt correctly, they mostly focus on the last, while any LLM can provide the first 2? I'm sorry for being ignorant, I've not read about this before.",3
post46tec,technical,0.5155927130725672,lowest,"Right, but you could achieve the same ""structure"" with normal language.",3
post46tec,technical,0.5155927130725672,lowest,"You could, but then you'd have to worry about and implement response parsing. That's the key about structured output. You get a guaranteed grammatically correct output every time. Semantically correct is up to you (via prompting, examples provided, etc.)

The blog post explains this. You can't prompt the model *without an example* and expect it to provide semantically correct json automagically. I have no idea why the paper did that, but it's extremely bad.",4
post46tec,technical,0.5155927130725672,lowest,"> do you think structure means ""json""

I could be misunderstanding who ""you"" is in this context, but in our rebuttal this is one of our major points: structured generation is not about specifically JSON, but rather running the results parser in reverse.

It just happens that in this example JSON (even unstructured) *does* yield better results on the last letter eval.

Most of my personal use of structured generation rarely uses JSON directly and typically starts with modeling the structure of the task as it appears in natural language. 

I do have an experiment I would like to run at some point that does iterate on a variety of formats for a variety of tasks *and* a variety of models ([here is an example](https://blog.dottxt.co/images/gsm8k_consistency_result.png) where JSON, unstructured, does worse than a NL style prompt) and see if we can find any evidence of consistently better formats.",2
post46tec,technical,0.5155927130725672,lowest,"Well right, I'm just saying more, I would think that structured output that is created with natural language rather than programmatically parsed languages, would improve things much further than either of those.

Essentially, applying a translation layer that outputs a deterministically varied natural language output, which can be transformed in either direction to and from the underlying ""structure"".",3
post32tec,technical,0.5317495100556942,lowest,do you have a kenyan army paid 2$ an hour?,1
post32tec,technical,0.5317495100556942,lowest,What kind’ve dataset does it take to train the reward model? I’m really a bit unsure on how much data/compute it takes for PPO to be effective in general.,2
post32tec,technical,0.5317495100556942,lowest,"For simple tasks, you might get by with a few thousand samples; complex tasks may require millions. PPO's effectiveness varies greatly with the task complexity",3
post32tec,technical,0.5317495100556942,lowest,I’ve always seen datasets for alignment being in the 100k’s not much more. Do you know a paper using 1M+ samples?,4
post32tec,technical,0.5317495100556942,lowest,For a really good dataset you need Americans making a bit more than that. Evaluating text quality isn't trivial.,2
post32tec,technical,0.5317495100556942,lowest,"i was referring to what openai used

https://time.com/6247678/openai-chatgpt-kenya-workers/",3
post32tec,technical,0.5317495100556942,lowest,Americans are possibly the worst cost to perf ratio,3
post35tec,technical,0.5376217798030576,lowest,"About this bit

> At the moment, TRLX has an API capable of production-ready RLHF at the scales required for LLM deployment (e.g. 33 billion parameters). Future versions of TRLX will allow for language models up to 200B parameters. As such, interfacing with TRLX is optimized for machine learning engineers with experience at this scale.

Has TRLX been used to tune models in production already? Or if not, what did the blog post mean by ""capable of production-ready RLHF""? I haven't seen any RLHF-ed models built on open source software yet, much less a 33B parameter one.

EDIT: Also hi @FerretDude",1
post35tec,technical,0.5376217798030576,lowest,It's already being used in production with a number of our partners. We have some chonky models coming out really soon. Expect things well into the tens of billions in the coming months.,2
post35tec,technical,0.5376217798030576,lowest,"Who? Who's even using RLHF in production yet, besides OpenAI (and maybe Cohere)?",3
post35tec,technical,0.5376217798030576,lowest,"Not allowed to share, many groups are looking into using RLHF in production though",4
post14tec,technical,0.5394749631521015,lowest,"I'm pretty sure they fine-tune lora adapters on top of a common base model. Then you can dynamically apply and remove adapters depending on the task. So they have a summarisation Lora, tone editing Lora... It's actually not that difficult to do. Llama cpp, vllm already have this capability",1
post14tec,technical,0.5394749631521015,lowest,[deleted],2
post14tec,technical,0.5394749631521015,lowest,"They mention LoRA [here](https://machinelearning.apple.com/research/introducing-apple-foundation-models):

>For on-device inference, we use low-bit palletization, a critical optimization technique that achieves the necessary memory, power, and performance requirements. To maintain model quality, we developed a new framework using LoRA adapters that incorporates a mixed 2-bit and 4-bit configuration strategy — averaging 3.5 bits-per-weight — to achieve the same accuracy as the uncompressed models.",3
post14tec,technical,0.5394749631521015,lowest,"Hi, 
Thanks for the info
Can you provide  link of paper or tutorial video that helps me understand loading of lora adapter dynamically please.


From my understanding lora will add a new layer or extension to the orginal model which will result in a new model

I've not seen a trained one as a adapter which we can load dynamically


Correct me if my understanding is wrong.",2
post14tec,technical,0.5394749631521015,lowest,"Yeah sure np. 

Here is an example, https://docs.vllm.ai/en/v0.4.0/models/lora.html

So you're right in that Lora adds a new part to the model. It adds a low rank matrix on top of specific layers. (Mainly the q,k,v). However these are kept seperate. They are only merged while inference for speed. But recently there have been implementations like s-lora(https://arxiv.org/abs/2311.03285) punica (https://arxiv.org/abs/2310.18547) where you can keep the adapters seperate from the base model and ""apply"" it at will. 

Which makes hosting multiple finetunes much more efficient.",3
post14tec,technical,0.5394749631521015,lowest,"A LoRA module does not add a layer, it is just a stored (and factored) weight update that you can add and subtract from the model weights freely.",3
post14tec,technical,0.5394749631521015,lowest,"There is also predibase... 

[https://arxiv.org/pdf/2405.00732](https://arxiv.org/pdf/2405.00732)

They recently published a research paper, they have a solution called Lorax, which allows for swapping of adapters... 

'Finally, we evaluate the latency and concurrency capabilities of LoRAX, an open-source Multi-LoRA inference server that facilitates the deployment of multiple LoRA fine-tuned models on a single GPU using shared base model weights and dynamic adapter loading. LoRAX powers LoRA Land, a web application that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA A100 GPU with 80GB memory. LoRA Land highlights the quality and cost-effectiveness of employing multiple specialized LLMs over a single, general-purpose LLM.'",3
post14tec,technical,0.5394749631521015,lowest,"No they don’t, you can’t swap loras on the fly, people request this feature for months.",2
post14tec,technical,0.5394749631521015,lowest,"https://docs.vllm.ai/en/v0.4.0/models/lora.html

Yes they do?",3
post14tec,technical,0.5394749631521015,lowest,"I was talking about llama.cpp, vllm don’t support k- quants, useless for most cases.",4
post24tec,technical,0.5476299545108965,lowest,how would this method be adapted to trying to train a base code LLM to learn a new python frameworks api?,1
post24tec,technical,0.5476299545108965,lowest,"The method is the same, you just need a ""good"" dataset for this task.",2
post24tec,technical,0.5476299545108965,lowest,so how would that look like? a cheat sheet format?,3
post24tec,technical,0.5476299545108965,lowest,I would suggest taking a look at existing code datasets. You can search for them on huggingface hub.,4
post24tec,technical,0.5476299545108965,lowest,"I haven't fine tuned yet but made a dataset for my Machine learning course by converting PDF contents to XML and then extracting the non meta-data as each ""response"" in a traditional ""prompt""-""response"" pair. I then synthetically generate a prompt for each response using OpenAI's API.

So your data could be extracted as the response  you are expecting from the LLM and then generate the prompts.",4
post10tec,technical,0.5623351252324088,lowest,"The traditional language modeling loss (negative log-likelihood) is misaligned with human expectations. One negation radically changes the meaning of a sentence. It doesn't radically change the loglikelihood. It isn't more important than a ""the"" or a superfluous word.

With RLHF, important words have important impact, and the loss is exactly aligned to human interests.",1
post10tec,technical,0.5623351252324088,lowest,"But isn't this only if you train it on the  loss (negative log-likelihood) via next-word prediction, i.e., what they do during pretraining?

If you use the ranks (from having users rank the documents) to compute the loss on the instead of the words as labels, would that still be the case?",2
post10tec,technical,0.5623351252324088,lowest,"Yes but the LM has to take many steps to produce the text

We need to train the LM to maximize a far-away reward and we need RL to do that",3
post10tec,technical,0.5623351252324088,lowest,"Could you help me understand what the far-away rewards represent here in this context? The steps are generating the individual words? So in this case you mean words that occur early in the text? In this case, a weighting scheme for the cross-entropy loss components could be used?",4
post23con,controversial,0.5760008377494001,lowest,I believe AI could be used to manage the wastefulness of modern society to redistribute this goods that are usually discarded for people in extreme needs around the world. From connecting people to developing more efficient trading systems. I believe what is needed is just the want to do it.,1
post23con,controversial,0.5760008377494001,lowest,"Fuk no, I want neoliberalism. Doggy dog world.",2
post23con,controversial,0.5760008377494001,lowest,Why would the powers that be treat AI any differently than any other technological innovation? When has new tech ever not been used to gain a competitive advantage?,2
post23con,controversial,0.5760008377494001,lowest,"I believe tech has been used for altruistic objectives, while it might not be such a sudden change as many of us would want, tech will grow to be adapted by the whole world, and this means non-profits doing the best they can to help their cause using emerging scientific discovery, the internet is an incredible tool that today is not yet completely globally accessible but efforts are made to make people more connected in areas of low resources.

If you want to be more theoretical, if AI manages to improve the economic status of humans where most people buy habits are not determined by the cost, maybe people would start supporting companies that uphold humanitarian values, i do believe that if people had the choice, there was the availability, quality and it didn't personally affect them, then they would choose the ethical product.",3
post23con,controversial,0.5760008377494001,lowest,"The internet is a great tool, but I block malicious attempts to hijack my webserver weekly, and I get scammers trying to trick my mother out of money from a computer in India. 

Tools are only tools. The people who weild them  determine the altruistic value of the action of using the tools. 100% of people are not benevolent, and if global unfettered access is given, narafious actors will use them for their gain. 

I've seen enough of human nature in my years to know that what you're dreaming up is pure fantasy. 

Technology is only as valuable as the ethics of those that use it. Not everyone wants good. There are a lot of people in this world, if given the opportunity, who wouldn't blink at the thought of eradicating entire groups of people from existence. 

Do you think Hamas, having control of a super intelligent technological marvel, would all of a sudden not want all Jews to die?

The first thing Americans did once they developed the technology behind the atomic bomb, was drop it on two cities in Japan. That's what technology can do. I'm not anti technology, I'm just a realist.",4
post23con,controversial,0.5760008377494001,lowest,We already can redistribute stuff right now If we wanted to. We have more than enough resources and capability for that.,2
post23con,controversial,0.5760008377494001,lowest,"Unfortunately a lot of wastefulnes in our system is by design to purposely keep price higher. AI will be able to improve productivity and overall world wealth, but the problem even today, is not how much goods are produced, but its distribution and much likely AI, at least at first, will make that issue worse.",2
post36con,controversial,0.5760008377494001,lowest,I believe AI could be used to manage the wastefulness of modern society to redistribute this goods that are usually discarded for people in extreme needs around the world. From connecting people to developing more efficient trading systems. I believe what is needed is just the want to do it.,1
post36con,controversial,0.5760008377494001,lowest,"Fuk no, I want neoliberalism. Doggy dog world.",2
post36con,controversial,0.5760008377494001,lowest,Why would the powers that be treat AI any differently than any other technological innovation? When has new tech ever not been used to gain a competitive advantage?,2
post36con,controversial,0.5760008377494001,lowest,"I believe tech has been used for altruistic objectives, while it might not be such a sudden change as many of us would want, tech will grow to be adapted by the whole world, and this means non-profits doing the best they can to help their cause using emerging scientific discovery, the internet is an incredible tool that today is not yet completely globally accessible but efforts are made to make people more connected in areas of low resources.

If you want to be more theoretical, if AI manages to improve the economic status of humans where most people buy habits are not determined by the cost, maybe people would start supporting companies that uphold humanitarian values, i do believe that if people had the choice, there was the availability, quality and it didn't personally affect them, then they would choose the ethical product.",3
post36con,controversial,0.5760008377494001,lowest,"The internet is a great tool, but I block malicious attempts to hijack my webserver weekly, and I get scammers trying to trick my mother out of money from a computer in India. 

Tools are only tools. The people who weild them  determine the altruistic value of the action of using the tools. 100% of people are not benevolent, and if global unfettered access is given, narafious actors will use them for their gain. 

I've seen enough of human nature in my years to know that what you're dreaming up is pure fantasy. 

Technology is only as valuable as the ethics of those that use it. Not everyone wants good. There are a lot of people in this world, if given the opportunity, who wouldn't blink at the thought of eradicating entire groups of people from existence. 

Do you think Hamas, having control of a super intelligent technological marvel, would all of a sudden not want all Jews to die?

The first thing Americans did once they developed the technology behind the atomic bomb, was drop it on two cities in Japan. That's what technology can do. I'm not anti technology, I'm just a realist.",4
post36con,controversial,0.5760008377494001,lowest,We already can redistribute stuff right now If we wanted to. We have more than enough resources and capability for that.,2
post36con,controversial,0.5760008377494001,lowest,"Unfortunately a lot of wastefulnes in our system is by design to purposely keep price higher. AI will be able to improve productivity and overall world wealth, but the problem even today, is not how much goods are produced, but its distribution and much likely AI, at least at first, will make that issue worse.",2
post2tec,technical,0.5789064115118601,lowest,"I think in the long run we won’t be using either of these approaches for what people are currently trying to do with them. In my view both these ultra long context LLMs and RAG are both hacky ways of trying to dynamically teach an LLM new things.

I believe that in the long run someone will come up with a better way of dynamically encoding and retrieving memories in an LLM. The memories will not be stored in plaintext like with rag, but will instead be highly compressed embeddings of some sort, or maybe even small sub-networks.",1
post2tec,technical,0.5789064115118601,lowest,"I don't doubt that you can come up with something smarter than what we already have, but to store more information without forgetting something you learned previously, we need to either increase the compression ratio, which becomes infeasible at some point or increase the ""storage"" space. In a way, longer context follows the second route, but you end up with quadratic growth (at least with standard attention) and it becomes harder to find what you're looking for in all that data. I think we'd definitely need something with at most log-linear increase in compute and memory, but filtering out relevant data from an increasing amount of total data while also scaling better than attention seems challenging.",2
post2tec,technical,0.5789064115118601,lowest,"The thing about both longer context and rag is that they both need to store the original text uncompressed. With longer context there is also the quadratic scaling problem you mention, and with ordinary RAG the retrieval mechanism isn’t dynamically tuned.

Somehow the human brain is capable of storing new memories dynamically and also holding onto these memories indefinitely. There is obviously some kind of compression going on along with a system for determining when memories should be created and retrieved.

With LLMs I could see it going a couple of different ways. Maybe like a more dynamic form of MoE where new experts can be dynamically created without impacting existing experts. It could also be more like RAG, but instead of storing the raw text, the model learns to store and retrieve some kind of compressed embedding. There could also be some system for “forgetting” stale information that seems to be of low value.",3
post2tec,technical,0.5789064115118601,lowest,but that's not true at all about the human mind.  Its is constantly killing unused memory and rewriting and linking memories and hallucinating freely. Its why human recollection of events is one of the least reliable bits of evidence.,4
post2tec,technical,0.5789064115118601,lowest,"You just invented fine-tuning which has its drawbacks as well, mainly it's relatively compute intensive.",2
post2tec,technical,0.5789064115118601,lowest,"No, it’s not fine tuning, at least not in the form that we currently have it. Fine tuning is not effective at adding new memories to an LLM, and in many cases seems to “overwrite” or “suppress” information learned during pre training. Fine tuning is only really effective for guiding the model, e.g. to follow chat prompts.

There needs to be new techniques that can reliably and efficiently add new information to a model without overwriting any previously learned information. RAG and long contexts are just hacks imo.",3
post2tec,technical,0.5789064115118601,lowest,"This is continual learning, and there's a bunch of research into it especially for RL where iid data is not possible.

Survey of the field: https://arxiv.org/abs/2302.00487",4
post2tec,technical,0.5789064115118601,lowest,"Continual learning could be a solution, but for the moment is a bit tricky. I have seen the KAN article about continual learning but it is still not convincing. Also there was a bit of hype of continual backpropagation. I have seen people coming with nice approach with memory augmented LLM, I think it is early to say it will work great",2
post39con,controversial,0.6184141460270253,lowest,"the world is competing at multiple levels.  citizens of a country are competing for jobs and compensation, and would benefit from UBI.  at the same time, countries are competing against each other, and restrictions that hinder the development of AI makes it more likely to lose on the international stage.  

how would the world be different, if the US decided that the development of nuclear weapons should be slowed out of concern for its impacts to society, and Germany developed the weapon first?

AGI has the potential to be more impactful than the invention of nuclear weapons.  Any country that can be the first to develop and control it will have a huge advantage over every other country",1
post39con,controversial,0.6184141460270253,lowest,"I'm not proposing for the slowing of AI development, not at all! In fact, this post was inspired by another post that suggested banning AI development. I think that AI will--and should--continue advancing. 

Further, I think that my plan to tax businesses based upon replacement of workers with AI will actually lead to an *increased* rate of development, because if we base an annual tax upon the prior year's employee cost plus a small percentage (1-5%) to secure against inflation and future growth (perhaps subtracting the cost of the AI units?), then they could buy 2-3 times as many AI units and produce significantly more products at a lower cost. This drastically increases their profits, such that they are more than overperforming by the end of the year, and the tax is a drop in the pond. 

This provides a strong incentive for businesses to encourage AI development, reduces the incentive for people to discourage its development (as the economic issues disappear), and creates a system that *supports* the adaptation of AI by businesses.

ETA: It's also the only way that I see those businesses continuing to be able to grow, like they want to: as more people become unemployed, they won't have enough customers to support any growth. That is why the tax needs to grow as a percentage of their gross revenue: so that the UBI can accommodate more consumers.",2
post39con,controversial,0.6184141460270253,lowest,"two companies in different countries are competing in the same market.  both can replace workers with AI, but one is forced to redirect a portion of their profits into UBI for the displaced workers, while the other is free to reinvest in the company. 

which company has the advantage?  which company has more incentive to fund AI development?",3
post39con,controversial,0.6184141460270253,lowest,"The one with UBI, because guess who is going to be the target of strong tarrifs from everyone else if they try this? For the most part, the upper class isn't made up entirely of fools.

If they forced everyone out of the country due to unemployment, leaving only the AI and businesses exporting to other countries, then other countries would both have to take in the refugees and deal with their impossible-to-compete-with margins. 

The natural response would be massive unilateral tarrifs on the country choosing to expel their middle class--both as a means of protecting local businesses and paying for the care of refugees--and then the subsequent collapse of whatever is left of the upper class, as there is no more local economy to hold them up without exports.",4
post42con,controversial,0.6212317456722539,lowest,"Not sure how accurate the model is. I read through your methodology and it feels more like picking and choosing random things to direct the AI.

Aside from that, polling data is already unreliable and adds multiple layers of complication.",1
post42con,controversial,0.6212317456722539,lowest,"I understand. The model does not use any polling data. The basic instructions are quite simple:

* Understand the U.S. system for presidential elections.
* Do not use polling data.
* Use and analyze actual events.

The model utilizes **Google Trends (a great starting point to identify an initial event)** to identify when candidates receive more online attention. At that point, it is prompted to investigate why this attention occurs. The question ""but why?"" is asked repeatedly until a topic is broken down to its core. Then, it is evaluated against the values of various social groups that can vote and how it might influence their opinions. Since the media in the U.S. can be biased (e.g., Fox = Republican, CNN = Democratic), the model avoids relying on news articles. If it must use an article from a U.S. news source (or any worldwide source), the article is broken down into an abstract event, removing the human element of the reporter.

While the article is fact-checked, the model also assesses how fact-checking influences voter perceptions. For example, if a candidate were to say, ""I'm Tom Cruise,"" which is obviously false, the model checks whether people actually believe this statement. The model is designed to distinguish between sarcasm and honest beliefs in conspiracy theories, and this is taken into account.

As you suggest, this is an important point, and the model should be adjusted to eliminate this possibility. Currently, it is not very random, but it could and should be more precise. I asked the model how it would be able to make a calculated prediction for an election in a fictional country resembling the U.S. It identified what data would be important to know, and, of course, ""polling data"" was one of the suggestions. I then asked it to propose an alternative way to make predictions based on daily events. This model requires significant refinement, and your feedback has made me acutely aware of this.

**Would it be beneficial to include vice presidential candidates in the equation as well? I**'m uncertain about the impact their rallies and speeches have on the presidential election. I believe this is a unique election cycle, and the influence of vice presidential candidates in the past may differ significantly from the situation in 2024.

At this point, the model is not comparing the personalities of Harris and Trump. I might consider adding this comparison, but I'm unsure if it really matters since everyone is already familiar with Trump's style, as historical data demonstrates. Harris is more difficult to analyze because most people base their opinions on her campaign statements. For a test run, I may incorporate their personalities (as far as they can be identified) into the model, but I believe it will not significantly impact the Electoral College. The candidates have such differing political views and agendas that the race or gender of the candidate might have minimal impact. Regardless, this should still be examined.",2
post42con,controversial,0.6212317456722539,lowest,"Google trends is still pretty unreliable because it only tells you that people are looking something up, not why. And the events still feel random",3
post42con,controversial,0.6212317456722539,lowest,"The model iteratively tracks the trend, identifying specific causes and effects until the trend ceases to exist. It's important to recognize that any trend is merely a starting point; the model continuously asks 'Why is this a trend?' and delves deeper to achieve a fully abstract understanding. If applicable, it applies the same process to other variables. The impact on any state or social group is then calculated based on this data.

While 100% accuracy is unrealistic, the goal is to develop a model capable of predicting elections with over 90% accuracy using this abstract, data-driven approach. The model should be able to predict the outcome of any state with complete accuracy, though failures may occur at the county level, which is acceptable within the overall prediction framework.",4
post3tec,technical,0.6249596264971391,lowest,"The focus of LoRA isn't inference, rather it's all about optimising training. If you need to shrink / optimise your model for inference then yes, I reckon you'd use other tools like quantisation or knowledge distillation.",1
post3tec,technical,0.6249596264971391,lowest,Oh shoot sorry I actually had a typo in my post - I actually meant that LoRA doesn't significantly improve GPU memory consumption or runtime during training for my custom model.,2
post3tec,technical,0.6249596264971391,lowest,"Oh, OK. So you ran tests & found LoRA didn’t improve your memory? What size was your base model & what was the LoRA rank? AdamW? Were you doing full LoRA or targeted to KQ?

Ideal case for LoRA would be targeted, low rank, large model, with a costly backprop like Adam compared against full fine tune with the same backprop algorithm.

I’ve heard the memory & inference savings are there due the reduced gradients but vaguely recall someone saying something similar, i.e that they didn’t see much memory spent on their gradients.",3
post3tec,technical,0.6249596264971391,lowest,"Yes so my base model was \~50M parameters. The lora rank was rank 4, typical Adam scheduler (no weight decay). I applied it to the value, query, key, and attention layer output matrices (so not only KQ). I did also fine tune the decoder aka the last few layers (I have an large encoder to small decoder arch) but when I computed the trainable parameters, it came to only \~3% of parameters. But yeah that was the run that only reduced GPU memory from 8.5G->8.1G.",4
post3tec,technical,0.6249596264971391,lowest,"When you say ""my custom model"" what do you mean?  Are you training a base model from scratch?  My understanding of LoRA is limited, but I was under the impression that it is only for use in fine tuning a pretrained model.",3
post3tec,technical,0.6249596264971391,lowest,"Yeah so LoRA really is just a framework, and you can theoretically use it to parameter-efficient tune any model. In this case, I tuned only the attention layers (all query/key/value/attention output matrix) and the small decoder in my model and froze all other layers.",4
